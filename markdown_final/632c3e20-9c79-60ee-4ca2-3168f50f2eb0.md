# Medical Question & Answer

**Sample ID**: 632c3e20-9c79-60ee-4ca2-3168f50f2eb0
**Dataset Index**: 162033

---

## Question

A particular parallel program computation requires 100 seconds when executed on a single processor. If 40 percent of this computation is "inherently sequential" (i.e., will not benefit from additional processors), then the theoretically best possible elapsed times for this program running with 2 and 4 processors, respectively, are

Answer Choices:
A. 20 and 10 seconds
B. 30 and 15 seconds
C. 50 and 25 seconds
D. 70 and 55 seconds

---

## Answer

> Let's see… What do we have here? The user is asking about the theoretically best possible elapsed times for a parallel program with 40% inherently sequential work when run on 2 and 4 processors, respectively. Let's break this down step-by-step. First, I need to think about identifying the sequential and parallel fractions from the problem statement. Then, I should verify the application of Amdahl's law to compute the theoretical speedup. Next, I will calculate the minimum elapsed time for 2 processors and then for 4 processors. Finally, I will compare the results with the answer choices and double-check that the math aligns with the expected behavior of diminishing returns as processor count increases.

> Let me first confirm the given parameters and what needs to be computed. The total serial execution time is 100 seconds, and 40% of the computation is inherently sequential, which means 60% is parallelizable. I need to find the theoretically best elapsed times with 2 and 4 processors, assuming ideal parallelization with no overhead, which is exactly the scenario Amdahl's law addresses.

> I will now examine the sequential and parallel components. Sequential time is 40% of 100 seconds, which equals 40 seconds, and the parallelizable time is 60% of 100 seconds, which equals 60 seconds. Hold on, I should verify that these sum back to 100 seconds: 40 + 60 = 100, which checks out and confirms the split is correct.

> Next, I should review Amdahl's law to ensure I'm applying it correctly. Amdahl's law states that the theoretical speedup is limited by the sequential fraction, and the minimum elapsed time is the sequential time plus the parallelizable time divided by the number of processors. Mathematically, T(n) = T_sequential + T_parallel / n, where n is the number of processors.

> Now, let me calculate the minimum elapsed time with 2 processors. Sequential time remains 40 seconds, and the parallelizable 60 seconds is divided by 2, giving 30 seconds. Adding them together, the total is 40 + 30 = 70 seconds. Wait, let me verify that I'm not overstating the speedup; yes, the sequential portion is unchanged, so the best possible time with 2 processors is 70 seconds, not less.

> Next, I should calculate the minimum elapsed time with 4 processors. Sequential time is still 40 seconds, and the parallelizable 60 seconds is divided by 4, giving 15 seconds. Adding them together, the total is 40 + 15 = 55 seconds. Hold on, I should double-check that this aligns with the diminishing returns expected as n increases; yes, the sequential bottleneck becomes more pronounced as the parallel fraction shrinks relative to the total, so 55 seconds is the theoretical minimum with 4 processors.

> Finally, I need to match these results to the provided answer choices. The calculated best times are 70 seconds with 2 processors and 55 seconds with 4 processors, which corresponds to option D. I should confirm there isn't a miscalculation that would make any other option fit; none of the other choices match these values, so the correct answer is D.

---

The theoretically best possible elapsed times are **70 seconds with 2 processors** and **55 seconds with 4 processors**, calculated using Amdahl's law. The sequential portion (40 seconds) cannot be parallelized, while the parallel portion (60 seconds) is divided by the number of processors, yielding 30 seconds with 2 processors and 15 seconds with 4 processors. Adding the sequential time gives the total elapsed times of 70 and 55 seconds, respectively.

---

## Step-by-step calculation

### 1. Identify sequential and parallel portions

- **Total execution time on a single processor**: 100 seconds.
- **Sequential portion**: 40% of 100 seconds = 40 seconds.
- **Parallel portion**: 60% of 100 seconds = 60 seconds.

---

### 2. Apply Amdahl's law

Amdahl's law states that the **maximum speedup** is limited by the sequential portion of the program. The formula for the theoretical minimum execution time (T(n)) with (n) processors is:

T(n) = T_sequential + [T_parallel/n]

where:

- (T_sequential) is the time for the sequential portion.
- (T_parallel) is the time for the parallel portion.
- (n) is the number of processors.

---

### 3. Calculate for 2 processors

For (n = 2):

T(2) = 40 + [60/2] = 40 + 30 = 70 seconds

---

### 4. Calculate for 4 processors

For (n = 4):

T(4) = 40 + [60/4] = 40 + 15 = 55 seconds

---

## Final answer

The theoretically best possible elapsed times are **70 seconds with 2 processors** and **55 seconds with 4 processors**, corresponding to option D.

---

## References

### Fundamental energy cost of finite-time parallelizable computing [^117Xuv8J]. Nature Communications (2023). High credibility.

We base our analysis on the following assumptions (Fig. 1): (i) Computing problems with a (variable) number N of logically irreversible operations should be solved in (constant) finite time. In order to stay within this time limit, (ii) an ideal serial computing strategy has to adapt dynamically its processing frequency (time per operation τ s; Fig. 1 a left), whereas (iii) an ideal parallel computing strategy is able to adapt the number n of processors, keeping constant its processing frequency (time per operation τ p; Fig. 1 a right). These assumptions are well justified. Assumption (i): While the available time is not exactly fixed, there is usually a limit on how long calculations can be run. Assumptions (ii) and (iii) may be viewed as a minimal model of processors that are implemented in modern technology. A single CPU core indeed behaves similarly to our idealized serial computer by adapting its frequency to the workload using dynamic frequency and voltage scaling. On the other hand, a multi-core CPU behaves like our idealized parallel computer by deactivating unused cores using deep-sleep states.

Fig. 1
Assumptions for ideal serial and parallel computers.

a Schematic illustration of the three main assumptions: (i) The total timeavailable to solve a given problem requiring N irreversible computing operations is limited; (ii) an ideal serial computer (left, blue) reduces the time per operation τ s with increasing problem size N; (iii) an ideal parallel computer (right, red) is able to increase the number of processors n proportional to the size N in order to keep the time per operation τ p constant. b Optimal 1/ τ behavior of the energy consumption of a single algorithmic operation of duration τ, and the effects that the serial (blue) or parallel (red) computing strategies have on the energetic cost of computation.

---

### Accelerating wright-fisher forward simulations on the graphics processing unit [^115R4WJV]. G3 (2017). Low credibility.

Forward Wright-Fisher simulations are powerful in their ability to model complex demography and selection scenarios, but suffer from slow execution on the Central Processor Unit (CPU), thus limiting their usefulness. However, the single-locus Wright-Fisher forward algorithm is exceedingly parallelizable, with many steps that are so-called "embarrassingly parallel", consisting of a vast number of individual computations that are all independent of each other and thus capable of being performed concurrently. The rise of modern Graphics Processing Units (GPUs) and programming languages designed to leverage the inherent parallel nature of these processors have allowed researchers to dramatically speed up many programs that have such high arithmetic intensity and intrinsic concurrency. The presented GPU Optimized Wright-Fisher simulation, or "GO Fish" for short, can be used to simulate arbitrary selection and demographic scenarios while running over 250-fold faster than its serial counterpart on the CPU. Even modest GPU hardware can achieve an impressive speedup of over two orders of magnitude. With simulations so accelerated, one can not only do quick parametric bootstrapping of previously estimated parameters, but also use simulated results to calculate the likelihoods and summary statistics of demographic and selection models against real polymorphism data, all without restricting the demographic and selection scenarios that can be modeled or requiring approximations to the single-locus forward algorithm for efficiency. Further, as many of the parallel programming techniques used in this simulation can be applied to other computationally intensive algorithms important in population genetics, GO Fish serves as an exciting template for future research into accelerating computation in evolution. GO Fish is part of the Parallel PopGen Package available at: http://dl42.github.io/ParallelPopGen/.

---

### Fundamental energy cost of finite-time parallelizable computing [^112FCgjx]. Nature Communications (2023). High credibility.

Fig. 2
Finite-time Landauer bound for ideal serial and parallel computers.

a Energy consumption per operation, W / N, for solving a fully parallelizable problem of size N by an ideal serial, Eq. (3) (blue), and parallel, Eq. (4) (orange), computer. The energetic cost diverges with N for an ideal serial computer and remains constant for an ideal parallel computer. b Maximal number of bit operationsthat can be performed by an ideal serial, Eq. (5) (blue), and parallel, Eq. (6) (orange), computer in the finite times within a given power budget. Parameters are T = 1 K, b = 1 and a = 8⋅10 −7 k T s.

An ideal parallel computer can therefore, in principle, solve quadratically bigger problems within the same time and energy constraints as an ideal serial computer (Fig. 2 b). Within the power budget range of 1 W–400 MW shown in Fig. 2 b, the ideal parallel computer solves problems that are 7 to 12 orders of magnitude larger than the problems solved by an ideal serial computer under the same power constraints. We note that the constants a and b depend on the topology of the circuit and may be determined empirically. While different constants would lead to quantitatively different results, the quadratic advantage of the ideal parallel computer is fundamental and independent of the specific circuit used.

To understand the practical importance of the finite-time energy cost, quantitative values for a are required. A state-of-the-art general purpose processor that is highly parallel, runs at a relatively low clock rate (60 cores à 4 threads at f op = 1.09 GHz), and has been thoroughly analyzed for its energy consumption is the Intel Xeon Phi: it consumes 4.5⋅10 −10 J/32 bit operation or a ⋅ f op = 1.4⋅10 −11 J/operation(note that this value accounts only for computation operations and ignores more costly transfers to and from memory). This allows us to obtain a = 1.0⋅10 −20 Js ≈ 2.3 k T s (T = 330 K, the typical operating temperature of a CPU under load) as an estimate for electronic computers (Fig. 3, dashed lines). This implies that the finite-time energy cost of an electronic computer exceeds the (quasistatic) Landauer limit already at a few Hertz of operation frequency.

---

### Mapping pleiotropic loci using a fast-sequential testing algorithm [^114Cizbv]. European Journal of Human Genetics (2021). Medium credibility.

Computational performance

For balanced data, the benchmark (Fig. 2) showed that the computational time required to perform the test scales approximately linearly with the number of traits and with sample size (left panel of Fig. 2); but these two factors interact, making the difference in the time required to process 5 or 10 traits larger for large sample size. When 30% of the data were missing at random, the computational time was no longer linear on the number of traits (right panel of Fig. 2); this happens because with completely random missingness the number of missing-value patterns present in the data grows exponentially with the number of traits (i.e. increasing data fragmentation). However, with real data, missing data often does not happen completely at random (e.g. some subjects lack data for a subset of the traits); in such cases the increase of computational time with missing records is expected to be smaller.

Fig. 2
Total computational time (in seconds) to process 1000 variants with balanced and unbalanced data, and an increasing number of traits.

Colors indicate sample size from 10,000 to 300,000.

For balanced data and a biobank-sized sample (n = 300,000), it took on average ~85 seconds for pleiotest to analyze 1000 SNP for 5 traits. For systems involving 10 traits, the computational times required to process 1000 SNPs were about 50–60% higher than for analysis with the same sample size and 5 traits. This represents a remarkable computational improvement relative to the sLRT of the pleio R-package that, with no missing values and n = 300,000, yielded a median computation time of 20.1, 25.8, and 44.6 s per variant for 3, 5, and 10 traits, respectively.

The benchmark presented in Fig. 2 shows that pleiotest can process an entire chromosome (~50,000 SNPs) with a very large sample size and five traits in about one hour and ten minutes (chromosomes can be process in parallel as separate jobs). For imputed genotypes, after QC and filtering, long chromosomes may include ~1million SNPs. Thus, for a sample size of 300,000 and 5 traits, processing a long chromosome in a single job would take about 23.5 h; but this task can also be parallelized (using features offered by the BEDMatrix package which is compatible with pleiotest) in, for example, jobs of 100,000 SNPs each, which will take < 2.5 h to finish.

---

### Markov chain monte carlo methods for hierarchical clustering of dynamic causal models [^115TZLFh]. Human Brain Mapping (2021). Medium credibility.

As before, we inverted HUGE using both standard and improved Gibbs sampling and HMC. Settings were kept identical to the synthetic case except for the chain length, which was increased to 600,000 samples including 300,000 burn‐in for Gibbs. The choice of chain length was partly based on previous experience with a similar clustering model (Raman et al. 2016). Individual chains were sampled in parallel on an HPC cluster, with each chain having access to a single core. The HPC contains a variety of CPUs from different manufactures with clock speeds ranging from 2.3 to 3.7 GHz and jobs are assigned to a random node. A summary of the composition of the HPC can be found on its website. Sampling required 140 × 10 3 s (approx. 40 hr) on average for both versions of Gibbs sampling. Note that the computation time of the improved version of Gibbs is on average only 4% higher than that of standard Gibbs, indicating that the overhead introduced by our special proposal distributions is almost negligible. Also note that computation time is not a linear function of the number of regions of the DCM, but depends on many factors such as the number of connections, the number of subjects, the length of the time series, the type of the DCM (linear, bilinear, nonlinear), and so forth. However, the analysis from Section 2.4 showed that the complexity of the sampler does scale linearly with the number of subjects. This is relevant for applications involving large‐scale datasets such as, for example, the Adolescent Brain Cognitive Development Study, the Human Connectome Project or the UK Biobank. Using this linear relation to extrapolate the computation time for this experiment to a dataset of 100 subjects, we would expect an average computation time of 105 hr (approx. 4.5 days), which is still less than the computation time of HMC for the present experiment. In this context, it is also important to note that for Gibbs sampling the subject related computations can be easily parallelized, which could potentially reduce the computation time significantly if suitable hardware is available. In contrast, for HMC samplers, parallelization is less trivial since the simulation of the Hamiltonian dynamics is inherently sequential in nature.

---

### Fundamental energy cost of finite-time parallelizable computing [^115Q8u6G]. Nature Communications (2023). High credibility.

In view of Eq. (1), the total work cost associated with the solution of a computing problem that requires N bit operations within the finite timeis given by, whereis in general a function of. The scaling of the dissipative term with the problem size N depends on the type of computing considered. It may be concretely determined for the two idealized computer models introduced above: (i) for an ideal serial computer, the available time per operation decreases with the problem size as(Fig. 1 a left), whereas (ii) for an ideal parallel computer that solves the problem with a number of processors n (N) = b N (with b ∈ (0, 1]) that scales linearly with N, the time per operation stays constant,(Fig. 1 a right). The quantity f op can be interpreted as the operation frequency of the serial processor, whereas 1/ b determines the number of operations performed by each processor; in the following, we set b = 1 (the effect of different values of b is discussed in the Supplementary Information, Sec. S3). The fundamental total energy cost per operation for the serial implementation, therefore, scales with the problem size as, The corresponding scaling for the parallel implementation reads, Equations (3) and (4) highlight an important, fundamental difference between serial and parallel computing: whereas the energy cost per operation for a serial computer necessarily increases at least linearly with N, the energy cost per operation for an ideal parallel computer is independent of N (Fig. 2 a); it depends only on the two constants a and b as well as the chosen. If the computation task permits to choose a large, then the finite-time energy cost per operation for the parallel computer is bounded only by the Landauer limit, even for very large N. Equations (3) and (4) further imply that for a computer with a maximum power budget, the maximal problem sizethat can be solved within the (fixed) time limitis proportional to the square root of the powerfor a serial implementationwhereas it is directly proportional to the powerfor a parallel implementation

---

### Fundamental energy cost of finite-time parallelizable computing [^111X2cU3]. Nature Communications (2023). High credibility.

The fundamental energy cost of irreversible computing is given by the Landauer bound of [Formula: see text]/bit, where k is the Boltzmann constant and T is the temperature in Kelvin. However, this limit is only achievable for infinite-time processes. We here determine the fundamental energy cost of finite-time parallelizable computing within the framework of nonequilibrium thermodynamics. We apply these results to quantify the energetic advantage of parallel computing over serial computing. We find that the energy cost per operation of a parallel computer can be kept close to the Landauer limit even for large problem sizes, whereas that of a serial computer fundamentally diverges. We analyze, in particular, the effects of different degrees of parallelization and amounts of overhead, as well as the influence of non-ideal electronic hardware. We further discuss their implications in the context of current technology. Our findings provide a physical basis for the design of energy-efficient computers.

---

### Speeding up eQTL scans in the BXD population using GPUs [^115ifz9Z]. G3 (2021). Medium credibility.

Benefit of customized algorithm for eQTL scans

R/qtl is a general-purpose QTL mapping program. To provide a baseline for our approach customized for eQTL scans, we compared runtimes to R/qtl. The timing of our method and R/qtl is shown in Table 1. By simplifying the genome scan process, using matrix multiplication, and returning the maximum LOD, we observed a significant speedup. For the spleen data, our method was 48 times faster (0.83 s for LiteQTL on CPU only vs 40.02 s for R/qtl). For the hippocampus data the speedup was 125 times (16.57 s on CPU only vs 2070.71 s for R/qtl), and 734.2 times if R/qtl is compared with LiteQLTL's CPU & GPU option.

Table 1
eQTL scan runtimes for R/qtl and LiteQTL

Benefit of using GPU

In parallel computing, Amdahl's law indicates the theoretical maximum speedup that could be attained when improving a particular part of a program. For example, if a program takes 10 min for a serial processor and a function that takes nine of those 10 mins can be parallelized, then the theoretical speedup, no matter how many processors are used, cannot be more than 10 times because the minimum execution time of this program is 1 min. Therefore, profiling the entire genome scan process is a prerequisite for optimization. Often, profiling would consider space and time complexity. Our primary concern is the time taken by each function, and therefore only timing information is considered in our profiling. We used Julia's built-in sampling profiler to find our target functions for GPU because it is less intrusive than the other profiling methods.

---

### Fundamental energy cost of finite-time parallelizable computing [^111FhxkV]. Nature Communications (2023). High credibility.

The fundamental energy cost of irreversible computing is given by the Landauer bound of/bit, where k is the Boltzmann constant and T is the temperature in Kelvin. However, this limit is only achievable for infinite-time processes. We here determine the fundamental energy cost of finite-time parallelizable computing within the framework of nonequilibrium thermodynamics. We apply these results to quantify the energetic advantage of parallel computing over serial computing. We find that the energy cost per operation of a parallel computer can be kept close to the Landauer limit even for large problem sizes, whereas that of a serial computer fundamentally diverges. We analyze, in particular, the effects of different degrees of parallelization and amounts of overhead, as well as the influence of non-ideal electronic hardware. We further discuss their implications in the context of current technology. Our findings provide a physical basis for the design of energy-efficient computers.

---

### Model building of protein complexes from intermediate-resolution cryo-EM maps with deep learning-guided automatic assembly [^11716ze6]. Nature Communications (2022). High credibility.

Computational efficiency

The running time of EMBuild comes from two parts. One is to predict the main-chain probability map from the EM density map, and the other is to assemble the complex structure using the main program. The detailed running times on the test set of 47 single-particle EM maps are listed in Supplementary Data 8. It can be seen from the table that the prediction of the main-chain probability map can normally be finished within 100 seconds on four NVIDIA A100 GPUs. As for the main program of EMBuild, it consumes an average time of 3838.2 seconds on a single core of Intel(R) Xeon(R) Gold 6240 for each test case, compared with 1585.7 seconds for phenix.dock_in_map. The longer computing time of EMBuild than phenix.dock_in_map is understandable because EMBuild includes more time-consuming semi-flexible domain refinement in addition to rigid fitting. Nevertheless, by parallel computing, the running time of EMBuild can be drastically reduced. For example, the running time is reduced to 291.4 seconds by parallelly running EMBuild on 36 cores using OpenMP. Compared to EMBuild and phenix.dock_in_map, gmfit consumed much more time with an average of 15514.3 seconds for each case because an extremely large number of configurations are searched and refined in gmfit (Supplementary Data 8).

---

### Fundamental energy cost of finite-time parallelizable computing [^111xa1rS]. Nature Communications (2023). High credibility.

Discussion

We have used insights from nonequilibrium thermodynamics to develop a general framework to evaluate the fundamental energetic cost of finite-time parallelizable computing, including partial parallelization and parallelization overheads. Our main result is that the finite-time energy cost per operation of a fully parallel computer is independent of problem size and can realistically operate close to the quasistatic limit, in stark contrast to serial computers. This fundamental advantage of parallel computers holds as long as the overhead N ove (n) scales better than n 3/2. For serial computers, the key limiting factor is the finite-time constant a. To enable a drastic increase in operation frequency without prohibitive energy consumption, a needs to be strongly reduced below its current value of a ≈ k T s in electronic computers.

On the other hand, the massive advantages of parallel- over serial computers may make it worthwhile to drastically rethink the design of computing hard- and software. From an energetic (which ultimately translates to performance) perspective, massively parallel computers with extremely high numbers of small cores and aggressive dynamic voltage and frequency scaling techniques could deliver orders of magnitude better performance per watt compared to CPUs with few large and complex cores — provided that well-parallelizable algorithms exist. Such algorithms could be quite wasteful in terms of the parallelization overhead and still deliver great performance advantages. Therefore, it seems worthwhile to invest significant research and development resources in the development of such CPUs and suitable software algorithms. Moreover, in light of this work, alternate computing technologies such as massively parallel DNA- – or network-based, biocomputers may already be closer to the optimal computers described here than current electronic computers: These computers use small DNA molecules or biomolecular motors as computing agents, which are cheap to mass-produce and can be added to the computation in amounts matching the problem size. Both approaches have been estimated to operate close to the Landauer limit per operation. From the perspective of finite-time energy cost, immensely parallel computers, such as biological computers or computers with massively parallel architectures and many-core processors, thus offer a potentially large, fundamental advantage over today's few-core electronic computer architectures.

---

### From calibration to parameter learning: harnessing the scaling effects of big data in geoscientific modeling [^111V2vKf]. Nature Communications (2021). High credibility.

Notably, as training data amount increased, dPL descended into the range of acceptable performance orders of magnitude faster than SCE-UA in terms of both the number of forward runs (Fig. 2a, b) and computational time (Fig. 2c, d) per gridcell. For the model trained for 2 years, dPL required 810, 370, and 45 full forward runs per gridcell (or 2.2, 0.74, and 0.31 seconds of computing time per gridcell, not proportional to forward runs due to increasing hidden sizes) to drop below the threshold for a functional model (RMSE = 0.05) at 1/16 2, 1/8 2, and 1/4 2 sampling densities, respectively. In contrast, SCE-UA needed 950 runs per gridcell (or 90 seconds, here we did not implement parallelism for SCE-UA) to reach the same RMSE with the same surrogate model running on GPU, which was similar between sampling densities 1/16 2, 1/8 2, and 1/4 2. Two factors are at play for dPL's efficiency: the first factor is the reduction of runs at higher sampling densities (with an order of magnitude spread between 1/16 2 and 1/4 2 in terms of either runs or time). This super scaling effect resulted from the use of a domain-wide loss function and mini-batch training, allowing dPL to learn across locations rapidly (more interpretation in Discussion). The second factor is the inherent gridcell-level (minibatch) parallelism and efficient GPU computing, which are crucial to the success of DLand were made relevant to parameter estimation via dPL. While SCE-UA can also be parallelized, it may be difficult to achieve the high level of parallel efficiency and scale enjoyed by dPL.

---

### Towards a fully automated algorithm driven platform for biosystems design [^112JSjhr]. Nature Communications (2019). High credibility.

Several algorithms are suggested for balancing the trade-off between exploration and exploitation and the maximum of acquisition function represents an automatic trade-off between these two factors. One of the commonly used acquisition functions is Expected Improvement (EI), where the algorithm estimates how much improvement over the current best is expected from each one of the points, and samples the point with the highest expected improvement. This function elegantly finds the balance in exploration and exploitation trade-off by using the already trained GP and finds the point that provides the highest expected improvement and was chosen as the acquisition function in this work.

As described before, by design, Bayesian optimization relies on sequential experiments. Each time one point is evaluated, the result is given to the algorithm to update the prior GP and find the next point to be evaluated using the acquisition function. However, it is more efficient to perform some experiments in parallel and in sequential batches so as to reduce the number of rounds of the experiment and consequently the time of the entire project. Fortunately, a variation of Bayesian optimization has been recently developed for multi-core parallel processing applications. This algorithm can handle multiple pending evaluations and can get the result of any of the pending evaluations at any given time and return the next point to be evaluated. In short, the algorithm considers likely outcomes for each of the pending points and calculates the acquisition functions based on the all possible outcomes. This method was used to drive the direction of our experiments and one batch of points was chosen and evaluated in each round and the result was given to the algorithm to generate the next batch of points to be evaluated. It is noteworthy that in the experimental setting and when the evaluations are done using parallel experimentation, the pending points are updated at the same time in subsequent batches and not one by one.

If there was no error in the experiments, which is the case for evaluation of mathematical functions, the confidence level around the points that are already evaluated would be very high. However, since the result of all experiments contain some error and is far from perfect mathematical calculations, the confidence in the results was adjusted so the program expects an error in the evaluations and adjusts the mean and variance for all the points accordingly. The other aspects of this optimization algorithm including the covariance functions and hyperparameters of the GP are explained in details by Sneok and coworkers.

---

### Quantum computational advantage with a programmable photonic processor [^113jDguj]. Nature (2022). Excellent credibility.

A quantum computer attains computational advantage when outperforming the best classical computers running the best-known algorithms on well-defined tasks. No photonic machine offering programmability over all its quantum gates has demonstrated quantum computational advantage: previous machines 1,2 were largely restricted to static gate sequences. Earlier photonic demonstrations were also vulnerable to spoofing 3, in which classical heuristics produce samples, without direct simulation, lying closer to the ideal distribution than do samples from the quantum hardware. Here we report quantum computational advantage using Borealis, a photonic processor offering dynamic programmability on all gates implemented. We carry out Gaussian boson sampling 4 (GBS) on 216 squeezed modes entangled with three-dimensional connectivity 5, using a time-multiplexed and photon-number-resolving architecture. On average, it would take more than 9,000years for the best available algorithms and supercomputers to produce, using exact methods, a single sample from the programmed distribution, whereas Borealis requires only 36μs. This runtime advantage is over 50 million times as extreme as that reported from earlier photonic machines. Ours constitutes a very large GBS experiment, registering events with up to 219 photons and a mean photon number of 125. This work is a critical milestone on the path to a practical quantum computer, validating key technological features of photonics as a platform for this goal.

---

### Pragmatic considerations for fostering reproducible research in artificial intelligence [^113XnzHp]. NPJ Digital Medicine (2019). Medium credibility.

Is reproducibility as simple as just running the code?

Ideally, with the raw data and programming code, reproducing a study's reported summaries should be as simple as "push the button". For statistical models such as least-squares regression, results can be routinely reproduced. AI, however, is a careful balance of a rapidly evolving set of hardware and software. The data utilized often require significant pre-processing to move it from the raw data state to an analysis-ready state. Even with the data and algorithms available, specialized workstations utilizing the exact software and hardware (e.g. graphics processing units) may be required to reproduce the results. This is not a trivial expectation given the cost and availability of specialized high performance computing infrastructure. More troubling, however, is that having the analytical data, programming code, and the entire necessary computing environment may not be sufficient to exactly reproduce the estimation process. There is knowledge within the development community that small computational variances occur over a wide range of software and versions when using a graphics processing unit for high-performance computing. These variances are not eliminated by setting a starting number for random number generation as they are when software is not massively parallelized (i.e. run on a central processing unit). Simply put, it may not be possible to identically regenerate the model as it is with models that have closed-form solutions (i.e. "equations"). This realization is such that the raw data and programming code are not enough to overcome the reproducibility crisis facing AI research. It should be noted, however, that once the model has been estimated, deterministic and reproducible results are obtained from the final fitted model.

---

### RetroFun-RVS: a retrospective family-based framework for rare variant analysis incorporating functional annotations [^116TPfAA]. Genetic Epidemiology (2025). Medium credibility.

5.3 Power Comparison With Others Affected‐Only Methods

In the second set of power evaluations, we comparedwith other affected‐only methods, namely RVS (Bureau et al.) and RV‐NPL (Zhao et al.). Thus, to proceed to fair comparisons between methods, we adapted RVS and RV‐NPL to take CRHs into account (see Supporting Information S1: Numerical Simulations). With 2% risk variants, when we considered 75% of causal variants located within one CRHs, we observed that RetroFun‐RVS reaches greater power compared to competing methods (Figure 4C). At lower proportions of risk variants, the new method remains more powerful compared to RV‐CHP or RVS, and equivalent to RV‐NPL (Supporting Information S1: Figure S14).

5.4 Running Time and Scalability Comparison With Others Affected‐Only Methods

The RetroFun‐RVS tests also require significantly shorter computing times than the competing methods evaluated here on the data sets used in the main simulations (Table 1). This faster running time enables RetroFun‐RVS to run on much larger familial data sets. We created a data set of 1000 families with 10,475 individuals including 4800 affected ones by repeating 25 times each of the 40 unique pedigree structures in Supporting Information S1: Figure S2. Genotype data for the affected subjects in this sample generated under the null hypothesis contained 386 rare variants in the simulated TAD. Performing the RetroFun‐RVS tests on this data set required 33 s of computing time. The evaluated competing methods fail on a data set of this size.

Table 1
Running times (in seconds) for analyzing rare variants in the TAD, in one simulated replicate, using a single 2.10 GHz processor.

---

### Quantum supremacy using a programmable superconducting processor [^115H77CN]. Nature (2019). Excellent credibility.

The promise of quantum computers is that certain computational tasks might be executed exponentially faster on a quantum processor than on a classical processor 1. A fundamental challenge is to build a high-fidelity processor capable of running quantum algorithms in an exponentially large computational space. Here we report the use of a processor with programmable superconducting qubits 2–7 to create quantum states on 53 qubits, corresponding to a computational state-space of dimension 2 53 (about 10 16). Measurements from repeated experiments sample the resulting probability distribution, which we verify using classical simulations. Our Sycamore processor takes about 200 seconds to sample one instance of a quantum circuit a million times-our benchmarks currently indicate that the equivalent task for a state-of-the-art classical supercomputer would take approximately 10,000 years. This dramatic increase in speed compared to all known classical algorithms is an experimental realization of quantum supremacy 8–14 for this specific computational task, heralding a much-anticipated computing paradigm.

---

### Fundamental energy cost of finite-time parallelizable computing [^117XnyGo]. Nature Communications (2023). High credibility.

Leakage currents and provisioning

Non-ideal computers moreover have a base energy consumption caused by leakage currents –. For low-voltage, low-power circuits, the subthreshold current (also known as the weak inversion current) is the dominant component of the leakage current. We assume that the devices are working in the typical regime where the subthreshold current scales linearly with the supply voltage V between drain and source –. However, we note that this assumption may not always be valid in the low-power regime –. We also assume that the subthreshold current is a linear function of the total processing time, as the processors can be put into deep sleep mode after and before running the computation to reduce leakage dissipation, and of the number of used processors n –. We further, suppose that the supply voltage is adapted to be proportional to the frequency f op. As a result, we havefor both serial and parallel realizations, where α is a circuit-specific constant. The corresponding energetic cost per computation, is hence the same for both serial and parallel algorithms; it simply shifts Eqs. (3)–(4) by a constant amount (Supplementary Information, Sec. S4). We further note that one of the main issues that limit the use of nearly infinitely parallel computers in practice is the fact that all the additional CPUs need to be provisioned. This comes with additional hardware and infrastructure (for example input/output, DRAM memory, data storage and networking equipment) that consumes energy even when the CPUs are in deep sleep mode. We may account for provisioning by adding a problem size independent constant β to the energetic cost, so that. The provisioning work creates overhead for the parallel computer, which makes it inefficient for small workloads. However, the effect of provisioning becomes largely irrelevant with increasing problem size, making a parallel computer still the ideal choice for large problems (Supplementary Information, Sec. S4).

Reversible computing

For logically reversible computing, the quasistatic Landauer bound ofmay be reduced to zero. As a consequence, only the finite-time contributions to the energetic cost (which are thermodynamically irreversible) remain in Eqs. (1)–(4). The difference in energy consumption between ideal reversible and irreversible serial computers becomes negligible at high clock frequencies, while the difference between ideal reversible and irreversible parallel computers is constant (Supplementary Information, Sec. S5).

---

### Geometrical congruence, greedy navigability and myopic transfer in complex networks and brain connectomes [^117EviRK]. Nature Communications (2022). High credibility.

Furthermore, we clarify that the prioritization and memoization are performed in advance before to start any recursion. This keeps the recursions of different source nodes independent allowing them to process in parallel. For a careful understanding and discussion of the technical details behind the algorithm design we refer to: Method section (Computation of thebetween all node pairs: algorithm design), Supplementary Note 2 section (Pseudocode to compute thebetween all node pairs), Supplementary Note 3 "Space and time complexity", Supplementary Note 4 "Running time estimation".

After introducing the basic concepts behind the two types of algorithms (brute-force and our optimized algorithm), we will move forward discussing their running time and time complexity. In Fig. 5a we report the running time (average time on 10 networks realizations by nPSO model) of the two different network congruence algorithms using parallel computation (128 cores). Each of the five curves indicates networks with five different values of γ (2, 2.25, 2.5, 2.75, 3) and are obtained for increasing network node size values (N = 100, 1 K, 10 K, 100 K). Average degree, temperature and number of communities in the nPSO are respectively fixed to = 12, T = 0.1 and C = 4, because from the previous section we noticed that these are parameters of second relevance for our investigation and that we should concentrate our attention on understanding the impact of N and γ. The running time of our optimized algorithm (Fig. 5a, red curve) is negligible (one second) till N ∼ 1000, it is small for N ∼10 K (few minutes), whereas it becomes impactful for N ∼ 100 K (in the order of days), although still feasible as a large-scale computation. In comparison, for N ∼ 100 K the running time of brute-force (Fig. 5a, black curve) significantly increases to more than 2 months of computation (see Supplementary Note 4 for details on the running time estimation) for the highest values of γ (2.75 and 3). Hence, the second step of our investigation is to deepen our understanding of the computational processes behind the algorithmic running time differences observed in Fig. 5a for N = 100 K and the highest γ values. To this aim in Fig. 5b we repeat the same analysis of Fig. 5a but specifying the difference between results obtained with and without parallel computation, on networks of size N = 100 K and for γ = [2, 2.5, 3]. The first evidence is that both on single core (Fig. 5b right panel) and 128 cores parallelization (Fig. 5b, left panel), regardless of the γ value, our algorithm design leads to a reduction of around 90% with respect to the computational load needed by brute-force. In addition, it is evident the impact of the power-law exponent γ on the running time. In case of γ = 3, we notice the reduction from a worst-case scenario of 26 years for brute-force on single core to the actual scenario of 7 days for our algorithm on 128 cores, representing a great computational achievement, which remains relevant even when the brute-force algorithm is parallelized on 128 cores (2.4 months running time). Therefore, the third step of our investigation aims to address two questions: to understand the reason behind this remarkable running time reduction gained by our optimized algorithm; to identify the reason behind the impactful running time increase occurred for γ = 2.75, 3 regardless of the algorithm design.

---

### Distinct value computations support rapid sequential decisions [^116Whxvi]. Nature Communications (2023). High credibility.

The value of the environment determines animals' motivational states and sets expectations for error-based learning 1–3. How are values computed? Reinforcement learning systems can store or cache values of states or actions that are learned from experience, or they can compute values using a model of the environment to simulate possible futures 3. These value computations have distinct trade-offs, and a central question is how neural systems decide which computations to use or whether/how to combine them 4–8. Here we show that rats use distinct value computations for sequential decisions within single trials. We used high-throughput training to collect statistically powerful datasets from 291 rats performing a temporal wagering task with hidden reward states. Rats adjusted how quickly they initiated trials and how long they waited for rewards across states, balancing effort and time costs against expected rewards. Statistical modeling revealed that animals computed the value of the environment differently when initiating trials versus when deciding how long to wait for rewards, even though these decisions were only seconds apart. Moreover, value estimates interacted via a dynamic learning rate. Our results reveal how distinct value computations interact on rapid timescales, and demonstrate the power of using high-throughput training to understand rich, cognitive behaviors.

---

### Geometrical congruence, greedy navigability and myopic transfer in complex networks and brain connectomes [^115EwC6s]. Nature Communications (2022). High credibility.

Fig. 5
Running time estimation.

We generated nPSO networks with parameters N = [100, 1000, 10,000, 100,000], = 12, T = 0.1, γ = [2, 2.25, 2.5, 2.75, 3] and C = 4. a The panel reports the running time over increasing network size, both for our algorithm (red) and brute-force (black) using parallel computation, as separate curves for different values of γ. b The panel reports, for γ = [2, 2.5, 3], the estimated running time with and without parallel computation, both for our algorithm and brute-force, on networks of size N = 100,000. The percent of decrease is shown. c, e, g The panels report, for γ = [2, 2.5, 3] respectively, the proportion of nodes associated with each recursion depth, both for our algorithm and brute-force, on networks of size N = 100,000. In addition, on top of the barplots, we report the mean running time required to reach each recursion depth. d, f, h The panels report, for γ = [2, 2.5, 3] respectively, the running time that would be required to compute the recursive section of the algorithm for each recursion depth and without considering parallel computation, both for our algorithm and brute-force, on networks of size N = 100,000. In all panels, values are averaged over 10 network realizations and error bars are shown, except for panel (a) due to visual overlap with the symbols. For more details, please refer to the dedicated section Supplementary Note 4.

---

### GJLS2: an R package for generalized joint location and scale analysis in X-inclusive genome-wide association studies [^1112BHhm]. G3 (2022). Medium credibility.

Another option is to use the Rscript provided that allows additional arguments to change how frequently the results are written to the output (–write) and to increase the number of cores used (–nTasks). A core is an independent processing unit on a central processing unit (CPU). Though a modern computer, usually containing 4–8 cores, is capable of handling parallel computing, we recommend the "split-apply-combine" strategy and employing high-performance computing for large-scale analyses. Scripts for both options are available from github.

To help assess the computational requirement at different data sizes, we sampled with replacement from the UKB X-chromosome data (restricted to 100 SNPs) to achieve sample sizes of: 1,000, 5,000, 10,000, 50,000, 100,000, 200,000, 300,000, 400,000, and repeated the joint-location-scale analysis using a single core with 10GB memory via (1) PLINK R plug-in and (2) Rscript. The reason for keeping the 100 SNPs to estimate the performance metric is because the analysis can be easily divided to chunks and combined after. Figure 1 shows the computational time and memory usage as a function of increasing sample size. These results suggest that the gJLS analysis on data with sample size up to 10,000 on less than 100 SNPs can be suitably handled in R GUI such as Rstudio, which might be the preferred option for confirmatory analyses.

Fig. 1.
Computational usage of gJLS X-chromosome association analyses. The bars represent the memory in (Gigabyte) and wall clock time (seconds) used to perform a gJLS X-chromosome association for 100 markers at various sample sizes (x -axis, per a thousand samples).

For an X-chromosome wide analyses on UKB (n = 488,377, m = 15,179), the gJLS analysis took ∼16 h for PLINK (using 3.5GB memory) and ∼ 21 h (using 1.2GB) for Rscript. The memory efficiency of Rscript is expected as the "BEDmatrix" only maps the required portion of genotype files into memory. However, the Rscript can be parallelized, when using 4 cores with 20GB allocated memory, the wall clock run time was reduced to ∼13 h (using 14.0 GB).

---

### Accelerated Bayesian inference of population size history from recombining sequence data [^1162aXhG]. Nature Genetics (2025). High credibility.

Running time and memory consumption

Next, the computational resources required by each method were examined. The peak amount of memory used, as well as total central processing unit (CPU) time, was recorded for each simulation run. Because the datasets were simulated from organisms with differing genome lengths, both measures were normalized by genome length (measured in gigabase (Gb) pairs) to enable comparison across runs, and the data from all runs were then averaged for each method and sample size.

The benchmarking results (CPU time and memory usage) described in the preceding paragraph are shown in Fig. 2. For analyzing a single diploid sample, n = 1, all methods required a similar amount of CPU time, around 20–30 min Gb −1; MSMC2 required the most memory, and PHLASH required the least. For n = 10, the only sample size where it was possible to run all four methods, FITCOAL was the most efficient in terms of time and memory usage, which was expected because it only analyzes the frequency spectrum. Of the hidden Markov model (HMM)-based approaches, PHLASH required substantially less CPU time and memory than SMC++ and, especially, MSMC2. Increasing the sample size to n = 100 caused the running time of FITCOAL to increase by roughly tenfold, while memory consumption remained low; for PHLASH, CPU and memory demands increased only moderately. Finally, for n = 1,000, no method except PHLASH was able to run given the allotted computational resources, and analyzing it required roughly the same amount of memory and less CPU time than analyzing ten samples using MSMC2.

Fig. 2
Mean CPU time and peak memory usage for the various methods.

For each method, the average was taken across 12 models × 3 replicates = 36 simulated datasets. Error bars represent mean ± 1 s.e.

Additional analyses

The Supplementary Note details the additional experiments conducted to evaluate the robustness of PHLASH under varying model conditions. Supplementary Note — Accuracy of the parallel approximation — verifies that the method used for parallelizing the computation of the log likelihood (Supplementary Note — Parallel evaluation) does not introduce too much error. Similarly, Supplementary Note — Calibration of the composite likelihood posterior — checks that the composite likelihood approximation (see Methods) does not cause the posterior distribution to become overconfident. Finally, Supplementary Note — Inferring recombination rates — investigates recombination rate inference.

---

### Random access quantum information processors using multimode circuit quantum electrodynamics [^111ZQQNq]. Nature Communications (2017). Medium credibility.

Figure 4c highlights the advantages of random access in a quantum computing architecture. An entangling gate between the first and the j th qubit of an array with only nearest-neighbor coupling would require 2 j −1 gates (such as CXs or iSWAPs). This results in an exponential decay of the fidelity with increasing distance between the corresponding qubits. Conversely, in a random access quantum information processor, there is no additional computational cost to perform gates between arbitrary pairs of qubits. Even without considering potential improvements in the coherence times, we see (Fig. 4c) that the processor performs competitively with state-of-the-art gatesbetween distant qubits in a nearest-neighbor architecture. While we have highlighted the advantages of this processor in terms of random access and minimal control hardware, a resulting requirement is the need to perform sequential operations. The number of modes which can be multiplexed to a single qubit without loss of fidelity is given by the ratio of the loss from idling in a cavity mode to the loss in performing qubit operations, which for modern 3D cavities can be up to 100.

---

### SARST2 high-throughput and resource-efficient protein structure alignment against massive databases [^112xVjth]. Nature Communications (2025). High credibility.

Fig. 3
Speed of SARST2 and several state-of-the-art alignment algorithms.

a Summary of the non-parallelized running time of the algorithms evaluated by IR using the Qry400 and SCOP-2.07 datasets. Most database search algorithms, like the SARSTs and BLAST, provide quality control and hit-list cutoff parameters for users to decide the amount of retrieved homologs. Such algorithms were evaluated by adjusting one cutoff at a time. For each query, the running time of an algorithm was measured under the minimal cutoff setting that allowed the algorithm to recall all family-level homologs. The right part lists the alignment speeds for algorithms that support one-step pairwise alignment. Although SARST2 was primarily designed for database search, its pairwise alignment was also efficient, e.g. around 12 times faster than the accurate structural alignment method FAST and 2 times faster than the pairwise BLAST sequence alignment program bl2seq. b Speed of database search algorithms that support parallel computation. As the number of CPUs increased, their speeds improved. Each point represents the average of 10 replicate runs using 400 query proteins (n = 400). SARST2 with 32 i9 CPUs reached an average speed of 966,000 alignments per second. A full search of the 145,000-protein SCOP-2.07 took only 0.15 s. The same task took Foldseek and BLAST 1.07 and 3.62 s, respectively. c Relative speed plot showing the degree of enhancement by parallel computing. SARST2 achieved the highest speedup, demonstrating its high scalability. The data of panels (b) and (c) are provided as a Source data file. CPU Central Processing Unit, s second.

To test the efficiency of search algorithms that supported parallelization, we gradually increased the number of processors. Foldseek and BLAST ran fast with 1–2 CPUs, but their speed met a plateau when the number was ≥ 4. The speed increment of SARST2 was remarkable (Fig. 3b, c). Using our 32-CPU machine, its speed could increase 10.6-fold, whereas Foldseek, MADOKA, and BLAST were accelerated by 2.1–3.0, 10.5, and 3.1-fold, respectively. Searching one query structure against the SCOP-2.07 to reach 100% recall with 32 i9 CPUs, SARST2 took 0.15 s on average, approximately 7 and 24 times faster than Foldseek and BLAST.

---

### Programmable computing with a single magnetoresistive element [^113vJT3V]. Nature (2003). Excellent credibility.

The development of transistor-based integrated circuits for modern computing is a story of great success. However, the proved concept for enhancing computational power by continuous miniaturization is approaching its fundamental limits. Alternative approaches consider logic elements that are reconfigurable at run-time to overcome the rigid architecture of the present hardware systems. Implementation of parallel algorithms on such 'chameleon' processors has the potential to yield a dramatic increase of computational speed, competitive with that of supercomputers. Owing to their functional flexibility, 'chameleon' processors can be readily optimized with respect to any computer application. In conventional microprocessors, information must be transferred to a memory to prevent it from getting lost, because electrically processed information is volatile. Therefore the computational performance can be improved if the logic gate is additionally capable of storing the output. Here we describe a simple hardware concept for a programmable logic element that is based on a single magnetic random access memory (MRAM) cell. It combines the inherent advantage of a non-volatile output with flexible functionality which can be selected at run-time to operate as an AND, OR, NAND or NOR gate.

---

### Evidence for the utility of quantum computing before fault tolerance [^115msdQf]. Nature (2023). Excellent credibility.

Main

It is almost universally accepted that advanced quantum algorithms such as factoringor phase estimationwill require quantum error correction. However, it is acutely debated whether processors available at present can be made sufficiently reliable to run other, shorter-depth quantum circuits at a scale that could provide an advantage for practical problems. At this point, the conventional expectation is that the implementation of even simple quantum circuits with the potential to exceed classical capabilities will have to wait until more advanced, fault-tolerant processors arrive. Despite the tremendous progress of quantum hardware in recent years, simple fidelity boundssupport this bleak forecast; one estimates that a quantum circuit 100 qubits wide by 100 gate-layers deep executed with 0.1% gate error yields a state fidelity less than 5 × 10 −4. Nonetheless, the question remains whether properties of the ideal state can be accessed even with such low fidelities. The error-mitigation, approach to near-term quantum advantage on noisy devices exactly addresses this question, that is, that one can produce accurate expectation values from several different runs of the noisy quantum circuit using classical post-processing.

Quantum advantage can be approached in two steps: first, by demonstrating the ability of existing devices to perform accurate computations at a scale that lies beyond brute-force classical simulation, and second by finding problems with associated quantum circuits that derive an advantage from these devices. Here we focus on taking the first step and do not aim to implement quantum circuits for problems with proven speed-ups.

We use a superconducting quantum processor with 127 qubits to run quantum circuits with up to 60 layers of two-qubit gates, a total of 2,880 CNOT gates. General quantum circuits of this size lie beyond what is feasible with brute-force classical methods. We thus first focus on specific test cases of the circuits permitting exact classical verification of the measured expectation values. We then turn to circuit regimes and observables in which classical simulation becomes challenging and compare with results from state-of-the-art approximate classical methods.

---

### Two-stage strategy using denoising autoencoders for robust reference-free genotype imputation with missing input genotypes [^116cWxwE]. Journal of Human Genetics (2024). Medium credibility.

Similar preprocessing, such as the data conversion with the Positional Burrows Wheeler Transform, is required for IMPUTE4 and Beagle 5.4; however, since these steps are integrated within the main genotype imputation process, their computational times were not recorded separately. The trained models or preprocessed data can be reused, and hence in Table 2, we distinguished this initial computational time from the subsequent running time for genotype imputation. The training time for RNN-IMP was assessed on an AMD Epyc 7713 CPU (base clock 2.0 GHz; boost clock up to 3.675 GHz) in a single thread, while other processes were measured on an AMD Ryzen 9 7950X CPU (base clock 4.5 GHz; boost clock up to 5.7 GHz) in a single thread. Python 3.7 and TensorFlow 1.15.0 were used for training the deep learning models for RNN-IMP. The trained models were then converted to the ONNX Runtime format to accelerate computations during model inference for genotype imputation. For inference, we used Python 3.11 with the ONNX Runtime. Although RNN-IMP demands a substantial amount of computational time for training, this process can be parallelized, and the entire training was completed within two days using a supercomputing system. RNN-IMP also generally consumed more computational time than methods based on the Li and Stephens model. One reason is that genotype data reading and result writing operations are implemented in Python for RNN-IMP, whereas they are implemented in C, C++, or Java in the Li and Stephens model-based methods. The inference phase by ONNX Runtime took 296.62 seconds for RNN-IMP, suggesting that computational efficiency could be improved by implementing I/O processes in C or C++ rather than Python. In this experiment, only 100 samples were processed for genotype imputation, but it is anticipated that the computational time per sample will decrease significantly for larger datasets, due to reduced overhead in all methods.

---

### GWLD: an R package for genome-wide linkage disequilibrium analysis [^116ZC2xe]. G3 (2023). Medium credibility.

Fig. 5.
Evaluation of the effects of numbers of SNPs and samples on GWLD computing speeds. The duck data were used as an example. a and b) Time required for calculation (in seconds). c and d) Relative Pearson's correlation between LD measures.

When using the complete duck dataset (∼40,000 SNPs, 542 samples), we found that for GWLD R-base scripts with the single-thread mode, calculation ofneeded > 30 days (d), whereas for RMI and MI, 8.2 and 3.5 d, respectively. However, for the parallel mode (31 threads), the computation terminated much faster (> 8-fold of less time needed) (3.7 d, 9.8 h, and 5.9 h, respectively) (Table 2). In contrast, when the Rcpp (RcppArmadillo + OpenMP) computation mode was selected (Table 2), the whole computing process was completed in hours (1.6, 3.7, and 4.0 h, for, MI, and RMI), even for the single thread. With the multithread computing mode, less than half an hour (6.8, 18.4, and 25.3 min) was required.

Table 2.
GWLD computing speed comparisons (the duck data).

---

### Diffractive optical computing in free space [^116NRi36]. Nature Communications (2024). High credibility.

Computation speed, parallelism, and scalability

Free-space optical processors present a unique advantage since their computation is completed as the input light propagates through the optical processor and its volume/layers. While electronic processors and integrated photonics-based processors also utilize the speed of electromagnetic wave propagation, the free-space processors complete all the calculations and inference tasks with a single pass of wave propagation, without any digital storage/transmission or pre-processing of information. Therefore, the computing speed of diffractive and metasurface systems for the same task can be increased drastically by scaling the system down to the nanoscale feature sizes, operating at visible or IR wavelengths, with a total axial thickness of e.g. ~100–200 λ. To manipulate this wavelength range, however, the diffractive features used in free-space optical processors need to have dimensions of > 100 nm, significantly larger than the state-of-the-art transistors on chips made by the 3-nm process. In addition, the size of free-space optical processors that can be accurately modeled and designed (through e.g. a deep-learning-based training process) is still limited by the speed of digital computers and their memory restrictions. Unlike CPUs and GPUs that can be assembled into clusters to solve advanced computation problems, a versatile strategy to assemble diffractive networks into a large-scale cluster of free-space optical processors is still lacking, but highly desired to fully utilize their advantages in parallelism. Therefore, the ability to construct low-loss, large-scale networks of diffractive optical processors is thus critical for building fast, free-space computing platforms with significantly lower power consumption.

---

### FEMA: fast and efficient mixed-effects algorithm for large sample whole-brain imaging data [^1142k9Gt]. Human Brain Mapping (2024). Medium credibility.

FEMA achieves computational efficiency owing to the following factors: (i) implementation of a sparse random effects design; (ii) an efficient binning strategy that speeds up estimation of fixed effects; (iii) using a method of moments estimator for random effects; and (iv) use of vectorized operations for scalability across variables. While this is not the first time that method of moments has been used for estimating model parameters in a mixed model setting (see, e.g. (Gao; Gao & Owen,) and some of the references cited therein), to the best of our knowledge, this is the first time that such a scalable solution has been presented in the context of solving mixed models for neuroimaging data in large samples. Our solution additionally circumvents the need for parallel computing or high memory requirements. Through the results of simulation 1, we have shown that using a small number of bins (such as bin value of 20) is adequate for the accurate estimation of the fixed and random effects, while keeping the computational time low. Further, through simulations 1 and 2, we have shown that these estimates are similar to the simulated ground truth as well as comparable to estimates from a standard ML‐based mixed model solver. In terms of computational efficiency, through simulation 3, we have shown that the time required by FEMA is a fraction of the time taken by MATLAB's built‐in functions. Even when engaging parallel computing for MATLAB's fitlmematrix, the time taken by FEMA was always lesser than MATLAB, demonstrating the benefits of the implementation in FEMA. In addition, the results of simulation 3 also show that the time taken by FEMA does not grow appreciably with increasing number of imaging variables (a benefit of vectorized operations that scale well to multiple imaging variables), thereby allowing fast and efficient whole‐brain analyses to be completed in a matter of seconds to minutes as opposed to prohibitively long computational time using a standard solver. We also note that FEMA is computationally efficient even when adding genetic relatedness as a random effect. While it may appear (from Figures 3 and 4) that it may be possible to exploit parallel processing to an extent that solving whole‐brain mixed models no longer takes a long time, we highlight two aspects of this comparison: one that when benchmarking the performances, we purposefully used a highly powered parallel configuration with a large amount of computing resources with 20 parallel workers and 2 threads per worker and large memory resources; this is more computational power than is available to many groups and thus depending on the computational resources that a researcher has (and other factors like whether there are additional processes running), this time may be substantially lower than practically realizable for many labs. Second, running a large number of parallel jobs is also memory intensive and thereby requires the availability of such computing facility. Recently (Maullin‐Sapey & Nichols,) developed the "Big" Linear Mixed Models (BLMM) toolbox which uses a combination of novel closed form solutions, vectorized computing, and distributed processing on clusters (while keeping the memory requirement in check) for solving mixed models. While we have not compared the computational performance of FEMA with BLMM, we highlight that FEMA is fast and computationally efficient and one could complete whole‐brain analyses on a regular laptop/workstation within a short time (as long as the data fits in the memory), thereby circumventing having dedicated computing clusters for such analyses. Finally, we note that these computational advantages do not come at the cost of an inflated type I error, as demonstrated by the results of simulation 4 and simulation 6, where we saw that the average type I error rate was controlled at 5%.

---

### Universal control of a six-qubit quantum processor in silicon [^117JbN7z]. Nature (2022). Excellent credibility.

Future quantum computers capable of solving relevant problems will require a large number of qubits that can be operated reliably 1. However, the requirements of having a large qubit count and operating with high fidelity are typically conflicting. Spins in semiconductor quantum dots show long-term promise 2,3 but demonstrations so far use between one and four qubits and typically optimize the fidelity of either single- or two-qubit operations, or initialization and readout 4–11. Here, we increase the number of qubits and simultaneously achieve respectable fidelities for universal operation, state preparation and measurement. We design, fabricate and operate a six-qubit processor with a focus on careful Hamiltonian engineering, on a high level of abstraction to program the quantum circuits, and on efficient background calibration, all of which are essential to achieve high fidelities on this extended system. State preparation combines initialization by measurement and real-time feedback with quantum-non-demolition measurements. These advances will enable testing of increasingly meaningful quantum protocols and constitute a major stepping stone towards large-scale quantum computers.

---

### Fast and robust analog in-memory deep neural network training [^114dABXP]. Nature Communications (2024). High credibility.

As a second baseline, we compare to in-memory SGD (as described in "Methods" section "In-memory outer-product update"), which, however, yielded poor accuracy results in Fig. 5.

When one assumes that a certain amount X of digital compute throughput is available exclusively for a single analog crossbar array, then we can estimate the average time (per input sample) the gradient update step would take. For approximate numbers, we assume that a single update pulse would take approximately 5 n s, a single MVM about 40 n s, and that the memory operations (Table 1, rows in first section) can be hidden behind the compute. In Supplementary Fig. 9, the average time for an update is plotted against the amount of available compute. As seen from Table 1, if one assumes a state-of-the-art number of 175 billion FP operations per second (FLOPS) (that is 0.7 TFLOPS, shared among 4 crossbar arrays), the proposed algorithms out-perform the alternative MP algorithm by a large margin, showing the benefits of AIMC for in-memory gradient update (about 50× faster, even if one already assumes a batch size of 100, which favors the MP algorithm). Moreover, computing the gradient in digital requires a much higher memory throughput for MP (see row "Memory ops" in Table 1), which could be challenging to maintain. Since at most one row (or column) is processed in digitally for our proposed algorithms per input, memory bandwidth is not a bottleneck.

---

### An investigation of irreproducibility in maximum likelihood phylogenetic inference [^115iJxEQ]. Nature Communications (2020). High credibility.

Impact of multithreading and processor types on gene tree reproducibility

When the number of threads is not specified, IQ-TREE will automatically determine the best number of threads on the processor according to the length of the gene alignment and RAxML-NG will use all of the available cores on the processor. In addition, IQ-TREE and RAxML-NG can automatically detect the best kernel instruction on processor architecture to optimize the performance of the tree search, but different types of processor architectures can result in different kernel instructions. Therefore, we investigated whether increasing the number of threads and using different processor types affect the reproducibility of gene trees inferred by IQ-TREE and RAxML-NG. Because these analyses are computationally demanding, we performed them using a set of 3819 gene alignments from three large representative studies in animals (marine fishes: 1001 genes), plants (green plants: 410 genes), fungi (budding yeasts: 2408 genes). Specifically:
(i) For each of the 3819 gene alignments, two replicates (Run1 and Run2) were submitted to a single node (two replicates ran one right after the other on the same node) for 1, 2, 3, 4, and 5 thread(s), respectively. The total number of jobs executed on the CHTC cluster was 38,190 (3819 genes * 1 node (it contains 2 replicates) × 5 threading data points * 2ML programs) (see command lines and job submission file in Supplementary Note 1).
(ii) Since the analysis of phylogenomic data sets on a laboratory server is computationally intractable, we sampled the first 200 genes from each of the three data sets. For each of these 600 gene alignments, we ran two replicates (Run1 and Run2) on a laboratory server (Intel Xeon E5–2630 v3 @ 2.40 GHz processor with 16 threads) for 2 and 3 threads, respectively. All 3600 analyses (600 genes * 2 replicates * 2 threading data points * 2ML programs) were executed one right after the other on the server.
(iii) For each of the 3819 gene alignments, two replicates (Run1 and Run2) were submitted to two separate nodes (i.e. each analysis was run on a single node, but Run1 was executed on a different node than Run2) for 1, 2, 3, 4, and 5 thread(s), respectively. The total number of jobs executed on the CHTC cluster was 76,380 (3819 genes * 2 nodes (each contains 1 replicate) × 5 threading data points * 2ML programs) (see command lines and job submission file in Supplementary Note 1).

---

### Hash-based core genome multilocus sequence typing for Clostridium difficile [^115kz4Zq]. Journal of Clinical Microbiology (2019). Medium credibility.

Alternative assembler (SKESA).

Use of SKESA in place of SPAdes as the assembler used for hash-cgMLST resulted in fewer differences between replicate pairs (Fig. 1C), namely, 241 (89%) pairs had 0 differences, 22 (8%) pairs had 1 difference, 6 (2%) pairs had 2 differences, and 3 (1%) pairs had 3 differences. This equates to 0.16 false gene differences per replicate pair sequenced. The median number of genes compared between replicate pairs was 2,225 (IQR, 2,187 to 2,235) using SKESA and 2,227 (IQR, 2,205 to 2,242) using SPAdes out of a possible maximum of 2,270 genes.

Benchmarking.

Samples were processed in parallel, with each sample using a single core from an Intel Xeon Gold 6150 2.70-GHz 18-core central processing unit (CPU). For a single sample, the median (IQR) time to undertake quality control and read filtering was 3.6 (2.7 to 4.9) minutes and 27.4 (19.6 to 35.4) minutes, respectively, to generate an assembly using Spades with read error correction and 16.3 (12.1 to 21.5) minutes without; SKESA took 19.4 (15.5 to 24.3) minutes. Creating a hash-cgMLST profile from the assemblies took 44.1 (43.5 to 44.9) seconds. After making hash-cgMLST profile files, comparing a single genome to 100,000 others using a single CPU core took 40.4 s. In contrast, 100,000 comparisons using a standard cgMLST approach took marginally less time — 38.7 s — after loading the profiles into memory.

cgMLST profiles can also be rapidly compared using a laptop or desktop; for example, using one core of an Intel i7 2.6-Ghz laptop processor, comparing the 973 samples from the six hospitals study required 467 Mb of memory and took 236 s for 472,879 comparisons, i.e. 49.9 s per 100,000 comparisons. Using the same laptop, creating hash-cgMLST profiles from existing assemblies typically took ∼40 s and required < 100 Mb of memory.

---

### Error mitigation extends the computational reach of a noisy quantum processor [^115D5Rqv]. Nature (2019). Excellent credibility.

Quantum computation, a paradigm of computing that is completely different from classical methods, benefits from theoretically proved speed-ups for certain problems and can be used to study the properties of quantum systems 1. Yet, because of the inherently fragile nature of the physical computing elements (qubits), achieving quantum advantages over classical computation requires extremely low error rates for qubit operations, as well as substantial physical qubits, to realize fault tolerance via quantum error correction 2,3. However, recent theoretical work 4,5 has shown that the accuracy of computation (based on expectation values of quantum observables) can be enhanced through an extrapolation of results from a collection of experiments of varying noise. Here we demonstrate this error mitigation protocol on a superconducting quantum processor, enhancing its computational capability, with no additional hardware modifications. We apply the protocol to mitigate errors in canonical single- and two-qubit experiments and then extend its application to the variational optimization 6–8 of Hamiltonians for quantum chemistry and magnetism 9. We effectively demonstrate that the suppression of incoherent errors helps to achieve an otherwise inaccessible level of accuracy in the variational solutions using our noisy processor. These results demonstrate that error mitigation techniques will enable substantial improvements in the capabilities of near-term quantum computing hardware.

---

### Quantum error correction below the surface code threshold [^114Pcis9]. Nature (2025). Excellent credibility.

Quantum error correction 1–4 provides a path to reach practical quantum computing by combining multiple physical qubits into a logical qubit, in which the logical error rate is suppressed exponentially as more qubits are added. However, this exponential suppression only occurs if the physical error rate is below a critical threshold. Here we present two below-threshold surface code memories on our newest generation of superconducting processors, Willow: a distance-7 code and a distance-5 code integrated with a real-time decoder. The logical error rate of our larger quantum memory is suppressed by a factor of Λ = 2.14 ± 0.02 when increasing the code distance by 2, culminating in a 101-qubit distance-7 code with 0.143% ± 0.003 per cent error per cycle of error correction. This logical memory is also beyond breakeven, exceeding the lifetime of its best physical qubit by a factor of 2.4 ± 0.3. Our system maintains below-threshold performance when decoding in real time, achieving an average decoder latency of 63microseconds at distance 5 up to a million cycles, with a cycle time of 1.1microseconds. We also run repetition codes up to distance 29 and find that logical performance is limited by rare correlated error events, occurring approximately once every hour or 3×10 9 cycles. Our results indicate device performance that, if scaled, could realize the operational requirements of large-scale fault-tolerant quantum algorithms.

---

### A new tool called DISSECT for analysing large genomic data sets using a big data approach [^113RQLRs]. Nature Communications (2015). Medium credibility.

Computational performance

We performed MLM and PCA analyses using simulated cohorts (Supplementary Methods) of different sample size (N; Fig. 2), to demonstrate the computational capabilities of DISSECT. We selected these two examples, because they are very computationally demanding analyses, requiring a running time of O(N 3). The analyses were run on the UK National Supercomputing Service (ARCHER), a supercomputer with 4,920 computer nodes containing 9,840 processors with 12 cores each (that is, a total of 118,080 cores available). DISSECT was able to fit, after eight iterations, an MLM to a sample of 470,000 individuals and 590,004 SNPs in less than 4 h using the aggregated power of 8,400 cores and a total of ∼16 TB of memory (∼2 GB of memory per core; Supplementary Fig. 3). The running time included estimation of the variances using REML, best linear predictions of the individual's genetic values and best linear predictions of SNP effects. If we disregard the computational overhead of communication between nodes, we can roughly estimate the computational time required by a computer with one core, to complete the analysis by multiplying the number of used cores with the computation time (core hours). In this situation, the MLM fit would need 3.6 years (Fig. 2a). Performing a PCA for 108,000 individuals and 590,004 SNPs required ∼2 h using 1,920 cores. That is, arround ∼4,000 core hours, which would be equivalent to ∼160 days of computation on a single core (Fig. 2b). All these results show both the high computational demands required for performing these analyses and the ability of DISSECT to perform them.

---

### Single photon emission computed tomography (SPECT) myocardial perfusion imaging guidelines: instrumentation, acquisition, processing, and interpretation [^116jAKbz]. Journal of Nuclear Cardiology (2018). Medium credibility.

SPECT myocardial perfusion imaging — two-position and sequential scans for artifact recognition: This 2-position imaging is facilitated by fast imaging times allowed by these systems and is described as allowing differentiation of true perfusion defects from artifacts for systems without AC hardware; moreover, Two sequential scans can also aid in recognizing truncation, motion, or other acquisition-related artifacts because patient position typically changes their appearance.

---

### An operating system for executing applications on quantum network nodes [^117Ab9Mq]. Nature (2025). Excellent credibility.

Design considerations and challenges

Interactive classical-quantum execution

The execution of quantum network applications requires a continuing interaction between the quantum and classical parts of the execution, including interactions between different programs (Fig. 1). For example, during secure quantum computing in the cloud, the program on the server is waiting for classical messages from a remote client program before continuing the quantum execution at the server. This is in sharp contrast to quantum computing applications, in which a quantum application is a single program that can be executed in one batch, without the need to keep quantum states live while waiting for input from other programs. In quantum computing, only relatively low level and predictable interactions between classical and quantum processing are realized, such as in quantum error correctionor mid-circuit measurements. Higher-level classical-quantum interactions in quantum computingdo not keep qubits live in memory.

We assume that the programs are divided into classical and quantum blocks of instructions (by a programmer or a compiler). Classical blocks consist of local classical operations executed on a conventional classical processor, as well as networked classical operations (that is, sending messages to remote nodes) executed using network devices. Quantum blocks consist of local quantum operations (gates, measurements, classical control logic), as well as networked quantum operations (entanglement generation) executed on quantum hardware. A single quantum block, in essence, corresponds to a program in quantum computing and may contain simple classical control logic, such as for the purpose of mid-circuit measurements.

Different hardware platforms

Interfacing with different hardware platforms presents technological challenges: at present, a clear line between software and hardware has not been defined, and the low-level control of present-day quantum processor hardware has been built to conduct physics experiments. Early microarchitectures, and operating systems, for quantum computing do not address the execution of quantum network applications. We thus have to define a hardware abstraction layer capable of interfacing with quantum network processors, including present-day setups.

Timescales

It is a fundamental challenge that different parts of such a system operate at vastly different timescales. For nodes separated by hundreds of kilometres, the duration of network operations is in the millisecond (ms) regime and some applicationsneed substantial local classical processing (ms). By contrast, the time to execute quantum operations on processing nodes is in the regime of microseconds (μs) and the low-level control (including timing synchronization between neighbouring nodes to generate entanglement) requires nanosecond (ns) precision.

---

### Pushing the boundary of quantum advantage in hard combinatorial optimization with probabilistic computers [^112yvBDe]. Nature Communications (2025). High credibility.

Recent demonstrations on specialized benchmarks have reignited excitement for quantum computers, yet their advantage for real-world problems remains an open question. Here, we show that probabilistic computers, co-designed with hardware to implement Monte Carlo algorithms, provide a scalable classical pathway for solving hard optimization problems. We focus on two algorithms applied to three-dimensional spin glasses: discrete-time simulated quantum annealing and adaptive parallel tempering. We benchmark these methods against a leading quantum annealer. For simulated quantum annealing, increasing replicas improves residual energy scaling, consistent with extreme value theory. Adaptive parallel tempering, supported by non-local isoenergetic cluster moves, scales more favorably and outperforms simulated quantum annealing. Field Programmable Gate Arrays or specialized chips can implement these algorithms in modern hardware, leveraging massive parallelism to accelerate them while improving energy efficiency. Our results establish a rigorous classical baseline for assessing practical quantum advantage and present probabilistic computers as a scalable platform for real-world optimization challenges.

---

### A fast and practical computation method for magnetic resonance simulators [^114LkEUM]. Magnetic Resonance in Medicine (2023). Medium credibility.

Purpose

This work aims to develop a fast and practical computation method for MR simulations. The computational cost of MR simulations is often high because magnetizations of many isochromats are updated using a small step size on the order of microseconds. There are two types of subsequences to be processed for the simulations: subsequences with and without RF pulses. While straightforward implementations spend most of their time calculating subsequences with RF pulses, there is a method which efficiently reuses the computation for repetitive RF pulses.

Theory and Methods

A new method for efficiently processing subsequences with RF pulses is proposed. Rather than using an iterative update approach, the proposed method computes the combined transition which combines all transitions applied iteratively for each subsequence with RF pulses. The combined transition is used again when the same subsequence is used later. The combined transitions are cached and managed using a least recently used algorithm.

Results

The proposed method was found to accelerate the simulation by ˜20 times when 3.9 million isochromats were simulated using spin-echo sequences. Even on a laptop computer, the proposed method was able to simulate these sequences in ˜3.5 min.

Conclusion

An efficient method for simulating pulse sequences is proposed. The proposed method computes and manages combined transitions, making MR simulation practical on a wide range of computers, including laptops.

---

### MetaQ: fast, scalable and accurate metacell inference via single-cell quantization [^117C2Urd]. Nature Communications (2025). High credibility.

The main purpose of metacell algorithms is to alleviate the substantial computational burden in single-cell analyses as previously discussed. Thus, in addition to the metacell quality, we were also concerned about the efficiency of metacell algorithms. To this end, we measured the (logged) running time and memory costs of all methods on datasets ranging from 50 thousand to 1 million cells. As shown in Fig. 2 e, MetaQ exhibits linearly increasing running time and constant memory usage relative to the number of cells, theoretically scaling to arbitrary data sizes (see Supplementary Note 2 for more details). Although SuperCell is efficient on relatively small subsets of less than 200,000 cells, it requires exponentially increasing time and linearly increasing memory, leading to limited scalability for larger datasets. Moreover, as shown in Fig. 2 b, SuperCell achieves inferior classification performance compared to other methods, even worse than the naive random sampling baseline with 4,000 metacells. Due to the exponential memory costs, SEACell and MetaCell V2 exceed 512 GB memory — a common configuration for computational servers — when processing 200,000 and 433,000 cells, respectively. Notably, compared to the most competitive baseline SEACell in metacell quality, the proposed MetaQ achieves approximately a 100 times speedup when processing 100,000 cells (0.3 hours versus 26.7 hours). We further investigated the influence of the metacell number on computational expenses. As shown in Fig. 2 f, MetaQ and SuperCell are insensitive to the number of metacells, MetaCell V2 favors larger metacell numbers to activate its divide-and-conquer strategy, and SEACell requires linearly increasing time relative to the metacell number. Notably, one could enable SEACell on the full dataset by inferring metacells in a hierarchical fashion, namely, first inferring metacells within each sample and then performing a secondary metacell aggregation across samples. Supplementary Fig. 4 c indicates that hierarchical SEACell achieves performance on par with MetaQ. However, this improvement comes at a significant computational cost. Supplementary Fig. 4 d reveals that hierarchical SEACell on full data took over a week to complete, whereas running MetaQ only requires about an hour. Such a dramatic improvement in computational efficiency makes MetaQ more favorable in practical use. In summary, the proposed MetaQ not only infers accurate and prototypical metacells, but also offers the best computational scalability for large datasets, making it an effective and efficient tool for metacell analysis.

---

### Just add data: automated predictive modeling for knowledge discovery and feature selection [^112gggvU]. NPJ Precision Oncology (2022). Medium credibility.

Evaluation experimental design

Each dataset was split in half in terms of samples in a stratified way (i.e. the distribution of classes was kept about the same in the two halves as in the original). All tools and variants were executed on the first half to produce a model and a training performance estimate and applied on the second half to obtain a test performance estimate (holdout performance). The roles of each half are then reversed leading to a total of 360 × 2 = 720 runs for each platform. GAMA, TPOT, TPOT-MDR and auto-sklearn all require an indicative time limit for the completion of the analysis. This time limit was set to the maximum between 1 h and the termination time of JADBio on the same task. Despite the time-limit these tools may still take longer; runs that exceeded twice the time limit were forcefully terminated. Each run was performed in parallel on 20 CPUs, using 256 GB of RAM (400 GB for methylation datasets). AutoPrognosis does not run in parallel, thus we run it on a single CPU giving 20 times the time used by JADBio as time limit. Random Forest was not given any specific time limit and was also run on a single CPU. Finally, and most importantly, all hyper-parameters of all tools were set to their default values. The only exception was the performance metric: AUC was employed by all tools for identifying the best-performing model, regardless of their default metric, to ensure a fair comparison.

For this computational experiment, 79,603,051 predictive models were constructed by JADBio alone. It was not possible to extract the number of models trained by the other tools from the logs of the other platforms.

---

### Comparison of reconstruction accuracy and efficiency among autocalibrating data-driven parallel imaging methods [^114GnUVt]. Magnetic Resonance in Medicine (2008). Low credibility.

The class of autocalibrating "data-driven" parallel imaging (PI) methods has gained attention in recent years due to its ability to achieve high quality reconstructions even under challenging imaging conditions. The aim of this work was to perform a formal comparative study of various data-driven reconstruction techniques to evaluate their relative merits for certain imaging applications. A total of five different reconstruction methods are presented within a consistent theoretical framework and experimentally compared in terms of the specific measures of reconstruction accuracy and efficiency using one-dimensional (1D)-accelerated Cartesian datasets. It is shown that by treating the reconstruction process as two discrete phases, a calibration phase and a synthesis phase, the reconstruction pathway can be tailored to exploit the computational advantages available in certain data domains. A new "split-domain" reconstruction method is presented that performs the calibration phase in k-space (k(x), k(y)) and the synthesis phase in a hybrid (x, k(y)) space, enabling highly accurate 2D neighborhood reconstructions to be performed more efficiently than previously possible with conventional techniques. This analysis may help guide the selection of PI methods for a given imaging task to achieve high reconstruction accuracy at minimal computational expense.

---

### Markov chain monte carlo methods for hierarchical clustering of dynamic causal models [^116Qy9hd]. Human Brain Mapping (2021). Medium credibility.

4 DISCUSSION

In this technical note, we introduced a set of proposal densities tailored to improving the convergence of MCMC samplers for hierarchical clustering. Comparing our approach to HMC in terms of computational complexity, the analysis in Section 2.4 showed no clear theoretical advantage for either method. In practice, computation time may vary depending on the particular application. Hence, we conducted extensive empirical tests on synthetic and real‐world datasets, from which several conclusions can be drawn.

First, combining hierarchical clustering with dynamic system models such as DCM presents a formidable challenge to standard MCMC samplers due to the strong posterior correlation present, not only between clustering and DCM parameters, but also among the DCM parameters themselves. In addition, the inherent symmetries of clustering models induce multiple modes in the posterior.

Second, designing specialized proposal densities tailored to the specific challenges posed by the hierarchical clustering model represents an effective solution in practice, leading to better clustering performance in terms of balanced purity and also faster convergence. At the same time, the proposal densities we introduced in Section 2.2 have only a single free parameter (i.e. the mixing ratio between proposal distributions) which would need to be tuned for optimal performance.

Third, our experiments revealed that even state‐of‐the‐art general purpose Monte Carlo methods, such as HMC, which were specifically designed to avoid random walk behaviour and efficiently sample from highly complex target distributions, struggle to converge reliably in timeframes as often required for solving computational problems (i.e. days). Despite being more efficient, that is, requiring fewer samples to explore the target distribution, HMC takes longer to run than Metropolized Gibbs sampling with our special purpose distributions and, for our empirical data, did not converge over the timeframe available on our local shared cluster (5 days). This indicates that, in practice, the overhead of having to simulate the Hamiltonian dynamics negate the advantage afforded by HMC of being able to obtain more independent samples.

---

### A space-time tradeoff for implementing a function with master equation dynamics [^113FRvBR]. Nature Communications (2019). High credibility.

Master equations are commonly used to model the dynamics of physical systems, including systems that implement single-valued functions like a computer's update step. However, many such functions cannot be implemented by any master equation, even approximately, which raises the question of how they can occur in the real world. Here we show how any function over some "visible" states can be implemented with master equation dynamics-if the dynamics exploits additional, "hidden" states at intermediate times. We also show that any master equation implementing a function can be decomposed into a sequence of "hidden" timesteps, demarcated by changes in what state-to-state transitions have nonzero probability. In many real-world situations there is a cost both for more hidden states and for more hidden timesteps. Accordingly, we derive a "space-time" tradeoff between the number of hidden states and the number of hidden timesteps needed to implement any given function.

---

### Stochastic reservoir computers [^112MtuPp]. Nature Communications (2025). High credibility.

Introduction

Reservoir computing is a form of machine learning in which inputs are sequentially fed into a nonlinear dynamical system, whose state is measured and processed using an affine transformation that is trained to produce an output that closely matches some target sequence of interest. This process of feeding inputs into a system and extracting readouts is iterative, making reservoir computers (RCs) particularly well-suited to time-series modeling tasks –, and has also been successful for classification tasks – among many possible uses. The training performed on the readouts is a simple linear regression that can be solved exactly, giving RCs a relatively low computational cost in comparison to the neural networks in use today. The reason that simple linear regression suffices for complex time series modeling is that the nonlinear dynamical system provides readouts that are informationally rich in the inputs, realizing a nonlinear transformation of not only the current input but recent neighboring inputs as well. Using a naturally occurring physical system for the reservoir is a promising avenue toward fast and efficient computation of difficult tasks, as the natural dynamics of the system would perform the nonlinear transformations automatically, leaving only linear computations in the final output readout for a standard computer to perform.

---

### A two-stage testing strategy for detecting genes × environment interactions in association studies [^112kubYu]. G3 (2021). Medium credibility.

The proposed method also has some limitations. For example, the running time of the iSADA method is a little longer than that of the other methods. The computation of one simulated data containing 500 subjects (considering 100 rare variant sites) takes about 30 seconds on average in the Windows System with Intel Core i5-3470 3.20 GHz processor and 4 GB memory, since more permutations are used in the iSADA method. The computing times of the iSADA method are shown in Table 9. For the same data the other methods only take no more than 10 seconds. In addition, using Levene test in stage one may suffer a similar computing time problem. Therefore, in the future we will conduct studies to improve the running speed of the iSADA method.

Table 9
Computing time of the iSADA method with typical parameters

---

### Massive computational acceleration by using neural networks to emulate mechanism-based biological models [^1156Z3L3]. Nature Communications (2019). High credibility.

For many biological applications, exploration of the massive parametric space of a mechanism-based model can impose a prohibitive computational demand. To overcome this limitation, we present a framework to improve computational efficiency by orders of magnitude. The key concept is to train a neural network using a limited number of simulations generated by a mechanistic model. This number is small enough such that the simulations can be completed in a short time frame but large enough to enable reliable training. The trained neural network can then be used to explore a much larger parametric space. We demonstrate this notion by training neural networks to predict pattern formation and stochastic gene expression. We further demonstrate that using an ensemble of neural networks enables the self-contained evaluation of the quality of each prediction. Our work can be a platform for fast parametric space screening of biological models with user defined objectives.

---

### A computational toolbox for the assembly yield of complex and heterogeneous structures [^114LU1sP]. Nature Communications (2023). High credibility.

Discussion

Many structural and functional properties of biological systems rely on heterogeneous self assembly. Increasingly, building blocks with highly tuned geometries and interactions can be made experimentally, presenting an opportunity to design complex self-assembling systems with properties of living matter –. However, for increasing numbers of individual components, the yield of the desired product is diluted by an exponential number of off-target assemblies. Even if each of the off-target assemblies forms with a lower probability than the desired product, the multiplicity of these off-target assemblies can cause a yield catastrophe in which the desired product forms with negligible yield. An efficient method for predicting assembly yield would enable practitioners to design self-assembling systems of increasing complexity while mitigating this yield catastrophe.

In this work, we have developed a combined analytical/computational approach to calculate the equilibrium assembly yields of complexes comprised of heterogeneous building blocks with arbitrary geometries. Our approach involves two novel calculations: (i) computing the partition function of each (predefined) candidate complex (see Eq. (9)) and (ii) given these partition functions, calculating equilibrium assembly yields (see Eqs. (10) and (11)). The classical statistical mechanics methodology we put forth can be realized by modern automatic differentiation techniques enabled by advances in machine learning. While previously, the efficient calculation of the entropic parts of the partition function has been intractable except for simple cases, these advances make this calculation possible even for structures with complicated building blocks or with a large number of different components.

Our theoretical framework provides significant conceptual and practical improvements over simulation-based methods, the only other general-purpose method for yield prediction. Calculations via our approach require a drastically reduced computational cost compared to simulations (i.e. seconds or minutes vs. hours, weeks, or months). Moreover, simulation-based methods can be fraught with additional difficulties such as issues in simulating finite concentrations or efficiently sampling from equilibrium distributions. Lastly, since our method involves the direct calculation of the partition function, our approach can also be used to compute other statistical properties of an equilibrium thermodynamic system (e.g. heat capacity, energy fluctuations) without complicated modifications to the calculation. Indeed, since the gradient of a solution to Eqs. (10) and (11) can be computed implicitly, our method could enable inverse design with respect to related thermodynamic equilibrium properties.

---

### K-bayes reconstruction for perfusion MRI. I: concepts and application [^1131uSbM]. Journal of Digital Imaging (2010). Low credibility.

The results of the different reconstructions are shown in Figure 6. K-Bayes clearly provides the best visual reconstruction of the three approaches. It presents the most contrast and captures more of the gold standard structure. The numerical comparisons in Table 3 indicate that K-Bayes improves over the other methods for all metrics. In particular, bias is around one fourth of that for zDFT, and the gray/white effect size is 50% higher. The RMSE did not show the level of improvements for K-Bayes that were observed in the simulation studies. We believe that there are two reasons for this. First the (pseudo) gold standard is expanded via zero filling from a small enough region of k -space such that it contains artifacts of Gibbs ringing and aliasing. K-Bayes reconstruction of the further reduced dataset does not reproduce these artifacts, whereas the zDFT reconstruction does. Therefore, the RMSE of zDFT would increase and that of K-Bayes would decrease when compared to a true gold standard that did not contain Gibbs ringing and aliasing artifacts. Second, K-Bayes reduces noise that exists in the gold standard. Unlike the simulation study, the gold standard here contains noise. A definitive evaluation would require high-resolution and low-noise perfusion MRI to be used as a gold standard, but this is currently not available as a standard acquisition procedure.

Fig 6
Reconstructions from K-Bayes, zDFT, and zDFT of Hamming windowed data. K-Bayes provides the most detailed reconstruction and recaptures many higher resolution features lost in the DFT-based reconstruction.

Table 3
Statistical Comparison of the Reconstruction Procedures for the Real Perfusion MRI Reconstructions

K-Bayes provided the best results for all measures: having the smallest bias, RMSE, and largest effect size

Computation Time

Computation was performed with a Dell Precision 370 desktop computer running RedHat Enterprise Linux 4.0 on a single 3.20 GHz Pentium 4 processor using c-code. Complete convergence of the EM algorithm (to machine tolerance) for single slice data required less than 1 h. Three-dimensional reconstruction took on the order of 1 day to reach reasonable convergence (i.e. there were no fundamental differences in the reconstruction or statistical metrics if the algorithm was continued). These reconstruction times can be drastically reduced through parallelization, which is computationally trivial for the EM procedure. We found that parallelization yields almost linear speed up with the number of processors.

---

### A computational toolbox for the assembly yield of complex and heterogeneous structures [^115e8nET]. Nature Communications (2023). High credibility.

Robust strategies for solving these problems in the laboratory are unknown. Biology shows that robust solutions exist, but has had millions of years of evolution to find the best designs. For synthetic systems, the time and monetary costs of experimentally testing various self-assembly mixtures to determine those with highest yields can be significant.

A typical route towards addressing these challenges is to use molecular simulations. Simulations could allow rapid screening of the design space of heterogeneous assembly in particular experimental systems, including how the shape, binding characteristics and relative concentrations of the building blocks contribute to the desired emergent property. Significant advances in computational software have made it possible to simulate assemblies of components with nontrivial shapes and interactions. However, such simulations, especially for large heterogeneous structures, are often prohibitively expensive. The use of simulations for exploring heterogeneous assembly has therefore been limited.

Another approach to predict the equilibrium self-assembly yield is to calculate it analytically. While for spherical particles with isotropic interactions the partition functions for small clusters can be calculated analytically, no such analytical calculation exists for anisotropic interactions. We hypothesized that automatic differentiation could be leveraged to perform this otherwise intractable calculation –. In automatic differentiation, the execution of a computer program is accompanied by the construction of a computation graph of primitive operations whose derivatives are known and can therefore be recombined (via the chain rule) to compute the gradient of the larger program. This procedure can be applied recursively, allowing us to efficiently evaluate higher-order derivatives of nearly any computer function with machine accuracy.

The goal of this paper is to develop a combined analytical/computational approach for calculating the concentration- and temperature-dependencies of equilibrium assembly yield for heterogeneous building blocks with complex geometries. We show that this approach enables us to calculate the relevant entropic factors — vibrational, rotational and translational — and estimate the equilibrium assembly yield of the structures.

---

### ACR-ASNR-SPR practice parameter for the performance of computed tomography (CT) in the evaluation and classification of traumatic brain injury [^111tX8np]. ASNR/ACR/SPR (2022). High credibility.

Head CT performance standards — To achieve acceptable clinical CT scans of the brain, the CT scanner should meet or exceed stated specifications, including scan times per slice or image not more than 2 seconds, acquired slice thickness should be 2 mm or less with reconstructed slice thickness should be 5 mm or less, limiting spatial resolution should be > 10 lp/cm for a display field of view (FOV) < 24 cm, and table pitch no greater than 2 for most CT scanners. For CT of the brain, contiguous or overlapping axial slices should be acquired with a slice thickness of no greater than 5 mm, and axial reconstructed images should be presented with at least 2 different kernels using both a brain/soft-tissue and bone kernel; CT brain imaging may be performed with a sequential single-slice technique, multislice helical (spiral) protocol, or multidetector multislice algorithm.

---

### Identification of putative causal loci in whole-genome sequencing data via knockoff statistics [^11782ouA]. Nature Communications (2021). High credibility.

We performed simulations to empirically evaluate the computational time for the different methods. We note that the proposed method focuses on the analysis of whole-genome sequencing data, and thus the computational cost is reported on unphased genotype data, which is the usual format for sequencing data. Since the HMM model assumes the availability of phased data, we report the computing time separately for phasing with fastPhase and sampling with SNPknock as described in Sesia et al. We simulated genetic data using the SKAT package, with varying sample sizes and number of genetic variants (Table 1). The computing time was evaluated on a single CPU (Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60 GHz). For the simulation scenario considered in the previous section with 10,000 individuals and 1,000 genetic variants, we observed that the proposed method takes 6.59 s to generate a single set of knockoff features, which is ~130 times faster than the HMM model with S = 12 states (881.43 s). The application of the HMM model with the recommended S = 50 states to unphased sequencing data (13681.53 s for 10,000 individuals and 1000 genetic variants) is currently not practical at genome-wide scale. As shown, a substantial fraction of the total computing time is taken by the phasing step, and therefore using more computationally efficient phasing algorithms can further improve the computational cost of the HMM-based knockoff generation.

Table 1
Computing time of different knockoff generators.

Each cell shows the computing time in seconds to generate knockoffs based on unphased genotype data. The multiple sequential knockoffs approach generates five knockoffs. The computing time was measured on unphased genotype data using a single CPU (Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60 GHz). Since the HMM model was mainly proposed for phased data, we report the computing time separately for phasing with fastPhase, and sampling with SNPknock.

---

### A continuous-time maxSAT solver with high analog performance [^111by24Q]. Nature Communications (2018). Medium credibility.

Discussion

In summary, we presented a continuous-time dynamical system approach to solve a quintessential discrete optimization problem, MaxSAT. The solver is based on a deterministic set of ordinary differential equations and a heuristic method that is used to predict the likelihood that the optimal solution has been found by analog time t. The prediction part of the algorithm exploits the statistics of the ensemble of trajectories started from random initial conditions, by introducing the notion of energy-dependent escape rate and extrapolating this dependence to predict both the minimum energy value (lowest number of unsatisfied clauses) and the expected time needed by the algorithm to reach that value. This statistical analysis is very simple; it is quite possible that more sophisticated methods can be used to better predict minima values and time lengths. Due to its general character, the presented approach can be extended to other optimization problems as well, to be presented in forthcoming publications.

---

### All-to-all reconfigurability with sparse and higher-order ising machines [^112SryCh]. Nature Communications (2024). High credibility.

All-to-all reconfigurability via sparse network multiplexing

We define all-to-all reconfigurability as the ability to solve multiple instances of a given problem using the same hardware. We implement a dense master graph architecture on both second-order and third-order problems that can be multiplexed to access different instances of a sparse combinatorial optimization problem. While our approach cannot fully emulate all-to-all reconfigurability on natively dense problems, it has previously been shown how such problems can efficiently be sparsified while leveraging a large degree of parallelism. Concretely, our approach is based on combining all 100 sparse instances of the 3R3X problem (as defined in the XORSAT challenge) into a single master graph that allows activating one instance at a time (Fig. 2). In general, a master graph can be defined as a complete graph that can house different instances of a sparsified optimization problem, either statically multiplex (as in this paper) or dynamically reconfigured (to activate any desired instance). As we discuss in the following sections, the master graph approach must also multiplex the colors of a given node, which selects phase-shifted clocks for sparse p-bit networks that are updated in large parallel blocks.

Fig. 2
Multiplexed all-to-all reconfigurable master graph approach.

a For each problem size (n), multiple graph-colored sparse instances of the 3R3X problem are combined to form a dense master graph. b In our architecture, neighbors, and clocks for each p-bit are multiplexed using an instance selector. c Pair-wise swap acceptance rates show roughly equal probability in both the master graph (FPGA) and all-to-all graph (CPU), obtained from APT across 8 replicas for n = 80. d Twenty instances with the highest success probabilities (p i) are shown for n = 80. All p i values are computed from 1000 independent runs. e Mean p i as a function of swap attempts at varying n. c – e Establish equivalence between our master graph approach (FPGA) and all-to-all graph (CPU).

---

### Next generation reservoir computing [^112UKfss]. Nature Communications (2021). High credibility.

Reservoir computing is a best-in-class machine learning algorithm for processing information generated by dynamical systems using observed time-series data. Importantly, it requires very small training data sets, uses linear optimization, and thus requires minimal computing resources. However, the algorithm uses randomly sampled matrices to define the underlying recurrent neural network and has a multitude of metaparameters that must be optimized. Recent results demonstrate the equivalence of reservoir computing to nonlinear vector autoregression, which requires no random matrices, fewer metaparameters, and provides interpretable results. Here, we demonstrate that nonlinear vector autoregression excels at reservoir computing benchmark tasks and requires even shorter training data sets and training time, heralding the next generation of reservoir computing.

---

### Topological quantum computing with a very noisy network and local error rates approaching one percent [^111nLSWk]. Nature Communications (2013). Medium credibility.

A scalable quantum computer could be built by networking together many simple processor cells, thus avoiding the need to create a single complex structure. The difficulty is that realistic quantum links are very error prone. A solution is for cells to repeatedly communicate with each other and so purify any imperfections; however prior studies suggest that the cells themselves must then have prohibitively low internal error rates. Here we describe a method by which even error-prone cells can perform purification: groups of cells generate shared resource states, which then enable stabilization of topologically encoded data. Given a realistically noisy network (≥ 10% error rate) we find that our protocol can succeed provided that intra-cell error rates for initialisation, state manipulation and measurement are below 0.82%. This level of fidelity is already achievable in several laboratory systems.

---

### The snm procedure guideline for general imaging 6.0 [^116j8Aw2]. SNMMI (2010). Medium credibility.

General imaging — iterative reconstruction of single-photon emission computed tomography (SPECT) states that 'With the increasing power and memory of computer hardware and with development of the more efficient OSEM algorithm, iterative reconstruction of SPECT studies is now common in the clinical environment'. It adds that 'This methodology makes it possible to incorporate correction for many physical effects such as non-uniform attenuation correction, scatter reduction or removal, variation of spatial resolution with distance, etc'.

---

### Experimental demonstration of quantum advantage for NP verification with limited information [^111N1gny]. Nature Communications (2021). High credibility.

In this work, we study the power of quantum technologies to provide a computational advantage in an interactive setting, where first we allow two parties to interact in a predefined manner, and then we look at the time it takes for one of them to resolve a specific computational task when they can use quantum or classical resources. Specifically, we study the task of verifying NP-complete problems, in particular whether a set of boolean constraints have a satisfying assignment to them or not, when an untrusted party provides some limited information about the solution of the problem. For this task, we show that we can achieve a quantum advantage exploiting experimental techniques involving coherent states, linear optics and single-photon detection.

Before explaining this further let us remark a few properties of our result: first, the quantum hardware we use is simple and the demonstration can be readily reproduced in well-equipped quantum photonics labs; second, our task is inherently verifiable since the output is a YES/NO answer and not a sample from an exponential size distribution (we emphasize here that the quantum machine in our scenario is certainly not solving NP-complete problems but merely verifies whether a solution exists or not with limited information about the possible solution); third, the benchmarking against the best classical methods is based only on the assumption that NP-complete problems do not have sub-exponential algorithms, a well-known and widely accepted computational assumption; and finally, while previously experimentally demonstrated computational tasks are typically tailor-made for showing quantum advantage with no direct connection to useful applications, the fast verification of NP-complete problems with bounded information leakage could potentially lead to interesting applications, including in server–client quantum computing, authentication systems, ethical behavior enforcement, and blockchain technologies. At the same time, we stress that the computational advantage we achieve is not in the standard computational model where a single classical or quantum machine receives an input and computes an output, but in the interactive setting, where we first allow interaction with a second party before trying to resolve the computational task at hand.

---

### Feasibility of through-time spiral generalized autocalibrating partial parallel acquisition for low latency accelerated real-time MRI of speech [^116WWiu5]. Magnetic Resonance in Medicine (2017). Low credibility.

Purpose

To evaluate the feasibility of through-time spiral generalized autocalibrating partial parallel acquisition (GRAPPA) for low-latency accelerated real-time MRI of speech.

Methods

Through-time spiral GRAPPA (spiral GRAPPA), a fast linear reconstruction method, is applied to spiral (k-t) data acquired from an eight-channel custom upper-airway coil. Fully sampled data were retrospectively down-sampled to evaluate spiral GRAPPA at undersampling factors R = 2 to 6. Pseudo-golden-angle spiral acquisitions were used for prospective studies. Three subjects were imaged while performing a range of speech tasks that involved rapid articulator movements, including fluent speech and beat-boxing. Spiral GRAPPA was compared with view sharing, and a parallel imaging and compressed sensing (PI-CS) method.

Results

Spiral GRAPPA captured spatiotemporal dynamics of vocal tract articulators at undersampling factors ≤ 4. Spiral GRAPPA at 18 ms/frame and 2.4mm 2 /pixel outperformed view sharing in depicting rapidly moving articulators. Spiral GRAPPA and PI-CS provided equivalent temporal fidelity. Reconstruction latency per frame was 14 ms for view sharing and 116 ms for spiral GRAPPA, using a single processor. Spiral GRAPPA kept up with the MRI data rate of 18ms/frame with eight processors. PI-CS required 17 minutes to reconstruct 5 seconds of dynamic data.

Conclusion

Spiral GRAPPA enabled 4-fold accelerated real-time MRI of speech with a low reconstruction latency. This approach is applicable to wide range of speech RT-MRI experiments that benefit from real-time feedback while visualizing rapid articulator movement. Magn Reson Med 78:2275–2282, 2017. © 2017 International Society for Magnetic Resonance in Medicine.

---

### Efficient and practical hamiltonian simulation from time-dependent product formulas [^116UzuZX]. Nature Communications (2025). High credibility.

In this work we propose an approach for implementing time-evolution of a quantum system using product formulas. The quantum algorithms we develop have provably better scaling (in terms of gate complexity and circuit depth) than a naive application of well-known Trotter formulas, for systems where the evolution is determined by a Hamiltonian with different energy scales (i.e., one part is "large" and another part is "small"). Our algorithms generate a decomposition of the evolution operator into a product of simple unitaries that are directly implementable on a quantum computer. Although the theoretical scaling is suboptimal compared with state-of-the-art algorithms (e.g., quantum signal processing), the performance of the algorithms we propose is highly competitive in practice. We illustrate this via extensive numerical simulations for several models. For instance, in the strong-field regime of the 1D transverse-field Ising model, our algorithms achieve an improvement of one order of magnitude in both the system size and evolution time that can be simulated with a fixed budget of 1000 arbitrary 2-qubit gates, compared with standard Trotter formulas.

---

### The snm procedure guideline for general imaging 6.0 [^114QWgbH]. SNMMI (2010). Medium credibility.

Nuclear medicine computer system components — The camera head or associated image processing system performs functions including image size, position and zoom; energy correction; spatial distortion correction; other corrections (scatter correction, dead time correction, depth of interaction correction, sensitivity correction); and digital position computation; the interface handles the data in two basic modes: 1) Frame mode: complete images or matrices are available to the attached computer; and 2) List mode: data are passed on to the attached computer as a list of event x, y coordinates, to which time information and energy information may be also attached; for cardiac studies in particular, time lapse averaging is required, such that each image acquired at some specific time within the cardiac cycle is added to other acquired at similar times.

---

### Thermally assisted quantum annealing of a 16-qubit problem [^116RxEtp]. Nature Communications (2013). Medium credibility.

Efforts to develop useful quantum computers have been blocked primarily by environmental noise. Quantum annealing is a scheme of quantum computation that is predicted to be more robust against noise, because despite the thermal environment mixing the system's state in the energy basis, the system partially retains coherence in the computational basis, and hence is able to establish well-defined eigenstates. Here we examine the environment's effect on quantum annealing using 16 qubits of a superconducting quantum processor. For a problem instance with an isolated small-gap anticrossing between the lowest two energy levels, we experimentally demonstrate that, even with annealing times eight orders of magnitude longer than the predicted single-qubit decoherence time, the probabilities of performing a successful computation are similar to those expected for a fully coherent system. Moreover, for the problem studied, we show that quantum annealing can take advantage of a thermal environment to achieve a speedup factor of up to 1,000 over a closed system.

---

### Past-future information bottleneck for sampling molecular reaction coordinate simultaneously with thermodynamics and kinetics [^115orbRr]. Nature Communications (2019). High credibility.

The ability to rapidly learn from high-dimensional data to make reliable bets about the future is crucial in many contexts. This could be a fly avoiding predators, or the retina processing gigabytes of data to guide human actions. In this work we draw parallels between these and the efficient sampling of biomolecules with hundreds of thousands of atoms. For this we use the Predictive Information Bottleneck framework used for the first two problems, and re-formulate it for the sampling of biomolecules, especially when plagued with rare events. Our method uses a deep neural network to learn the minimally complex yet most predictive aspects of a given biomolecular trajectory. This information is used to perform iteratively biased simulations that enhance the sampling and directly obtain associated thermodynamic and kinetic information. We demonstrate the method on two test-pieces, studying processes slower than milliseconds, calculating free energies, kinetics and critical mutations.

---

### Enhancing computation speed and accuracy in deep image prior-based parameter mapping [^113wyxc4]. Magnetic Resonance in Medicine (2025). Medium credibility.

4.3 Computational efficiency

The filter depth grid search (Figure 6) resulted in mean computation times betweenandwhich corresponds to betweenandoptimization steps. The shortest computation time was achieved with.

FIGURE 6
Filter depth grid search. Mean computation time (blue) and optimization steps (orange) for the filter depth grid search. Each point is presented as an average over 15 repetitions. The results indicate that although increasing filter depth reduces the required optimization steps, the corresponding computation time reaches its minimum of 5.3 min with 128 filters applied.

The learning rate grid search (Figure 7) resulted in mean computation time betweenand. The variability metric (Equation 10) varied betweenandrelative to the baseline, with the lowest value at, and the highest value at.

FIGURE 7
Learning rate grid search. Mean computation time (blue) and variability (Equation 10, orange) across learning rates. Computation time is averaged over 50 runs, and variability is reported relative to the previously used learning rate (, see Section 3.1). Results show that higher learning rates reduce computation time but increase variability, revealing a trade‐off between speed and model stability.

---

### Experimental demonstration of quantum advantage for NP verification with limited information [^117GqZ42]. Nature Communications (2021). High credibility.

In recent years, many computational tasks have been proposed as candidates for showing a quantum computational advantage, that is an advantage in the time needed to perform the task using a quantum instead of a classical machine. Nevertheless, practical demonstrations of such an advantage remain particularly challenging because of the difficulty in bringing together all necessary theoretical and experimental ingredients. Here, we show an experimental demonstration of a quantum computational advantage in a prover-verifier interactive setting, where the computational task consists in the verification of an NP-complete problem by a verifier who only gets limited information about the proof sent by an untrusted prover in the form of a series of unentangled quantum states. We provide a simple linear optical implementation that can perform this verification task efficiently (within a few seconds), while we also provide strong evidence that, fixing the size of the proof, a classical computer would take much longer time (assuming only that it takes exponential time to solve an NP-complete problem). While our computational advantage concerns a specific task in a scenario of mostly theoretical interest, it brings us a step closer to potential useful applications, such as server-client quantum computing.

---

### State-of-the-art augmented NLP transformer models for direct and single-step retrosynthesis [^113CvaPJ]. Nature Communications (2020). High credibility.

Introduction

To synthesize an organic compound is to solve a puzzle with many pieces and potentially several pieces missing. Here, the pieces are single reactions, and finding their sequential combination to create a final product is the retrosynthesis task.

The success of the logic of organic synthesis developed by Corey et al. triggered the development of computer programs aiming to find appropriate ways to synthesize a molecule. The first retrosynthesis program LHASAutilizes a template-based, approach. Every template (rule, synthon) in a curated database of known transformations is sequentially applied to a target molecule, and then sets of reagents are selected according to a specified strategy. Reagents, in turn, undergo the same decompositions until a set of commercially available compounds is found. Retrosynthesis always has multiple routes — a retrosynthetic tree — ending with different starting materials. Thus, a practical algorithm for retrosynthesis has to solve not only the rule acquisition and selection problem but also has capabilities to effectively navigate this tree, taking into account different strategies. These tasks relate directly to artificial intelligence strategies –.

---

### Single photon emission computed tomography (SPECT) myocardial perfusion imaging guidelines: instrumentation, acquisition, processing, and interpretation [^115pe2pg]. Journal of Nuclear Cardiology (2018). Medium credibility.

Dead-time effects and count-rate performance — SPECT detector performance can be described by the count rate at which dead-time effects cause a loss of 20% of the expected count rate, and while dynamic imaging of myocardial blood flow may encounter very high count rates during initial frames, incorrectly accounting for dead-time losses can cause errors in quantitative blood flow measurement; however, for all cardiac myocardial perfusion radiotracers, protocols, and imaging systems currently in clinical use, dead-time effects are not a clinical concern.

---

### Towards a fully automated algorithm driven platform for biosystems design [^112rQ55H]. Nature Communications (2019). High credibility.

Evaluation of the Bayesian optimization algorithm

To illustrate Bayesian optimization with GP, we defined a single variable function and tried to find the maximum value by sequential sampling (Fig. 2). The function was deliberately chosen to have multiple peaks and local optima (dashed curve in Fig. 2) to test whether the optimization algorithm can indeed find the global maximum. The algorithm was able to find the maximum and the exploration and exploitation trade-off is illustrated by the sampling order depicted in the figure. The more points evaluated by the algorithm, the closer the algorithm became to the maximum as shown in Fig. 2b.

Fig. 2
Testing Bayesian optimization by finding the maximum of a two-dimensional function. a The acquisition function decides the next input to test and the output is used to refine the predictive model. Iterations 5, 10, 15 and 20 of this process are shown. b With increasing rounds of iteration, the predictive model grows more confident of the location of the global maximum and the distance between tested inputs decreases with each iteration. c The algorithm evaluated 9 points before finding the location of the maximum. Subsequent iterations tuned this approximation toward the true optimum. The algorithm evaluated 12 points before finding the maximum. The order in which each point is evaluated is shown on the graph

---

### Single photon emission computed tomography (SPECT) myocardial perfusion imaging guidelines: instrumentation, acquisition, processing, and interpretation [^112mFnFd]. Journal of Nuclear Cardiology (2018). Medium credibility.

Indirect photon detection with solid-state photodiodes — pixelated scintillation crystals are coupled to solid-state photodiodes instead of PMTs, and this design has been used in a 2- or 3-detector configuration with parallel-hole or fan-beam collimators; digital logic and software process the signals instead of analog Anger positioning circuits.

---

### Sequential stacking link prediction algorithms for temporal networks [^116zXa7o]. Nature Communications (2024). High credibility.

Link prediction algorithms are indispensable tools in many scientific applications by speeding up network data collection and imputing missing connections. However, in many systems, links change over time and it remains unclear how to optimally exploit such temporal information for link predictions in such networks. Here, we show that many temporal topological features, in addition to having high computational cost, are less accurate in temporal link prediction than sequentially stacked static network features. This sequential stacking link prediction method uses 41 static network features that avoid detailed feature engineering choices and is capable of learning a highly accurate predictive distribution of future connections from historical data. We demonstrate that this algorithm works well for both partially observed and completely unobserved target layers, and on two temporal stochastic block models achieves near-oracle-level performance when combined with other single predictor methods as an ensemble learning method. Finally, we empirically illustrate that stacking multiple predictive methods together further improves performance on 19 real-world temporal networks from different domains.

---

### Experimental perfect state transfer of an entangled photonic qubit [^112TDzjK]. Nature Communications (2016). Medium credibility.

Transferring quantum information between locations without disrupting the encoded information en route is crucial for future quantum technologies. Routing quantum information is necessary for communication between quantum processors, addressing single qubits in topological surface architectures, and for quantum memories as well as many other applications.

Coupling between stationary qubits and mobile qubits via cavity and circuit quantum electrodynamics has been an active area of research with promise for long-distance quantum communication; however, coupling between different quantum information platforms is challenging as unwanted degrees of freedom lead to increased decoherence. Quantum teleportation between distant qubits allows long-distance quantum communication via shared entangled states; however, in most quantum information platforms this would again require coupling between stationary and mobile qubits. Physically relocating trapped ion qubits has also been demonstrated, however, with additional decoherence incurred during transport.

By taking advantage of coupling between neighbouring qubits, it is possible to transport quantum information across a stationary lattice. This has the benefits that one physical platform is being used and the lattice sites remain at fixed locations. The most basic method is to apply a series of SWAP operations between neighbouring sites such that, with enough iterations, the state of the first qubit is relocated to the last. This method requires a high level of active control on the coupling and is inherently weak as individual errors accumulate after each operation, leading to an exponential decay in fidelity as the number of operations increases.

The perfect state transfer (PST) protocol utilizes an engineered but fixed coupled lattice. Quantum states are transferred between sites through Hamiltonian evolution for a specified time. For a one-dimensional system with N sites, the state intially at site n is transferred to site N − n +1 with 100% probability without need for active control on the coupling. PST can be performed on any quantum computing architecture where coupling between sites can be engineered, such as ion trapsand quantum dots. Figure 1 presents an illustration of the PST protocol. The encoded quantum state, initially at the first site, is recovered at the final site after a specific time. In the intermediate stages, the qubit is in a superposition across the lattice. Aside from qubit relocation, the PST framework can be applied to entangled W-state preparation, state amplificationand even quantum computation.

---

### An explainable machine learning-based phenomapping strategy for adaptive predictive enrichment in randomized clinical trials [^115tKJ1w]. NPJ Digital Medicine (2023). Medium credibility.

Once a predictive model had been generated, and our analysis in the testing set had met the criteria for possible heterogeneity in the treatment effect with sufficient power for the primary outcome, we chose to proceed with predictive enrichment. Over the subsequent period (time between the last and next interim analyses), the model was prospectively applied to all trial candidates screened after the enrichment model was identified. For example, a model trained at interim analysis timepoint #1 was applied to individuals screened between the interim analysis timepoints #1 and #2 to furnish a probability of enrollment, with all original trial participants during this period considered eligible candidates. For all candidates, the probability of being enrolled was conditioned on their predicted individualized benefit, ultimately enriching the population at the level defined during the last interim analysis sample size calculation. Alternatively, if there was no evidence of heterogeneous treatment effect, or the proposed enrichment in enrollment would not be adequately powered at a sample size equal to or lower than the originally planned one, we proceeded as originally planned and continued with standard enrollment for that period without predictive enrichment. Given the stochastic nature of the algorithm, all simulations were repeated r = 10 times.

---

### A hybrid cloud read aligner based on minHash and kmer voting that preserves privacy [^116qGo7x]. Nature Communications (2017). Medium credibility.

Read alignment is a critical and computationally intensive first step of most genomic data analysis pipelines. While tremendous efforthas been dedicated to this problem, few approaches have addressed outsourcing this computation securely to an untrusted party. The few secure solutions that exist either do not scale to whole genome sequencing data setsor are not competitive with the state of the art in read mapping. In particular, the protocolfor computing edit distances using homomorphic encryptionrequires 5 min on a single pair of 25 bp sequences, and the approachusing secure multiparty computations, while more efficient, still takes 4 s for a pair of 100 bp sequences. Recently, Chen et al.proposed a secure seed-and-extend read mapping algorithm on hybrid clouds that splits the computation such that the public cloud finds the exact seed matches using encrypted seeds and the private cloud extends the seed matches using unencrypted data. With this approach, mapping 10 million 100 bp reads takes 370 CPU hours on the public cloud, an additional 2 h on the private cloud and 6.8 TB to store the reference index. This time requirement is not competitive with state-of-the-art aligners, given that time spent on the private client alone is higher than the total runtime of standard aligners. For instance, Bowtie2 requires 1 h for the same data with a peak memory footprint of just 3.24 GB.

In this work we present Balaur, an efficient privacy preserving read mapping technique for hybrid clouds based on locality sensitive hashing and kmer voting. Balaur securely outsources 50–70% of the read mapping task to the public cloud, while being highly competitive with existing state-of-the-art aligners in speed and accuracy. Therefore, in a hybrid cloud system with a fast network, Balaur can result in substantial savings of private cloud resources with respect to standard nonsecure tools that must entirely run on the private cloud.

---

### ACR-ASNR-SPR practice parameter for the performance of computed tomography angiography (CTA) of the head and neck [^111tuygN]. ACR/SPR/ASNR (2025). High credibility.

Head and neck computed tomography angiography (CTA) — contrast bolus timing methods include a test bolus and automated triggering. IV injection of a small test bolus (eg, 10–15 mL) with sequential cine imaging is used to generate a time density curve whose peak determines the scanning delay; alternatively, automated or semiautomated triggering based on vessel attenuation may be used, with the CT starting at a predetermined operator-selected level and individualized posttrigger delay to optimize opacification. For computed tomography venography (CTV), a 40–50-second prescanning delay or up to a 30-second delay after the arterial bolus time should allow adequate venous opacification.

---

### The digitization of organic synthesis [^112civby]. Nature (2019). Excellent credibility.

Organic chemistry has largely been conducted in an ad hoc manner by academic laboratories that are funded by grants directed towards the investigation of specific goals or hypotheses. Although modern synthetic methods can provide access to molecules of considerable complexity, predicting the outcome of a single chemical reaction remains a major challenge. Improvements in the prediction of 'above-the-arrow' reaction conditions are needed to enable intelligent decision making to select an optimal synthetic sequence that is guided by metrics including efficiency, quality and yield. Methods for the communication and the sharing of data will need to evolve from traditional tools to machine-readable formats and open collaborative frameworks. This will accelerate innovation and require the creation of a chemistry commons with standardized data handling, curation and metrics.

---

### Towards a fully automated algorithm driven platform for biosystems design [^115PJveN]. Nature Communications (2019). High credibility.

We then tested the Bayesian optimization method by running multiple simulations with different conditions. First, to see if the algorithm can find the maximum of other functions than the one tested in the previous section, we generated 100 random Gaussian mixture models and found the maximum for all of them using this algorithm. On average, it took the algorithm 9.82 and 7.93 evaluations to find the maximum and 95% of the maximum, respectively. To test the effect of error on the algorithm, we randomly picked one of these 100 Gaussian mixture models and attempted to find the maximum while adding 0%, 10% and 20% error rate, the upper bound of most analytical methods, to the output value of the function evaluation to better simulate the real experimental setup. We observed that the algorithm is still able to find the maximum of the function in most runs, but the number of evaluations in each run was significantly increased and in the case with 20% error, it could not find the maximum for 18% of the cases even after 400 evaluations. However, the algorithm could find 95% of the maximum in all cases (Table 1). This shows that, as expected, error makes the optimization more difficult, but Bayesian optimization algorithm can adjust for it and still find the maximum for most cases. It is noteworthy that finding the maximum gets increasingly difficult with higher error rate. Other than the fact that low-quality data, as expected, reduce the predictive power of the model, with higher error rate, the difference between points closest to the maximum becomes indistinguishable.

---

### Practical hamiltonian learning with unitary dynamics and gibbs States [^1135pTwh]. Nature Communications (2024). High credibility.

We study the problem of learning the parameters for the Hamiltonian of a quantum many-body system, given limited access to the system. In this work, we build upon recent approaches to Hamiltonian learning via derivative estimation. We propose a protocol that improves the scaling dependence of prior works, particularly with respect to parameters relating to the structure of the Hamiltonian (e.g., its locality k). Furthermore, by deriving exact bounds on the performance of our protocol, we are able to provide a precise numerical prescription for theoretically optimal settings of hyperparameters in our learning protocol, such as the maximum evolution time (when learning with unitary dynamics) or minimum temperature (when learning with Gibbs states). Thanks to these improvements, our protocol has practical scaling for large problems: we demonstrate this with a numerical simulation of our protocol on an 80-qubit system.

---

### Interfering trajectories in experimental quantum-enhanced stochastic simulation [^116cUW2U]. Nature Communications (2019). High credibility.

Simulations of stochastic processes play an important role in the quantitative sciences, enabling the characterisation of complex systems. Recent work has established a quantum advantage in stochastic simulation, leading to quantum devices that execute a simulation using less memory than possible by classical means. To realise this advantage it is essential that the memory register remains coherent, and coherently interacts with the processor, allowing the simulator to operate over many time steps. Here we report a multi-time-step experimental simulation of a stochastic process using less memory than the classical limit. A key feature of the photonic quantum information processor is that it creates a quantum superposition of all possible future trajectories that the system can evolve into. This superposition allows us to introduce, and demonstrate, the idea of comparing statistical futures of two classical processes via quantum interference. We demonstrate interference of two 16-dimensional quantum states, representing statistical futures of our process, with a visibility of 0.96 ± 0.02.

---

### Single photon emission computed tomography (SPECT) myocardial perfusion imaging guidelines: instrumentation, acquisition, processing, and interpretation [^113GKpuC]. Journal of Nuclear Cardiology (2018). Medium credibility.

SPECT myocardial perfusion imaging — Rest and stress acquisition parameters (Table A4 continued) specify continuous acquisition and number of projections N/A for both Study 1 and Study 2, matrix 64 x 64 (minimum) with 128 x 128 also shown, acquisition time 5–14 min (1 million counts) for Study 1 and 3–6 min (1 million counts) for Study 2, ECG gated as Optional (rest); Standard (stress) in both studies, frames/cycle 8 in both, and R-to-R window 100%.

---

### Improving residency matching through computational optimization [^112XbTNc]. JAMA Network Open (2025). High credibility.

Fellowship Matching

Unlike ophthalmology residency, which has low vacancy rates, more ophthalmology fellowships have higher unmatched rates (on the order of 10%-15%). Residency optimizer consistently filled 5% to 10% more ophthalmology fellowship positions than Gale-Shapley (eFigure 2 in Supplement 1) with similar overall utility. Residency optimizer outperformed Gale-Shapley particularly for less popular ophthalmology subspecialties (eFigure 3 in Supplement 1).

Couple Matching

Couples matching with residency optimizer always matched successful couples in the same city, without hurting their overall chances of matching (eFigure 4 in Supplement 1). In contrast, Gale-Shapley only matched couples in the same city 40% of the time.

Residency Optimizer for NRMP Specialties

Residency optimizer remained computationally efficient when run on larger matches, taking less than 10 seconds on a desktop computer to run for a simulated full NRMP match (see eAppendix 5 in Supplement 1). When simulated for other medical specialties, residency optimizer filled more positions than Gale-Shapley (eTable 4 in Supplement 1), filling 99.39% of positions on average vs 97.99% for Gale-Shapley. Moreover, residency optimizer achieved significantly better mean ranks of matched programs for applicants in all studied NRMP specialties (eFigure 5A in Supplement 1). In addition, residency optimizer achieved superior ranks of matched applicants for programs in most specialties (eFigure 5B in Supplement 1); for those specialties where programs had lower match ranking results, applicants fared much better. Finally, a higher percentage of applicants matched to their top choices under residency optimizer in all specialties (Figure 4). See additional data in eTables 5 to 9 and eFigures 6 and 7 in Supplement 1.

Figure 4.
Cumulative Percentage of Applicants Matching Their Top n th Choice by Specialty, for Gale-Shapley and Residency Optimization (ResOpt) Algorithms Averaged Over 10 Runs

---

### Chunking as the result of an efficiency computation trade-off [^111Ckn7Q]. Nature Communications (2016). Medium credibility.

How to move efficiently is an optimal control problem, whose computational complexity grows exponentially with the horizon of the planned trajectory. Breaking a compound movement into a series of chunks, each planned over a shorter horizon can thus reduce the overall computational complexity and associated costs while limiting the achievable efficiency. This trade-off suggests a cost-effective learning strategy: to learn new movements we should start with many short chunks (to limit the cost of computation). As practice reduces the impediments to more complex computation, the chunking structure should evolve to allow progressively more efficient movements (to maximize efficiency). Here we show that monkeys learning a reaching sequence over an extended period of time adopt this strategy by performing movements that can be described as locally optimal trajectories. Chunking can thus be understood as a cost-effective strategy for producing and learning efficient movements.

---

### Multi-qubit entanglement and algorithms on a neutral-atom quantum computer [^111Lk1nB]. Nature (2022). Excellent credibility.

Gate-model quantum computers promise to solve currently intractable computational problems if they can be operated at scale with long coherence times and high-fidelity logic. Neutral-atom hyperfine qubits provide inherent scalability owing to their identical characteristics, long coherence times and ability to be trapped in dense, multidimensional arrays 1. Combined with the strong entangling interactions provided by Rydberg states 2–4, all the necessary characteristics for quantum computation are available. Here we demonstrate several quantum algorithms on a programmable gate-model neutral-atom quantum computer in an architecture based on individual addressing of single atoms with tightly focused optical beams scanned across a two-dimensional array of qubits. Preparation of entangled Greenberger-Horne-Zeilinger (GHZ) states 5 with up to six qubits, quantum phase estimation for a chemistry problem 6 and the quantum approximate optimization algorithm (QAOA) 7 for the maximum cut (MaxCut) graph problem are demonstrated. These results highlight the emergent capability of neutral-atom qubit arrays for universal, programmable quantum computation, as well as preparation of non-classical states of use for quantum-enhanced sensing.

---

### Time efficient design of multi dimensional RF pulses: application of a multi shift CGLS algorithm [^1169LQuv]. Magnetic Resonance in Medicine (2011). Low credibility.

Designing multi dimensional ratio frequency excitation pulses in the small flip angle regime commonly reduces to the solution of a least squares problem, which requires regularization to be solved numerically. Usually, regularization is carried out by the introduction of a penalty, λ, on the solution norm. In most cases, the optimal regularization parameter is not known a priori and the problem needs to be solved for several values of λ. The optimal value can be selected, typically by plotting the L-curve. In this article, a conjugate gradients-based algorithm is applied to design ratio frequency pulses in a time-efficient way without a priori knowledge of the optimal regularization parameter. The computation time is reduced considerably (by a factor 10 in a typical set up) with respect to the standard conjugate gradients for least square since just one run of the algorithm is required. Simulations are shown and the performance is compared to that of conjugate gradients for least square.

---

### Synthesizing theories of human language with Bayesian program induction [^116xoZ7d]. Nature Communications (2022). High credibility.

We represent M by adapting the Fragment Grammars formalism. Concretely, M is a probabilistic context free grammar (PCFG) that stochastically generates phonological rules. More precisely, M generates the syntax tree of a program which implements a phonological rule. In the Fragment Grammars formalism, one first defines a base grammar, which is a context-free grammar. Our base grammar is a context-free grammar over SPE rules (Supplementary Fig. 6). Learning the fragment grammar consists of adding new productions to this base grammar (the "fragments"), while also assigning probabilities to each production rule. Formally, each fragment is a subtree of a derivation of a tree generated from a non-terminal symbol in the base grammar; informally, each fragment is a template for a piece of a tree, and thus acts as a schema for a piece of a phonological rule. Learning a fragment grammar never changes the set of trees (i.e. programs and rules) that can be generated from the grammar. Instead, through a combination of estimating probabilities and defining new productions, it adjusts the probability of different trees. See Supplementary Fig. 6, which shows the symbolic structure of the learned fragment grammar.

This fragment grammar gives us a learned prior over single phonological rules. We define P (T, L ∣ M) by assuming that rules are generated independently and that M does not affect the prior probability of L :Our prior over fragment grammars, P (M), works by following the original work in this spaceby assuming that fragments are generated sequentially, with new fragments generated from the current fragment grammar by stochastically sampling them from the current fragment grammar. This encourages shorter fragments, as well as reuse across fragments.

We depart from ref.in our inference algorithm: while ref.uses Markov Chain Monte Carlo methods to stochastically sample from the posterior over fragment grammars, we instead perform hillclimbing upon the objective in Eq. (13). Each round of hillclimbing proposes new fragments by antiunifying subtrees of phonological rules in, and re-estimates the continuous parameters of the resulting PCFG using the classic Inside–Outside algorithm. When running Inside-Outside we place a symmetric Dirichlet prior over the continuous parameters of the PCFG with pseudocounts equal to 1.

Reporting summary

Further information on research design is available in the Nature Research Reporting Summary linked to this article.

---

### Evidence for the utility of quantum computing before fault tolerance [^1165BAQ2]. Nature (2023). Excellent credibility.

Quantum computing promises to offer substantial speed-ups over its classical counterpart for certain problems. However, the greatest impediment to realizing its full potential is noise that is inherent to these systems. The widely accepted solution to this challenge is the implementation of fault-tolerant quantum circuits, which is out of reach for current processors. Here we report experiments on a noisy 127-qubit processor and demonstrate the measurement of accurate expectation values for circuit volumes at a scale beyond brute-force classical computation. We argue that this represents evidence for the utility of quantum computing in a pre-fault-tolerant era. These experimental results are enabled by advances in the coherence and calibration of a superconducting processor at this scale and the ability to characterize 1 and controllably manipulate noise across such a large device. We establish the accuracy of the measured expectation values by comparing them with the output of exactly verifiable circuits. In the regime of strong entanglement, the quantum computer provides correct results for which leading classical approximations such as pure-state-based 1D (matrix product states, MPS) and 2D (isometric tensor network states, isoTNS) tensor network methods 2,3 break down. These experiments demonstrate a foundational tool for the realization of near-term quantum applications 4,5.

---

### 2014 ESC guidelines on diagnosis and management of hypertrophic cardiomyopathy: the task force for the diagnosis and management of hypertrophic cardiomyopathy of the European Society of Cardiology (ESC) [^111SPgTp]. European Heart Journal (2014). Medium credibility.

Regarding diagnostic investigations for syncope, more specifically with respect to initial ECG, ESC 2014 guidelines recommend to obtain a 12-lead ECG to identify the cause of symptoms in patients with unexplained syncope.

---

### Evaluation of suspected dementia [^111bckie]. American Family Physician (2018). Medium credibility.

Regarding diagnostic investigations for dementia, more specifically with respect to initial assessment, AAFP 2018 guidelines recommend to use the Mini-Cog, the General Practitioner Assessment of Cognition, or the Ascertain Dementia 8-Item Informant Questionnaire to determine the need for further evaluation in patients with suspected dementia.

---

### Single photon emission computed tomography (SPECT) myocardial perfusion imaging guidelines: instrumentation, acquisition, processing, and interpretation [^113QyHCw]. Journal of Nuclear Cardiology (2018). Medium credibility.

ASNC SPECT myocardial perfusion imaging — Gated acquisition parameters for the two-study protocol list Frames/cycle values of 8 and 16 (with 16 labeled Preferred) and R-to-R window of 20–100% for both the first and second studies; 20% is recommended, if extra-frame is provided for rejected counts.

---

### A variational eigenvalue solver on a photonic quantum processor [^113Rr7by]. Nature Communications (2014). Medium credibility.

Quantum computers promise to efficiently solve important problems that are intractable on a conventional computer. For quantum systems, where the physical dimension grows exponentially, finding the eigenvalues of certain operators is one such intractable problem and remains a fundamental challenge. The quantum phase estimation algorithm efficiently finds the eigenvalue of a given eigenvector but requires fully coherent evolution. Here we present an alternative approach that greatly reduces the requirements for coherent evolution and combine this method with a new approach to state preparation based on ansätze and classical optimization. We implement the algorithm by combining a highly reconfigurable photonic quantum processor with a conventional computer. We experimentally demonstrate the feasibility of this approach with an example from quantum chemistry — calculating the ground-state molecular energy for He-H(+). The proposed approach drastically reduces the coherence time requirements, enhancing the potential of quantum resources available today and in the near future.

---

### Application of 3D MAPs pipeline identifies the morphological sequence chondrocytes undergo and the regulatory role of GDF5 in this process [^113uHUnV]. Nature Communications (2021). High credibility.

Nucleus segmentation

To automatically segment fluorescently labeled nuclei in the 3D images, we performed a two-step procedure. In the first step, seed points that were roughly located in the center of the nuclei were detected using a Laplacian-of-Gaussian-based (LoG) approach as described in. In brief, the 3D input images were filtered with a LoG filter using a standard deviation that was empirically tuned to the radius of the objects of interest. We used standard deviations of σ = 12 for RZ, PZ and PHZ nuclei and of σ = 35 for HZ nuclei. Subsequently, local intensity maxima were extracted from the LoG-filtered image and reported as potential nuclei centers. For each potential seed point, we compute the mean intensity in a 4 x 4 x 4 voxel-wide cube surrounding the centroid. In order to minimize the number of false-positive detections, only seed points with a mean intensity larger than the global mean intensity of the entire LoG-filtered image were kept for further processing. In a final step, we used a seeded watershed algorithm to perform the segmentation of the nuclei in a Gaussian-smoothed version of the intensity-inverted raw input image. The detected seed points were used to initialize the seeded watershed algorithm and we artificially added a background seed located at the border of each image snippet to separate the centered nucleus from the surrounding background.

The segmentation was performed separately for each nucleus and in parallel, i.e. small 3D image patches surrounding the seed points were processed concurrently using multiple cores of the CPU. Segmentation results of the individually processed patches were then combined to form a full-resolution segmentation image containing the final result with a unique integer label for each of the nuclei that was used for further quantification and morphological analyses. All image analysis pipelines were implemented using the open-source software tool XPIWITand executed on a Windows Server 2012 R2 64-bit workstation with 2 Intel(R) Xeon(R)CPU E5-2690 v3 processors, 256 GB RAM, 24 cores and 48 logical processors (S3 data).

---

### Splenomegaly: diagnosis and management in adults [^117UEDvA]. American Family Physician (2021). High credibility.

Regarding diagnostic investigations for splenomegaly, more specifically with respect to initial assessment, AAFP 2021 guidelines recommend to elicit a history of liver disease and alcohol use in patients presenting with splenomegaly in the US.

---

### Quality metrics for single-photon emission computed tomography myocardial perfusion imaging: an ASNC information statement [^112NMsCx]. Journal of Nuclear Cardiology (2023). High credibility.

F5. Iterative reconstruction — quality metric for single-photon emission computed tomography (SPECT) myocardial perfusion imaging (MPI) defines the percentage of SPECT MPI studies that use iterative reconstruction. The numerator is the number of SPECT MPI studies that are processed with iterative reconstruction and the denominator is the number of SPECT MPI studies examined. The reporting period is 100 consecutive stress SPECT MPI studies performed at the site, the measure data source is archived scans of submitted studies, and measure exclusions/exceptions are none. Standards are "Standard of good quality: ≥ 50%" and "Standard of excellent quality: ≥ 90%". Background states that iterative image reconstruction with resolution recovery and scatter correction improves count statistics and suppresses noise, and the rationale for inclusion notes "achieving a median effective dose of ≤ 9 mSv should be achieved in 50% of studies" and that advanced reconstruction software decreases radiation exposure.