# Medical Question & Answer

**Sample ID**: d84093c5-4e7a-0922-176f-274a3a027ca0
**Dataset Index**: 124270

---

## Question

6. What is the maximum number of natural numbers not exceeding 2016 that can be marked so that the product of any two marked numbers is a perfect square?

---

## Answer

> Let's see… What do we have here? The user is asking for the maximum number of natural numbers not exceeding 2016 that can be marked so that the product of any two marked numbers is a perfect square. Let's break this down step-by-step. First, I need to think about the algebraic condition that makes the product of two numbers a perfect square. Then, I should classify numbers by their square-free parts and determine how many square-free parts appear up to 2016. Next, I will count how many numbers share each square-free part and identify the largest group. Finally, I will verify edge cases and confirm the maximum size, making sure I haven't overlooked any constraints or exceptions.

> Let me first confirm the core condition: the product of two natural numbers is a perfect square if and only if, in their prime factorizations, every prime appears with an even total exponent. This means that for any two marked numbers a and b, the product ab is a perfect square precisely when the square-free parts of a and b are identical, because the square parts already have even exponents and the square-free parts must cancel to even exponents in the product [^notfound].

> Wait, let me verify the implication: if all marked numbers share the same square-free part, then the product of any two will have even exponents for all primes, hence be a perfect square. Conversely, if two marked numbers have different square-free parts, their product will have at least one prime with an odd total exponent, so it cannot be a perfect square. Therefore, the largest valid set is the largest collection of numbers ≤ 2016 that share the same square-free part [^notfound].

> I need to check the square-free parts that actually occur up to 2016. The square-free part of a number is what remains after factoring out the largest perfect square divisor; for example, 18 = 2 × 3² has square-free part 2, and 50 = 2 × 5² has square-free part 2. The square-free parts themselves are products of distinct primes, and they must be ≤ 2016 because the square-free part of a number n is always ≤ n [^notfound].

> Hold on, I should verify the counting strategy: for each square-free k ≤ 2016, the numbers ≤ 2016 with square-free part k are exactly the numbers of the form k × m² where k × m² ≤ 2016. So, for a given k, the count is the number of integers m such that m² ≤ 2016/k, i.e., m ≤ floor(√(2016/k)). The largest possible set will correspond to the k that maximizes this count [^notfound].

> Let me consider which k maximizes floor(√(2016/k)). Intuitively, the count decreases as k increases, so the maximum should occur at the smallest square-free k, which is 1. For k = 1, the count is floor(√2016) = 44, since 44² = 1936 ≤ 2016 and 45² = 2025 > 2016. For k = 2, the count is floor(√(2016/2)) = floor(√1008) ≈ 31.74, so 31. For k = 3, it is floor(√(2016/3)) = floor(√672) ≈ 25.92, so 25. The trend continues downward, confirming that k = 1 yields the largest group [^notfound].

> I should double-check that k = 1 is indeed square-free and valid. Yes, 1 is square-free by definition, and the numbers with square-free part 1 are precisely the perfect squares ≤ 2016. There are 44 such perfect squares: 1², 2²,…, 44², and the product of any two perfect squares is again a perfect square, so this set satisfies the condition [^notfound].

> But wait, what if I tried to mix different square-free parts in a clever way? Could that yield a larger set? Let me reconsider: if I include numbers with different square-free parts, say k1 and k2, then the product of a number with square-free part k1 and a number with square-free part k2 will have square-free part k1k2, which is not 1 and therefore not a perfect square unless k1 = k2. Hence, mixing square-free parts violates the pairwise product condition, so the maximum must still be the largest single square-free-part group, which is 44 [^notfound].

> Final answer: The maximum number of natural numbers not exceeding 2016 that can be marked so that the product of any two marked numbers is a perfect square is 44, achieved by marking all perfect squares up to 2016, i.e., {1, 4, 9,…, 1936} [^notfound].

---

The maximum number of natural numbers not exceeding 2016 that can be marked such that the product of any two marked numbers is a perfect square is **44**. This is achieved by selecting all perfect squares up to 2016, i.e. ({1², 2², …, 44²}), because the product of any two perfect squares is itself a perfect square. No larger set is possible, as any non-square would introduce a non-square product with at least one other element.

---

## Mathematical reasoning

### Perfect squares property

Perfect squares have the form n², where (n) is a natural number. The product of two perfect squares is also a perfect square:

a² × b² = (a × b)²

Thus, any set of perfect squares satisfies the condition that the product of any two elements is a perfect square.

---

### Identifying perfect squares up to 2016

We need to find the largest integer (n) such that n² ≤ 2016:

n ≤ √2016 ≈ 44.9

Thus, the largest integer is 44, and the perfect squares up to 2016 are:

{1², 2², 3², …, 44²}

This set contains **44 elements**.

---

### Verification of the condition

For any two elements (a²) and (b²) in this set, their product is:

a² × b² = (a × b)²

which is a perfect square. Therefore, the condition is satisfied.

---

### Why no larger set is possible

Suppose we add a non-square number (k) to the set. Then there exists at least one perfect square (m²) in the set such that the product (k × m²) is not a perfect square. This is because (k) introduces a non-square factor that cannot be canceled by the square factor (m²). Therefore, the maximum number of elements is achieved by selecting only perfect squares.

---

## Conclusion

The maximum number of natural numbers not exceeding 2016 that can be marked such that the product of any two marked numbers is a perfect square is **44**, corresponding to the perfect squares ({1², 2², …, 44²}).

---

## References

### Current sample size conventions: flaws, harms, and alternatives [^f34a2bb9]. BMC Medicine (2010). Low credibility.

Value of information methods

Many methods have already been described in the statistical literature for choosing the sample size that maximizes the expected value of the information produced minus the total cost of the study. See for an early discussion, for recent examples, and the introduction of for additional references. These require projecting both value and cost at various different sample sizes, including quantifying cost and value on the same scale (note, however, that this could be avoided by instead maximizing value divided by total cost). They also require formally specifying uncertainty about the state of nature; although this can be criticized as being subjective, it improves vastly on the usual conventional approach of assuming that one particular guess is accurate. These methods can require considerable effort and technical expertise, but they can also produce the sort of thorough and directly meaningful assessment that should be required to justify studies that are very expensive or that put many people at risk.

Simple choices based on cost or feasibility

Recent work has justified two simple choices that are based only on costs, with no need to quantify projected value or current uncertainty about the topic being studied. Because costs can generally be more accurately projected than the inputs for conventional calculations, this avoids the inherent inaccuracy that besets the conventional approach. One choice, called n min, is the sample size that minimizes the total cost per subject studied. This is guaranteed to be more cost-efficient (produce a better ratio of projected value to cost) than any larger sample size. It therefore cannot be validly criticized as inadequate. The other, called n root, is the sample size that minimizes the total cost divided by the square root of sample size. This is smaller than n min and is most justifiable for innovative studies where very little is already known about the issue to be studied, in which case it is also guaranteed to be more cost efficient than any larger sample size. An interactive spreadsheet that facilitates identification of n min and n root is provided as Additional file 2.

A common pragmatic strategy is to use the maximum sample size that is reasonably feasible. When sample size is constrained by cost barriers, such as exhausting the pool of the most easily studied subjects, this strategy may closely approximate use of n min and therefore share its justification. When constraints imposed by funders determine feasibility, doing the maximum possible within those constraints is a sensible choice.

---

### Abstract representations emerge naturally in neural networks trained to perform multiple tasks [^472ba8eb]. Nature Communications (2023). High credibility.

Next, we quantify how the level of abstraction developed in the representation layer depends on the number of classification tasks used to train the model (Fig. 3 c). For each number of classification tasks, we train 10 multi-tasking models to characterize how the metrics depend on random initial conditions. As the number of classification tasks P exceeds the number of latent variables D, both the classification and regression generalization metrics saturate to near their maximum possible values (classifier generalization metric: exceeds 90% correct with 8 tasks; regression generalization metric: exceeds r 2 = 0.8 with 9 tasks; Fig. 3 c, right of the gray line). Saturation of the classifier generalization metric indicates that the broad organization of the latent variables is perfectly preserved; while saturation of the regression generalization metric indicates that even the magnitude information that the multi-tasking model did not receive supervised information about is preserved and represented in a fully abstract format. Importantly, both the training and testing set split and the classification boundary for the classifier generalization metric are randomly selected — they are not the same as classification tasks used in training.

The multi-tasking model also reduces the number of samples required to both learn and generalize novel tasks. For a multi-tasking model trained to perform P = 10 tasks with D = 5 latent variables, we show how the performance of a novel classification task depends on the number of samples. We compare this performance to a lower bound (Fig. 3 f, dark gray), from when the task is learned from the standard input representation; as well as an upper bound (Fig. 3 f, light gray), from when the task is learned directly from the latent variables. The performance of the multi-tasking model nearly saturates this upper bound (Fig. 3 f, left). Next, we perform a similar novel task analysis, but where the novel task is learned from one half of the stimulus space and is tested in the other half – just like our generalization analysis above (and schematized in Fig. 1 c, top). We compare the same lower and upper bound as before and show that, again, the multi-tasking model representation nearly saturates the upper bound (Fig. 3 d, right). Thus, not only does the multi-tasking model produce representations with good generalization properties, it also produces representations that lend themselves to the rapid (i.e. few sample) learning of novel tasks.

---

### Universal and uniquely human factors in spontaneous number perception [^5cea3268]. Nature Communications (2017). Medium credibility.

Some researchers have suggested that a combination of non-numeric dimensions is used to quantify sets of objects, instead of representing number directly. To test this we implemented a regression analysis, which included the combination of variables suggested (convex hull, density, cumulative surface area and average diameter) and also included number (equation (12)) on our results from Experiment 1. Again group was entered as a random effects term. If number is represented via these alternative dimensions then we should find no significant effect of number when this collection of variables is included in the model. This is not what we found. Instead, number was a significant predictor over and above all other measures (β number = 1.05, P < 0.001, n = 4,785). This persistent effect of number even when other variables are controlled shows that the effect of number cannot be accounted for by alternative variables. Additionally, the effect of number was significantly greater than any of the variables tested (z -tests: Number versus Convex Hull: z = 7.39, P < 0.001, n = 4,785; Number versus Density: z = 2.79, P < 0.01, n = 4,785; Number versus Area: z = 2.15, P < 0.05, n = 4,785; Number versus Dot Diameter: z = 5.51, P < 0.001, n = 4,785). The results suggest that numerical information is perceived directly, rather than indirectly through the representation of other quantitative dimensions.

---

### Quantum theory based on real numbers can be experimentally falsified [^f7cef749]. Nature (2021). Excellent credibility.

Although complex numbers are essential in mathematics, they are not needed to describe physical experiments, as those are expressed in terms of probabilities, hence real numbers. Physics, however, aims to explain, rather than describe, experiments through theories. Although most theories of physics are based on real numbers, quantum theory was the first to be formulated in terms of operators acting on complex Hilbert spaces 1,2. This has puzzled countless physicists, including the fathers of the theory, for whom a real version of quantum theory, in terms of real operators, seemed much more natural 3. In fact, previous studies have shown that such a 'real quantum theory' can reproduce the outcomes of any multipartite experiment, as long as the parts share arbitrary real quantum states 4. Here we investigate whether complex numbers are actually needed in the quantum formalism. We show this to be case by proving that real and complex Hilbert-space formulations of quantum theory make different predictions in network scenarios comprising independent states and measurements. This allows us to devise a Bell-like experiment, the successful realization of which would disprove real quantum theory, in the same way as standard Bell experiments disproved local physics.

---

### Number of samples-hypothesis testing [^69e3a758]. Nuclear Medicine and Biology (2008). Low credibility.

This note reviews "further discussions on choosing the number of animals for an experiment". Particular emphasis will be given to the table therein.

---

### What is Ne, anyway? [^8a832536]. The Journal of Heredity (2022). Medium credibility.

Few doubt that effective population size (Ne) is one of the most important parameters in evolutionary biology, but how many can say they really understand the concept? Ne is the evolutionary analog of the number of individuals (or adults) in the population, N. Whereas ecological consequences of population size depend on N, evolutionary consequences (rates of loss of genetic diversity and increase in inbreeding; relative effectiveness of selection) depend on Ne. Formal definitions typically relate effective size to a key population genetic parameter, such as loss of heterozygosity or variance in allele frequency. However, for practical application to real populations, it is more useful to define Ne in terms of 3 demographic parameters: number of potential parents (adult N), and mean and variance in offspring number. Defined this way, Ne determines the rate of random genetic drift across the entire genome in the offspring generation. Other evolutionary forces (mutation, migration, selection)-together with factors such as variation in recombination rate-can also affect genetic variation, and this leads to heterogeneity across the genome in observed rates of genetic change. For some, it has been convenient to interpret this heterogeneity in terms of heterogeneity in Ne, but unfortunately, this has muddled the concepts of genetic drift and effective population size. A commonly repeated misconception is that Ne is the number of parents that actually contribute genes to the next generation (NP). In reality, NP can be smaller or larger than Ne, and the NP/Ne ratio depends on the sex ratio, the mean and variance in offspring number, and whether inbreeding or variance Ne is of interest.

---

### The snm procedure guideline for general imaging 6.0 [^638aaec4]. SNMMI (2010). Medium credibility.

SNM Procedure Guideline for General Imaging — matrix size and pixel depth in nuclear medicine acquisitions notes that matrix size is almost always a power of 2 with typical values 64 x 64, 128 x 128, 256 x 256 and 512 x 512, and that non-square matrix sizes also exist for whole-body studies. Each pixel can be represented with a single byte (pixel values ranging from 0 to 255 counts) or with 16 bit words (pixel values ranging up to a maximum of 32k or 64k). Overflow occurs when the number of counts recorded at some given position exceeds the maximum number of counts, and overflow is more likely to occur when using a byte matrix.

---

### Universal and uniquely human factors in spontaneous number perception [^7b54d8ff]. Nature Communications (2017). Medium credibility.

Once subjects were trained to accurately categorize the training stimuli, a small percentage of 'probe' stimuli were added in with the presentation of 'standard' training stimuli. On these probe trials, number and cumulative area were systematically uncorrelated so that responses to these stimuli would show which dimension subjects spontaneously used (Fig. 2). On probe trials, all responses were treated as correct and positively reinforced. This procedure allowed us to precisely measure the contribution of each dimension to subjects' underlying categorization of the quantities. If number was the primary basis of subjects' categorization judgments during training, then as number of dots increases, the percentage of trials where subjects select the 'more' category should also increase, regardless of the cumulative area of the dots. In contrast if surface area was the primary basis of categorization then subjects' choice of the 'more' category will increase with the cumulative surface area values of the sets. If both dimensions were used relatively equally, subjects' category choices would be modulated to a similar degree by the number and cumulative surface area values. We found that all groups had a bias to use the numerical dimension of the stimuli regardless of age, culture or species. The number bias was greatly enhanced in humans compared to non-human primates and correlated with age and math education in humans.

---

### Why is a small sample size not enough? [^1f4c7fd4]. The Oncologist (2024). Medium credibility.

Background

Clinical studies are often limited by resources available, which results in constraints on sample size. We use simulated data to illustrate study implications when the sample size is too small.

Methods and Results

Using 2 theoretical populations each with N = 1000, we randomly sample 10 from each population and conduct a statistical comparison, to help make a conclusion about whether the 2 populations are different. This exercise is repeated for a total of 4 studies: 2 concluded that the 2 populations are statistically significantly different, while 2 showed no statistically significant difference.

Conclusions

Our simulated examples demonstrate that sample sizes play important roles in clinical research. The results and conclusions, in terms of estimates of means, medians, Pearson correlations, chi-square test, and P values, are unreliable with small samples.

---

### Response of polygenic traits under stabilizing selection and mutation when loci have unequal effects [^82a8ea8f]. G3 (2015). Low credibility.

Dynamics when most effects are small

As the distribution of quantitative trait loci measured in experimental and natural populations find most effects to be small, it is important to study this situation in detail. Here we have obtained analytical expressions for the dynamics by assuming the genetic variance to be constant. Although the fact that the variance does not change much in time when most effects are small was observed numerically in, an explanation of this behavior was not provided. Here, as explained in Appendix A, it is a good approximation to assume the variance to be time-independent provided the product of the initial values of the mean deviation and skewness is small.

In the absence of mutations, have considered the effect of background with a time-independent genetic variance on the frequency at a single focal locus. Their results match the ones obtained here using the short-term dynamics model with directional selection only; in particular, (11) and (12) match the results (21) and (25) of, respectively, on identifying their parametersand a withand γ from this study.

Our basic result concerning the dynamics of the phenotypic mean is that it relaxes over a time scale that is inversely proportional to the initial variance. Because the variance is of order ℓ, we thus have the important result that the mean approaches the optimum faster if a larger number of loci is involved. Moreover, this time depends nonmonotonically on the shape parameter of the gamma distribution. Note that the phenotypic mean deviation relaxes to zero when the phenotypic optimum is far below the upper boundon the phenotypic mean. However, when the phenotypic optimum exceeds the maximum typical value of the mean, such that the mean deviation remains substantially different from zero at late times, (10b) shows that all higher cumulants vanish at the end of the phase of directional selection.

---

### Number needed to treat (or harm) [^d0b683ae]. World Journal of Surgery (2005). Low credibility.

The effect of a treatment versus controls may be expressed in relative or absolute terms. For rational decision-making, absolute measures are more meaningful. The number needed to treat, the reciprocal of the absolute risk reduction, is a powerful estimate of the effect of a treatment. It is particularly useful because it takes into account the underlying risk (what would happen without the intervention?). The number needed to treat tells us not only whether a treatment works but how well it works. Thus, it informs health care professionals about the effort needed to achieve a particular outcome. A number needed to treat should be accompanied by information about the experimental intervention, the control intervention against which the experimental intervention has been tested, the length of the observation period, the underlying risk of the study population, and an exact definition of the endpoint. A 95% confidence interval around the point estimate should be calculated. An isolated number needed to treat is rarely appropriate to summarize the usefulness of an intervention; multiple numbers needed to treat for benefit and harm are more helpful. Absolute risk reduction and number needed to treat should become standard summary estimates in randomized controlled trials.

---

### The cost of large numbers of hypothesis tests on power, effect size and sample size [^4b1c0c24]. Molecular Psychiatry (2012). Low credibility.

Results

For a fixed targeted effect size and fixed sample size, power decreases as the number of tests and corresponding critical value increase (Table 1, Figure 2a). If the power for a single test is 80%, the power is approximately 50% for 10; 10% for 1000; and 1% for 100 000 Bonferroni-adjusted tests. To avoid a drop in nominal power without increasing sample size, an investigator may target larger effect sizes (Table 1, Figure 2b) using equation (1). For one million tests, the effect size multiplier is 2.25 at 80% power and 2.08 at 90% power. Suppose it has been determined that a sample of 100 yields 80% power to detect a difference in group means when the mean in group 1 is 10 and the mean in group 2 is 12. The original sample size of 100 would also yield 80% power to detect a mean difference of 2.25 × 2 = 4.50, with Bonferroni adjustment for one million tests. The effect size multiplier works on the scale of the underlying Normally distributed test statistic. For example, effect size multiplication for an OR should be carried out after conversion to the natural log scale. Suppose 500 total cases and controls provide 90% power to detect an OR = 1.2, the effect size multiplier for 90% power and 1000 tests is 1.65. Since log e (1.2) = 0.18, the same sample of 500 yields 90% power in 1000 tests to detect a log e (OR) = 1.65 × 0.18 = 0.30 or, equivalently, an OR = exp(0.30) = 1.35.

---

### Optimizing natural fertility: a committee opinion [^098c7f49]. Fertility and Sterility (2013). Medium credibility.

Natural fertility — intercourse frequency: Cycle fecundity was similar with intercourse that occurred daily, every other day, and even every 3 days in the fertile window, but was lowest when intercourse occurred only once in the fertile window. Couples should be informed that reproductive efficiency increases with the frequency of intercourse and is highest when intercourse occurs every 1 to 2 days during the fertile window. Intercourse more frequently than every 1 to 2 days is not associated with lower fecundity, and couples should not be advised to limit the frequency of intercourse when trying to achieve pregnancy.

---

### Global potential for natural regeneration in deforested tropical regions [^ff04e9bc]. Nature (2024). Excellent credibility.

Modelling approach

The statistical modelling and validation is based on a set of six million random points generated within the regeneration and non-regeneration spatial domains defined above, stratified so that each level of the dependent variable was equally represented but with no other form of spatial stratification. For each of these random points, we used geographical information systems to extract values for a range of biophysical and socioeconomic variables (see the 'Predictor variables' section). We based the spatial predictions on the model containing biophysical variables only (the rationale is provided in Supplementary Information 1).

We used randomForeststo fit models of natural regeneration occurrence and estimate spatially explicit predictions of the potential for natural regeneration in the near future (2030), assuming that overall conditions from 2000 to 2016 apply to future scenarios. We adopted a variable-selection procedure to improve parsimony and reduce the high computational burden of making predictions at 30-m resolution over large scales. RandomForests involves stochastic fitting processes, so two models fit to the same data may differ. We therefore fit ten models with all biophysical covariates by randomly selecting 500,000 training records (balanced with respect to the levels of the dependent variable) from a pool of six million records. The variables were ordered by the mean decrease in accuracy resulting from their omission, and the overall ordering of variable importance was based on summing the rank position of each variable among each of these ten models. Following a previous study, we then iteratively fit new models adding one additional variable in order of importance each time to identify the point at which the addition of new variables does not improve model accuracy. The final model was fit using only those variables with one million records. Model validation was based on an independent set of random points that was balanced with respect to the dependent variable. A concern with this approach to validation is that it may overestimate model performance, as it does not account for autocorrelation. We evaluated this effect by quantifying the distance between the validation and training locations and assessing model performance in 1-km distance increments from the training locations.

---

### Oversimplifying quantum factoring [^9011a539]. Nature (2013). Excellent credibility.

Shor's quantum factoring algorithm exponentially outperforms known classical methods. Previous experimental implementations have used simplifications dependent on knowing the factors in advance. However, as we show here, all composite numbers admit simplification of the algorithm to a circuit equivalent to flipping coins. The difficulty of a particular experiment therefore depends on the level of simplification chosen, not the size of the number factored. Valid implementations should not make use of the answer sought.

---

### Dynamics of ranking [^58e0eeab]. Nature Communications (2022). High credibility.

Virtually anything can be and is ranked; people, institutions, countries, words, genes. Rankings reduce complex systems to ordered lists, reflecting the ability of their elements to perform relevant functions, and are being used from socioeconomic policy to knowledge extraction. A century of research has found regularities when temporal rank data is aggregated. Far less is known, however, about how rankings change in time. Here we explore the dynamics of 30 rankings in natural, social, economic, and infrastructural systems, comprising millions of elements and timescales from minutes to centuries. We find that the flux of new elements determines the stability of a ranking: for high flux only the top of the list is stable, otherwise top and bottom are equally stable. We show that two basic mechanisms - displacement and replacement of elements - capture empirical ranking dynamics. The model uncovers two regimes of behavior; fast and large rank changes, or slow diffusion. Our results indicate that the balance between robustness and adaptability in ranked systems might be governed by simple random processes irrespective of system details.

---

### Using the number needed to treat in clinical practice [^646ae153]. Archives of Physical Medicine and Rehabilitation (2004). Low credibility.

The number needed to treat (NNT) is gaining attention as a method of reporting the results of clinical trails with dichotomous outcome measures. The NNT is defined as the number of patients who would need to be treated, on average, with a specific intervention to prevent 1 additional bad outcome or to achieve 1 desirable outcome in a given time period. Because it reports outcomes in terms of patient numbers, it is extremely useful to clinicians for making decisions about the effort expended with a particular intervention to achieve a single positive outcome. This special communication describes the NNT statistic and its utility for choosing clinical interventions.

---

### Graphs [^c6c6d5a7]. Chest (2006). Low credibility.

Two rules of good graphs are presented and explicated.

---

### Development and external validation of Indian population-specific garbhini-GA2 model for estimating gestational age in second and third trimesters [^b85120b4]. The Lancet Regional Health: Southeast Asia (2024). High credibility.

Development of population-specific gestational dating model

We employed multiple approaches to establish the relationship between foetal biometric variables and GA. These included Random Forest and Gradient Boosting machine learning methods implemented in R and multivariable polynomial regression developed using an optimisation technique inspired by the process of natural selection and genetics called "Genetic Algorithm" implemented in MATLAB (Supplementary Methods). We decided to implement these methods because they were the most appropriate for achieving our objectives, which involved identifying the most essential population-specific characteristics to determine GA accurately. Furthermore, these methods are well-suited for addressing the expected collinearity in the data, as we were working with variables associated with foetal growth, which were likely to be correlated and also capture if any non-linear relationships existed between these features. Random Forest and Gradient Boosting models were developed after undergoing hyperparameter tuning, which involved finding the optimal hyperparameter settings. This included parameters such as the number of decision trees and the maximum number of features considered for node splitting in the case of Random Forest and parameters like the number of boosting rounds, the maximum tree depth, and others for Gradient Boosting. The developed models were evaluated using the TEST and VALIDATION datasets (Supplementary Methods; Figure S3).

In the genetic algorithm method, we generated several candidate multivariate polynomial equations using the combinations of the USG variables in the TRAINING set, including their natural logarithms and square root forms (Table S3). Due to the stochastic nature of genetic algorithms, an optimal solution was not guaranteed. Therefore, the algorithm was run ten times independently to ensure reproducibility and consistency. We selected those equations that resolved in more than 5 out of 10 trials. These selected equations were evaluated using the TEST and VALIDATION datasets. The best-performing multivariate polynomial equation was named the Garbhini-GA2 formula.

---

### Summary benchmarks-full set – 2024 [^b9555fdb]. AAO (2024). High credibility.

Dry eye syndrome — staging notes — one or more options concurrently within each category can be considered within that step of the dry eye disease state, options within a category are not ranked according to importance and may be equally valid, the evidence available to support options differs and may be lower for newer options so each treatment option should be considered in accordance with the level of evidence available at the time management is instigated, and the severity and etiology of the dry eye disease state will dictate the range and number of options selected from one or more steps.

---

### To portray clonal evolution in blood cancer, count your stem cells [^90574c79]. Blood (2021). Medium credibility.

Clonal evolution, the process of expansion and diversification of mutated cells, plays an important role in cancer development, resistance, and relapse. Although clonal evolution is most often conceived of as driven by natural selection, recent studies uncovered that neutral evolution shapes clonal evolution in a significant proportion of solid cancers. In hematological malignancies, the interplay between neutral evolution and natural selection is also disputed. Because natural selection selects cells with a greater fitness, providing a growth advantage to some cells relative to others, the architecture of clonal evolution serves as indirect evidence to distinguish natural selection from neutral evolution and has been associated with different prognoses for the patient. Linear architecture, when the new mutant clone grows within the previous one, is characteristic of hematological malignancies and is typically interpreted as being driven by natural selection. Here, we discuss the role of natural selection and neutral evolution in the production of linear clonal architectures in hematological malignancies. Although it is tempting to attribute linear evolution to natural selection, we argue that a lower number of contributing stem cells accompanied by genetic drift can also result in a linear pattern of evolution, as illustrated by simulations of clonal evolution in hematopoietic stem cells. The number of stem cells contributing to long-term clonal evolution is not known in the pathological context, and we advocate that estimating these numbers in the context of cancer and aging is crucial to parsing out neutral evolution from natural selection, 2 processes that require different therapeutic strategies.

---

### Number needed to treat and number needed to harm are not the best way to report and assess the results of randomised clinical trials [^f243221a]. British Journal of Haematology (2009). Low credibility.

The inverse of the difference between rates, called the 'number needed to treat' (NNT), was suggested 20 years ago as a good way to present the results of comparisons of success or failure under different therapies. Such comparisons usually arise in randomised controlled trials and meta-analysis. This article reviews the claims made about this statistic, and the problems associated with it. Methods that have been proposed for confidence intervals are evaluated, and shown to be erroneous. We suggest that giving the baseline risk, and the difference in success or event rates, the 'absolute risk reduction', is preferable to the number needed to treat, for both theoretical and practical reasons.

---

### The role of evolution in the emergence of infectious diseases [^f965afb1]. Nature (2003). Excellent credibility.

It is unclear when, where and how novel pathogens such as human immunodeficiency virus (HIV), monkeypox and severe acute respiratory syndrome (SARS) will cross the barriers that separate their natural reservoirs from human populations and ignite the epidemic spread of novel infectious diseases. New pathogens are believed to emerge from animal reservoirs when ecological changes increase the pathogen's opportunities to enter the human population and to generate subsequent human-to-human transmission. Effective human-to-human transmission requires that the pathogen's basic reproductive number, R(0), should exceed one, where R(0) is the average number of secondary infections arising from one infected individual in a completely susceptible population. However, an increase in R(0), even when insufficient to generate an epidemic, nonetheless increases the number of subsequently infected individuals. Here we show that, as a consequence of this, the probability of pathogen evolution to R(0) > 1 and subsequent disease emergence can increase markedly.

---

### Natural genetic variation underlying tiller development in barley (L) [^89bfeb2e]. G3 (2020). Medium credibility.

Figure 3
Minor trade-offs between tiller number and other traits in 2015. A) Percent variance associated with predictor variables in multiple linear regression models of maximum tiller number in 2-rows and 6-rows in 2015. White numbers on bars represent percent variance associated with individual predictor variables (< 2% if number is not shown). Traits shaded in red and blue are positively and negatively associated with tiller number, respectively. Numbers beside bars are total percent variance (R 2) represented by the entire model. B) (Left) Representative density plot illustrating assignment of 2-row and 6-row lines into percentile groups based on maximum and productive tiller number. (Right) Comparison of tiller number in percentile groups based on maximum and productive tiller number. C) Box plots showing traits that were significantly different between percentile groups based on maximum and productive tiller number. D) Box plots of percent productive tillers (tillers that survive and form grain-bearing spikes). Diamonds on box plots represent mean trait values, and letters indicate whether groups were significantly different (Tukey Test, FDR-adjusted p-value, 0.01).

---

### Global potential for natural regeneration in deforested tropical regions [^e3a6982b]. Nature (2024). Excellent credibility.

Tropical natural regeneration potential

We build on the methods used for a recently published pantropical remote sensing analysisthat identified 31.6 ± 11.9 Mha of natural regrowth in 4.78 million patches globally between 2000 and 2012 that persisted to 2016 (hereafter, 2000–2016), with an average patch area of 1.2 ha, to assess the scale and locations where natural regeneration has potential in tropical forest biomes (± 25° latitude). The analysisdistinguished areas of natural regrowth from plantations on the basis of extensive training data and ground-truth information; patches of natural regrowth were defined as at least 0.45 ha in area with vegetation taller than 5 m in height.

Based on a sample of 5.4 Mha of natural regrowth detected previously, we used machine learning methods to distinguish areas in which natural regeneration did or did not occur across the global tropics as a function of a suite of geospatial, biophysical and socioeconomic variables. To predict potential tropical forest regeneration across Earth, we selected predictor variables spanning local (site) and landscape scales (from 30 m to country level) that are known to influence the potential for tropical forest regrowth. These biophysical and socioeconomic variables include distance to nearest tree cover; local forest density in a 1 km 2 area; land cover; 12 soil metrics reflecting conditions in the top 30 cm of soil; 19 bioclimatic variables reduced to 5 principal component axes; slope; net primary productivity; monthly average of burned area due to wildfires over the period of 2001–2017; distance to water; population density; gross domestic product; human development index; road density; distance to urban areas; and protected area status (Supplementary Table 2). Spatially explicit values of predictor variables were then used to model the relative potential for natural regeneration in the present (2015) and in the near future (2030), assuming that overall conditions from 2000 to 2016 apply to future scenarios (Fig. 1). For reporting purposes, we translate the resulting continuous potential for natural regeneration value (0–1) to area-based values by multiplying the relative potential for natural regeneration by the area of each pixel, resulting in a weighted-area value. We provide a sensitivity analysis to this calculation where area is regarded as cells that have greater than 50% potential (Supplementary Table 4). The continuous potential for natural regeneration can be interpreted as the probability of natural regeneration per pixel; the weighted-area value is therefore the expected area of natural regeneration per pixel.

---

### Optimizing natural fertility: a committee opinion [^894bdac0]. Fertility and Sterility (2013). Medium credibility.

Age-related fertility — summary statement: The chances of pregnancy and live birth for women significantly decrease after age 35, as the risks of aneuploidy and miscarriage increase.

---

### 2018 ACC / HRS / NASCI / SCAI / SCCT expert consensus document on optimal use of ionizing radiation in cardiovascular imaging: best practices for safety and effectiveness [^a428588c]. Catheterization and Cardiovascular Interventions (2018). Medium credibility.

Nuclear cardiology image quality and counts — signal-to-noise relationships and acquisition parameters are characterized by noise being approximately the square root of number of counts, so with 100 counts noise would be about 10 and signal to noise ratio 10, whereas with 10,000 counts the signal to noise ratio would be 100. Three variables affect the number of registered counts — the imaging system's sensitivity, the amount of radioactivity administered to the subject, and the length of time that counts are acquired — and a given scan is typically conducted until a requisite number of counts has been collected; if the radiopharmaceutical dose is small a longer scan acquisition time will be required, but patients cannot lie still indefinitely, placing practical limits on acquisition duration.

---

### Natural genetic variation underlying tiller development in barley (L) [^1b2c8eea]. G3 (2020). Medium credibility.

We considered the possibility that larger trade-offs or trade-offs that were not indicated by correlations or MLR modeling could be identified by comparing traits in lines with extremely different tillering capacities. Therefore, 2-rows and 6-rows were split into 10 th and 90 th percentile groups based on maximum and productive tiller number (Figure 3B). Despite at least 2.5-fold or higher change in average tiller number between percentile groups (Figure 3B), very few traits were significantly different between percentile groups. Stem diameter was lower in high tillering 6-rows (90 th percentile, maximum and productive) than low tillering 6-rows (10 th percentile, maximum and productive) but was not significantly different between high and low tillering 2-rows (Figure 3C). Fifty kernel weight was also lower and lodging severity increased in high tillering 6-rows (90 th percentile, maximum) than low tillering 6-rows (10 th percentile, maximum), but they were not significantly different between high and low tillering 2-rows (Figure 3C). Interestingly, the trend in percent productive tillers between percentiles based on maximum tiller number was reversed in percentiles based on productive tiller number (Figure 3D). This suggests that tiller survival had a major impact on final productive tiller number in 2015 and that variation in tiller survival may alleviate trade-offs between tiller number and other traits. Overall, our results suggest that trade-offs between tiller number and other traits were very minor and were slightly more pronounced in 6-rows than 2-rows, but, in general, there were no major trade-offs between tiller number and other traits independent of spike row-type.

Results from our study and others show that tiller number is consistently lower in 6-rows than 2-rows, possibly due to a trade-off with seeds per spike, which is inherently higher in 6-rows (Figure S2B). However, there was no evidence from our study that more seeds per spike within 2-row or 6-row groups was associated with lower tiller number. Overall, results from this study indicated that trade-offs between tiller number and seeds per spike probably only exist if the difference in seeds per spike is very large, as it is between 2-rows and 6-rows. Similarly, other studies reported no or low association between tiller number and yield-related traits.

---

### Crisis + emergency risk communication. 2012 edition [^45b9a99d]. CDC (2012). Medium credibility.

Epidemiology terms — Population-based is defined as "Pertaining to a general population defined by geopolitical boundaries" and Prevalence as "The number of instances of a given disease or other condition in a given population at a designated time". Prevention states "The goals of public health and medicine are to promote health, to preserve health, to restore health when it is impaired, and to minimize suffering and distress". Primary case is "The individual who introduces the disease into the family or group under study", and Quantitative data are "Data in numerical quantities, such as continuous measurements or counts". Quarantine is "The limitation of freedom of movement of well persons or animals exposed to a communicable disease, for a period of time not longer than the longest usual incubation period of the disease". Random "Occurs by chance", and Rate is a "Ratio whose essential characteristic is that time is an element of the denominator and in which there is a distinct relationship between numerator and denominator". Relative risk is "The ratio of the risk of disease or death among the exposed to the risk among the unexposed", and Reservoir is "The natural habitat of the infectious agent". Risk is "A probability that an event will occur", and a Risk factor is "An attribute of exposure that is associated with an increased probability of a specified outcome, such as the occurrence of a disease". Sample is "A selected subset of a population", and Screening is "The use of tests or examinations to identify unrecognized disease". Seroepidemiology is "Epidemiologic study or activity based on the detection on serological testing of characteristic change in the serum level of specific antibodies", and Statistical significance indicates that "Statistical methods allow an estimate to be made of the probability for the observed or greater degree of association between independent and dependent variables under the null hypothesis".

---

### The impact of reproductive factors on the metabolic profile of females from menarche to menopause [^7819a2ce]. Nature Communications (2024). High credibility.

Parity

Genetic instruments were selected from a GWAS of number of children ever born, as a proxy of parity, which included 785,604 men and women of European ancestry from 45 studies (Supplementary data 14). Number of children ever born was treated as a continuous measure and included both parous and nulliparous women. Linear regression models were used to estimate the association between genetic variants and number of children ever born adjusting for principal components of ancestry, birth year, its square and cubic, to control for non-linear birth cohort effects. Family-based studies controlled for family structure or excluded relatives. The sex-combined metanalysis also included interactions of birth year and its polynomials with sex. For our analyses, we used the 32 independent SNPs reported by the GWAS to be strongly associated with number of children ever born (P -value < 5*10–8) in either the sex-combined (28 SNPs) or female-specific (4 SNPs) metanalyses and summary association data from the female-specific metanalyses. The GWAS included UK Biobank (maximum estimated sample overlap: 14%).

Age at natural menopause

Genetic instruments were selected from a GWAS of age at natural menopause conducted in 201,323 women of European ancestry (Supplementary data 14). Linear regression models were used to estimate the association between genetic variants and age at natural menopause (in years). For our analyses, we selected 290 SNPs reported by the GWAS to be strongly associated with age at natural menopause (P -value < 5*10–8). Where available, we used association data from the sample combining discovery and replication stages (N = 496,151). Given the age at menarche GWAS included UK Biobank participants (maximum estimated sample overlap: 13% considering the GWAS combined discovery and replication samples), we have also selected an additional set of age at natural menopause-associated genetic variants (N = 42 SNPs) using data from a previous GWAS that did not include UK Biobank (details in 'Sensitivity analyses' below and Supplementary data 14).

---

### Model selection: finding the right fit [^6940b900]. The Journal of Foot and Ankle Surgery (2019). Medium credibility.

There are numerous possible goals for building statistical models. Those statistical goals, the associated model types, and each statistical tool involved in model building come with its own assumptions and requirements. In turn, these requirements must be met if we are to ensure that our models produce meaningful, interpretable results. However, beyond these technical details is the intuition, and the additional set of tools and algorithms, used by the statistician, to build the contextually appropriate model: not only must we build an interpretable model, we must build a model that answers the particular question at hand and addresses the particular goal we have in mind. In this column we discuss the methods by which statisticians build models for description, risk factor identification, and prediction.

---

### Playing games with multiple access channels [^fe448e9d]. Nature Communications (2020). High credibility.

Theorem 1 If a nonlocal game G does not admit a perfect strategy (using the available resources), the sum rate R 1 + R 2 of the MAC N G defined in (2) is strictly bounded away from.

Consider now a nonlocal game G that cannot be won with certainty using any classical strategy, and assume there exists a perfect quantum strategy. If we allow shared entanglement between the two senders of the MAC, then Theorem 1 provides a provable separation between the capacity region of the unassisted MAC N G and the entanglement-assisted achievable rate region. A well-known example of such a nonlocal game is the magic square game G MS –, in which Alice and Bob have to fill a given row and column of a 3 × 3-square with a bit string such that the parity of Alice's row is even, the parity of Bob's column is odd, and the assignments are consistent in the overlapping cell. It is well-known that any classical strategy has winning probability at most, as illustrated in the left panel in Fig. 2. For the MACdefined in terms of the magic square game, we can use Theorem 1 to obtain an upper bound on the achievable sum rate of 3.13694, bounding it away from the maximal value of.

---

### Non-linear relationships in clinical research [^945c0776]. Nephrology, Dialysis, Transplantation (2025). Medium credibility.

How to select the best model for modeling the non-linear trajectory of serum albumin over time

As linear regression alone does not fit the trajectory of serum albumin over time, in Fig. 6 we explore several alternatives. (i) Linear regression using time categorized into four intervals to capture changes in trend, (ii) linear regression with time modeled as a cubic polynomial, (iii) linear regression with time modeled as a piecewise linear spline with four segments (knots placed at –3, –2 and –1 years), (iv) linear regression with time modeled as a natural cubic spline with four knots (placed at the 1st, 33rd, 66th and 99th percentiles) and (v) a GAM modeling time as a spline with automated smoothness selection.

The many options available to us raises the question on how to select the best model? Besides visual inspection of the fitted curve over the raw data, as well as the residual plots, there are several "goodness-of-fit" metrics available to help evaluate how well a model captures a relationship. The "root mean squared error" (RMSE) is the square root of the average of the squared residuals, and is a measure for the difference between the true values of the independent variable and the fitted values from the model, with lower values indicating a better model fit. Another commonly used metric to help in the selection of the best model is the Akaike Information Criterion (AIC), with lower values indicating a better fit. This metric helps find a balance between how well a model fits the data and the model's complexity. By including a penalty term for overly complex models (i.e. a model containing many covariates), the AIC helps reduce the risk of overfitting when used to select a model. The R-squared statistic is another metric of model fit, and represents the proportion of the variance in the outcome that can be explained by the independent variables in the model, with higher values indicating a better fit. The adjusted version of the R-square statistic also penalizes model complexity (represented by the number of covariates in the model). The RMSE, adjusted R-squared and the AIC are provided for each method in Fig. 6. Comparing these statistics among models indicates that the GAM may provide the best fit. Visual inspection of the residual plots leads to the same conclusion. If, however, two models were to have comparable performance, the principle of parsimony suggests opting for the simpler, least complex model.

---

### Natural genetic variation underlying tiller development in barley (L) [^a8f7064b]. G3 (2020). Medium credibility.

Results And Discussion

Tiller number in the two- and six-row diversity panel

In 2014 and 2015, 761 lines were grown in the field, and data were collected for weekly and productive tiller number, days to heading, plant height, stem diameter, leaf width (2015 only), seeds per spike, fifty kernel weight, and lodging (2015 only) (Table S3). Fifty-four lines that were grown in 2014 and 2015 were also grown in 2016 in three complete, randomized blocks, and data for weekly and productive tiller number, days to heading, plant height, seeds per spike, and fifty kernel weight were collected (Table S3). Phenotypic data were analyzed in all lines and in subsets of lines based on spike row-type and PPD-H1 alleles. Tiller number data from 2014 and 2015 are summarized in Table 1, and all trait data from all years are summarized in Table S5.

Genetic variance for tiller number was significant (p-value < 0.0001) in 2014, 2015, and 2016 for most time points (Table 1, Table S5). In both years for all line subsets, variance was highest for maximum tiller number and tiller number measured at later time points (5-7WPE), and it decreased for productive tiller number (Table 1). Tiller number at 6WPE, the time point at which maximum tiller number occurred on average for all lines, also had the highest heritability estimate (0.53) of all tiller counts. Decreased heritability from 6WPE to productive tiller number was likely due to variability in tiller survival, which appears to be strongly influenced by environment as genetic variance for percent productive tillers was not significant (Table S5). Heritability estimates for tillering traits were lower than other traits measured (Table S5).

---

### Reconstructing propagation networks with natural diversity and identifying hidden sources [^d752aa57]. Nature Communications (2014). Medium credibility.

Although the number of base strings is relatively small compared with the network size, we need a set of strings at different time with respect to a base string to formulate the mathematical framework for reconstruction. We study how the length of time series affects the accuracy of reconstruction. Figure 3a, b show the success rate as a function of the relative length n t of time series for SIS and CP dynamics on both homogeneous and heterogeneous networks, where n t is the total length of time series from the beginning of the spreading process divided by the network size N. The results demonstrate that even for very small values of n t, most links can already be identified, as reflected by the high values of the success rate shown. Figure 3c, d show the minimum lengthrequired to achieve at least 95% success rate for different network sizes. For both SIS and CP dynamics on different networks, decreases considerably as N is increased. This seemingly counterintuitive result is due to the fact that different base strings can share strings at different times to enable reconstruction. In general, as N is increased, will increase accordingly. However, a particular string can belong to different base strings with respect to the threshold Δ, accounting for the slight increase in the absolute length of the time series (see Supplementary Fig. 4 and Supplementary Note 5) and the reduction in(see Supplementary Note 3 on the method to choose base and subordinate strings). The dependence of the success rate on the average node degree ‹ k › for SIS and CP on different networks has been investigated as well (see Supplementary Fig. 5 and Supplementary Note 5). The results in Figs 2 and 3, Supplementary Figs 2–5 and Table 1 demonstrate the high accuracy and efficiency of our reconstruction method based on small amounts of data.

---

### Natural genetic variation underlying tiller development in barley (L) [^5f1d10c7]. G3 (2020). Medium credibility.

Tiller number was compared using data for the 54 lines (27 2-rows and 27 6-rows) grown in all three years. Due to waterlogging in the field early in development in 2014, the onset of tiller development was delayed and maximum and productive tiller number was much lower than 2015 and 2016 (Figure 1A, B). By 2WPE in 2014, 25.4% of all lines grown had not yet developed at least one tiller per plant on average, whereas all lines grown in 2015 had developed at least one tiller per plant by 2WPE. Maximum tiller number was not significantly different between 2015 and 2016, but productive tiller number was lower in 2016 than 2015 due to lower tiller survival (Figure 1B). Despite differences between years, they all followed a similar trend where average tiller number increased linearly until 5WPE, after which it either slowed or began decreasing (Figure 1A).

Figure 1
Overview of tiller development (tillering) and other traits. A) Progression of average tiller number throughout the growing season for 54 lines grown in 2014, 2015, and 2016. B) Box plots summarizing tillering traits for 54 lines grown in all three years. Diamonds represent mean trait values, and letters indicate whether groups are significantly different based on FDR-adjusted p-values from ANOVA in conjunction with Tukey Test. C) Box plots summarizing non-tillering traits show similar relationship between years as average productive tiller number.

Average plant height, stem diameter (measured in 2014 and 2015 only), seeds-per-spike, and fifty kernel weight followed a similar trend as productive tiller number across the three years, where trait values were highest in 2015 and lowest in 2014 (Figure 1C). In years when plants developed more productive tillers on average, they were also taller with thicker stems, more seeds per spike, and heavier seeds on average (Figure 1C), indicating that productive tiller number is correlated with overall plant fitness.

---

### Optimizing natural fertility: a committee opinion [^63aa3a95]. Fertility and Sterility (2013). Medium credibility.

Clinical pregnancy probability — cycle day and age: The probability of clinical pregnancy increases from 3.2% on cycle day 8 to 9.4% on cycle day 12 and decreases to less than 2% on cycle day 21, and the likelihood of success decreases with increasing age.

---

### 2018 ACC / HRS / NASCI / SCAI / SCCT expert consensus document on optimal use of ionizing radiation in Cardiovascular imaging: best Practices for safety and effectiveness: a report of the American college of cardiology task force on expert consensus decision pathways [^4ae49d9f]. Journal of the American College of Cardiology (2018). Medium credibility.

Nuclear cardiology signal-to-noise and count acquisition — detector dose explains that noise is approximately the square root of number of counts, with illustrative examples that if 100 counts are collected, noise would account for about 10 counts (square root of 100 is 10), and signal to noise ratio would be 10, whereas if 10,000 counts are collected, the signal to noise ratio would be 100. Three variables affect registered counts — the imaging system's sensitivity, the amount of radioactivity administered to the subject (which affects the subject's dose), and the length of time that counts are acquired to form the image — and typically a scan continues until a requisite number of counts has been collected; if the administered dose is small, a longer acquisition time will be required, yet patients cannot lie still indefinitely, limiting practical duration to avoid motion artifacts.

---

### Playing games with multiple access channels [^17a40303]. Nature Communications (2020). High credibility.

Fig. 2
Classical and quantum strategies for the magic square game.

a An optimal classical strategy for the magic square game that allows Alice and Bob to win the game for 8 of the 9 possible questions. Filling the bottom right square consistently with the parity constraints for the rows (even) and columns (odd) is impossible. b A perfect quantum strategy defined by measuring the observables in the cells on two maximally entangled states. Note that the observables along each row and column commute pairwise.

On the other hand, there is a perfect quantum strategy in which Alice and Bob make measurements on two maximally entangled states shared between them. This perfect quantum strategy, depicted in the right panel of Fig. 2, allows Alice and Bob to send individual messages to the receiver at the rates, yielding the maximal sum rate, which is not achievable by any classical coding strategy by Theorem 1. Therefore, the unassisted capacity region ofis separated from the point, while the entanglement-assisted achievable rate region includes this point. This separation between classical strategies and entanglement-assisted strategies occurs for any nonlocal game G with no perfect classical strategies and perfect quantum strategies, a so-called "pseudo-telepathy game". Each game in this class yields a separation for the corresponding MAC between the unassisted capacity region and the entanglement-assisted region via Theorem 1.

---

### Methodology for ACOEM's occupational medicine practice guidelines-2025 revision [^5b3929ab]. ACOEM (2025). High credibility.

Newcastle - Ottawa Quality Assessment Scale — case control studies scoring is defined as follows: A study can be awarded a maximum of one star for each numbered item within the Selection, Exposure, and Outcome categories, with a maximum of two stars for Comparability and a maximum of 9 points overall. Under Selection, items include 1) Is the case definition adequate? with options a) yes, with independent validation; b) yes, e.g. record linkage or based on self reports; c) no description; 2) Representativeness of the cases with options a) consecutive or obviously representative series of cases; b) potential for selection biases or not stated; and 3) Selection of Controls with options a) community controls; b) hospital controls; c) no description.

---

### Natural genetic variation underlying tiller development in barley (L) [^cfd9f701]. G3 (2020). Medium credibility.

Days to heading and spike row type were associated with a high proportion of variance in tiller number models (Figure 2A). Together they were associated with 28% of the total variance in maximum tiller number models and 12% of the total variance in productive tiller number models (Figure 2A). Interestingly, a very small proportion of variance in the productive tiller number model was associated with days to heading (1.9%) (Figure 2A), probably due to variability in tiller survival between lines. Furthermore, average differences in tiller survival represented by percent productive tillers between 2-rows and 6-rows (Figure S2A) could explain why seeds per spike accounted for a larger proportion of variance in productive tiller number than maximum tiller number.

Principal coordinates (PCo) analysis based on tiller number throughout development and productive tiller number also indicated that a large proportion of variance in tiller number was associated with days to heading and spike row-type. Groups based on spike row-type and days to heading were more strongly correlated than any other single trait with PCo1 (R = 0.59, P < 2.2e-16), which accounted for 86% of the total variance in the PCo model (Figure 2B). Furthermore, although 6-rows produced fewer tillers on average than 2-rows, maximum tiller number in late heading 6-rows (> 60 days) was not significantly different from earlier heading 2-rows (< 60 days), indicating that high tiller number can be achieved in late heading 6-rows (Figure 2C).

Trade-offs between tillering and other traits

Tiller number and other traits were compared to evaluate trade-offs associated with high tiller number. Because spike row-type is correlated with tiller number and other traits, trade-offs were assessed separately in 2-row and 6-row subsets and using 2015 data only for the same reasons as previously described. Results of MLR modeling indicated minor trade-offs between tiller number and other vegetative traits. Leaf width and stem diameter were associated with a significant proportion of variance in productive and maximum tiller number MLR models (Figure 3A), and their coefficients were consistently negative, indicating a tendency for leaf width and stem diameter to decrease as tiller number increased. Both traits were also weakly, negatively correlated with productive tiller number (Table 2 and Table S6).

---

### MIRD pamphlet no. 27: MIRDcell V3, a revised software tool for multicellular dosimetry and bioeffect modeling [^95177843]. Journal of Nuclear Medicine (2022). Medium credibility.

FIGURE 4.
Complex radiobiologic parameters.

From the "Multicellular Geometry" tab, the "3-D Cluster" tab is selected and the radius of the cluster is set to 125 μm. The distance between cells is adjusted until the number of cells matches the experimental observations (described in the "Complex Radiobiological Parameters" section). A drug penetration depth of 12 μm is set, and a radial exponential activity distribution is selected from the drop-down labeling-method menu; the exponential factor is set to 0.4. Since the drug penetrates to only a single cell layer (∼12 μm), the selection of the activity distribution has minimal effect on the rest of the cluster. The time-integrated activity coefficient is set to T p /ln(2) = 1.11 h (where T p is the physical half-life of the radionuclide). Even though what ultimately matters is the product of the time-integrated activity coefficient and the maximum mean activity per cell, which provides the mean number of decays per cell (after correcting h to s), it is helpful to know the time-integrated activity coefficient for reasonability checks. The percentage of cells that are labeled in MIRDcell is set to 100%. The maximum mean activity per cell (all cells) (Bq) is adjusted until the maximum mean absorbed dose to cells in the MIRDcell SF curve matches the maximum average dose (Gy) given in Figure 6 of Kennel et al. When the "Compute" button is clicked the first time, an error message will pop up indicating that 100% of the cells cannot be labeled because this number exceeds the number of cells within the drug penetration depth; the percentage of labeled cells will be automatically set to the maximum number allowed when the error message is accepted. Therefore, a good rule is to let the program decide the percentage of labeled cells when a drug penetration depth is specified. Alternatively, if there is a specific desired percentage, the value can be set before the "Compute" button is clicked. Once the model is run, the SF as a function of different domains can be visualized under the "SF Curve" tab. The maximum mean activity per cell that matched the desired absorbed dose was 0.02 Bq (Fig. 5). The radial activity histograms and tomographic sections (Fig. 6) of the selected cell cluster geometry are displayed in the "Radial Histogram" and "3-D Slice" tabs, respectively. Tomographic sections of each cell layer can be displayed by specifying the value, in terms of cell diameters, in the box labeled "Axial Height". Alternatively, the sections can be scrolled through using the wheel of the mouse.

---

### Current sample size conventions: flaws, harms, and alternatives [^8e4148bc]. BMC Medicine (2010). Low credibility.

Background

The belief remains widespread that medical research studies must have statistical power of at least 80% in order to be scientifically sound, and peer reviewers often question whether power is high enough.

Discussion

This requirement and the methods for meeting it have severe flaws. Notably, the true nature of how sample size influences a study's projected scientific or practical value precludes any meaningful blanket designation of < 80% power as "inadequate". In addition, standard calculations are inherently unreliable, and focusing only on power neglects a completed study's most important results: estimates and confidence intervals. Current conventions harm the research process in many ways: promoting misinterpretation of completed studies, eroding scientific integrity, giving reviewers arbitrary power, inhibiting innovation, perverting ethical standards, wasting effort, and wasting money. Medical research would benefit from alternative approaches, including established value of information methods, simple choices based on cost or feasibility that have recently been justified, sensitivity analyses that examine a meaningful array of possible findings, and following previous analogous studies. To promote more rational approaches, research training should cover the issues presented here, peer reviewers should be extremely careful before raising issues of "inadequate" sample size, and reports of completed studies should not discuss power.

Summary

Common conventions and expectations concerning sample size are deeply flawed, cause serious harm to the research process, and should be replaced by more rational alternatives.

---

### Composite rating scales [^2fbad343]. Journal of the Neurological Sciences (2010). Low credibility.

Rating scales are instruments that are very frequently used by clinicians to perform patient assessments. Typically, rating scales grade the attribute on an ordinal level of measurement, i.e., a rank ordering, meaning that the numbers assigned to the different ranks (item scores) do not represent 'real numbers' or 'physical magnitudes'. Single-item scales have some advantages, such as simplicity and low respondent burden, but they may also suffer from disadvantages, such as ambiguous score meanings and low responsiveness. Multi-item scales, in contrast, seem more adequate for assessment of complex constructs, allowing for detailed evaluation. Total scores representing the value of the construct may be quite precise and thus the responsiveness of the scale may be high. The most common strategy for obtaining the total score is the sum of the item scores, a strategy that constitutes one of the most important problems with these types of scales. A summative score of ordinal figures is not a 'real magnitude' and may have little sense. This paper is a review of the theoretical frameworks of the main theories used to develop rating scales (Classical Test Theory and Item Response Theory). Bearing in mind that no alternative is perfect, additional research in this field and judicious decisions are called for.

---

### Sample size estimation: current practice and considerations for original investigations in MRI technical development studies [^ff5f6795]. Magnetic Resonance in Medicine (2021). Medium credibility.

Purpose

To investigate and to provide guidance for sample size selection based on the current practice in MR technical development studies in which healthy volunteers are examined.

Methods

All original articles published in Magnetic Resonance in Medicine between 2017 and 2019 were investigated and categorized according to technique, anatomical region, and magnetic field strength. The number of examined healthy volunteers (ie, the sample size) was collected and evaluated, whereas the number of patients was not considered. Papers solely measuring patients, animals, phantoms, specimens, or studies using existing data, for example, from an open databank, or consisting only of theoretical work or simulations were excluded.

Results

The median sample size of the 882 included studies was 6. There were some peaks in the sample size distribution (eg, 1, 5, and 10). In 49.9%, 82.1%, and 95.6% of the studies, the sample size was smaller or equal to 5, 10, and 20, respectively.

Conclusion

We observed a large variance in sample sizes reflecting the variety of studies published in Magnetic Resonance in Medicine. Therefore, it can be concluded that it is current practice to balance the need for statistical power with the demand to minimize experiments involving healthy humans, often by choosing small sample sizes between 1 and 10. Naturally, this observation does not release an investigator from ensuring that sufficient data are acquired to reach statistical conclusions.

---

### Brain damage: neglect disrupts the mental number line [^3653f4cb]. Nature (2002). Excellent credibility.

A popular metaphor for the representation of numbers in the brain is the 'mental number line', in which numbers are represented in a continuous, quantity-based analogical format. Here we show that patients with hemispatial neglect misplace the midpoint of a numerical interval when asked to bisect it (for example, stating that five is halfway between two and six), with an error pattern that closely resembles the bisection of physical lines. This new form of representational neglect constitutes strong evidence that the mental number line is more than simply a metaphor, and that its spatial nature renders it functionally isomorphic to physical lines.

---

### Skillful multiyear predictions of ocean acidification in the California current system [^980bad1e]. Nature Communications (2020). High credibility.

We compute the anomaly correlation coefficient (ACC) via a Pearson product-moment correlation to quantify the linear association between predicted and target anomalies (where the target is either the model reconstruction or the observational product). If the predictions perfectly match the sign and phase of the anomalies, the ACC has a maximum value of 1. If they are exactly out of phase, it has a minimum value of −1. The ACC is a function of lead time, :Where F ′ is the forecast anomaly, O ′ is the verification field anomaly, and the ACC is calculated over the initializations spanning 1954–2017 (N = 64) relative to the reconstruction and CESM-LE, and over initializations covering 1990–2005 (N = 16) relative to the JMA observational product. We quantify statistical significance in the ACC using a t test at the 95% confidence level with the null hypothesis that the two time series being compared are uncorrelated. We follow the methodology of Bretherton et al. using the effective sample size in t tests to account for autocorrelation in the two time series being correlated:Where N is the true sample size andandare the lag 1 autocorrelation coefficients for the forecast and verification data. We assess statistical significance between two ACCs (e.g. between that of the initialized forecast and a simple persistence forecast for the same lead time) using a z test at the 95% confidence level with the null hypothesis that the two correlation coefficients are not different.

To quantify the magnitude of forecast error, or the accuracy in our forecasts, we use the normalized mean absolute error(NMAE), which is the MAE normalized by the interannual standard deviation of the verification data. The NMAE is 0 for perfect forecasts, < 1 when the forecast error falls within the variability of the verification data, and increases as the forecast error surpasses the variability of the verification data. MAE is used instead of bias metrics such as the root mean square error (RMSE), as it is a more accurate assessment of bias in climate simulations. Where N is the number of initializations andis the standard deviation of the verification data over the verification window.

---

### The perfect storm for obesity [^b38aa040]. Obesity (2013). Low credibility.

Obesity rates have risen steadily in the United States and around the world over the past century, with a marked escalation within the past two decades. Conventional wisdom within the medical community is that the burgeoning obesity epidemic is the product of poor nutrition and lack of exercise, but increasingly researchers are questioning whether those factors are wholly responsible. Emerging research about alternative factors is setting the stage for today's "Perfect Storm" for obesity.

---

### Neighbourhood level real-time forecasting of dengue cases in tropical urban Singapore [^abf44f56]. BMC Medicine (2018). Low credibility.

For each forecast window (from k = 1 to 12 weeks), a separate LASSO sub-model was developed, which used data available at the time of the forecast only. Each LASSO sub-model is as follows:where y t, i is the number of cases (natural log-transformed, with 1 added to avoid logging 0) in neighbourhood i in week t. The terms, andare the square, cubic and square root of the number of cases. Similarly, n t, i, 1 and n t, i, 2 are the total number of cases (similarly, natural log-transformed, with 1 added to avoid logging 0) from all neighbourhoods whose centroids are within 1 km radius and within a ring from 1 km to 2 km from the centroid of neighbourhood i, in week t, respectively. W t, i, c represents the climatic variable (average, minimum and maximum temperature, and average relative humidity) at time t in neighbourhood i. T t, i measures the number of cases moving into neighbourhood i in week t, derived from a one-time telco dataset on the movement of users. A t, i measures average building age in neighbourhood i in week t. N t is the national total number of cases (natural log-transformed, with 1 added) in week t. V i and U i measure the vegetation and connectivity index of neighbourhood i. Detailed information on the type of each set of variables are documented in Additional file 3: Table S1. Covariates in the LASSO regression were z -scored prior to estimation and the coefficients were rescaled afterwards.

Parameter estimation was subject to the LASSO constraint: Ten-fold cross validation was performed and the constraint term that optimised the out-of-sample performance was chosen as the optimal p for the forecast model.

As the models were built separately for each forecast window, the variables included in the final forecast model and their lags and parameter magnitude and sign may differ substantially.

LASSO models were built using all the data from the training dataset, which comprised information from 2010 to 2015. Out-of-sample validation was performed on the testing dataset consisting of data from 2016.

---

### Should the PANSS be rescaled? [^d91cc2a9]. Schizophrenia Bulletin (2010). Low credibility.

The design of the Positive and Negative Syndrome Scale (PANSS) with item levels ranging from 1 to 7 leads to the trivial result that the 30-item scale's zero level (no symptoms) is 30. This causes serious problems when ratios are calculated which always implicitly depend on a natural zero point (equals 0). Recent publications concerning efficacy of antipsychotics correctly suggest a subtraction of 30 points to every PANSS before calculating percent change (PC). Nevertheless, the traditional approach using uncorrected scores is still in common practice. This analysis aims to clarify which approach is the most appropriate from a statistical perspective. For analysis, data from a naturalistic study on 400 patients with a schizophrenic spectrum disorder and simulated data sets were used. While calculations concerning absolute score values and their differences are not affected, considerable problems arise in calculations of PC and related response criteria. Even significance levels of estimated treatment effects change, depending on the structure of the data (eg, baseline symptom severity). Using a PANSS version with items ranging from 0 to 6 would avoid such often neglected pitfalls.

---

### Equivalence between haseman-elston and variance-components linkage analyses for sib pairs [^796e0ce6]. American Journal of Human Genetics (2001). Low credibility.

The Haseman-Elston regression method offers a simpler alternative to variance-components (VC) models, for the linkage analysis of quantitative traits. However, even the "revisited" method, which uses the cross-product — rather than the squared difference — in sib trait values, is, in general, less powerful than VC models. In this report, we clarify the relative efficiencies of existing Haseman-Elston methods and show how a new Haseman-Elston method can be constructed to have power equivalent to that of VC models. This method uses as the dependent variable a linear combination of squared sums and squared differences, in which the weights are determined by the overall trait correlation between sibs in a population. We show how this method can be used for both the selection of maximally informative sib pairs for genotyping and the subsequent analysis of such selected samples.

---

### Mammalian skull dimensions and the golden ratio (φ) [^3a12f466]. The Journal of Craniofacial Surgery (2019). Medium credibility.

The Golden Ratio characterizes a unique relationship in the study of integers, a branch of mathematics called Number Theory. Numerically, Φ can be calculated as a ratio within the Fibonacci series as the series approaches infinity. The Fibonacci series is a well-known, infinite series in which the next term in the series is the sum of the prior 2 terms. The Fibonacci series starts as 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55… Initially, the ratios of consecutive terms in the Fibonacci series (F n / F n-1) fluctuate between 1 and 2, but rapidly converge on the value Φ as the series grows: 1/1 = 1, 2/1 = 2, 3/2 = 1.5, 5/3 = 1.667, 8/5 = 1.6, 13/8 = 1.625, 21/34 = 1.615, 34/21 = 1.619, 55/34 = 1.618,…

The division of the 40th Fibonacci term (102,334,155) by the 39th (63,245,986) matches Φ to 15 decimal places. For convenience, authors and mathematicians typically truncate Φ to 1.618. Interestingly, the reciprocal of Φ (1/Φ or 1/1.61803…) is 1 integer less than Φ (0.61803…) and has the same decimal extension as Φ, and the square of Φ (Φ² or 1.61803…²) is 1 integer more than Φ (2.61803…) and also has the same decimal extension. Φ is the only number that has these 2 properties. By convention, the reciprocal of Φ is represented by the Greek lower-case letter phi (ϕ), although sometimes this convention is reversed.

---

### Plants capable of selfing are more likely to become naturalized [^898f6c8a]. Nature Communications (2016). Medium credibility.

As in the previous models, we separately analysed naturalization incidence (whether the species have been recorded as naturalized outside their native ranges), and for the subset of only the naturalized species, their naturalization extent (the number of regions where they are naturalized). For the analysis of the binary variable naturalization incidence, we used a diagonally weighted least square estimation method with robust standard errors. For the analysis of naturalization extent, we log-transformed the number of regions where the species is native to and the number of regions, where the species is naturalized, and used a maximum likelihood estimation method with robust standard errors (MLR). To facilitate comparisons among the different path coefficients, we standardized all continuous variables to a mean of zero and s.d. of 1. We assessed model fit using a relative measure, the Comparative Fit Index, and an absolute measure, the root mean square error of approximation. A Comparative Fit Index > 0.95 and a root mean square error of approximation < 0.06 are considered good. In addition, we reported R 2 for the endogenous variables naturalization incidence and naturalization extent. We did the path analysis using the 'lavaan' packagein R. The results remained qualitatively similar when we used one average index per species (Supplementary Fig. 5).

Data availability

Data for this article, including four selfing-ability indices and information on data sources, species name, life history, native range and global naturalization as assessed based on the Global Naturalized Alien Flora database may be found in the Supplementary Information (Supplementary Data 1). Fruit set and seed production data in pollinator exclusion, self-pollination and cross-pollination treatments are available from the corresponding author upon request.

---

### Deep learning driven biosynthetic pathways navigation for natural products with bioNavi-NP [^76b3c282]. Nature Communications (2022). High credibility.

Data set

MetaNetXintegrated a number of major resources of metabolites and biochemical reactions including MetaCyc, KEGG, The SEED, Rhea, BiGGand so on. However, the information regarding reaction directionality and compartmentalization were generally disregarded, which are important to predict the precursors of the NPs. So we first extracted reactions directly from MetaCyc (version 23.5) pathway ontology and KEGG (accessed in January, 2021) pathway maps, where the "irrelevant" agents such as cofactors and minor substrates or products were removed. These reactions were excluded from MetaNetX (version 4.1) dataset, and following the previous work, we derived pairs of precursors and metabolites where the maximum common substructure exceeds 40% of the atoms of the metabolite for the rest of the reactions. Then, multi-product reactions were decomposed to multiple mono-product reactions with the same substrates and the cofactors were removed. Compounds containing Coenzyme A (CoA) play an important role in the biosynthesis of natural products especially polyketides and lipids. Although CoA is a complex structure that contains 84 heavy atoms, it is usually not involved in the reaction center, so the CoAs in the molecules were replaced by " in the SMILES format and restored when the results were output. Considering that the stereochemistry is often poorly annotated in the structure databases, we firstly checked the structures in our reaction dataset. The potential stereochemical centers exist in 15,372 out of 20,710 structures, and 10,909 (71.0%) of them are completely annotated with the stereochemistry, 12,763 (83.0%) are annotated with at least half of the stereochemistry. Thus the stereochemistry was preserved in our dataset. The resulting BioChem dataset consists of 33,710 unique pairs of precursors and metabolites, 1000 of which were selected as the test set and another 1000 as the validation set (see Supplementary Fig. 2). Additionally, in order to learn more syntactic rules for chemical reactions, we screened reactions with components similar to natural products from the organic chemistry dataset for model pre-training. Specifically, we used molecular ECFP4 fingerprintsto select the reactions with a maximum similarity of ≥ 0.8 (calculated by RDKit) between any component (reactants and/or products) of each reaction in the USPTO database and the NPs from the DNPdatabase (version 27.2), resulting in 60,000 chemical reaction equations. The USPTO_NPL dataset was merged with the BioChem dataset to train the single-step model.

---

### Juno spacecraft gravity measurements provide evidence for normal modes of jupiter [^5f37f190]. Nature Communications (2022). High credibility.

Model selection: AIC

We performed the task of model selection using the AIC. This well-established statistical criterion is based on the use of entropy as a measure of information and provides a quantitative value of the information loss when a given data fitting model is selected. It selects models using their likelihood L in fitting the Doppler data while favoring low complexity models, expressed by the associated regression-effective degree of freedom k:

AIC consists of a trade-off between the goodness of fit and the simplicity of the model itself. Its minimization requires the maximization of the (log-) likelihood function of the model to the data, while keeping the solution's degrees of freedom to minimum (low value of k). See dedicated subsection in Methods for how to compute the number of degrees of freedom of a given dynamical solution. In the case of a least square estimation filter, by substituting the corresponding likelihood function, the AIC can be computed as:With n obs being the number of Doppler data points, RSS obs their root-sum-square, normalized with the noise standard deviation, and C a constant which depends only on the number of data points (which is unchanged in the different solutions we aim to compare).

Given the possibility of incurring into a solution with a large difference between estimated and model amplitudes for the normal modes, the likelihood is constructed as the product of the data-driven and amplitude-based likelihoods. The additional, amplitude-based, likelihood term is:

here n modes is the number of modes estimated (for all the arcs), RSS modes the root-sum-square of the difference between the model and estimated amplitudes of the modes, normalized to the uncertainty of the mode amplitude, and = RSS modes / n modes is the maximum likelihood estimate of the variance of a model's residuals distribution. This term becomes important when the data are not compatible with the amplitude of the modes we try to impose. The final AIC function reads:

Since only differences in AIC among different solutions are important, we select the constant C to have AIC equal to zero for the best solution found in the analysis. Gravity solutions with a small AIC value are selected as the best models. Generally, solutions having ΔAIC larger than ~500 still show signature in the Doppler data of a few passes.

---

### Spontaneous perception of numerosity in humans [^d412d5ba]. Nature Communications (2016). Medium credibility.

Data analysis

In the odd-one-out experiment, for each numerosity data were analysed by plotting per cent correct responses as a function of the area and density of the odd stimulus. Yield maps like those of Fig. 2, with the abscissa showing change in area and ordinate change in patch density. As the axes are logarithmic, the forward diagonal represents numerosity (area times density), and the other diagonal constant numerosity. Data were fitted with 2D elliptical Gaussian functions:

where x and y are obtained by clockwise rotation of the area and density space (and). The elliptical Gaussian has five free parameters: orientation of short-radius, widths of the short and long radii and the position of the centre in the area and density space. The orientation of the short radius is the axis of maximal sensitivity as subjects discriminate best when the odd stimulus varied in that direction. The ratio between long and short radii is an index of selectivity of the sensitivity.

In the explicit judgment experiment, for each numerosity and task requirement we plotted the percentage of times the test patch was perceived as 'more' as a function of area- and density-change. Again, this yields a 2D map of density change plotted against area. Both area and density employed in the analysis were calculated stimulus by stimulus via estimation of the surface covered by the convex hull of the dot cloud.

The subject response maps are then interpolated filling a space spanning from −0.75 to 0.75 octaves and fit with a 2D psychometric cumulative Gaussian function. The fitted function was a one-dimension cumulative Gaussian function operating on an arbitrary axis x obtained via rotation of the area–density space. The function has three parameters: rotation angle, width and offset. The width indicates the amount of change required to go from 50 to 84% 'more' responses. For choice planes off the area or density axis, the width is expressed as the sum of the changes in area and density required to attain 84% categorization.

---

### Selective fishing induces density-dependent growth [^2560991d]. Nature Communications (2014). Medium credibility.

Over the last decades, views on fisheries management have oscillated between alarm and trust in management progress. The predominant policy for remedying the world fishing crisis aims at maximum sustainable yield (MSY) by adjusting gear selectivity and fishing effort. Here we report a case study on how striving for higher yields from the Eastern Baltic cod stock by increasing selectivity has become exceedingly detrimental for its productivity. Although there is a successive increase in numbers of undersized fish, growth potential is severely reduced, and fishing mortality in fishable size has increased. Once density-dependent growth is introduced, the process is self-enforcing as long as the recruitment remains stable. Our findings suggest that policies focusing on maximum yield while targeting greater sizes are risky and should instead prioritize catch rates over yield. Disregarding the underlying population structure may jeopardize stock productivity, with dire consequences for the fishing industry and ecosystem structure and function.

---

### Many plants naturalized as aliens abroad have also become more common within their native regions [^b40dec53]. Nature Communications (2025). High credibility.

Following the protocol of Telfer et al. we first calculated the proportion of occupied grid cells in a native region for each species in each time period as (x + 0.5)/(n + 1), where x is the number of grid cells in which the species has been recorded and n is the total number of grid cells in the respective region. As recommended by Telfer et al. we added 0.5 to x and 1 to n to avoid proportion values of zero and one. This was necessary because in the second step, the proportion values were logit transformed, and the logit of zero would be undefined and the logit of one would be infinite. Next, we performed a weighted least-squares linear regression of the logit-transformed proportions of occupied grid cells during the later period as a function of the logit-transformed proportion of occupied grid cells during the early period. The weights were equal to the reciprocal of the variance of the logit-transformed proportions. Data visualization using scatterplots showed that species with low occupancy proportions in the early period deviated from the linear relationship. Therefore, as recommended by Telfer et al. we excluded species occupying fewer than five grid cells during the early period. The resulting regression plots are shown in Fig. S3. The values of the occupancy-change index then correspond to the standardized residuals (i.e. the deviations of the logit-transformed proportions of grid cells occupied by the species in the later period from the regression line). Therefore, for each early occupancy value, we have a more or less symmetrical distribution of negative and positive occupancy-change index values (i.e. standardized residuals; Fig. S3). It should be noted that a positive value of the change index does not necessarily mean that the occupancy of a species has increased between the two periods. Instead, it means that the species occupied a higher proportion of grid cells in the later period than expected based on its occupancy in the early period.

---

### Single photon emission computed tomography (SPECT) myocardial perfusion imaging guidelines: instrumentation, acquisition, processing, and interpretation [^c6205197]. Journal of Nuclear Cardiology (2018). Medium credibility.

Computed tomography (CT) calibration and uniformity for SPECT/CT attenuation correction require daily and periodic checks with defined HU tolerances. The CT calibration is checked daily with a water-filled cylinder, usually 24 cm in diameter provided by the manufacturer, and if the error is greater than 5 to 10 HU (i.e., different than 0 HU), the CT system is considered to be out of calibration; an air calibration (blank-scan) can be tried and, if unresolved, the manufacturer's field service engineer must be called, and calibration must be checked by a qualified person on an annual basis or after any major repair or calibration. For field uniformity, small square or circular ROIs are placed at 90° intervals near the edge of the cylinder image and compared with the center, and the maximum difference in mean region CT number should not exceed 5 HU or the manufacturer's specified value, since nonuniformities greater than this may affect SPECT AC based on the CT image.

---

### Precision medicine and the cursed dimensions [^fb2c53c4]. NPJ Digital Medicine (2019). Medium credibility.

Our intuition regarding "average" is rooted in one-dimensional thinking, such as the distribution of height across a population. This intuition breaks down in higher dimensions when multiple measurements are combined: fewer individuals are close to average for many measurements simultaneously than for any single measurement alone. This phenomenon is known as the curse of dimensionality. In medicine, diagnostic sophistication generally increases through the addition of more predictive factors. Disease classes themselves become more dissimilar as a result, increasing the difficulty of incorporating (i.e., averaging) multiple patients into a single class for guiding treatment of new patients. Failure to consider the curse of dimensionality will ultimately lead to inherent limits on the degree to which precision medicine can extend the advances of evidence-based medicine for selecting suitable treatments. One strategy to compensate for the curse of dimensionality involves incorporating predictive observation models into the patient workup.

---

### Super-emitters in natural gas infrastructure are caused by abnormal process conditions [^6eca421e]. Nature Communications (2017). Medium credibility.

Methods for compression systems

Supplementary Fig. 11 shows a flow chart summarizing the methodology followed to estimate emissions from compressors. There are four major sources of emissions related to compression systems on production sites: fugitive emissions, engine exhaust emissions, compressor start-ups and compressor blowdowns.

To determine the total number of compressors at natural gas production sites in the region, we derived from the GHGRPan activity factor (for the Barnett Shale) of 0.14 compressors per well. For a total number of 25,700 wells in the region we estimated 3,600 compressors. We determined a distribution for the number of compressors per production site using the Texas Commission on Environmental Quality (TCEQ)'s 2009 Barnett Shale Special Emissions Inventory (BSEI). For the subset of sites in the inventory that reported at least one compressor, the average was 1.2 compressors per site with a range of 1 to 5. We used this distribution to assign a specific number of compressors to randomly selected sites until we reached the total count of compressors in the region. The number of compressors at each site never exceeded the well count. For each site, we then estimated total compressor emissions (considering fugitives, engine exhaust, start-ups and blowdowns).

---

### Measuring the burden of hundreds of bioBricks defines an evolutionary limit on constructability in synthetic biology [^ebec7d3b]. Nature Communications (2024). High credibility.

Burden analysis

To analyze the burden assay data for one plate, we first subtracted the average values of all media blanks from the OD and fluorescence measurements. Next, to deal with well-to-well variation in background levels, we shifted the values to force the means of the points over the first hour of measurements for each strain to match the grand mean for those data points over all replicates of that strain. We then fit growth rates using nonlinear least-squares regression to an exponential model: C (t) = C 0 e rt. We assumed that OD is directly proportional to the number of cells at a given time, C (t). C 0 is the initial number of cells, and r is the specific growth rate. We fit C 0 and r for all sets of nine consecutive measurements (a 90-min window in the standard assay) after the OD exceeded 0.03 and recorded the largest value of r as the maximum specific growth rate for that strain. To determine the fluorescent protein (e.g. GFP) production rate per cell, p, we repeated this procedure while fitting fluorescence values to the equation: F (t) = F 0 + C 0 (p/ r) (e rt − 1). F 0 is the initial fluorescence and F(t) is the fluorescence at time t. This equation is derived by integrating the relationship dF / dt = p C(t). We fit F 0 and p in this model to the data while keeping C 0 and r fixed to the values determined from the OD curve fit for the corresponding time window. Again, we recorded the largest value of p across all time points as the maximum fluorescent protein production rate.

---

### 111In-leukocyte scintigraphy for suspected infection / inflammation 3.0 [^7c3eefef]. SNMMI (2024). High credibility.

111In-leukocyte quality control includes determining labeling efficiency by recentrifugation (approximately 450 g for 5 min) of washed cells resuspended in 5 mL of buffered saline, with the leukocyte pellet resuspended in 5 mL of cell-free plasma; leukocyte clumping may be checked on a hemocytometer slide; and the average number of cells per 50-micron (small) square is then determined with No. cells/cm3 (mL) = average number of cells/small square x (2 x 10⁶).

---

### [XLS] as permitted by the freedom of information act (FOIA), I am hereby… [^bd81233e]. FDA (2025). Medium credibility.

Collinsville, Connecticut United States End Dates of inspection: 23 Aug 2024 Project Area: Human Compliance: Devices FEI: 3009424796 Classification: VAI I confirm that Clarivate Analytics will be charged accordingly by FOI for this request, however please contact me if the charges exceed $ 200". 2024–9667, 11/01/2024, Clarivate Analytics SAS, "To Whom it May Concern: As permitted by the Freedom of Information Act, I am hereby requesting the following document: Form FDA 483: Francos. Villebarou, France United States End Dates of inspection: 19 Apr 2024 Project Area: Drug Quality Assurance FEI: 3010164501 Classification: VAI I confirm that Clarivate Analytics will be charged accordingly by FOI for this request, however please contact me if the charges exceed $ 200". 2024–9702, 11/01/2024, FOIA Professional Services, "I would like to request full reports of any and all adverse events reported to the FAERS database for IZERVAY or avacincaptad pegol intravitreal solution between October 17, 2024, and October 23, 2024. Please provide the documents electronically if possible". 2024–9711, 11/01/2024, VENABLE LLP, "Any premarket tobacco product application submitted by or on the_x000D_ behalf of Alien Vape LLC, regardless of whether such PMTA was the subject of a_x000D_ marketing denial order". 2024–9739, 11/04/2024, "DAKO TRADING CORPORATION, INC", LIST OF FDA EMAIL ADDRESSES. 2024–9748, 11/04/2024, Muira McCammon, " I request all agency records from December 2008 to the present concerning Tweets_x000D_ deleted or drafted and not sent from the @FDArecalls account associated with the Food_x000D_ and Drug Administration". 2024–9755, 11/04/2024, Gcmedica Enterprise Ltd, We request files regarding 510 submission: k030935 and k171440. 2024–9756, 11/04/2024, Sovish, Request for released records.

---

### Sample sizes based on three popular indices of risks [^4a61a58f]. General Psychiatry (2018). Low credibility.

Sample size justification is a very crucial part in the design of clinical trials. In this paper, the authors derive a new formula to calculate the sample size for a binary outcome given one of the three popular indices of risk difference. The sample size based on the absolute difference is the fundamental one, which can be easily used to derive sample size given the risk ratio or OR.

---

### Too many digits: the presentation of numerical data [^a41454ae]. Archives of Disease in Childhood (2015). Low credibility.

Thus a decimal places rule that ignores significant digits does not work. But equally, and perhaps surprisingly, a significant digits rule that ignores decimal places does not always work either. Reporting risk ratios to three significant digits for example leads to the largest ratio below 1 being reported as 0.999 and the smallest above 1 as 1.01, with three and two decimal places, respectively. This is clearly unsatisfactory as they differ in precision by a factor of ten. In this instance a combination of significant digits and decimal places, the rule of four, works best: round the risk ratio to two significant digits if the leading non-zero digit is four or more, otherwise round to three.

The rule of four gives three decimal places for risk ratios from 0.040 to 0.399, two from 0.40 to 3.99 and one from 4.0 to 39.9. Applying it to the example of 22.68 above gives 22.7 (95% CI 7.5 to 74). Alternatively one can apply the rule with one less significant digit, giving 23 with CI 8 to 70.

Another example is the reporting of test statistics such as t or F. Specifying one decimal place would permit say t = 30.1, where 30 is clearly sufficient as it is so highly significant. Conversely specifying two significant digits would permit t = −0.13, where again the extra precision is irrelevant as it is far from significant. A suitable rule specifies up to one decimal place and up to two significant digits.

When comparing group means or percentages in tables, rounding should not blur the differences between them. This is the basis for the Hopkins two digits rule, whereby the mean has enough decimal places to ensure two significant digits for the SD. An analogous rule for percentages might be to use enough decimal places to ensure two significant digits for the range of values across groups, eg, if the range is 10% or more use whole numbers, if less than 1% use two decimal places, and otherwise one. In practice percentages are usually given along with their corresponding frequencies, so precision is less critical as the exact values can be calculated.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^5f767c11]. CDC (2011). Medium credibility.

Table 2 — additional community viral load (VL) measures — includes a cumulative measure defined as "Sum of VLs for all cases" that reflects "The number of virus particles in the population who have a VL; affected by the number of cases; reflects both prevalence of HIV cases and burden of viremia". A suppression proportion is defined by "The numerator is the number of cases whose most recent VL was suppressed; the denominator is the number of cases with a VL" and interpreted as "Maximal Virologic Suppression, or the percent virologically suppressed reflects ART uptake, adherence, and effectiveness in clinical settings; a quality of HIV care measure; in jurisdictions offering universal treatment, an important marker of uptake of universal ART guidelines". Calculation timing guidance notes "Time needs to be defined for each calculation, usually a 12-month period. Pooling over a few years can increase VL completeness but is harder to interpret". The abbreviation is specified as "ART, antiretroviral treatment".

---

### The universal visitation law of human mobility [^b6530c01]. Nature (2021). Excellent credibility.

Human mobility impacts many aspects of a city, from its spatial structure 1–3 to its response to an epidemic 4–7. It is also ultimately key to social interactions 8, innovation 9,10 and productivity 11. However, our quantitative understanding of the aggregate movements of individuals remains incomplete. Existing models-such as the gravity law 12,13 or the radiation model 14 -concentrate on the purely spatial dependence of mobility flows and do not capture the varying frequencies of recurrent visits to the same locations. Here we reveal a simple and robust scaling law that captures the temporal and spatial spectrum of population movement on the basis of large-scale mobility data from diverse cities around the globe. According to this law, the number of visitors to any location decreases as the inverse square of the product of their visiting frequency and travel distance. We further show that the spatio-temporal flows to different locations give rise to prominent spatial clusters with an area distribution that follows Zipf's law 15. Finally, we build an individual mobility model based on exploration and preferential return to provide a mechanistic explanation for the discovered scaling law and the emerging spatial structure. Our findings corroborate long-standing conjectures in human geography (such as central place theory 16 and Weber's theory of emergent optimality 10) and allow for predictions of recurrent flows, providing a basis for applications in urban planning, traffic engineering and the mitigation of epidemic diseases.

---

### The basic reproduction number (R) of measles: a systematic review [^f3ec52f0]. The Lancet: Infectious Diseases (2017). Medium credibility.

The basic reproduction number, R nought (R 0), is defined as the average number of secondary cases of an infectious disease arising from a typical case in a totally susceptible population, and can be estimated in populations if pre-existing immunity can be accounted for in the calculation. R 0 determines the herd immunity threshold and therefore the immunisation coverage required to achieve elimination of an infectious disease. As R 0 increases, higher immunisation coverage is required to achieve herd immunity. In July, 2010, a panel of experts convened by WHO concluded that measles can and should be eradicated. Despite the existence of an effective vaccine, regions have had varying success in measles control, in part because measles is one of the most contagious infections. For measles, R 0 is often cited to be 12–18, which means that each person with measles would, on average, infect 12–18 other people in a totally susceptible population. We did a systematic review to find studies reporting rigorous estimates and determinants of measles R 0. Studies were included if they were a primary source of R 0, addressed pre-existing immunity, and accounted for pre-existing immunity in their calculation of R 0. A search of key databases was done in January, 2015, and repeated in November, 2016, and yielded 10883 unique citations. After screening for relevancy and quality, 18 studies met inclusion criteria, providing 58 R 0 estimates. We calculated median measles R 0 values stratified by key covariates. We found that R 0 estimates vary more than the often cited range of 12–18. Our results highlight the importance of countries calculating R 0 using locally derived data or, if this is not possible, using parameter estimates from similar settings. Additional data and agreed review methods are needed to strengthen the evidence base for measles elimination modelling.

---

### Modelling the species-area relationship using extreme value theory [^02417f0a]. Nature Communications (2025). High credibility.

The nested species-area relationship, obtained by counting species in increasingly larger areas in a nested fashion, exhibits robust and recurring qualitative and quantitative patterns. When plotted in double logarithmic scales it shows three phases: rapid species increase at small areas, slower growth at intermediate scales, and faster rise at large scales. Despite its significance, the theoretical foundations of this pattern remain incompletely understood. Here, we develop a theory for the species-area relationship using extreme value theory, and show that the species-area relationship is a mixture of the distributions of minimum distances to a starting sampling focal point for each individual species. A key insight of our study is that each phase is determined by the geographical distributions of the species, i.e. their ranges, relative to the focal point, enabling us to develop a formula for estimating the number of species at phase transitions. We test our approach by comparing empirical species-area relationships for different continents and taxa with our predictions using Global Biodiversity Information Facility data. Although a SAR reflects the underlying biological attributes of the constituent species, our interpretations and use of the extreme value theory are general and can be widely applicable to systems with similar spatial features.

---

### Committee opinion no. 680 summary: the use and development of checklists in obstetrics and gynecology [^6d342952]. Obstetrics and Gynecology (2016). Medium credibility.

A Checklist for Checklists — development, drafting, and validation prompts provide concrete design and testing considerations. Development prompts include "Do you have clear, concise objectives for your checklist?" Drafting prompts ask whether a checklist will "Utilize natural breaks in workflow (pause points)?", "Fit on one page?", and "Is the font: Sans serif? Upper and lower case text? Large enough to be read easily? Dark on a light background?", and whether "Are there fewer than 10 items per pause point?" and "Is the date of creation (or revision) clearly marked?" Validation prompts include "Trialed the checklist with frontline users (either in a real or simulated situation)?", "Modified the checklist in response to repeated trials?", and whether it will "Fit the flow of work?" and "Detect errors at a time when they can still be corrected?", with attention to planning for future revision; additionally, "Please note: A checklist is NOT a teaching tool or algorithm".

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^6517cbfd]. Annals of the American Thoracic Society (2023). High credibility.

Basic stereological measurements — first-order parameters and probes identify that "First-order parameters are volume (three-dimensional [3D]), surface (two-dimensional [2D]), length (one-dimensional [1D]), and number (zero-dimensional [0D])". Measurements "are performed using geometric test probes, such as points (0D), lines (1D), planes (2D), or volumes (3D)". These probes create countable events in the image, and "Raw counts provide ratios… that are multiplied by the reference space volume to obtain absolute measures for the lung or subcompartment".

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^b1513dd5]. CDC (2011). Medium credibility.

Table 1b — Minimum sample size for detecting geometric mean (GM) ratio k with α = 0.05 and W = 0.9 — presents required sample sizes by standard deviation (S) strata and defines parameters; α is significance level, W is power, GM is geometric mean, and S is standard deviation, noting that 1.2 is the standard deviation of national viral load (VL) data. Each jurisdiction will need to assess the standard deviation of their local VL data and then determine the appropriate sample size needed to assess VL, and if the sample size is inadequate to meet the recommended case inclusion criterion, an alternate method may need to be used, such as combining multiple years of data. For k = 3, sample sizes across S = 1, 1.1, 1.2, 1.3, 1.4, 1.5 are 75, 91, 108, 127, 147, 169, and Table 1b uses power = 90%; jurisdictions may also explore differences in means of viral loads, including categorical differences in the proportion with undetectable or very low VL.

---

### The rate and effects of spontaneous mutation on fitness traits in the social amoeba, dictyostelium discoideum [^f583307b]. G3 (2013). Low credibility.

Second, some traits may be under stabilizing selection in nature because of their effects on spore production. The number of FBs and the number of spores are positively correlated (Table 4). However, if the relationship between spore number (i.e. fitness) and FB has a nonlinear component, that could indicate stabilizing selection. In that situation, we would expect significant nonlinearity, with an optimum close to the ancestral value. Put another way, mutations that increase or decrease the number of FBs relative to the ancestor may all decrease spore number. To examine this hypothesis, a nonlinear regression between spore number and FB number was calculated by fitting a cubic relationship. This analysis indicated a significant nonlinearity, with a local optimum occurring at a relative fitness of 1.06 (A). A similar nonlinear regression analysis was also performed for spores per FB, which is also positively correlated with spore number (Table 4). Again there was significant nonlinearity, with a maximum at 1.21 (B). These results suggest that both of these fitness components may be under stabilizing selection, which would imply that mutations that sufficiently increase the value of one of these fitness components are not in fact beneficial overall. None of the others fitness components exhibited significant nonlinearity in a regression with spore number. This supports the idea that it is stabilizing selection on spores per FB, and total number of FBs, that causes the high frequency of "beneficial" mutations that increase these trait values.

Third, the high frequency of trait-increasing mutations may indicate the trait is not actually a fitness component. If correct, the ancestor's particular trait value has little predictive power regarding the effects of mutations. This is because there has been no evolutionary history of selection removing only those mutations that reduce the trait value, as there has for directionally selected fitness traits, which enriches them among random mutations. For the remaining two components exhibiting high estimates of the frequency of trait-increasing mutations, the distance that a slug travels and the relative competitive ability, is this a likely explanation? Slug migration is thought to be an important component of fitness in D. discoideum. Slug migration allows D. discoideum to move to regions of more abundant resources. It also assists in long-range spore dispersal and may assist in predator protection (f1;). The fact that the sizeable majority of mutations reduced slug distance supports the notion that slug migration distance being at least partially under direct selection in nature.

---

### Mean-field theory of random close packings of axisymmetric particles [^90ce5be7]. Nature Communications (2013). Medium credibility.

Finding the optimal random packing of non-spherical particles is an open problem with great significance in a broad range of scientific and engineering fields. So far, this search has been performed only empirically on a case-by-case basis, in particular, for shapes like dimers, spherocylinders and ellipsoids of revolution. Here we present a mean-field formalism to estimate the packing density of axisymmetric non-spherical particles. We derive an analytic continuation from the sphere that provides a phase diagram predicting that, for the same coordination number, the density of monodisperse random packings follows the sequence of increasing packing fractions: spheres < oblate ellipsoids < prolate ellipsoids < dimers < spherocylinders. We find the maximal packing densities of 73.1% for spherocylinders and 70.7% for dimers, in good agreement with the largest densities found in simulations. Moreover, we find a packing density of 73.6% for lens-shaped particles, representing the densest random packing of the axisymmetric objects studied so far.

---

### 2022 AHA / ACC key data elements and definitions for cardiovascular and noncardiovascular complications of COVID-19: a report of the American college of cardiology / American Heart Association task force on clinical data standards [^7c00ac0e]. Journal of the American College of Cardiology (2022). High credibility.

Maximum fraction of inspired oxygen (FiO2) is defined as the maximum molar fraction of oxygen in an inhaled gas, with permissible value type Numeric.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^dee6cacc]. CDC (2011). Medium credibility.

Community viral load sample size calculations — sample size to detect differences in geometric mean (GM) viral load depends on power and standard deviation (S). The table specifies "α = 0.05 and W = 0.8", with S columns 1 to 1.5; for k = 3 the minimum sample sizes by S are 54, 66, 78, 92, 106, and 122, and "1.2 is the standard deviation of national VL data", so at S = 1.2 the needed sample size is 78.

---

### Recommendations for prevention and control of influenza in children, 2023–2024 [^9e7d9395]. Pediatrics (2023). High credibility.

Table 4 — Recommended seasonal influenza vaccines for children and adolescents (United States, 2023–2024) lists product, age group, presentation with antigen content, recommended dose, and Thimerosal Mercury Content† (µg Hg/0.5-mL Dose). Examples include: Fluarix Quadrivalent (GlaxoSmithKline) for ≥ 6 mo as a 0.5-mL prefilled syringe (15 µg/0.5 mL) with recommended dose 0.5 mL and Thimerosal Mercury Content† 0; Fluzone Quadrivalent (Sanofi Pasteur) for ≥ 6–35 mo as a 0.5-mL single-dose vial* (15 µg/0.5 mL) with recommended dose 0.25 or 0.5 mL and Thimerosal Mercury Content† (µg Hg/0.5-mL Dose) 0, and for 6–35 mo as a 5.0-mL multidose vial* (15 µg/0.5 mL) with recommended dose 0.25 or 0.5 mL and Thimerosal Mercury Content† (µg Hg/0.5-mL Dose) 25; Flucelvax Quadrivalent (Seqirus) ≥ 6 mo in a 5.0-mL multidose vial* (15 µg/0.5 mL) with recommended dose 0.5 mL and Thimerosal Mercury Content† (µg Hg/0.5-mL Dose) 25; Flublok Quadrivalent (Sanofi Pasteur) for ≥ 18 y as a 0.5-mL prefilled syringe (45 µg/0.5 mL) with recommended dose 0.5 mL and Thimerosal Mercury Content† 0; and FluMist Quadrivalent (AstraZeneca) for 2–49 y as a 0.2-mL prefilled intranasal sprayer (virus dose: 10^(6.5–7.5) FFU/0.2 mL) with recommended dose 0.2 mL and Thimerosal Mercury Content† 0. Handling notes state: "A total of 0.25 mL drawn from a single or multidose vial is an acceptable dose for children 6 to 35 months of age", "For vaccines that include a multidose vial presentation, the maximum number of doses withdrawn should not exceed the number specified in the package insert (eg, 10 doses for Fluzone, 20 doses for Afluria). Residual product should be discarded", and "Single-dose vials should be used for only 1 dose (0.25 mL or 0.5 mL). Residual product remaining in the vial should be discarded".

---

### Specialized hub beta cells trade maximal insulin production for perfect timing [^b27b923b]. Cell Metabolism (2016). Low credibility.

The pulsatility of insulin release is disturbed early in type 2 diabetes, but it is not clear whether specialized pacemaker cells drive islet oscillations. In this issue of Cell Metabolism, Johnston et al. (2016) show that specialized hubs, identified as 1%-10% of beta cells with more active mitochondria and less insulin, synchronize beta cell oscillations.

---

### An analytical theory of balanced cellular growth [^a5efc002]. Nature Communications (2020). High credibility.

Proof: The active stoichiometric matrix A may have more rows than columns. In this case, m + 1 > n, and the rows for exactly n metabolites are linearly independent, as row and column rank must equal. As a consequence, the remaining m + 1 − n metabolite concentrations are linearly dependent on the concentrations of the n independent metabolites. These dependent concentrations are not free variables, and hence they can be put aside and dealt with separately.

We decompose the linear system of equations represented by constraint (11) into two parts, rearranging the rows of A into matrices B, C such that B contains the rows for the independent reactants. As A has full column rank, choosing linearly independent rows results in a square matrix B of full rank (#rows(B) = rank(B) = rank(A)). Let b be the subvector of reactant concentrations a that correspond to the rows of B, and let c be the subvector of the remaining reactant concentrations corresponding to the rows of C. We can then split the mass balance constraint (11) into two separate equations: B is a square matrix of full rank, so there is always a unique inverse B⁻¹. Multiplying both sides of the first equation by B⁻¹ from the left, we obtain the desired equation for v. Inserting this result into the second equation results in the desired equation for c. □

---

### A faster and more accurate algorithm for calculating population genetics statistics requiring sums of stirling numbers of the first kind [^7a69c229]. G3 (2020). Medium credibility.

Ewen's sampling formula is a foundational theoretical result that connects probability and number theory with molecular genetics and molecular evolution; it was the analytical result required for testing the neutral theory of evolution, and has since been directly or indirectly utilized in a number of population genetics statistics. Ewen's sampling formula, in turn, is deeply connected to Stirling numbers of the first kind. Here, we explore the cumulative distribution function of these Stirling numbers, which enables a single direct estimate of the sum, using representations in terms of the incomplete beta function. This estimator enables an improved method for calculating an asymptotic estimate for one useful statistic, Fu's [Formula: see text] By reducing the calculation from a sum of terms involving Stirling numbers to a single estimate, we simultaneously improve accuracy and dramatically increase speed.

---

### Recommendations for prevention and control of influenza in children, 2015–2016 [^2c61e3c9]. Pediatrics (2015). Medium credibility.

Egg-independent influenza vaccine technologies and product selection — 2015–2016: Two trivalent influenza vaccines manufactured using technologies that do not use eggs will be available for people 18 years or older during the 2015–2016 season — cell culture–based inactivated influenza vaccine (ccIIV3) and recombinant influenza vaccine (RIV3); for ccIIV3, only trace amounts of ovalbumin are detectable and egg allergy is not listed as a contraindication, and for RIV3, it is licensed for people 18 years or older and there are no egg proteins in this version of influenza vaccine. Vaccination should not be delayed to obtain a specific product.

---

### A general strategy for diversifying complex natural products to polycyclic scaffolds with medium-sized rings [^b15c000f]. Nature Communications (2019). High credibility.

The interrogation of complex biological pathways demands diverse small molecule tool compounds, which can often lead to important therapeutics for the treatment of human diseases. Since natural products are the most valuable source for the discovery of therapeutics, the derivatization of natural products has been extensively investigated to generate molecules for biological screenings. However, most previous approaches only modified a limited number of functional groups, which resulted in a limited number of skeleta. Here we show a general strategy for the preparation of a library of complex small molecules by combining state-of-the-art chemistry - the site-selective oxidation of C-H bonds - with reactions that expand rigid, small rings in polycyclic steroids to medium-sized rings. This library occupies a unique chemical space compared to selected diverse reference compounds. The diversification strategy developed herein for steroids can also be expanded to other types of natural products.

---

### ACC appropriate use criteria methodology: 2018 update: a report of the American college of cardiology appropriate use criteria task force [^0573d900]. Journal of the American College of Cardiology (2018). Medium credibility.

ACC Appropriate Use Criteria (AUC) rating panel composition and balance — Rating panels consist of 13 to 17 members and must be composed of an odd number of individuals; at a minimum they include a practicing physician with expertise in the clinical topic, a practicing physician in a closely related discipline, a primary care physician, an expert in statistical analysis, and an expert in clinical trial design. Rating panel members may serve more than 1 function; the Task Force may incorporate a public-sector member and/or a health services payer representative, and a fellow-in-training may be included. To maintain balance, specialists whose key area of interest is the primary focus of the specific set of AUC should be a minority of rating panel members.

---

### Colloidal gel elasticity arises from the packing of locally glassy clusters [^898df107]. Nature Communications (2019). High credibility.

The connections between subgraphs determines the average cluster contact number z c and the square length scale 〈 r 2 〉. Figure 2c depicts n c, z c, and 〈 r 2 〉 computed from the graph decomposition. Like the correlation length ξ, 〈 r 2 〉 is not a systematic function of the attraction strength. However, z c is an increasing function, and crucially, the product n c z c is a convex function of U / kT, such that the number of elastically active contacts alone is able to reproduce trends in the elastic modulus measured from experiments and simulations. When these measures of the connectivity among the fundamental elastic subunits are substituted into the Cauchy-Born theory (Fig. 1c), the elastic modulus is reproduced quantitatively as well.

We compute a histogram of the number of bonds formed by particles within the individual clusters to verify that the clusters identified by the l -balanced graph decomposition are themselves rigid (Fig. 3). This distribution reveals that the mean number of bonds per particle exceeds six for all cases studied. For particles with central interactions, six bonds are necessary for iso-staticity and thus rigidity. The contact number distribution cannot prove definitively that the clusters identified by the decomposition are internally rigid. However, that the mean number of contacts per particle in a cluster is larger than six is consistent with clusters having a rigid core. Specific notions of iso-staticity and rigidity of the whole gel structure or parts has been highlighted recently through alternative methods –.

Fig. 3
The nearest neighbor distribution for particles within clusters. The l -balanced graph decomposition identifies minimally connected clusters among the different gels studied. The analysis itself is agnostic to whether the clusters themselves are rigid. By identifying minimally connected clusters in a homogeneous network, there is a bias toward clusters with large number of internal bonds. To quantify this effect and confirm that the clusters are indeed rigid, we examine each cluster in isolation and compute the number of bonded neighbors each particle within the cluster possesses. This number excludes any neighbors that might reside within another cluster. The plot shows this distribution for all the simulated gels. Clearly, the mean number of nearest neighbors is larger than six for all cases. Therefore, the clusters exceed the Maxwell iso-staticity condition and contain a rigid core that can serve as an elastic sub-unit for the Cauchy-Born theory

---

### Shape analysis, a field in need of careful validation [^3754a883]. Human Brain Mapping (2014). Low credibility.

In the last two decades, the statistical analysis of shape has become an actively studied field and finds applications in a wide range of areas. In addition to algorithmic development, many researchers have distributed end-user orientated toolboxes, which further enable the utilization of the algorithms in an "off the shelf" fashion. However, there is little work on the evaluation and validation of these techniques, which poses a rather serious challenge when interpreting their results. To address this lack of validation, we design a validation framework and then use it to test some of the most widely used toolboxes. Our initial results show inconsistencies and disagreement among four different methods. We believe this type of analysis to be critical not only for the community of algorithm designers but also perhaps more importantly to researchers who use these tools without knowing the algorithm details and seek objective criteria for tool selection.

---

### Existing and potential infection risk zones of yellow fever worldwide: a modelling analysis [^620837bb]. The Lancet: Global Health (2018). Medium credibility.

Figure 2
Predicted distribution of yellow fever within contemporary risk zones

(A) Predicted spatial variation in individual risk of yellow fever virus infection. (B) Predicted average annual incidence of yellow fever (99·8% of grid squares within the disease's range were predicted to have fewer than eight cases per 100 people per year, the highest predicted value was 20 cases per 100 people per year). (C) Average annual numbers of yellow fever cases (99·5% of grid squares within the disease's range were predicted to have fewer than three cases per year, the highest predicted value was 109 cases per year). Continental calibration factors have been applied to the outputs in B and C, calculated from Global Burden of Diseases, Injuries, and Risk Factors Study 2015 estimates of national incidence averaged from 1990 to 2015 (see full details in the Methods and appendix pp 17–18), and use population vaccination coverage rates achieved in 2016. All predictions were restricted to areas within the contemporary risk zones as defined by Jentes and colleaguesand averaged over the 47-year study period from 1970 to 2016.

---

### Cognitive arbitration between candidate dimensions of psychopathology [^1eb9c690]. Molecular Psychiatry (2025). Medium credibility.

As an alternative to the Diagnostic and Statistical Manual of Mental Disorders, transdiagnostic approaches that identify latent dimensions of psychopathology through factor analysis have gained prominence in recent years. A key critique of these approaches, however, is that they are performed at the level of symptoms only. This begs the question: are these dimensions truly more valid predictors of external outcomes than existing alternatives? Are there other ways, that are more data-driven, which can allow us to refine our definitions of clinical phenotypes? We tested this idea empirically, conducting a large-scale meta-scientific comparison of thousands of competing factor solutions that allowed us to determine if the latent structure underlying the covariation of psychiatric symptoms has robust and specific cognitive correlates. In nine independent datasets, comprising N = 7565 individuals including patients about to start mental health treatment, healthy individuals, paid and unpaid participants, a broad set of age ranges and cognitive task variants that measured model-based planning and metacognition, we found that factors with the best fit to cognition were those derived from a first-order factor analysis on the maximal number of theoretically informed self-report symptoms available. These factors ('Compulsivity and Intrusive Thought' and 'Anxious-depression') performed better than thousands of engineered alternatives and performed twice as well as traditional questionnaire total scores. Crucially, this unsupervised approached based on symptom correlation only performed on-par with a partial least squares analysis, a supervised approach to deriving factors based on cognition. These results provide evidence that unsupervised factor analysis of psychiatric symptoms is a viable method for rethinking how we define mental health and illness, affording clear opportunities for enhancing our understanding of specific underlying mechanistic processes.

---

### Scale-free networks are rare [^0cde0f97]. Nature Communications (2019). High credibility.

Robustness analysis

In order to assess the dependence of these results on the evaluation scheme itself, we conduct a series of robustness tests.

Specifically, we test whether the above results hold qualitatively when (i) we consider only network data sets that are naturally simple (unweighted, undirected, monoplex, and no multi-edges); (ii) we remove the power-law with cutoff from the set of alternative distributions; (iii) we lower the percentage thresholds for all categories to allow admission if any one constituent simple graph satisfies the requirements; and (iv) we analyze the scaling behavior of the degree distribution's first and second moment ratio. Details for each of these tests, and two others, are given in Supplementary Note 5. We also test whether the evaluation scheme correctly classifies four different types of synthetic networks with known structure, both scale free and non-scale free. Details and results for these tests are given in Supplementary Note 6.

The first test evaluates whether the extension of the scale-free hypothesis to non-simple networks and the corresponding graph-simplification procedure biases the results. The second evaluates whether the presence of finite-size effects drives the lack of evidence for scale-free distributions. Applied to the corpus, each test produces qualitatively similar results as the primary evaluation scheme (see Supplementary Note 5, and Supplementary Fig. 4), indicating that the lack of empirical evidence for scale-free networks is not driven by these particular choices in the evaluation scheme itself.

The third considers a "most permissive" parameterization, which evaluates the impact of our requirements that a minimum percentage of degree sequences satisfy the constraints of a category. Under this test, we specifically examine how the evidence changes if we instead require that only one degree sequence satisfies the given requirements. That is, this test lowers the threshold for each category to be maximally permissive: if scale-free structure exists in any associated degree sequence, the network data set is counted as falling into the corresponding category.

---

### Summary benchmarks-full set – 2024 [^a9241842]. AAO (2024). High credibility.

Dry eye syndrome — care management advises to treat any causative factors that are amenable to treatment as patients with dry eye symptoms often have many contributory factors, and that specific therapies may be chosen from any category regardless of the level of disease severity, depending on physician experience and patient preference.

---

### Clone-structured graph representations enable flexible learning and vicarious evaluation of cognitive maps [^ba346d8d]. Nature Communications (2021). High credibility.

Representation of paths and temporal order

To learn the CSCG on the maze in Fig. 4 f, we sample 5000 paths along each of the stochastic routes that are shown. The number in each square indicates the observation received at that location, and the arrows indicate possible transitions. We consider both sequences as independently generated by the model, and run EM to optimize the probability of both simultaneously. We allocate 20 clones for each other observation. By inspecting the sum-product forward messages of BP at each step as the agent navigates the two routes, we can see the distributions over clones. We observe they are over disjoint subsets of the clones. To generate the paths from the CSCG shown in Fig. 4 g (producing only paths that are consistent with the route), we sample an observation from the normalized messages from hidden state to observation during forward message passing. Finally, to extract the communities and generate the visualization in Fig. 4 g, we run the InfoMap algorithmon the graph defined by CSCG transition matrix.

In Fig. 5, we replicate the experiments of Sun et al.as follows. First, we learned a CSCG with 20 clones per observation on a sequence of observations sampled from four laps around the maze shown in Fig. 5 d. The start and end positions were unique observations. Training was terminated when perfect learning of the underlying graph was achieved. Community detection (explained below) revealed that each sensory observation was encoded by a unique clone, akin to the chunking cells found by Sun et al.

---

### Summary benchmarks-full set – 2024 [^6f68d3ad]. AAO (2024). High credibility.

Preferred Practice Pattern (PPP) guidelines — use and ethics note states that "The PPPs are intended to serve as guides in patient care, with greatest emphasis on technical aspects", that "true medical excellence is achieved only when skills are applied in a such a manner that the patients' needs are the foremost consideration", and that "The Academy is available to assist members in resolving ethical dilemmas that arise in the course of practice".

---

### A computational theory of the subjective experience of flow [^0063f495]. Nature Communications (2022). High credibility.

Fig. 9
Design and results of experiment 5.

a Schematic of Rock, Paper, Scissors. On each trial, participants choose one of three symbols (rock, paper, or scissors), then transition to one of three states depending on their (computer) opponent's symbol (rock, paper, or scissors) with a 33% chance of each state. Depending on the combination of symbols, the end state is a win (worth 3 cents), draw (worth 0 cents), or loss (worth -1 cent). We have illustrated the game with the example choice of "rock" for the participant. b Schematic of Odds vs. Evens. Participants first chose to represent "odds" or "evens" for the duration of the game. On each trial, participants select a number (one or two), then transition to one of two states depending on the number selected by their (computer) opponent (one or two), with a 50% chance of each state. If the sum of the numbers is odd, participants advance to an end state of "win" (worth 3 cents) if they represent "odds" — otherwise they advance to an end state of "lose" (worth -1 cents). If the sum of the numbers if even, participants advance to an end state of "win" if they represent "evens" — otherwise they advance to an end state of "lose". We have illustrated the game for the example choice of two for a participant who represents "odds". c Effects of game on flow and enjoyment. Points denote means, and error bars represent 95% CIs. Statistics are derived from LMMs (two-sided) performed over 794 observations across 397 participants. No corrections for multiple comparisons were applied. d Key statistics associated with each game. The values of, expected value (EV), marginal value, and the correlation betweenandwere computed analytically. The values for temporal difference prediction error and value of information (VOI) correspond to the average output of 1,000 simulations. The largest value of each variable is highlighted in gray.

Participants played 50 rounds of both games against a computer that chose hands at random. Participants earned $0.03 for every win and lost $0.01 for every loss. Draws, which are possible only in Rock, Paper, Scissors, were worth $0. After each game, we measured flow and enjoyment.

---

### Estimating the syphilis incidence and diagnosis rate in Japan: a mathematical modelling study [^e07d869d]. Sexually Transmitted Infections (2020). Medium credibility.

Objectives

The reported number of syphilis cases is increasing in Japan. In this study, we aimed to estimate both the incidence of infection and diagnosis-and-reporting rate using a mathematical model that captures the time course of infection.

Methods

We analysed yearly notification data from 1961 to 2016 and stratified according to clinical stage and sex. We built a compartmental ordinary differential equations model to describe the natural epidemiological course of syphilis in which the incidence of infection and diagnosis-and-reporting rate were included as time-varying parameters that we estimated via the maximum likelihood method.

Results

From 2008 to 2016, the estimated number of new syphilis cases among men and women was 1070 (95% CI 1037 to 1104) and 302 (95% CI 287 to 318), respectively, which was increased from the previous period (1999–2007) with 269 (95% CI 256 to 282) and 71 (95% CI 64 to 78) cases, respectively. The diagnosis-and-reporting rate did not vary greatly over time. We estimated the number of undiagnosed syphilis cases from either incubating or early clinical (ie, primary or secondary) infections in 2016 to be 262 (95% CI 249 to 275) and 79 (95% CI 74 to 84) for men and women, respectively.

Conclusions

The recent increase in reported cases of syphilis is owing to an increase in the number of infections. We found no evidence of time-dependent improvement in detection or reporting.

---

### Optimizing natural fertility: a committee opinion [^a88a4233]. Fertility and Sterility (2013). Medium credibility.

Frequency of intercourse — effects of abstinence duration on semen parameters: Whereas abstinence intervals greater than 5 days may adversely affect sperm counts, abstinence intervals as short as 2 days are associated with normal sperm densities. A widely held misconception is that frequent ejaculations decrease male fertility. In men with normal semen quality, sperm concentrations and motility remained normal, even with daily ejaculation, and in men with oligozoospermia, sperm concentration and motility may be highest with daily ejaculation. After longer abstinence intervals of 10 days or more, semen parameters begin to deteriorate.

---

### Pharmacovigilance and drug utilization review… [^3ead649a]. FDA (2025). Medium credibility.

Pediatric Labeling Approval Date: 20-Dec-2013 Application Type/Number: NDA 022145, NDA 203045, NDA 205786 Applicant/Sponsor: Merck Sharp & Dohme OSE RCM #: 2016–350 This document contains proprietary drug use data obtained by FDA under contract. The drug use data/information cannot be released to the public/non-FDA personnel without contractor approval obtained through the FDA/CDER Office of Surveillance and Epidemiology. Reference ID:

3957045. March 2013 through February 2016, aggregated. March 1, 2013
- February 29, 2016 Patient Count† N Share % Total Patients 133, 730 100% 0–16 years 1, 958 1% 0–1 year 26 1%. 2–16 years 1, 933 99% 17+ years 131, 464 98% Unknown age 944 1% Source: IMS Health, Total Patient Tracker. Mar 2013 – Feb 2016. Extracted Apr
2016. **File**: TPT 2016–350 raltegravir Isentress by age Mar2013-Feb2016. xlsx. * Patient age groups are inclusive of all patients up to the day before their next birthday.

For example, patients aged 0–16 years include patients less than 17 years of age. † Unique patient counts may not be added due to the possibility of double counting those patients aging during the. 10  FAERS case #10169889–1, USA. Literature case report: Schwenk H, Ramirez-Avila L, Sheu SH, et al. Progressive., conjugated bilirubin 76 umol/L, ALT 230 U/L, AST 366 U/L, GGT 170 U/L, LDH 538 U/L, CRP 54 mg/L.