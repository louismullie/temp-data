# Medical Question & Answer

**Sample ID**: 67d509f4-ebed-d838-3bff-7f5e34521917
**Dataset Index**: 356486

---

## Question

3. The range of the function y = √[7 − x] + √[9 + x] is

---

## Answer

> Let's see… What do we have here? The user is asking about finding the range of the function y equals the square root of 7 minus x plus the square root of 9 plus x. Let's break this down step-by-step. First, I need to think about the domain where both square roots are real. Then, I should verify the function's behavior at the endpoints of that domain. Next, I will check for critical points using calculus to see if there is a maximum or minimum in the interior. Finally, I will combine these findings to state the range and confirm it with a quick sanity check.

> Let me first confirm the domain, because square roots require non-negative arguments. For the first term, 7 minus x must be greater than or equal to 0, so x is less than or equal to 7. For the second term, 9 plus x must be greater than or equal to 0, so x is greater than or equal to negative 9. Combining these, the domain is the closed interval from negative 9 to 7, inclusive, which I should double-check before proceeding.

> Next, I should review the function's values at the endpoints to anchor the range. When x equals negative 9, y equals the square root of 7 minus negative 9 plus the square root of 9 plus negative 9, which simplifies to the square root of 16 plus the square root of 0, giving 4. When x equals 7, y equals the square root of 7 minus 7 plus the square root of 9 plus 7, which simplifies to the square root of 0 plus the square root of 16, again giving 4. So both endpoints yield y equals 4, which suggests the minimum might be 4, but wait, let me verify whether there is a lower value inside the interval before concluding that.

> I will now examine the derivative to find critical points. Let me compute y prime carefully: the derivative of the square root of 7 minus x is negative 1 over 2 times the square root of 7 minus x, and the derivative of the square root of 9 plus x is 1 over 2 times the square root of 9 plus x. Setting y prime equal to zero gives negative 1 over 2 times the square root of 7 minus x plus 1 over 2 times the square root of 9 plus x equals zero, which simplifies to the square root of 9 plus x equals the square root of 7 minus x. Squaring both sides yields 9 plus x equals 7 minus x, so 2x equals negative 2, and thus x equals negative 1. I should confirm that this critical point lies within the domain, which it does, since negative 1 is between negative 9 and 7.

> Now I need to evaluate the function at the critical point to see if it is a maximum or minimum. Plugging x equals negative 1 into y gives the square root of 7 minus negative 1 plus the square root of 9 plus negative 1, which is the square root of 8 plus the square root of 8, or 2 times the square root of 8. Simplifying, that is 2 times 2 times the square root of 2, which equals 4 times the square root of 2. Numerically, 4 times the square root of 2 is about 5.66, which is greater than 4, so this is a local maximum. Hold on, I should verify the second derivative test to be sure, but given the endpoint values and this interior value, the pattern is consistent with a maximum at x equals negative 1 and minima at the endpoints.

> Let me consider the function's concavity and behavior across the interval to ensure there are no other extrema. The derivative y prime is negative when x is less than negative 1 and positive when x is greater than negative 1, which means the function decreases from x equals negative 9 to x equals negative 1 and increases from x equals negative 1 to x equals 7, confirming that x equals negative 1 is indeed a global minimum wait, let me correct that, because the derivative analysis shows the function decreases to the left of negative 1 and increases to the right, so x equals negative 1 is a global maximum, not a minimum, and the minimum values occur at the endpoints where y equals 4. I should double-check that I stated that correctly, and yes, the minimum is 4 and the maximum is 4 times the square root of 2.

> Putting this together, the range spans from the minimum value of 4 at the endpoints to the maximum value of 4 times the square root of 2 at x equals negative 1. Therefore, the range is the closed interval from 4 to 4 times the square root of 2, which I can write as [4, 4√2]. To sanity-check, 4√2 is approximately 5.66, and since both square root terms are non-negative and the function is continuous on the closed interval, the Intermediate Value Theorem guarantees that every value between 4 and 4√2 is attained, which supports this conclusion.

---

The range of the function (y = √[7 − x] + √[9 + x]) is ([4, 4√2]). This is found by first determining the domain ([-9, 7]) from the square roots, then evaluating the function at the endpoints to get (y = 4) at both (x = −9) and (x = 7). Next, taking the derivative and setting it to zero yields a critical point at (x = −1), where (y = 4√2). Since the function is continuous on a closed interval, the minimum is 4 and the maximum is 4√2, so the range is ([4, 4√2]).

---

## Step 1: determine the domain

For the function to be real, the expressions under the square roots must be non-negative:

- (7 − x ≥ 0 ⇒ x ≤ 7)
- (9 + x ≥ 0 ⇒ x ≥ −9)

Thus, the domain is ([-9, 7]).

---

## Step 2: evaluate the function at the endpoints

At (x = −9): (y = √[7 − (−9)] + √[9 + (−9)] = √16 + √0 = 4 + 0 = 4).

At (x = 7): (y = √[7–7] + √[9 + 7] = √0 + √16 = 0 + 4 = 4).

So, at both endpoints, (y = 4).

---

## Step 3: find critical points

Differentiate (y) with respect to (x):

y' = d/dx (√[7 − x] + √[9 + x]) = −[1/(2√[7 − x])] + [1/(2√[9 + x])]

Set (y' = 0):

−[1/(2√[7 − x])] + [1/(2√[9 + x])] = 0 ⇒ 1/√[9 + x] = 1/√[7 − x]

Squaring both sides:

9 + x = 7 − x ⇒ 2x = −2 ⇒ x = −1

This critical point lies within the domain ([-9, 7]).

---

## Step 4: evaluate the function at the critical point

At (x = -1):

y = √[7 − (−1)] + √[9 + (−1)] = √8 + √8 = 2√8 = 4√2

---

## Step 5: determine the range

The function is continuous on the closed interval ([-9, 7]), so the extreme values occur at the endpoints or at the critical point. The minimum value is 4 (at (x = −9) and (x = 7)), and the maximum value is 4√2 (at (x = −1)).

Therefore, the range of the function is ([4, 4√2]).

---

The range of the function (y = √[7 − x] + √[9 + x]) is ([4, 4√2]).

---

## References

### Density-functional fluctuation theory of crowds [^1179K4wT]. Nature Communications (2018). Medium credibility.

Discretization H = Σ b

To arrive at the discretization described in the text, it is important to note that the density n (x) appearing in the probability functional P [n (x)] corresponds to the fluctuating crowd density, as opposed to the average density n ave (x). As such, in practice, this density must be described in terms of the discrete locations x a of all agents a in the crowd at any give time. The most natural description for the associated density operator iswhere δ (σ) (x, x a) is a function describing the range over which the presence of an agent at x a contributes to the density n (x) at point x. To conserve number of agents, this function must integrate to unity. The analysis carried out in the text divides space into bins b of area A b, and estimates the density in each bin as n = N b / A b where N b corresponds to the total number of agents in bin b. This definition sets the range function as

To capture relevant variations in vexation and density, the bins cannot be selected so large that these quantities vary significantly across each bin. Alternately, to avoid missing the effects of nearby agents, the bins cannot be selected to be smaller than the agent's interaction range.

Finally, combining equations 5, 7, and 8, yieldswhereand.

Bin occupation probability distributions P b (N)

To arrive at the final discrete probability expression in the text, there are now two routes. One can directly insert Eq. 9 above into Eq. 2 from the main text, or one can employ Eq. 9 directly to compute Δ H to determine the probabilities for moves. In the latter case, the predicted probability distribution becomes exact so long as we interpret f ′(n) in the main text at points x ′ and x to represent forward and reverse finite difference derivativesand, respectively, where Δ ≡ 1/ A b. Finally, because the Boltzmann factor above gives probabilities for individual arrangements of agents among bins, we must account for the multiple ways to realize a set of bin counts { N b } by permuting individuals among the bins. Multiplying by the combinatorial factor N tot !/(N 1 !… N b !…), we findwhere Z is a normalization factor.

---

### Artificial-intelligence-guided design of ordered gas diffusion layers for high-performing fuel cells via Bayesian machine learning [^114GG6CE]. Nature Communications (2025). High credibility.

Algorithm 1

Bayesian Optimization

1 Input: Objective function f, domain χ, initial observation D 0

2 Output: The best input x * ∈ χ and its function values f (x *)

3 Initialize Gaussian Process (GP) with prior p (f)

4 Define acquisition function α (x; D)

5 D ← D 0

6 for t = 1,2, …, T do

7 Select x t ∈ χ by optimizing the acquisition function:

8

9 Evaluate the objective function y t = f (x t) by multi-scale modeling:

10 1. Pore-scale calculation of GDLs

11 2. Cell-scale simulation of fuel cells

12 Augment the data: D ← D ∪ {(x t, y t)}

13 Update the GP model with the new data D

14 end for

15based on the GP posterior

16 return x *, f (x)

BO is used to maximize the LCD with the objective function by multi-scale modeling, within the domain (χ) of porosity, pore radius, and fiber orientation. During each iteration in the BO algorithm, we employed the acquisition function for selecting the next x t, evaluated the objective function and updated the Gaussian process model, to progressively approach the global optimum for complex fuel cell simulations. Figure 1c, d shows the convergence of optimum LCD within 40 iterations, where the optimal structures mainly lie in the regions with porosity in the range of 0.7–0.9, fiber radius of 3–6 µm, and a fiber orientation near 0°, representing a highly aligned fiber structure (Fig. 1e) with the reconstructed structure as a representative. However, fabricating aligned carbon fiber electrodes remains challenging, as fiber-fabricating techniques like wet-spinning and melt-spinning typically result in fibers with diameters much larger than 10 μm and poorly adjusted fiber orientation, and traditional electrospinning technique is commonly used for fabricating nanoscale fibers that are randomly dispersed.

---

### Glycemic deviation index: a novel method of integrating glycemic numerical value and variability [^116HhUdh]. BMC Endocrine Disorders (2021). Medium credibility.

The functional form of y 1 should meet the exponential increase of the x value at a certain symmetrical point. To ensure that y 1 satisfies the following two criteria: 1) the value range of x is symmetrical around a certain point, 2) the target range of x is also symmetrical around this point; the following equation was constructed:

The solution calculated using MATLAB was: a = − 0.801, b = 0.672. Inserting a and b into eq. 1–1 provided:

Furthermore, to adjust the value range to occupy the range of [0,10], and to meet the normal range at [0,1] synchronously, the following formula was introduced (Fig. 1 a):

Fig. 1
Functional images of MGI (a) and SDGI (b). The definitional domain of MGI is [2.8, 33.3], while the definitional domain of SDGI is [0, 7.4]. The value domain of both functions is [0, 10]

Next, to reduce the "neutralization" of hyperglycemia on hypoglycemia, MGI was calculated in two steps. MG 1 and MG 2 represent the average of blood glucose in the hypoglycemic and non-hypoglycemic periods separately, respectively substituted into formula 1–3 for the calculation to be performed. The weighting coefficient "c" was used to indicate duration in the hypoglycemic range (percentage of readings below 3.9 mmol/L per 72 h). The final formula of MGI was as follows:

The target range of SDG in this study was 0–1.4 mmol/L, based on previous studies, and the value range was defined as 0–7.4 mmol/L. The degree of deviation of glycemic variability was represented as the standard deviation of glucose index (SDGI). The SDGI value was exponentially augmented after SDG crossed over the normal threshold (Fig. 1 b). SDGI was listed as follows, where "z" represented the SDG, while "e" and "f" were constant parameters:

---

### Recovery after stroke: not so proportional after all? [^113pT5WW]. Brain (2019). Medium credibility.

Several recent studies report interquartile ranges, rather than standard deviations, for their fitter patients' baselines and outcomes. Accepting some room for error, we can also estimate σ Y /σ X from those interquartile ranges. In one case, r(X,Δ) = −0.97 and σ Y /σ X = 0.158, while in another, σ Y /σ X = 0.438 and r(X,Δ) ≈ −0.88. In both cases, Equation 1 implies that r(X,Δ) would be at least as strong as that reported, regardless of r(X, Y): these reported r(X,Δ) do not tell us how predictable outcomes actually were, given baseline scores.

Many studies in this literature only relate baselines to recovery through multivariable models; in these studies, we cannot demonstrate confounds directly with Equation 1. Nevertheless, these studies are also probably confounded, because any inflation in one variable's effect size will inflate the multivariable model's effect size as well. As discussed in the previous section, empirical studies of recovery after stroke should tend to encourage small σ Y /σ X, whether or not recovery is proportional. Consequently, the null hypothesis will rarely be that r(X,Δ) ≈ 0. For example, in the only multivariable modelling study, which reports IQRs for its fitter-patients' baselines and outcomes, σ Y /σ X ≈ 0.48, which implies that the weakest r(X,Δ) was −0.88, for any positive value of r(X, Y).

Finally, while r(X,Δ) can be misleading if it is extreme relative to r(X, Y), the reverse is also true. One study in this literature, which uses outcomes as the dependent variable rather than recovery, reports that r(X, Y) ≈ 0.8 and σ Y /σ X = 1.2 in their 'combined' group of 76 patients. By Equation 1, r(X,Δ) = −0.05: i.e. recovery was uncorrelated with baseline scores. These authors only reported proportional recovery in a subsample of their patients (but not the information we need to re-examine that claim), but their full sample seems better described by constant recovery (as in Fig. 3 B).

---

### Recommendations for a standardized pulmonary function report. An official American Thoracic Society technical statement [^112LaQt8]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Selecting and reporting reference values — procedural guidance states that PFT laboratories must select appropriate reference values generated from high-quality data from a large sample of healthy asymptomatic individuals who have never smoked or had other respiratory illness or significant exposures, reports should identify the source of reference values, any change in selected equations should be noted and prior percent predicted values recalculated if possible, pediatric–adult equation discontinuity is discouraged, and extrapolation beyond the age range should not be done during growth, increases uncertainty in the elderly, and must be noted in technician comments.

---

### Recommendations for a standardized pulmonary function report. An official American Thoracic Society technical statement [^113fNQHv]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Using reference data in interpretation — ATS/ERS recommend using limits of normal, with thresholds set to the fifth percentile (z-score −1.645) so that 95% of a healthy population falls within the normal range; use the upper limit where appropriate such as for lung volumes.

---

### Recovery after stroke: not so proportional after all? [^116Q32Lh]. Brain (2019). Medium credibility.

Spurious r(X,Δ) are likely when σ Y /σ X is small

For any X and Y, it can be shown that:

A formal proof of Equation 1 is provided in the Supplementary material, Appendix A [proposition 4 and theorem 1; also see]; its consequence is that r(X,Δ) is a function of r(X, Y) and σ Y /σ X. To illustrate that function, we performed a series of simulations (Supplementary material, Appendix B) in which r(X, Y) and σ Y /σ X were varied independently. Figure 2 illustrates the results: a surface relating r(X,Δ) to r(X, Y) and σ Y /σ X. Figure 3 shows example recovery data at six points of interest on that surface.

Figure 2
The relationship between r(X, Y), r(X,Δ) and σ Y /σ X. Note that the x -axis is log-transformed to ensure symmetry around 1; when X and Y are equally variable, log(σ Y /σ X) = 0. Supplementary material, proposition 7 in Appendix A, provides a justification for unambiguously using a ratio of standard deviations in this figure, rather than σ Y and σ X as separate axes. The two major regimes of Equation 1 are also marked in red. In Regime 1, Y is more variable than X, so contributes more variance to Δ, and r(X,Δ) ≈ r(X, Y). In Regime 2, X is more variable than Y, so X contributes more variance to Δ, and r(X,Δ) ≈ r(X,−X) (i.e. −1). The transition between the two regimes, when the variability ratio is not dramatically skewed either way, also allows for spurious r(X,Δ). For the purposes of illustration, the figure also highlights six points of interest on the surface, marked A–F; examples of simulated recovery data corresponding to these points are provided in Fig. 3.

---

### Recovery after stroke: not so proportional after all? [^116G78G8]. Brain (2019). Medium credibility.

σ Y /σ X may be small, whether or not recovery is proportional

Proportional recovery implies small σ Y /σ X, but small σ Y /σ X does not imply proportional recovery; for example, constant recovery with ceiling effects will produce the same result. To illustrate this, we ran 1000 simulations in which: (i) 1000 baseline scores are drawn randomly with uniform probability from the range 0–65 (i.e. impaired on the 66-point Fugl-Meyer upper-extremity scale); (ii) outcome scores were calculated as the baseline scores plus half the scale's range (33); and (iii) outcome scores greater than 66 were set to 66 (i.e. a hard ceiling). Mean r(X, Y) and r(X,Δ) were calculated both before and after shuffling the outcomes data for each simulation. After shuffling, r(X, Y) ≈ 0 and r(X,Δ) = −0.88: ceiling effects make σ Y /σ X small enough to encourage spurious r(X,Δ). And just as importantly, before shuffling, r(X, Y) = 0.89 and r(X,Δ) = −0.90: even when r(X,Δ) is not spurious [because r(X, Y) is similarly strong], we cannot conclude that recovery is really proportional.

---

### Size limits the sensitivity of kinetic schemes [^113ujoyg]. Nature Communications (2023). High credibility.

A different measure of sensitivity — the amplification of a fold-change in the input — provides a solution to this problem. Suppose that for some value x 0 of the input parameter x, scaling by a factor a scales the output by b, so f (a x 0) = b f (x 0). Then the quotientcan be thought of as a discrete approximation of the derivative defining the logarithmic sensitivity. And if f (x) is differentiable everywhere, then by the mean value theorem, there must be a value x * of x for whichThis means that careful measurement of any two points on the input-output curve (x versus f (x)) witnesses the (local, infinitesimal) logarithmic sensitivity somewhere. Importantly — unlike in the case of fitting to a Hill function — if error in the measurements is very low, then they are also telling us the derivative for some value of x very accurately.

Equation (25) leads us to another common definition of the effective Hill coefficient:where S 0.9 and S 0.1 are the values of the input variable (in our case, x) required to get 90% and 10% (respectively) of the maximum value of the output variable (in our case, f (x)). Note that (26) is like (25) with a = S 0.9 / S 0.1 and b = 9. It implies that somewhere between S 0.1 and S 0.9 there is a logarithmic sensitivity of H eff /2.

There is yet another common definition, specific to models of binding. Suppose x is the concentration of a ligand and 〈 n b 〉(x) is the expected number of sites bound by a ligand out of a total of n possible binding sites. It is common then, to takeor to report, as the Hill coefficient, the slope of a line fitted to x versusdata on a log-log plot.

---

### A practical guide for understanding confidence intervals and P values [^115vcziB]. Otolaryngology — Head and Neck Surgery (2009). Low credibility.

The 95 percent confidence interval about the mean demarcates the range of values in which the mean would fall if many samples from the universal parent population were taken. In other words, if the same observation, experiment, or trial were done over and over with a different sample of subjects, but with the same characteristics as the original sample, 95 percent of the means from those repeated measures would fall within this range. This gives a measure of how confident we are in the original mean. It tells us not only whether the results are statistically significant because the CI falls totally on one side or the other of the no difference marker (0 if continuous variables; 1 if proportions), but also the actual values so that we might determine if the data seem clinically important. In contrast, the P value tells us only whether the results are statistically significant, without translating that information into values relative to the variable that was measured. Consequently, the CI is a better choice to describe the results of observations, experiments, or trials.

---

### Recommendations for a standardized pulmonary function report. An official American Thoracic Society technical statement [^117LSRwm]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

American Thoracic Society pulmonary function report — core spirometry reporting elements emphasize that a uniform format for the presentation of PFT results in reports to users and in the medical record can reduce potential miscommunication or misunderstanding, and only information with validated clinical application should be included. The normal limit(s) of each test parameter should be displayed, and, consistent with other laboratory values, the measured value should be shown before reference values, ranges, or normal limits. Report and/or display of the displacement of the result from a predicted value in standard deviation units (z-score) can help in understanding abnormality. For spirometry, many parameters can be calculated but most do not add clinical utility and should not be routinely reported; only FVC, FEV1, and FEV1/FVC need be routinely reported. Measurement of slow VC and calculation of FEV1/VC are a useful adjunct in patients with suspected airflow obstruction. Reporting FEV1/FVC (or FEV1/VC) as a decimal fraction, and not reporting it as a percentage of the predicted value for this ratio, will help to minimize miscommunication. The nitrogen washout plot for multibreath tests and the tracings for plethysmograph tests can be shown graphically to aid quality assessment.

---

### Recommendations for a standardized pulmonary function report. An official American Thoracic Society technical statement [^116VXEZR]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Spirometry reporting specifies that numerical values are given for the FEV1, the FVC, and the FEV1/FVC ratio; the latter should be reported as a decimal fraction and the space for percent predicted value left blank, and if bronchodilators are given the LLN column need not be repeated with absolute and percent change given only for FEV1 and FVC. Other numerical values such as the forced inspiratory flow at 75% of FVC (FEF75%) and FEF25–75% are not recommended for routine use. Graph requirements include that for the volume–time curve the volume scale should be at least 10 mm/L, the time scale at least 20 mm/s, and 1 second prior to the start of expiration should be displayed; on the flow–volume plot the flow display should be at least 5 l/min/L/s, and the ratio of flow to volume should be 2 L/s to 1 L, and linear and log scales where values are plotted as z-scores relative to the predicted value (z = 0) give an intuitive sense of severity.

---

### Predicting metabolic adaptation from networks of mutational paths [^117SBoVW]. Nature Communications (2017). Medium credibility.

Centrality convergence (error) for incomplete networks

To characterize how a statistic of a centrality converges to its value when the network is complete as a function of the degree of a network's completion, we used a scale-free measure of error. Let x (c, Δ S max) be the value of the statistic for networks at completion c (fraction of edges of the complete network included in the incomplete network) and maximum size of mutation Δ S max. Then letbe the normalized value of x (c, Δ S max). The minimum and maximum are taken over all values of completion, c, and over all samples and so establish the observed range of values of the statistic. This normalization ensures that y (c, Δ S max) ∈ [0, 1] for all samples. We defined the normalized error for this sample process aswhere y (1, Δ S max) is the normalized value of the statistic when the entire graph is discovered (i.e. when graph completion is 1 and all edges are known). We define the feature convergence error as the mean of e (c, Δ S max) taken over all samples in the data set of partial graphs. We repeat this procedure for all twelve centrality statistics (Supplementary Fig. 9).

---

### Recovery after stroke: not so proportional after all? [^1129zvs9]. Brain (2019). Medium credibility.

Point A corresponds to the canonical example of spurious r(X,Δ), introduced in the last section: i.e. σ Y /σ X ≈ 1 and r(X, Y) ≈ 0, but r(X,Δ) ≈ −0.71 (Fig. 3 A). At Point B, σ Y /σ X ≈ 1 and r(X, Y) is strong, so recovery is approximately constant (Fig. 3 B) and r(X,Δ) ≈ 0, consistent with the view that strong r(X, Y) curtail spurious r(X,Δ). However, the situation is more complex when σ Y /σ X is more skewed.

When σ Y /σ X is large, Y contributes more variance to Y − X (Δ), and r(X,Δ) ≈ r(X, Y); this is Regime 1. Points C and D illustrate the convergence (Fig. 3 C and D). By contrast, when σ Y /σ X is small, X contributes more variance to Y − X, and r(X,Δ) ≈ r(X, −X): i.e. −1 (Supplementary material, Appendix A, theorem 2); this is Regime 2, where the confound emerges. Point E, near Regime 2, corresponds to data in which all patients recover proportionally (Δ = 70% of lost function; Fig. 2 E). Here, σ Y /σ X is already small enough (0.3) to be dangerous: after randomly shuffling Y, r(X, Y) ≈ 0, but r(X,Δ) is almost unaffected (Point F, and Fig. 3 F). In other words, if even the proportional recovery rule is approximately right, empirical data may enter territory, on the surface in Fig. 2, where over-optimistic r(X,Δ) are likely.

---

### How dieting might make some fatter: modeling weight cycling toward obesity from a perspective of body composition autoregulation [^112JCjDv]. International Journal of Obesity (2020). Medium credibility.

Two exponential fits are shown: a generalized linear model (GLM) with 95% confidence intervals (solid line), and a linear model (LM), with R 2 values (dotted line). The four figures correspond to the four methods presented in the main text for computing the value FAT END at the final time END at which the subjects would have completely recovered their initial FFM. The two red squares correspond to the US Army Ranger data points presented in the section on 'Applications of the Model'.

Their exact values depend on the method used for computing the values FAT END when the FFM has been completely recovered and on the type of statistical regression used. As explained previously, we consider four methods for computing FAT END. In Fig. 4, we show the fits obtained from a generalized linear model (GLM) with 95% confidence intervals (solid line) and from a linear model (LM), with R 2 values (dotted line). Following the performance of diagnostics tests to analyze the residuals, it is found that the GLM satisfies the major assumptions of regression analysis better than the LM, especially concerning the normality assumption of the error terms. Nevertheless, as shown in Fig. 4, both models give curves that are close to each other. In simple terms, the main difference between the above LM and GLM approaches to estimate the best parameter values for the constants a and b is the way the error term is handled. In the LM method, we write y = a × e^(b x) × ε, where ε is a random error variable that follows a log-normal distribution with parameters μ = 0 and σ². Taking the logarithm on both sides of the equality gives, which corresponds to the linear model, where, x ′ = x, β₁ = b andis a random error variable that follows a normal distribution with mean μ = 0 and variance σ². Note that in the LM method, the error term ε is multiplied with the exponential function y = a × e^(b x). In the GLM method, the error term is instead added to the exponential function. Formally, we write y = a × e^(b x) + ε, where ε is a random error variable that follows a normal distribution with mean 0 and variance σ². As a consequence, the GLM admits the possibility of the value y = 0 (which is excluded in LM) and assumes that the variance V (y) is constant, while the LM method assumes that V (y) varies with x. Finally, note that the LM method admits exact analytical solutions for the model parameters a and b, while the GLM method requires numerical optimization algorithms to find the best values (the maximum likelihood estimates) for a and b.

---

### Recommendations for a standardized pulmonary function report. An official American Thoracic Society technical statement [^111sXPXL]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Pulmonary function reporting conclusions — The ATS PFT Committee believes that wide adoption of the formats presented above and their underlying principles by equipment manufacturers and pulmonary function laboratories can improve the interpretation, communication, and understanding of test results. Limiting the number of parameters reported and showing the LLN next to the measured value should improve interpretive accuracy, particularly for those less experienced, and reserving the word percent to percent predicted value should reduce errors. Showing the measured values relative to the normal distribution in a simple linear graphic (with or without reporting a z-score) can enhance understanding of the result. Quality review of PFTs needs to move beyond "did, or did not, meet ATS standards", but a grading system is most helpful if the grades have a common meaning; therefore adoption of a uniform system is desirable.

---

### Recovery after stroke: not so proportional after all? [^115R6Zns]. Brain (2019). Medium credibility.

Figure 3
Exemplar points on the surface in Fig. 2. Simulated recovery data, corresponding to the points A–F marked on the surface in Fig. 1. (A) Baselines and outcomes are entirely independent [r(X, Y) = 0], yet r(X,Δ) is relatively strong; this is the canonical example of mathematical coupling, first introduced by. (B) Recovery is constant with minimal noise, so baselines and outcomes are equally variable (σ Y /σ X ≈ 1) and recovery is unrelated to baseline scores (r(X, Δ) ≈ 0). (C and D) Outcomes are more variable than baselines (σ Y /σ X ≈ 5), and r(X,Δ) converges to r(X, Y). (E) Recovery is 70% of lost function, so outcomes are less variable than baselines (σ Y /σ X ≈ 0.3); even with shuffled outcomes data (F) baselines and recovery still appear to be strongly correlated.

---

### Limits on the computational expressivity of non-equilibrium biophysical processes [^114C4gev]. Nature Communications (2025). High credibility.

To explain this difference, in the Supplementary Information we use the 1D rational polynomial form of the matrix-tree expression, Equation (3), to maximize ∂ π S /∂ F with respect to the learnable coefficientsand(treated for now as free and independent). The multi-index μ used in Equation (2) simplifies here to the single index m. We show thatwhere the derivative is evaluated at the location of the decision boundary F 0 and M R = M max − M min is the range in exponential powers of e^(F/2) among all directed spanning trees. This result shows that the sharpness of the classifier is fundamentally limited by the structure of the network. A tighter approximation to the bound can be obtained by replacing M R with, which is the range in exponential powers among only the directed spanning trees rooted on the output nodes. We further explain in the Supplementary Information that the directed spanning trees of the parallelly extended push-pull networks preventfrom scaling with n, whereas the spanning trees for serially extended networks allow, which enables increasingly sharp transitions as more edges are added. In serially extended networks, the structure allows this rangeto grow with the number of added edges n, leading to increasingly sharp transitions. In contrast, parallelly extended networks constrain all output-rooted spanning trees to use the same number of driven edges, keepingfixed and preventing sharper transitions.

Finally, even when the bound in Eq. (8) is large it may not be achieved in practice (see Supplementary Information for details). Saturating the bound requires that the coefficientsbe concentrated on trees with either the smallest or largest possible net input drive. However, in networks such as those with a ladder-like architecture, many spanning trees make intermediate contributions, and equality constraints among the functionsprevent the network from assigning large weights solely to the extremal trees. As shown in Fig. 4 B, overlapping spanning trees entangle the coefficients and reduce the effective degrees of freedom. This structural limitation suggests that sharp decision boundaries may be inherently inaccessible in densely interconnected biochemical networks. This finding resonates with, though is technically distinct from, recent results in refs.

---

### Closed-loop two-photon functional imaging in a freely moving animal [^116wNyxX]. Nature Communications (2025). High credibility.

Imaging settings

The scan amplitudes and frequencies were determined by the following user inputs: x scan range, a x (μm), aspect ratio, r x y, min dwell time τ m i n (μs), target pixel size, Δ x (μm), and maximum frequency (f m a x). From these, the target x- and y- scan patterns were calculated as follows:

Table 1 details the specific choices of imaging parameters along with the τ r a s t e r, the characteristic raster volume time (Fig. S5) for the corresponding figures.

Table 1
Imaging parameters

Figure: which figure in the text or supplement is referenced — when multiple experiments appear in a figure, these are identified separately by genotype and number of strides; x-freq the frequency of the triangle wave oscillation of the imaging x-galvo (Eq. (1)); y-freq the frequency of the triangle wave oscillation of the imaging y-galvo (Eq. (2)); x-amplitude — the programmed amplitude of the x-galvo triangle wave; y-amplitude the programmed amplitude of the y-galvo triangle wave; Δ x: the target pixel size; τ r a s t e r, the characteristic time to sample the volume using a bi-directional raster scan.

On the FPGA, the triangle wave was incremented by incrementing or decrementing the target locations by Δ x at fixed intervals set by an on-board 16 bit counter. The counter values were found bywhere c x and c y are the counter roll-over values, f f p g a = 80 MHz is the clock frequency of the loop containing the scan generation code, and p n (x) denotes the prime number nearest to x. Setting the counter values to prime numbers kept the scan pattern from repeating more often than required by the discrete logic of the FPGA.

For registration and display purposes, volumes and their projections were reconstructed using a 1 × 1 × 2 μm x, y, z voxel and discretized into time bins of 1/2 the characteristic raster time (See section "Creating volumetric images"); for the main display figures and movies, this results in a time step of 94 ms.

Temporal signals (e.g. Fig. 2 b–e) were lowpassed by convolving with a Gaussian filter of standard deviation of one time step (= 1/2 characteristic raster time).

---

### Identifying domains of applicability of machine learning models for materials science [^116Vjopm]. Nature Communications (2020). High credibility.

Methods

MBTR

The MBTR representation space X can vary depending on the employed many-body order (e.g. interatomic distances for a two-body model, and/or angles for a two- and/or three-body model, and/or torsions for up to four-body models). The results reported herein are calculated using a representation consisting of broadened histograms of element counts (one-body terms) and pairwise inverse interatomic distances (two-body terms), one for each unique pair of elements in the structure (i.e. for this dataset: Al–Al, Al–Ga, Al–In, Al–O, Ga–Ga, Ga–In, Ga–O, In–In, In–O, and O–O). These are generated according to:where a normal distribution function is centered at each inverse distance between pairs of atoms (1/ r i, j) to ensure smoothness of the representation. The functiondampens contributions from atoms separated by large distances and is defined as. The MBTR representation was generated using QMMLpack.

SOAP

The SOAP representation space is constructed by transforming pairwise atomic distances as overlapping densities of neighboring atoms and expanding the resulting density in terms of radial and spherical harmonics basis functions. The local density is modeled through a sum of gaussian distributions on each of the atomic neighbors j of atom i :where j ranges over neighbors within a specific cut-off radius (r cut) relative to i, where the cut-off function w SOAP is defined as:The density ρ i (r) is then expanded in terms of spherical harmonics Y k, m (r /∣ r ∣) and orthogonal radial functions g n (∣ r ∣):

The number of coefficients c n, k, m is given by the choice of basis set expansion values. Rotationally invariant features are then computed from the coefficients of the expansion and averaged to create a single per-structure representation, forming the input space X. A real-space radial cutoff of r cut = 10 Å and ϵ b = 0.5 Å are used in this work. The SOAP representation was computed with the QUIPPY package available at.

---

### Area under the expiratory flow-volume curve: predicted values by regression and deep learning methods and recommendations for clinical practice [^116uQKuS]. BMJ Open Respiratory Research (2021). High credibility.

Table 1
Simple linear regression model without interactions for predicting square root AEX-FV

Table 2
Linear regression model with interactions for predicting square root AEX-FV

Further attempts to improve the models' performance by using various response variable distributions (eg, gamma or lognormal transformations) and using optimisation (generalised regression) techniques such as ridge regression, elastic net, lasso and double lasso, with or without adaptive features, did not lead to major improvements in generalised R 2 of the validation set (table 3).

Table 3
Model Comparisons between linear regression (standard least squares) and various techniques of optimisation (generalised regression) for predicting square root AEX-FV

To counteract the issue of non-normal distributions and collinearities between variables (assumed by default in regression to be completely independent), we also developed a neural network model using the same variables (figure 3A–C). We ran several neural network architectures, and the optimised, fastest models were based on a deep learning architecture with one output (Sqrt AEX-FV, maintained as transformed for purpose of comparability with the other models), same five inputs (PEF, FVC, FEF 25, FEF 50 and FEF 75) and two hidden layers, each with three sigmoid [(e^(2x) − 1)/(e^(2x) + 1)], 3 gaussian ([1/e^(x·x)]) and three linear activation function nodes. We used transformed covariates (with either Johnson Su or Johnson Sb distributions), a robust fit method that uses absolute deviations instead of least squares (in order to minimise the effects of outliers), 10 additional tours for the fitting process, and a squared penalty function to avoid overfitting (the latter being preferred when all X variables are expected to contribute to the predictive ability of the model). Generalised R 2 in this instance simplifies the traditional R 2 for continuous normal responses in the standard least squares setting, and is also known as the Nagelkerke or Craig-Uhler R 2, which is a normalised version of Cox and Snell's pseudo R 2. The root mean square error (RMSE) is equivalent to the SD, while Mean Absolute Deviance is represented by the average of the absolute differences between the predicted response and the actual Y variable (figure 3A). The SSE (figure 3A) represents the sum of squares error, while the main effects of the variables is based on the k nearest neighbours' method of resampling dependent variables. Overall, the R 2 was very high (0.995 in both the training and the validation set), while the RMSE was even further reduced to 0.070 (figure 3A). The prediction profiler in figure 3B shows the non-linear relationship between Sqrt AEX-FV and the X variables, while the contour profilers show the response surfaces for various combination of variables versus Sqrt AEX-FV (figure 3B). Another illustration of the neural network model's performance is shown in figure 3C, which showcases the high correlations between Sqrt AEX-FV observed and predicted values, and the residual values plotted against the predicted Sqrt AEX-FV in the training and validation sets, respectively. The figure 3C also shows that the residuals do not increase at higher response variable values (visual inspection confirming that the homoscedasticity condition is met).

---

### Continuous growth reference from 24th week of gestation to 24 months by gender [^111Pr1ED]. BMC Pediatrics (2008). Low credibility.

Appendix

Assuming normality, to estimate the variance, skewness and kurtosis of the population based on the cases in the range of μ ± δσ in a sample for certain values of δ > 0.

Without lost of generality, let μ = 0. Let, where φ (x; σ) is the normal probability distribution function with mean 0 and variance σ 2. It can be deduced that

where Φ (x) is the standard normal cumulative distribution function. Letand. It can be shown that

Letand. It can be shown that

Let p = 2Φ(δ) - 1 be the proportion of observations in the range of μ ± δσ under a normal distribution. Considering a sample of n observations, x 1. x n. Let q 1 be the (1 - p)/2-th quantile and q 2 be the (1 - (1 - p)/2)-th quantile and X r = { x i: q 1 ≤ x i ≤ q 2 } be the reduced sample of size m. An empirical estimate of, j = 2, 3, 4, is given as

The general sample variance, the skewness and the kurtosis are given as

and

respectively. Let, Skew r and Kurt r be the corresponding statistics calculated based on the reduced sample. Substituting – into –, we have the statistics for the full dataset evaluated based on the reduced sample as

and

---

### Identifying domains of applicability of machine learning models for materials science [^113w9fho]. Nature Communications (2020). High credibility.

An illustrative example

Before describing the details of DA identification and its integration into the ML process, let us illustrate the concept and its utility via a synthetic example (see Fig. 1). We consider a simple two-dimensional representation consisting of independent features x 1 and x 2 that are each distributed according to a normal distribution with mean 0 and variance 2 (N (0, 2)) and a target property y that is a third-degree polynomial in x 1 with an additive noise component that scales exponentially in x 2 :That is, the y values are almost determined by the third-degree polynomial for low x 2 values but are almost completely random for high x 2 values. Discovering applicable domains reveals how different models cope differently with this setting even if they have a comparable average error. To show this, let us examine the error distributions obtained from three different kernelized regression models of the formwith parameter vector ν that are fitted around a training, or fitting (F), setwith three different choices for the kernel function k. We observe:
When using the linear (lin) kernel, the resulting linear model is globally incapable to trace the variation of the third-order polynomial except for a small stripe on the x 1 -axis where it can be approximated well by a linear function. Consequently, there is a very high error globally that is substantially reduced in the DA described by σ lin (x 1, x 2) ≡ −0.3 ≤ x 1 ≤ 0.3.
When using the Gaussian kernel), the resulting radial basis function (rbf) model is able to represent the target property well locally unless (a) the noise component is too large and (b) the variation of the target property is too high relative to the number of training points. The second restriction is because the rbfs have non-negligible values only within a small region around the training examples. Consequently, the discovered DA is not only restricted in x 2 -direction but also excludes high absolute x 1 -values: σ rbf ≡ −3.3 ≤ x 11 ≤ 3.1 ∧ x 2 ≤ 0.1.
In contrast, when using the non-local third-degree polynomial (poly) kernel, data sparsity does not prevent an accurate modeling of the target property along the x 1 -axis. However, this non-locality is counterproductive along the x 2 -axis where overfitting of the noise component has a global influence that results in higher prediction errors for the almost deterministic data points with low x 2 -values. This is reflected in the identified DA σ poly (x 1, x 2) ≡ −3.5 ≤ x 2 ≤ 0.1, which contains no restriction in x 1 -direction, but excludes both high and low x 2 -values. This highlights an important structural difference between the rbf and the polynomial model that is not reflected in their similar average errors.

---

### Estimation of the risk of a qualitative phenotype: dependence on population risk [^112uHqUg]. Journal of Human Genetics (2017). Low credibility.

Individual disease risk estimated based on the data from single or multiple genetic loci is generally calculated using the genotypes of a subject, frequencies of alleles of interest, odds ratios and the average population risk. However, it is often difficult to estimate accurately the average population risk, and therefore it is often expressed as an interval. To better estimate the risk of a subject with given genotypes, we built R scripts using the R environment and constructed graphs to examine the change in the estimated risk as well as the relative risk according to the change of the average population risk. In most cases, the graph of the relative risk did not cross the line of y = 1, thereby indicating that the order of the relative risk for given genotypes and the population average risk does not change when the average risk increases or decreases. In rare cases, however, the graph of the relative risk crossed the line of y = 1, thereby indicating that the order of the relative risk for given genotypes and the population average risk does change owing to the change in the population risk. We propose that the relative risk should be estimated for not only the point average population risk but also for an interval of the average population risk. Moreover, when the graph crosses the line of y = 1 within the interval, this information should be reported to the consumer.

---

### The simplicity of protein sequence-function relationships [^114PjDLu]. Nature Communications (2024). High credibility.

Fig. 1
Reference-free analysis (RFA) of genetic architecture.

a Illustration of RFA on a 2-site, 2-state genetic architecture. The four possible genotypes are arranged on a plane with their phenotype indicated by elevation. (First panel) The zero-order effect (e 0) is the mean phenotype of all genotypes, marked by the clear plane with cyan edges. The first-order effect of state A or B at site 1 [e 1 (A) or e 1 (B), green arrows] measures how the mean phenotype of all genotypes containing the state (dashed line) differs from the global mean. The green plane predicts the phenotype based on the state at site 1. (Second panel) First-order effects at site 2 are defined similarly and shown in pink. (Third panel) The first-order model predicts phenotypes by summing the first-order effects of all genetic states plus the global mean, shown as the gray plane tilted in both dimensions; the fraction of phenotypic variance explained is shown. (Fourth panel) The pairwise interaction between states A and B at sites 1 and 2 [e 1, 2 (A, B)] measures how the mean phenotype of all genotypes containing the two states [here just one genotype (A, B)] differs from the first-order prediction. b We implement RFA with a nonlinear link function to incorporate nonspecific epistasis. Each variant's genetic score (s) is the sum of the effects of its genetic states. The link function transforms s of each variant into its phenotype, y. Although the link function can take any form, here we use a simple sigmoid defined by two parameters representing the upper and lower bounds of the measurable phenotype. c (Left) A 5-site, 3-state genetic architecture was simulated by drawing reference-based effects from the standard normal distribution (but setting all fifth-order effects to zero); a small amount of simulated noise was added to the phenotypes. (Middle) Absolute error of RFA terms computed from the simulated measurements. Dashed lines, mean absolute error of individual phenotypes. Supplementary Fig. 1 shows the individual inferred terms. (Right) The fraction of phenotypic variance explained by the true, directly computed, and regression-estimated RFA terms. Supplementary Section 1.1 analyzes additional simulated genetic architectures.

---

### Quantitative assessment of full-width at half-maximum and detector energy threshold in X-ray imaging systems [^11655yGb]. European Journal of Radiology (2024). Medium credibility.

Background

The response function of imaging systems is regularly considered to improve the qualified maps in various fields. More the accuracy of this function, the higher the quality of the images.

Methods

In this study, a distinct analytical relationship between full-width at half-maximum (FWHM) value and detector energy thresholds at distinct tube peak voltage of 100 kV has been addressed in X-ray imaging. The outcomes indicate that the behavior of the function is exponential. The relevant cut-off frequency and summation of point spread function S(PSF) were assessed at large and detailed energy ranges.

Results

A compromise must be made between cut-off frequency and FWHM to determine the optimal model. By detailed energy range, the minimum and maximum of S(PSF) values were revealed at 20 keV and 48 keV, respectively, by 2979 and 3073. Although the maximum value of FWHM occurred at the energy of 48 keV by 224 mm, its minimum value was revealed at 62 keV by 217 mm. Generally, FWHM value converged to 220 mm and S(PSF) to 3026 with small fluctuations. Consequently, there is no need to increase the voltage of the X-ray tube after the energy threshold of 20 keV.

Conclusion

The proposed FWHM function may be used in designing the setup of the imaging parameters in order to reduce the absorbed dose and obtain the final accurate maps using the related mathematical suggestions.

---

### Scalable spatiotemporal prediction with Bayesian neural fields [^117WYRSC]. Nature Communications (2024). High credibility.

Prediction queries

We can approximate the posterior (13) using a set of samples, which may be obtained from either MAP ensemble estimation or stochastic variational inference (by sampling from the ensemble of M variational distributions). We can then approximate the posterior-predictive distribution(which marginalizes out the parameters Θ) of Y (r *) at a novel field index r ✱ = (s *, t *) by a mixture model with M equally weighted components:Equipped with Eq. (23), we can directly compute predictive probabilities of events { Y (r *) ≤ y }, predictive probability densities { Y (r *) = y }, or conditional expectationsfor a probe function. Prediction intervals around Y (r *) are estimated by computing the α -quantile y α (r *), which satisfiesFor example, the median estimate is y 0.50 (s *, t *) and 95% prediction interval is [y 0.025 (s *, t *), y 0.975 (s *, t *)]. The quantiles (24) are estimated numerically using Chandrupatla's root finding algorithmon the cumulative distribution function of the mixture (23).

Temporal seasonal features

Including seasonal features (c.f. Eq. (8)), where possible, is often essential for accurate prediction. Example periodic multiples p for datasets with a variety of time units and seasonal components are listed below (Y = Yearly; Q = Quarterly; Mo = Monthly; W = Weekly; D = Daily; H; Hourly; Mi = Minutely; S = Secondly):
Q: Y = 4
Mo: Q = 3, Y = 12
W: Mo = 4.35, Q = 13.045, Y = 52.18
D: W = 7, Mo = 30.44, Q = 91.32, Y = 365.25
H: D = 24, W = 168, Mo = 730.5, Q = 2191.5, Y = 8766
Mi: H = 60, D = 1440, W = 10080, Mo = 43830, Q = 131490, Y = 525960
S: Mi = 60, H = 3600, D = 86400, W = 604800, Mo = 2629800, Q = 7889400, Y = 31557600

---

### The XZZX surface code [^114fd6hd]. Nature Communications (2021). High credibility.

This structured noise model thus leads to two distinct regimes, depending on which failure process is dominant. In the first regime where, we expect that the logical failure rate will decay like. We find this behaviour with systems of a finite size and at high bias where error rates are near to threshold. We evaluate logical failure rates using numerical simulations to demonstrate the behavior that characterises this regime; see Fig. 6 (a). Our data show good agreement with the scaling ansatz. In contrast, our data are not well described by a scaling.

Fig. 6
Sub-threshold scaling of the logical failure rate with the XZZX code.

a Logical failure rateat high bias near to threshold plotted as a function of code distance d. We use a lattice with coprime dimensions d × (d + 1) for d ∈ {7, 9, 11, 13, 15} at bias η = 300, assuming ideal measurements. The data were collected usingiterations of Monte-Carlo (MC) samples for each physical rate sampled and for each lattice dimension used. The physical error rates used are, from the bottom to the top curves in the main plot, p = 0.19, 0.20, 0.21, 0.22 and 0.23. Error bars represent one standard deviation for the Monte-Carlo simulations. The solid lines are a fit of the data to, consistent with Eq. (2), and the dashed lines a fit to, consistent with Eq. (3) where we would expect, see Methods. The data fit the former very well; for the latter, the gradients of the best fit dashed lines, as shown on the inset plot as a function of, give a linear slope of 0.61(3). Because this slope exceeds the value of 0.5, we conclude that the sub-threshold scaling is not consistent with. b Logical failure ratesat modest bias far below threshold plotted as a function of the physical error rate p. The data (markers) were collected at bias η = 3 and coprime d × (d + 1) code dimensions of d ∈ {5, 7, 9, 11, 13, 15} assuming ideal measurements. Data is collected using the Metropolis algorithm and splitting method presented in refs. The solid lines represent the prediction of Eq. (3). The data show very good agreement with the single parameter fitting for all system sizes as p tends to zero.

---

### CurveCurator: a recalibrated F-statistic to assess, classify, and explore significance of dose-response curves [^116TKdLJ]. Nature Communications (2023). High credibility.

F-statistics and p -values

The basic idea behind the F-value in classical linear regression problems is to quantify how much better a more complex model (M 1 with k linear parameters) fits the data compared to a simpler model (M 0 with j linear parameters and j < k) given the n observed data points and the corresponding sum-squared errors (SSE) (Eq. 6).

Although not a linear model by nature, the log-logistic function still meets the required assumptions of random sampling, independence of observations, residual normality, and equal variance of the errors. The basic rationality behind CurveCurator's recalibrated F-statistic is similar to the linear F-statistic above. It also quantifies how much better the fitted log-logistic model (M 1) is compared to the mean model (M 0), which describes that there is no relationship between the applied dose and the observed response. We found, however, that n/k was a more appropriate scaling factor for the 4-parameter log-logistic function.

The obtained recalibrated F-value (Eq. 7) can then be used to calculate a p -value that quantifies how often a curve with a similar or bigger F-value can be found by random chance. We observed that these F-values follow a parameterized F-distribution with degrees of freedom that diverged from the case of linear models. Using extensive simulations under the null hypothesis (5 million curves for n = 5… 50), we obtained a simple quasi-linear function to calculate the "effective "degrees of freedom as a function of n (Eqs. 8–10).

---

### Dissection of gene expression datasets into clinically relevant interaction signatures via high-dimensional correlation maximization [^1136aMxX]. Nature Communications (2019). High credibility.

Signature focus

Purpose of the signature focus is to define where a detected effect ends. While this may seem easy for plateau-like clusters (having few highly correlated genes or samples while all others only have low correlations), often real effects have no clear edge. In this case, we prefer a smooth decrease in membership weights. The signature focus should not influence ranks of top members, as they need to be determined by signature strengths for optimal bimonotonic regression (cf. Eqs. 29 and 30). But non-members should be identified and excluded by the signature focus to minimize the influence of noise.

The signature focus consists of correlation-based gene and sample weights that allow the computation of all vectors and scores as specific as possible, even if the detected interaction only affects a small subset of measured genes and samples. To retain specificity for such small signatures, we set weight componentsandexactly to zero for all (and potentially very many) genes and samples that have only weak or insignificant correlations to detected signature axes.

For samples, letwhere the second factor decreases quadratically to zero with the noise probability of sample correlations (cf. Supplementary Note 2). Sample weights are defined relative to. Values ≥ 50% of the maximum of allare already mapped to full weight:

Consequently, weights have no influence on the signature's order of top samples, as is intended. To exclude any unspecific influence of samples with relatively weak or insignificant correlation, we finally set all weights to zero that are lower than two thirds of their quantile:

Analogously, gene weightsare given bywhereand.

For signature size estimation and qualification thresholds, mapping all x -values above 50% to full weight is not optimal. To keep the full dynamic range of weights for these tasks, we additionally define the extended signature focusby increasing the upper threshold from 50% to 100% (i.e.and) and by decreasing the lower specificity threshold from two thirds to 0.4. Otherwise, weightsandare computed in the same way.

Importantly, specificity thresholds of the signature focus exclude noise genes from computation that could otherwise reduce both specificity of signatures and detection robustness. In case of many measured genes and small signatures, this can also speed up computation.

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^115o8v1f]. Nature Communications (2025). High credibility.

After n refinement steps, we thus obtain a functiondefined at discrete points:which we extend to the entire interval [0, 1) by linear interpolation; see Fig. 5 d for an illustration. In practice we found that computations (specifically the risk computed by integrating) converge after about eight refinement steps.

This procedure has the important property that once a point is sampled, it does not change on further refinements:which follows from equation (67). Recall now that, as stated above, a process is self-consistent if "for small enough Δ Φ, the probability distribution at a point Φ [does] not depend on the level of refinement". Since equation (70) clearly satisfies that requirement, we see that the process obtained after infinitely many refinement steps is indeed self-consistent. We thus define the hierarchical beta (HB) process as

To complete the definition of, we need to specify how we choose the initial end pointsand. In our implementation, they are drawn from normal distributionswith Φ ∈ {0, 1}, where again c is determined via our proposed calibration procedure; this is simple and convenient, but otherwise arbitrary. We also need to explain how we choose the beta parameters α and β, which is the topic of the next subsection.

Choosing beta distribution parameters

All HB processes are monotone, continuous and self-consistent, but within this class there is still a lot of flexibility: since α and β are chosen independently for each subinterval, we can mouldinto a wide variety of statistical shapes. We use this flexibility to satisfy the two remaining desiderata: a) that realisationstrackover Φ ∈ [0, 1]; and b) that the variability ofbe proportional to. It is the goal of this subsection to give a precise mathematical meaning to those requirements.

Let x 1 ~ Beta(α, β) and x 2 = 1 − x 1. (The density function of a beta distribution is given in (24).) The mean and variance of x 1 areFor a given Φ, it may seem natural to select α and β by matchingtoandto. However both equations are tightly coupled, and we found that numerical solutions were unstable and unsatisfactory; in particular, it is not possible to make the variance large whenapproaches either 0 or 1 (otherwise the distribution of x 1 would exceed [0, 1]).

---

### Limits on the computational expressivity of non-equilibrium biophysical processes [^112Romk3]. Nature Communications (2025). High credibility.

Fig. 2
The matrix-tree theorem.

A Computing the steady-state occupancy π 1 by summing weights over directed spanning trees. Directed spanning trees are subgraphs containing all graph nodes but no cycles, with edges oriented toward a root node. In each directed spanning tree, the input forces make a positive, negative, or zero contribution to the tree weight. The structural vectorsare shown below each tree; these quantities enter into Equation (3) below. B Schematic illustration of the high-dimensional space of feature vectors ψ (i; θ) and χ (i, F). The depicted arrangement of vectors could solve a binary classification problem.

We define the input multiplicityas the number of edges affected per input variable, which we assume to be the same for each input. To focus on the functional way in which the input driving enters the steady-state probabilities, the driving contributions can be factored out in the algebraic expressions for the numerator and denominator of Equation (1). This has been previously been used to make analytical progress for M = D = 1 in, for example, refs. –. This equivalent formulation of Eq. (1) suggests that steady states of Markov jump processes implement a rational polynomial function of exponentiated input variables. Defining, we rewrite the matrix-tree expression for π i for general D and M We use the multi-index, whereis the set of D input labels and each componentof the multi-index runs over the values, to enumerate themonomials. These monomials y μ (F) in Equation (2) combinatorially depend on the different mixtures μ of input driving, representing a net total μ a of signed contributions from the input force F a, μ b such contributions for F b, and so on for each input. The coefficients, which are functions of the parameters θ, are the sums of weights over all directed spanning trees rooted at node i which have the corresponding mixture μ of signed input contributions. The monomial coefficientsthus represent learnable amplitudes of each polynomial basis function y μ (F). The coefficients in the denominator are defined as. Classification will be successful if, for F ρ drawn from class ρ, the coefficientsand monomials y μ (F ρ) are large for the same μ. In the subsequent sections of the paper and in the Supplementary Information we use the formulation in Equation (2) to show how the classification ability of a non-equilibrium Markov processes may be systematically modulated.

---

### Real-time estimates in early detection of SARS [^114Ri9ty]. Emerging Infectious Diseases (2006). Low credibility.

Distribution of X t + (T) | X t - (T)

We assume that X t is Poisson distributed with mean n t λ t and choose a vague gamma prior distribution for λ t with shape parameter α = 10⁻⁵ and rate β = 10⁻⁵.

Conditional on X t, the number X t - (T) of early secondary cases is binomial with parameters X t, W tT, where W tT is the probability that the generation interval is < T – t. It follows that X t - (T) | λ t is Poisson distributed with mean n t λ t W tT. The same argument would show that X t + (T) | λ t is Poisson with mean n t λ t (1 – W tT). Given λ t, X t + (T) and X t - (T) are independent so thatWith Bayes' theorem, whereEventually, we obtain that the distribution X t + (T) | X t - (T) = y is negative binomial with parameters p = (n t W tT + β)/(n t + β), m = y + α and probability

Distribution of X t - (T) | I(T)

In practice, the exact realization of X t - (T) is unknown, and inference must be based on I(T) alone. Wallinga and Teunis have shown that the probability that a case detected at day k < T has been infected by a case detected at day t < T iswhere 1{.} is the indicator function. The distribution of X t - (T) given I(T) is a sum of independent binomial distributionsThis probability distribution may be determined numerically.

---

### Recommendations for the evaluation of left ventricular diastolic function by echocardiography and for heart failure with preserved ejection fraction diagnosis: an update from the American Society of Echocardiography [^115fEorz]. Journal of the American Society of Echocardiography (2025). High credibility.

Key points — age and diastolic function metrics indicate that numerous studies have demonstrated the association between age and echocardiographic measurements of left ventricular (LV) diastolic function; normal ranges are not necessarily equivalent to "optimal" values, as the aging process itself may affect diastolic function; and E/e′ ratio and left atrial volume index (LAVI) have near linear associations with incidence of heart failure (HF) or death without evidence of a clear prognostic threshold.

---

### Bayesian log-normal deconvolution for enhanced in silico microdissection of bulk gene expression data [^115QWN3R]. Nature Communications (2021). High credibility.

Fig. 2
Comparison of normal, negative binomial, and log-normal distribution in fitting linear-scale gene expression data.

a A bar chart of average log-likelihood of the three types of distribution fitted to PBMC single-cell RNA-seq data. The genes were split by DEGs (red; n = 1723) and non-DEGs (blue; n = 1496). b Comparison of the distance of the estimated mode to the true mode (y -axis) per distribution type (x-axis). The standard boxplot notation was used (lower/upper hinges — first/third quartiles; whiskers extend from the hinges to the largest/lowest values no further than 1.5 * inter-quartile ranges). c Pairwise comparison of per-gene log-likelihood of log-normal distribution (y -axis) and that of normal (x -axis; top) and negative binomial distribution (x -axis; bottom). The genes were split into non-DEGs (left) and DEGs (right). d Density plots for raw-counts (red) and optimized log-normal (green), normal (blue), and negative binomial distribution (purple) for four example genes (gene name at the top) with low maximum log-likelihood for normal distribution. e, f Maximum log-likelihood values (e) and root mean squared error (root MSE: f) of each gene for log-normal (y -axis) and negative binomial (x -axis) convolutions of T = 8 cell types, applied to TCGA-MESO (left) and TCGA-SARC (right) data.

---

### Recommendations for the evaluation of left ventricular diastolic function by echocardiography and for heart failure with preserved ejection fraction diagnosis: an update from the American Society of Echocardiography [^11376qBH]. Journal of the American Society of Echocardiography (2025). High credibility.

Mitral annular e′ velocity age-specific cutoffs for impaired LV relaxation (Table 6) are: at 20–39 y, septal e′ < 7 cm/s, lateral e′ < 10 cm/s, and average e′ < 9 cm/s; at 40–65 y, septal e′ < 6 cm/s, lateral e′ < 8 cm/s, and average e′ < 7 cm/s; at > 65 y, septal e′ < 6 cm/s, lateral e′ < 7 cm/s, and average e′ < 6.5 cm/s.

---

### General population-based lung function trajectories over the life course: an accelerated cohort study [^115oeYAr]. The Lancet: Respiratory Medicine (2025). High credibility.

Background

Lung function is a key determinant of health, but current knowledge on lung function growth and decline over the life course is based on fragmented, potentially biased data. We aimed to empirically derive general population-based life course lung function trajectories, and to identify breakpoints and plateaus.

Methods

We created an accelerated cohort by pooling data from eight general population-based child and adult cohort studies from Europe and Australia. We included all participants with information on lung function, smoking status, BMI, and asthma diagnosis status from at least two visits. We used cross-classified three-level linear mixed models to derive sex-specific life course trajectories of FEV 1, forced vital capacity (FVC), and FEV 1 /FVC ratio based on observations at ages 4–80 years, and Bayesian time-series decomposition to identify breakpoints and plateaus. We repeated sex-specific analyses with separate stratification for asthma status (never had asthma vs persistent asthma, where persistent was defined as the risk factor being present at all participant visits) and smoking status (never smoker vs persistent smoker).

Findings

The accelerated cohort included 30438 participants born between 1901 and 2006 (15 703 [51.6%] female and 14735 [48.4%] male; mean age 26 [SD 16] years), who provided a total of 87666 observations (range 2–8 observations per participant). In female participants, FEV 1 increased non-linearly in two phases, at a mean of 234 (95% CI 223 to 245) mL/year until age 13 (95% credible interval [CrI] 12 to 15) years, then at 99 (76 to 122) mL/year until a peak at age 20 (18 to 22) years, and subsequently decreased throughout the rest of adulthood (-26 [-27 to -25] mL/year). In male participants, the pattern was similar, with an increase in FEV 1 of 271 (263 to 280) mL/year until age 16 (14 to 18) years, which slowed to 108 (93 to 124) mL/year until reaching a maximum at age 23 (21 to 25) years, decreasing thereafter (-38 [-39 to -37] mL/year), representing a later peak than in female participants. In female participants, FVC increased non-linearly in two phases, at 232 (95% CI 222 to 243) mL/year until age 14 (95% CrI 12 to 15) years, then at 77 (59 to 94) mL/year until peaking at age 20 (19 to 22) years, after which it decreased throughout the rest of adulthood (-26 [-27 to -25] mL/year). In male participants, FVC also increased in two phases, at 326 (315 to 337) mL/year until age 15 (13 to 17) years, then at 156 (144 to 168) mL/year until a peak at 23 (19 to 30) years, and subsequently declined in two phases (-22 [-29 to -14] mL/year until age 42 [38 to 50] years, then -36 [-38 to -34] mL/year thereafter). No plateau after the peak was observed for either lung function parameter in both sexes. FEV 1 /FVC ratio decreased throughout life from the starting age of 4 years in both sexes with some distinct patterns. Stratified analysis showed that persistent asthma (vs never had asthma) was related to an earlier FEV 1 peak, lower FEV 1 throughout adulthood, and lower FEV 1 /FVC ratio across the life course in both sexes. Persistent smoking (vs never smoking) was related to an accelerated decrease in FEV 1 and FEV 1 /FVC ratio during adulthood in both sexes. No statistically significant plateau was observed in any lung function parameter across the strata of asthma or smoking status.

Interpretation

In both sexes, FEV 1 and FVC increased in two phases, with a fast increase until around age 13–16 years, and then a slower increase until a peak. Neither parameter showed a plateau phase after the peak, and decreases started earlier than previously described. FEV 1 /FVC ratio decreased throughout life. These observations provide an essential, but previously unavailable, framework to assess and monitor lung health over the life course.

Funding

EU Horizon 2020, Wellcome, European Respiratory Society, AstraZeneca, Chiesi, GSK, Menarini Group, and Sanofi.

---

### The snm procedure guideline for general imaging 6.0 [^114QWgbH]. SNMMI (2010). Medium credibility.

Nuclear medicine computer system components — The camera head or associated image processing system performs functions including image size, position and zoom; energy correction; spatial distortion correction; other corrections (scatter correction, dead time correction, depth of interaction correction, sensitivity correction); and digital position computation; the interface handles the data in two basic modes: 1) Frame mode: complete images or matrices are available to the attached computer; and 2) List mode: data are passed on to the attached computer as a list of event x, y coordinates, to which time information and energy information may be also attached; for cardiac studies in particular, time lapse averaging is required, such that each image acquired at some specific time within the cardiac cycle is added to other acquired at similar times.

---

### Tracking COVID-19 using online search [^116zL3nM]. NPJ Digital Medicine (2021). Medium credibility.

We develop forecasting models using Gaussian Processes (GP), training a different model for each D days ahead forecasting task. The choice of GPs is justified by previous work on modelling infectious diseases. GPs are defined as random variables any finite number of which have a multivariate Gaussian distribution. GP methods aim to learn a functiondrawn from a GP prior. They are specified through a mean and a covariance (or kernel) function, i.e. where x and(both) denote rows of the input matrix X for the SAR-F model. By setting μ (x) = 0, a common practice in GP modelling, we focus only on the kernel function. The specific kernel function used in SAR-F is given bywhere k SE (⋅, ⋅) denotes a squared exponential (SE) covariance function (Supplementary Eq. 5), δ (⋅, ⋅) denotes a Kronecker delta function used for an independent noise component, ℓ 's and σ 's are lengthscale and scaling (variance) parameters, respectively. This composite covariance function uses separate kernels for online search and deaths data, as well as a kernel where they are modelled jointly. This provides the GP with more flexibility in adapting its decisions to one class of observations over the other. The covariance function of the AR-F model is a simplified version of Eq. (9), where search data is not used, i.e. The hyperparameters of both covariance functions (σ 's, ℓ 's) are optimised using Gaussian likelihood and variational inference.

In our analysis, we compare the aforementioned forecasting models with a basic persistence model (PER-F). For a D days ahead forecasting task, PER-F uses the most recently observed value of the ground truth, y t, as the forecasting estimate for the time instance t + D. Given the limited time span and the irregularities in reporting, this is a competitive baseline to improve upon.

---

### Guidelines for validation of next-generation sequencing-based oncology panels: a joint consensus recommendation of the Association for Molecular Pathology and college of American pathologists [^1112btau]. The Journal of Molecular Diagnostics (2017). Medium credibility.

Nonparametric tolerance intervals for non-normal distributions: The above estimate of the tolerance interval would only be applicable to a population that is normally distributed, but when the underlying population is often not normal (eg, when there is a natural boundary that the data cannot exceed (ie, 0% or 100%)) it is helpful to define tolerance intervals using nonparametric methods; the one-sided nonparametric tolerance interval can be determined by finding the value for k that satisfies the cumulative binomial equation, where CL is the confidence level (eg, 0.95), and by setting k = 0 (ie, 0 failures) the formula can be simplified.

---

### Perceptual response characterization in acute vestibular implant stimulation [^115z5bj1]. Journal of Neurology (2025). Medium credibility.

Perception intensity

Results regarding the intensity of the reported perception, scored on a VAS ranging from 0 (no percept) to 10 (uncomfortable) for each given stimulation is illustrated in Fig. 5. At UCL, the mean VAS score was 7.13 (range: 4–9) for LAN, 7.11 (range: 3–9) for SAN, and 7.64 (range: 3–9) for PAN stimulation.

Fig. 5
Perception intensity as a function of the stimulation amplitude (% of DR) for each vestibular electrode. The x -axis represents the percentage of the electrical dynamic range: 0% equals vestibular threshold, 100% equals upper comfortable limit. The y -axis represents the Visual Analog Scale (VAS) scores for perception intensity: zero represents no perception, 10 equals uncomfortable perception. Each dot corresponds to an individual observation, with the size of the dots reflecting the frequency of observations at that point. Linear regression lines for each electrode are shown in black. The formula for each linear regression is printed at the top of the plot. LAN = lateral ampullary nerve electrode, SAN = superior ampullary nerve electrode, PAN = posterior ampullary nerve electrode

---

### Recommendations for the evaluation of left ventricular diastolic function by echocardiography and for heart failure with preserved ejection fraction diagnosis: an update from the American Society of Echocardiography [^113oYYmc]. Journal of the American Society of Echocardiography (2025). High credibility.

Echocardiographic diastolic function — age-specific normal ranges (Table 5) provide E/A ratio reference intervals by decade: ages 20–39 years 0.88 (0.82–0.94) to 2.73 (2.66–2.81), 40–59 years 0.69 (0.66–0.73) to 2.07 (2.03–2.11), and 60–80 years 0.50 (0.45–0.56) to 1.40 (1.34–1.47); these normal ranges were identified on the basis of their fifth and 95th percentile limits.

---

### Estimating the effectiveness of routine asymptomatic PCR testing at different frequencies for the detection of SARS-CoV-2 infections [^117VE3R5]. BMC Medicine (2021). Medium credibility.

Fig. 3
Estimation of positivity over time, and probability that different testing frequencies with PCR would detect infection. a Ct value data for the PCR tests in the SAFER trial. This plot does not show data for every individual included in the analysis. The x -axis shows a time since infection using the median infection date inferred by the model. Points below the threshold of 37, indicating a positive result, are shown in red. Negative results above 37 are shown in black. All negative results for which there is no ct value specified are given the value of 40. b Temporal variation in PCR-positivity based on time since infection. The grey interval and solid black line show the 95% uncertainty interval and the mean, respectively, for the empirical distribution calculated from the posterior samples of the times of infection (see Additional file 1: Section D for methodology). The blue interval and dashed black line show the 95% credible interval and median, respectively, of the logistic piecewise regression described above. c Probability of detecting virus before expected onset of symptoms, based on curve in b, assuming delay from test to results is either 1 or 2 days. Dashed black box shows a site of possible trade-off between testing frequency and results delay discussed in the text. d Probability of detecting an asymptomatic case within 7 days, based on curve in b, assuming delay from test to results is either 24 or 48 h

---

### Thyroid function within the reference range and the risk of stroke: an individual participant data analysis [^116v5UNG]. The Journal of Clinical Endocrinology and Metabolism (2016). Low credibility.

Context

The currently applied reference ranges for thyroid function are under debate. Despite evidence that thyroid function within the reference range is related with several cardiovascular disorders, its association with the risk of stroke has not been evaluated previously.

Design and Setting

We identified studies through a systematic literature search and the Thyroid Studies Collaboration, a collaboration of prospective cohort studies. Studies measuring baseline TSH, free T 4, and stroke outcomes were included, and we collected individual participant data from each study, including thyroid function measurements and incident all stroke (combined fatal and nonfatal) and fatal stroke. The applied reference range for TSH levels was between 0.45 and 4.49 mIU/L.

Results

We collected individual participant data on 43 598 adults with TSH within the reference range from 17 cohorts, with a median follow-up of 11.6 years (interquartile range 5.1–13.9), including 449 908 person-years. Age- and sex-adjusted pooled hazard ratio for TSH was 0.78 (95% confidence interval [CI] 0.65–0.95 across the reference range of TSH) for all stroke and 0.83 (95% CI 0.62–1.09) for fatal stroke. For the free T 4 analyses, the hazard ratio was 1.08 (95% CI 0.99–1.15 per SD increase) for all stroke and 1.10 (95% CI 1.04–1.19) for fatal stroke. This was independent of cardiovascular risk factors including systolic blood pressure, total cholesterol, smoking, and prevalent diabetes.

Conclusion

Higher levels of TSH within the reference range may decrease the risk of stroke, highlighting the need for further research focusing on the clinical consequences associated with differences within the reference range of thyroid function.

---

### Overcoming the design, build, test bottleneck for synthesis of nonrepetitive protein-RNA cassettes [^1176712S]. Nature Communications (2021). High credibility.

In the analysis carried out in this paper, we chose to reduce the parameter space to a 3-dimensional space consisting of the following components: the slope (m) and goodness of fit (R 2) to a simple linear fit of the rescaled fluorescenceto inducer concentration values. The third component is the standard deviation (std) ofcomputed at the three highest concentration induction bins. We term this vector:

Based on the 3-dimensional space (R 2, m, and std) we conducted a multivariant Gaussian fit for the positive and negative control populations (Fig. 2), which in turn allowed us to compute the 3-dimensional pdf(pos, n) and pdf(neg, n). Finally, we computed the R score for each non-control variant by averaging the score over as many barcodes which passed our filters (each variant appeared in our library 5 times). The results of this computation are presented in the heatmaps of Fig. 2 and Fig. S3, which are arranged in accordance with decreasing R score. Up to this point, we have developed the R score to sort the different variants, but did not dive into what it means physically or from a binding perspective. The approach relied on mapping the behavior of the positive binding controls and non-binding negative controls in some 3-dimensional parameter space, and computing the likelihood that a given variant would belong to one or the other group. The R score is the log of the ratio of the two computations. In principle, R score can be computed from any number of probability density functions. We could have used the original 6D space consisting of the 6 inducer concentrations, or chosen any other combination. In the computation below, we will map the 6D space to a 1D space of binding affinities that can be in principle computed from each 6-vector using a Hill function fit. In the case of such a mapping, we can replace eqn. 7 and 8 in the paper with the following terms:

---

### Variability in estimated modelled insulin secretion [^1165rnTV]. Journal of Diabetes Science and Technology (2022). Medium credibility.

Background

The ability to measure insulin secretion from pancreatic beta cells and monitor glucose-insulin physiology is vital to current health needs. C-peptide has been used successfully as a surrogate for plasma insulin concentration. Quantifying the expected variability of modelled insulin secretion will improve confidence in model estimates.

Methods

Forty-three healthy adult males of Māori or Pacific peoples ancestry living in New Zealand participated in an frequently sampled, intravenous glucose tolerance test (FS-IVGTT) with an average age of 29years and a BMI of 33kg/m 2. A 2-compartment model framework and standardized kinetic parameters were used to estimate endogenous pancreatic insulin secretion from plasma C-peptide measurements. Monte Carlo analysis (N = 10000) was then used to independently vary parameters within ± 2 standard deviations of the mean of each variable and the 5th and 95th percentiles determined the bounds of the expected range of insulin secretion. Cumulative distribution functions (CDFs) were calculated for each subject for area under the curve (AUC) total, AUC Phase 1, and AUC Phase 2. Normalizing each AUC by the participant's median value over all N = 10000 iterations quantifies the expected model-based variability in AUC.

Results

Larger variation is found in subjects with a BMI > 30kg/m 2, where the interquartile range is 34.3% compared to subjects with a BMI ≤ 30kg/m 2 where the interquartile range is 24.7%.

Conclusions

Use of C-peptide measurements using a 2-compartment model and standardized kinetic parameters, one can expect ~ ± 15% variation in modelled insulin secretion estimates. The variation should be considered when applying this insulin secretion estimation method to clinical diagnostic thresholds and interpretation of model-based analyses such as insulin sensitivity.

---

### The tumor immune microenvironment architecture correlates with risk of recurrence in head and neck squamous cell carcinoma [^114c2KCx]. Cancer Research (2023). Medium credibility.

Neighborhood identification

To identify CNs that are regions with a characteristic local composition of cell phenotypes, we implemented in R the neighborhood analysis presented in ref. For each of the 792,349 cells from the nine samples, we identified a window consisting of W nearest neighboring cells (including the center cell) using the nn2 function of the RANN R package (version 2.6.1). The nn2 function uses a k-dimensional tree to find a given number of near neighbors (here, W) for each point identified by the X and Y coordinates in the input dataset. These windows were then clustered by their composition with respect to the 20 CTs previously identified by graph-based clustering and supervised annotation. Specifically, each window was converted to a vector of length 20 containing the frequency of each of the 20 CTs among the W neighbors. Subsequently, windows have been clustered using the MiniBatchKmeans function of the ClusterR package (version 1.2.9) implementing the Mini-batch K-means clustering algorithm with a given value of K. Each cell was then allocated to the CN of its surrounding window using the predict_MBatchKMeans function of the ClusterR package. We applied the entire procedure and identified CNs for different combinations of W (ranging from 7 to 200) and K (ranging from 7 to 14).

---

### Input-output maps are strongly biased towards simple outputs [^11354s9E]. Nature Communications (2018). Medium credibility.

Many systems in nature can be described using discrete input-output maps. Without knowing details about a map, there may seem to be no a priori reason to expect that a randomly chosen input would be more likely to generate one output over another. Here, by extending fundamental results from algorithmic information theory, we show instead that for many real-world maps, the a priori probability P(x) that randomly sampled inputs generate a particular output x decays exponentially with the approximate Kolmogorov complexity [Formula: see text] of that output. These input-output maps are biased towards simplicity. We derive an upper bound P(x)≲[Formula: see text], which is tight for most inputs. The constants a and b, as well as many properties of P(x), can be predicted with minimal knowledge of the map. We explore this strong bias towards simple outputs in systems ranging from the folding of RNA secondary structures to systems of coupled ordinary differential equations to a stochastic financial trading model.

---

### Article 5. An introduction to estimation – 2: from z to T [^117RsoLq]. Emergency Medicine Journal (2001). Low credibility.

Provided the sample size is large enough (that is, n greater than 100), the z statistic can be used to determine the confidence interval estimation of the population mean even when the sigma is not known. In these cases the estimation of the standard error of the mean is used. The z statistic is also valid when determining the population's proportion based upon a large sample. However, when dealing with smaller samples, the z statistic is replaced by the t statistic. This makes it possible to estimate, in a population with an unknown standard deviation: The probability of getting a sample mean greater than or equal to a particular value The value of a sample mean with a particular probability of occurring The probability of getting a sample mean between two particular values The confidence interval for the estimation of the population mean can also be determined using the t statistic.

---

### Sex-specific prediction equations for vmax (FRC) in infancy: a multicenter collaborative study [^1129MepA]. American Journal of Respiratory and Critical Care Medicine (2002). Low credibility.

Measurements of maximal flow at functional residual capacity (Vmax(FRC)) from partial forced expiratory maneuvers remain the most popular method for assessing small airway function in infants and young children. However, the lack of appropriate reference data that are both applicable outside the centers that developed them and reflect the normal variability between healthy subjects has limited interpretation of Vmax(FRC) results in both clinical practice and research. To address this problem, we collated Vmax(FRC) data from 459 healthy infants (226 boys) tested on 654 occasions during the first 20 months of life from three collaborating centers. Multiple linear regression analysis indicated that sex, age, and length were important predictors of Vmax (FRC), which was, on average, 20% higher in girls than in boys during the first 9 months of life. (Vmax(FRC))^(0.5) (ml × second⁻¹) = 4.22 + 0.00210 × length² (cm) for boys (RSD = 3.01; R² = 0.48), and −1.23 + 0.242 × length for girls (RSD = 2.72; R² = 0.49). Alternative models incorporating both age and length z scores are also described. Failure to use sex-specific prediction equations for Vmax(FRC) may preclude detection of clinically significant changes in girls and lead to false reports of diminished airway function in boys. Appropriate use of z scores, which indicate a "normal" range (z scores of 0 ± 2) for Vmax(FRC), during infancy should also improve interpretation of both clinical and research studies.

---

### Longitudinal assessment of lung function in patients following COVID-19 [^115coH8C]. Respiratory Medicine (2025). Medium credibility.

Background

While effects of COVID-19 on lung function are commonly described in those with severe illness, less is known about those with less severe illness, and most studies do not extend beyond 6 months following initial presentation.

Research Question

What are the effects of COVID-19 on lung function in a cohort of participants that included those with more severe and less severe disease over a 1 year period from time of enrollment?

Study Design

We enrolled 52 participants who included those with more severe (had been hospitalized) and less severe (not hospitalized) illness and measured spirometry, lung volumes, diffusing capacity, oscillometry, maximal muscle pressures, inspiratory drive, exercise capacity, and symptom and quality of life surveys at presentation, and repeated the measurements 6 and 12 months later.

Results

While participants who had been hospitalized had consistently lower lung function in all measures, all values were within normal reference ranges. The pattern of lung function change suggested a predominant restrictive physiologic defect with reduced exercise capacity. Over 1 year, there was no significant improvement in lung function. Similar findings were seen when participants were stratified by whether they had shortness of breath at presentation.

Interpretation

In our cohort of participants with both more severe and less severe disease, there were only minor differences in lung function associated with severe illness or whether participants had shortness of breath. COVID-19 resulted in subtle changes related to physiologic restriction, but overall lung function remained in the normal range with little change over time, suggesting that other factors besides lung function contribute to shortness of breath in participants following COVID-19.

---

### Structure and inference in annotated networks [^1138c43s]. Nature Communications (2016). Medium credibility.

Computationally, the most demanding part of the EM algorithm is calculating the sum in the denominator of equation (7), which has an exponentially large number of terms, making its direct evaluation intractable on all but the smallest of networks. Traditionally one gets around this problem by approximating the full distribution q (s) by Monte Carlo importance sampling. In our calculations, however, we instead use a recently proposed alternative method based on belief propagation, which is significantly faster, and fast enough in practice for applications to very large networks.

Final likelihood value

The EM algorithm always converges to a maximum of the likelihood but is not guaranteed to converge to the global maximum — it is possible for there to be one or more local maxima as well. To get around this problem we normally run the algorithm repeatedly with different random initial guesses for the parameters and from the results choose the one that finds the highest likelihood value. In the calculations presented in this paper we did at least 10 such 'random restarts' for each network. To determine which run has the highest final value of the likelihood we calculate the log-likelihood from the right-hand side of (6) using P (A | Θ, s) and P (s | Γ, x) as in equation (2), the final fitted values of the parameters Θ and Γ from the EM algorithm, and q (s) as in equation (7). (As we have said, the right-hand side of (6) becomes equal to the left, and hence equal to the true log-likelihood, when q (s) is given the value in equation (7).)

---

### KDIGO 2024 clinical practice guideline for the evaluation and management of chronic kidney disease [^114ToLK7]. Kidney International (2024). High credibility.

CKD mineral–bone parameters by albuminuria category — figure context: The y axis represents the meta-analyzed absolute difference from the mean adjusted value at an eGFR of 80 ml/ min per 1.73 m² and albumin excretion < 30 mg/g (< 3 mg/mmol), with albuminuria categories defined as A1, albuminuria < 30 mg/g (< 3 mg/mmol); A2, albuminuria 30–300 mg/g (3–30 mg/mmol); and A3, > 300 mg/g (> 30 mg/mmol).

---

### CurveCurator: a recalibrated F-statistic to assess, classify, and explore significance of dose-response curves [^116JUx2X]. Nature Communications (2023). High credibility.

Dose-response curves are key metrics in pharmacology and biology to assess phenotypic or molecular actions of bioactive compounds in a quantitative fashion. Yet, it is often unclear whether or not a measured response significantly differs from a curve without regulation, particularly in high-throughput applications or unstable assays. Treating potency and effect size estimates from random and true curves with the same level of confidence can lead to incorrect hypotheses and issues in training machine learning models. Here, we present CurveCurator, an open-source software that provides reliable dose-response characteristics by computing p-values and false discovery rates based on a recalibrated F-statistic and a target-decoy procedure that considers dataset-specific effect size distributions. The application of CurveCurator to three large-scale datasets enables a systematic drug mode of action analysis and demonstrates its scalable utility across several application areas, facilitated by a performant, interactive dashboard for fast data exploration.

---

### Reference values for 7 different protocols of simple functional tests: a multicenter study [^114oRRJb]. Archives of Physical Medicine and Rehabilitation (2022). Medium credibility.

Objectives

To establish reference values and equations (ages 20-80y) for 7 simple functional tests based on a multicenter study.

Design

Cross-sectional data collection in 4 research centers across different regions of a continental dimension country.

Setting

Healthy subjects from general community were assessed in different research laboratories.

Participants

Data collection of 296 volunteer subjects (N = 296; 45% men; aged 50 ± 18y, forced expiratory volume in the first second 95 ± 13% pred, body mass index 26.9 ± 4.5 kg/m 2) aged 20–80 years; representing both sexes; with the ability to understand and perform all proposed assessments; and with no severe and/or unstable condition that could limit functional assessments occurred simultaneously in all centers.

Interventions

Not applicable.

Main Outcome Measures

All participants randomly performed the following 7 functional tests twice: (1) the 4-meter gait speed test at usual walking speed; (2) the 4-meter gait speed test at maximal walking speed; (3) the Sit-to-Stand test performed with 5 repetitions; (4) the Sit-to-Stand test performed in 30 seconds; (5) the Sit-to-Stand test performed in 1 minute; (6) the Timed Up and Go test at usual speed; and (7) the Timed Up and Go test at maximal speed. Spirometry, quality of life, depression, anxiety, physical activity, and comorbidities were also assessed to better characterize the sample. The best performance of each test was used to propose reference values for men and women and reference equations for all.

Results

Participants similarly distributed by age groups from the 4 centers were included. All tests were correlated with age (0.34 < r < 0.53) and body mass index (0.24 < r < 0.31; P < .05 for all). Reference values with limits of normality were provided by each 10-year age group and regression models identified reference equations for all tests. Reliability of the reference equations were confirmed in an independent sample.

Conclusions

Reference values and equations for 7 widely used simple functional tests were provided in this study and might help researchers and clinicians to identify and quantify functional impairments using easy-to-perform assessments.

---

### Semiparametric estimation of the relationship between ROC operating points and the test-result scale: application to the proper binormal model [^114xDrbH]. Academic Radiology (2011). Low credibility.

Rationale and Objectives

Semiparametric methods provide smooth and continuous receiver operating characteristic (ROC) curve fits to ordinal test results and require only that the data follow some unknown monotonic transformation of the model's assumed distributions. The quantitative relationship between cutoff settings or individual test-result values on the data scale and points on the estimated ROC curve is lost in this procedure, however. To recover that relationship in a principled way, we propose a new algorithm for "proper" ROC curves and illustrate it by use of the proper binormal model.

Materials and Methods

Several authors have proposed the use of multinomial distributions to fit semiparametric ROC curves by maximum-likelihood estimation. The resulting approach requires nuisance parameters that specify interval probabilities associated with the data, which are used subsequently as a basis for estimating values of the curve parameters of primary interest. In the method described here, we employ those "nuisance" parameters to recover the relationship between any ordinal test-result scale and true-positive fraction, false-positive fraction, and likelihood ratio. Computer simulations based on the proper binormal model were used to evaluate our approach in estimating those relationships and to assess the coverage of its confidence intervals for realistically sized datasets.

Results

In our simulations, the method reliably estimated simple relationships between test-result values and the several ROC quantities.

Conclusion

The proposed approach provides an effective and reliable semiparametric method with which to estimate the relationship between cutoff settings or individual test-result values and corresponding points on the ROC curve.

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^114XJRus]. Annals of the American Thoracic Society (2023). High credibility.

V_A/Q distribution modeling — derived from cumulative plots of ventilation or perfusion versus V_A/Q — is binned into 50 equally spaced log-scale compartments, with shunt and dead space handled separately. Specifically, V_A/Q ratios < 0.005 (shunt) and > 100 (dead space) are calculated as separate components, and relative dispersion and gravitational gradients may also be assessed.

---

### Longitudinal cardio-respiratory fitness prediction through wearables in free-living environments [^111o5w6K]. NPJ Digital Medicine (2022). Medium credibility.

Model differences across tasks

Task 1 trains the main neural network of our study (see previous subsection). Task 2 re-trains an identical model to predict V O 2 m a x in the future (and the delta present-future). We note that the delta prediction task cannot be comparable with the models predicting the present and future outcomes. Essentially, the delta model predicts the difference between these two timepoints, which results in a range of values roughly from −10 to +10. This distribution is not normally distributed (Shapiro-Wilk test = 0.991, p = 0.002) and hence both linear and neural models cannot approximate the tails, with most of their predictions lying between −3 and +3. The negative/positive signs of this outcome make the error metrics not very interpretable. We do not believe this performance is caused by overfitting because the results of both linear and neural models are similar. This result motivated us to study the delta distribution as a binary problem. When we re-frame this problem as a classification task (see Fig. 5), we use significantly fewer participants when we focus on the tails of the change distribution. Therefore, to combat overfitting, we train a smaller network with one Dense layer of 128 units and a sigmoid output unit, which is appropriate for binary problems. Instead of optimizing the MSE, we now minimize the binary cross-entropy. In all other cases — such as in Task 3 or when visualizing the latent space —, we do not train new models; the model which was trained in Task 1 is used in inference mode (prediction).

Prediction equations

For reference, we compare our models' results to traditional non-model equations, which rely on Body Mass, RHR, and Age. We incorporate the popular equation proposed by Uth et al. which corresponds to VO₂max = 15.0 × Body Mass (kg) × (HRmax/HRrest), in combination with Tanaka's equationwhere H R m a x = 208 −. Other approaches rely on measurements such as the waist circumference, which however was not recorded in our cohorts.

Linear model

We begin our investigation by establishing a strong baseline with a linear regression model (as seen in Table 1). We compare differenet combinations of input data and finally compare the comprehensive model with the Dense neural network. We use the Python sklearn implementation for linear regression.

---

### Comparison of parameter optimization methods for quantitative susceptibility mapping [^111zGMk6]. Magnetic Resonance in Medicine (2021). Medium credibility.

Purpose

Quantitative Susceptibility Mapping (QSM) is usually performed by minimizing a functional with data fidelity and regularization terms. A weighting parameter controls the balance between these terms. There is a need for techniques to find the proper balance that avoids artifact propagation and loss of details. Finding the point of maximum curvature in the L-curve is a popular choice, although it is slow, often unreliable when using variational penalties, and has a tendency to yield overregularized results.

Methods

We propose 2 alternative approaches to control the balance between the data fidelity and regularization terms: 1) searching for an inflection point in the log-log domain of the L-curve, and 2) comparing frequency components of QSM reconstructions. We compare these methods against the conventional L-curve and U-curve approaches.

Results

Our methods achieve predicted parameters that are better correlated with RMS error, high-frequency error norm, and structural similarity metric-based parameter optimizations than those obtained with traditional methods. The inflection point yields less overregularization and lower errors than traditional alternatives. The frequency analysis yields more visually appealing results, although with larger RMS error.

Conclusion

Our methods provide a robust parameter optimization framework for variational penalties in QSM reconstruction. The L-curve-based zero-curvature search produced almost optimal results for typical QSM acquisition settings. The frequency analysis method may use a 1.5 to 2.0 correction factor to apply it as a stand-alone method for a wider range of signal-to-noise-ratio settings. This approach may also benefit from fast search algorithms such as the binary search to speed up the process.

---

### 'Knock down the brain': a nonlinear analysis of electroencephalography to study the effects of sub-concussion in boxers [^116cHFU2]. European Journal of Neurology (2025). Medium credibility.

The nPSD was estimated within the(2–4 Hz),(4–8 Hz),(8–13 Hz) and(13–30 Hz) frequency ranges. In detail, the power spectrum (PSD) was computed in the 1–45 Hz frequency band by applying the modified Welch periodogram on 1‐s Hamming windowed segments with 50% overlap, and the nPSD measure was obtained as follows:whererepresents the frequency range over which the total spectrum was computed, that is, 1–45 Hz, andare the boundary frequencies over which the specific nPSD measure was estimated, andis the PSD at the frequency.

On the other hand, the selected nonlinear algorithms considered are the following.
PLE describes changes in the scale‐free behaviour of the signal. The measure is obtained from the slope of the regression line computed on the PSD of the EEGs in log–log coordinates in the frequency bands 1–3.5 Hz, 4–7 Hz, 8–12 Hz and 13–35 Hz and goes beyond the classic linear measurement. Indeed, the PLE represents the contribution of the non‐oscillatory components in the EEGs, usually not highlighted by the linear spectral analysis performed through the PSD. In order to highlight such components and to avoid the influence of those characterized by rhythmic behaviour, a peak removal operation was performed as suggested by the work of Colombo et al.
DFA describes the degree of signal self‐similarity expressed as the slope of a regression line calculated from the root‐mean‐square fluctuations of the signals in log–log coordinates. The fluctuations are extracted from the integrated and detrended signals at different observation windows. The regression line is then computed by considering in log–log coordinates how the fluctuations change against the increasing size of the observation windows.
It is worth underlining that, from an applicative point of view, what seems to reflect neuronal activity in its dynamic nature are the so‐called long range temporal correlations (LRTC) present in the time series. For this reason, it was decided to estimate the LRTC measure, which consists of applying the DFA procedure to the amplitude envelopes of the EEG oscillatory activity, extracted by applying the band‐pass filter (finite impulse response filter, order 2000 and Hamming window) in the specific frequency band of interest and then the Hilbert transform. In this case, the considered frequency ranges were(4–8 Hz),(8–13 Hz), low‐(13–20 Hz) and high‐(20–30 Hz).
MSE estimates the sample entropy index (SampEn) on different scales to quantify the degree of intrinsic randomness in the signals. The SampEn computes the conditional probability that two similar sequences of m points remain similar at point m + 1. Thus, by considering different EEG sequences and by computing SampEn for each of them, the MSE curve is obtained and can be evaluated in terms of slopes. In detail, the sequences result through the coarse‐grained procedure, a method that generates a new time series by considering the average ofconsecutive samples of the original signal, whereis defined as a scale factor. As thevalue increases, different sequences are obtained. In this work, the number of scale factors considered is 30 and the MSE parameter was evaluated both at low (l‐MSE with) and at high (h‐MSE with) scale factors, whilst the parameters for the SampEn application are set to m = 2 and r = 0.2 times the standard deviation of the given signals.

---

### Reconstructing higher-order interactions in coupled dynamical systems [^111u6WhR]. Nature Communications (2024). High credibility.

The variable x i represents the abundance of species i. The local dynamics of x i is governed by the logistic functionwhere r i and k i are the growth rate and the carrying capacity. The pairwise interactions between species are encoded in the real coefficients of the N × N weighted matrixwith at most N (N − 1) non-zero elements, while the three-body interactions in the real coefficients of the N × N × N weighted tensorwith at mostnon-zero elements. This is because when the interaction does not depend on the permutation of the indices, as is the case of Lotka–Volterra type models, the number of non-zero entries of tensoris. Equation (5) is in the form of Eq. (1) with g (1) (x i, x j) = x i x j and g (2) (x i, x j, x k) = x i x j x k. As an example, we consider the system of N = 7 species with four cooperative and four antagonistic pairwise interactions, studied in ref.and shown in Fig. 1 a with blue and red arrows, respectively. In addition to these pairwise interactions, we have included two cooperative three-species interactions, shown as double arrows in the hypergraph in Fig. 1 a. These respectively correspond to a contribution to the dynamics of x 2 given byand one to x 4 given by, withand. The other system parameters, i.e. the values of r i, k i, i = 1, …, 7, and the initial conditions have been chosen as in ref. Namely, growth rates r i for all species have been randomly selected from a uniform distribution in the interval (0, 1), similarly, the carrying capacities k i are sampled from a uniform distribution in the interval (1, 100), and the initial conditions x i (0) are integers sampled in the interval (10, 100).

---

### A probabilistic Bayesian approach to recovermap and phase images for quantitative susceptibility mapping [^115XpbgJ]. Magnetic Resonance in Medicine (2022). Medium credibility.

Purpose

Undersampling is used to reduce the scan time for high-resolution three-dimensional magnetic resonance imaging. In order to achieve better image quality and avoid manual parameter tuning, we propose a probabilistic Bayesian approach to recover R₂* map and phase images for quantitative susceptibility mapping (QSM), while allowing automatic parameter estimation from undersampled data.

Theory

Sparse prior on the wavelet coefficients of images is interpreted from a Bayesian perspective as sparsity-promoting distribution. A novel nonlinear approximate message passing (AMP) framework that incorporates a mono-exponential decay model is proposed. The parameters are treated as unknown variables and jointly estimated with image wavelet coefficients.

Methods

Undersampling takes place in the y-z plane of k-space according to the Poisson-disk pattern. Retrospective undersampling is performed to evaluate the performances of different reconstruction approaches, prospective undersampling is performed to demonstrate the feasibility of undersampling in practice.

Results

The proposed AMP with parameter estimation (AMP-PE) approach successfully recovers R₂* maps and phase images for QSM across various undersampling rates. It is more computationally efficient, and performs better than the state-of-the-art l₁-norm regularization (L1) approach in general, except a few cases where the L1 approach performs as well as AMP-PE.

Conclusion

AMP-PE achieves better performance by drawing information from both the sparse prior and the mono-exponential decay model. It does not require parameter tuning, and works with a clinical, prospective undersampling scheme where parameter tuning is often impossible or difficult due to the lack of ground-truth image.

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^113JA7ko]. Annals of the American Thoracic Society (2023). High credibility.

Ventilation–perfusion (V A /Q) distribution modeling is binned into 50 equally spaced (log scale) compartments, with V A /Q ratios < 0.005 (shunt) and > 100 (dead space) separately calculated, and relative dispersion and gravitational gradients may also be assessed.

---

### Phase-encoded fMRI tracks down brainstorms of natural language processing with subsecond precision [^114CqMnH]. Human Brain Mapping (2024). Medium credibility.

2.8 Functional image analyses

For each BRIK file of a functional scan containing (x, y, z, t) = 64 × 64 × 55 × 256 data points, a 256‐point discrete Fourier transform was applied to the time series x m (t) of each voxel m at location (x, y, z) by:where X (ω) is the Fourier component at each frequency ω between 0 and 127 cycles per scan, and | X m (ω)| and θ m (ω) represents the amplitude and phase angle, respectively. The task frequency is defined as ω s (16 cycles per scan), denoting the frequency of periodic fluctuations of blood flow in response to periodic stimuli and tasks. The remaining nontask frequencies are defined as ω n. The signal and noise are defined as the Fourier components X m (ω) at frequencies ω s and ω n, respectively. The statistical significance of periodic fluctuations of blood flow is evaluated by the signal‐to‐noise ratio, an F ‐ratio (Chen et al; Huang et al; Sereno et al; Sereno & Huang; Sood & Sereno,), in each voxel m by:where df s = 2 and df n = 230 are the degrees of freedom of the signal and noise, respectively. The p ‐value in each voxel m is estimated by the cumulative distribution function F (2,230) = F (F m; df s, df n) (Chen et al; Huang et al; Press et al.). A complex F ‐value, (F m_r, F m_i), incorporating both the F ‐statistic value and the phase angle, θ m (ω s), of each voxel was computed by F m_r = f m cos(θ m (ω s)) and F m_i = f m sin(θ m (ω s)), where f m is the square root of F m. Voxels containing strong periodic signals at the task frequency (ω s = 16 cycles per scan) with F (2,230) > 4.7 (p < .01, uncorrected), F (2,230) > 7.1 (p < .001, uncorrected), or F (2,230) > 9.6 (p < .0001, uncorrected) were retained and their phase angles were color‐coded in a range between 0 and 2π (0–16 s) and painted on each individual subject's cortical surfaces for each scan using csurf (Figure 3). The complex F ‐values of corresponding voxels m were vector‐averaged (voxel‐wise) across two scans k = {1, 2} of the same task in each session for each subject S using:which was performed by the "Combine 3D Phase Stats" function of csurf. The resulting average complex F ‐valueswere then painted on individual subject's cortical surfaces.

---

### Variable field of view for spatial resolution improvement in continuously moving table magnetic resonance imaging [^115xMCjy]. Magnetic Resonance in Medicine (2005). Low credibility.

An approach is described in which the field of view (FOV) along the Y (right/left) phase encoding direction can be dynamically altered during a continuously moving table (CMT) coronal acquisition for extended FOV MRI. We hypothesize that with this method, regions of the anatomy exhibiting significantly different lateral widths can be imaged with a matching local FOV(Y), thereby improving local lateral spatial resolution. k-space raw data from the variable-FOV CMT acquisition do not allow simple Fourier reconstruction due to the presence of a mixture of phase encodes sampled at different Deltak(Y) intervals. In this work, we employ spline interpolation to reregister the mixed data set onto a uniformly sampled k-space grid. Using this interpolation scheme, we present phantom and peripheral contrast-enhanced MR angiography results demonstrating an approximate 45% improvement in local lateral spatial resolution for continuously moving table acquisitions.

---

### A more accurate method to estimate glomerular filtration rate from serum creatinine: a new prediction equation. modification of diet in renal disease study group [^113MM1BV]. Annals of Internal Medicine (1999). Low credibility.

The clinical calculator "MDRD GFR Equation" for diabetic nephropathy, chronic kidney disease, immunoglobulin A nephropathy, membranous nephropathy, focal segmental glomerulosclerosis, anemia of chronic kidney disease, anti-glomerular basement membrane disease, hepatorenal syndrome, immunoglobulin light chain amyloidosis, infection-related glomerulonephritis, kidney transplantation and membranoproliferative glomerulonephritis.

The MDRD GFR Equation estimates the Glomerular Filtration Rate (eGFR) using serum creatinine, age, gender, and race, adjusted for body surface area. It is widely used for early detection of Chronic Kidney Disease (CKD) and provides an accurate assessment of kidney function without requiring urine collection, making it practical for routine clinical use.

The MDRD GFR Equation calculator helps estimate how well your kidneys are filtering blood, expressed as the estimated glomerular filtration rate (eGFR). This measurement reflects kidney function and helps determine stages of chronic kidney disease (CKD). To use this calculator, you'll need to know four things: gender, age, serum creatinine level, and race.

Serum creatinine levels, integral to the calculation, must first be standardized. If your creatinine level is provided in micromoles per liter (µmol/L), convert it to milligrams per deciliter (mg/dL) by dividing by 88.4. This ensures that all calculations are consistent.

With this information, the MDRD formula estimates the eGFR:

- eGFR = 175 × (creatinine^(−1.154)) × (age^(−0.203))

Specific adjustments are made to this calculation for certain demographics:

- Multiply the result by 0.742 if you are female. This accounts for differences in muscle mass, as women generally have less muscle mass compared to men.
- Multiply by 1.212 if you are Black, as studies have shown distinct genetic factors that influence kidney function.

After computing the eGFR, the value is rounded to the nearest whole number. This final figure places you in one of the CKD stages, each reflecting a range of kidney function:

- **Stage 1**: eGFR of 90 or more, indicating normal kidney function with some kidney damage.
- **Stage 2**: eGFR between 60 to 89, suggesting mild decrease in kidney function.
- **Stage 3a**: eGFR between 45 to 59, indicating a mild to moderate decrease.
- **Stage 3b**: eGFR between 30 to 44, showing a moderate to severe decrease.
- **Stage 4**: eGFR between 15 to 29, indicating severely reduced function.
- **Stage 5**: eGFR less than 15, which is considered kidney failure.

This estimation assists healthcare providers in understanding kidney health over different stages, guiding treatment decisions and health management.

---

### Recommendations for the evaluation of left ventricular diastolic function by echocardiography and for heart failure with preserved ejection fraction diagnosis: an update from the American Society of Echocardiography [^112kY1jw]. Journal of the American Society of Echocardiography (2025). High credibility.

Diastolic function values and prognosis — ARIC cohort data show that the values provided are based on an estimate of the most extreme 10% of values from disease-free individuals, and among > 5,700 older adults (> 65 years of age) increases in incidence of heart failure (HF) or death were observed at septal e′ velocity < 6 cm/s (lower 10th percentile limit 4.6 cm/s) and lateral e′ velocity < 7 cm/s (lower 10th percentile limit 5.2 cm/s); additionally, in this study E/e′ ratio and LAVi demonstrated monotonic and near linear associations with incidence of HF or death without evidence of a clear threshold.

---

### Standardization of spirometry 2019 update. An official American Thoracic Society and European Respiratory Society technical statement [^113vrBWz]. American Journal of Respiratory and Critical Care Medicine (2019). High credibility.

Standardization of spirometry — display and digitization requirements specify acquisition, graph scaling, and start-of-test display. For digitization of the flow or volume signal, the sampling rate must be ≥ 100 Hz with a minimum resolution of 12 bits. For the flow–volume graph, expiratory flow must be plotted upward and expiratory volume toward the right, and a 2:1 aspect ratio must be maintained so that 2 L/s of flow and 1 L of volume are the same distance on their respective axes. For the start of test display, the volume–time graph must begin at the point of maximum inspiration or 1 second before Time 0, whichever occurs first, and should continue to the end of the plateau or the beginning of inspiration. Displays of flow versus volume provide more detail than volume–time graphs for the first 1 second of the FVC maneuver, whereas volume–time graphs provide more detail for the latter part of the maneuver.

---

### Data-driven modeling and prediction of non-linearizable dynamics via spectral submanifolds [^114hPZEj]. Nature Communications (2022). High credibility.

Embedding SSMs via generic observables

If at least some of the real parts of the eigenvalues in (2) are negative, then longer-term trajectory data for system (1) will be close to an attracting SSM, as illustrated in panel (b) of Fig. 2. This is certainly the case for data from experiments that are run until a nontrivial, attracting steady state emerges, see, e.g. in panel (e) of Fig. 1. Measurements of trajectories in the full phase space, however, are seldom available from such experiments. Hence, if data about system (1) is only available from observables, the construction of SSMs and their reduced dynamics has to be carried out in the space of those observables.

An extended version of Whitney's embedding theorem guarantees that almost all (in the sense of prevalence) smooth observable vectorsprovide an embedding of a compact subsetof a d -dimensional SSM, W (E, Ω t; ϵ), into the observable spacefor high enough p. Specifically, if we have p > 2(d + ℓ) simultaneous and independent continuous measurements, y (x), of the p observables, then almost all mapsare embeddings of, and hence the top right plot of Fig. 3 is applicable with probability one.

Fig. 3
Schematics of SSMLearn.

First, he data-driven, SSM-based model reduction algorithm implemented in SSMLearn diagnoses and approximates the dominant SSM from the input data. Next, it constructs a data-driven reduced-order model as an extended normal form on the SSM. Finally, the algorithm uses this model to predict individual unforced trajectories and the response of the system under additional forcing.

In practice, we may not have access to p > 2(d + ℓ) independent observables and hence cannot invoke Whitney's theorem. In that case, we invoke the Takens delay embedding theorem, which covers observable vectors built from p uniformly sampled, consecutive measured instances of a single observable. More precisely, if s (t) is a generic scalar quantity measured at times Δ t apart, then the observable vector for delay-embedding is formed as. We discuss the embedding, of an autonomous SSM, W (E, Ω t 0; 0), in the observable spacein more detail in the Methods section "Embedding the SSM in the observable space".

---

### CurveCurator: a recalibrated F-statistic to assess, classify, and explore significance of dose-response curves [^115E3kpa]. Nature Communications (2023). High credibility.

Curves were classified as significantly up or down-regulated if a curve's relevance score is above the decision boundary.

For many applications, it is also useful to know which dose–response curves show clear independence of the doses to obtain a high-quality negative data set. It is fundamentally impossible to prove the absence of an effect statistically. Thus, we developed a heuristic approach using the null model to classify only a clear non-responsive line to be not regulated. A clear non-responder has a mean-model intercept close to 1.0 and is allowed to maximally diverge +- fc lim /2. Additionally, the variance around the null model should be low and is quantified by the root-mean-squared error (RMSE). By default, the maximally tolerated variance is an RMSE of 0.1 but can be adjusted by the user. Optionally, the user can add additional criteria, such as a p -value threshold, to be even more restrictive to the non-classification.

False discovery rate estimation

The false discovery rate (FDR) was estimated using a target-decoy approach. Decoy curves were generated based on the sample variance distribution estimated from the input data directly (Eq. 15). From this variance distribution, decoys were constructed similarly to the null curves (Eq. 1).

The decoy curves are then subject to the identical analysis pipeline. Finally, the target-decoy relevance score allows the calculation of a q -value for each curve using the target-decoy approach, as well as the overall FDR corresponding to the user's pre-defined thresholds (Eq. 15). FDR estimation is enabled using the — fdr command line option.

Statistics and reproducibility

No new experimental data were generated for this study. Study designs, including sample sizes, were taken directly from the respective original studies. For the simulated curve data, no statistical method was used to determine sample size. The 5 million curves for each number of curve data points n was chosen as the number of curves that produced sufficient smooth characterization of p -value estimation behavior up to 10⁻⁵. No data were excluded from the analyses. Unless otherwise stated, p -values in this study were generated using the recalibrated F-statistic and multiple testing correction was performed by applying a target-decoy strategy on the relevance score. The CurveCurator version used throughout this manuscript is v0.2.1.

Reporting summary

Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article.

---

### STANCE: a unified statistical model to detect cell-type-specific spatially variable genes in spatial transcriptomics [^114GNjCq]. Nature Communications (2025). High credibility.

Simulation 2: evaluation of cell-type-specific test

Simulation 2 aims to evaluate the performance of the STANCE individual test in detecting ctSVGs. Similar to Simulation 1, we generated 4000 cells randomly distributed within a unit square, each assigned precise x and y coordinates. These cells were divided into two spatial domains: the spatially variable domain (S D) and the non-spatially variable domain (D) (Fig. 4 c), following the procedure outlined below. First, one of the 4000 cells was randomly selected to serve as the center of S D. The radius of this domain was then independently sampled from a uniform distribution between 0.2 and 0.4. Any cell that fell within this circular area was assigned to the S D domain, while the remaining cells were assigned to the D domain.

Each cell was assigned to one of three cell types based on a categorical distribution, with probabilities of 10% for cell type 1, 30% for cell type 2, and 60% for cell type 3, reflecting low, medium, and high proportions, respectively (Fig. 4 b). We simulated the expression of 1000 genes per cell using a series of negative binomial distributions with a mean of 1 and dispersion parameters of 0.7 and 1.5. Spot-resolution spatial transcriptomics data were then simulated based on the single-cell resolution data generated from the above procedure. The unit square was divided into 400 spots using a grid size of 0.05. For each spot, we aggregated the expression counts of all cells within it to determine spot-level expression and calculated cell type compositions. Additionally, the coordinates of each spot were based on the mean x and y coordinates of the cells within that spot.

---

### Real-time estimates in early detection of SARS [^111QcU8F]. Emerging Infectious Diseases (2006). Low credibility.

Detecting the Effect of Control Measures

In Figure 2, the method is used to estimate the impact of control measures implemented on day 20 in the simulated datasets with completely effective or limited control measures. The curves show the temporal pattern of R t based on an average over the 500 simulated datasets as a function of T. Even when control measures are completely effective, based on data available up to day 21, the average expectation of R 20 is ≈3. Based on data available up to day 25, a downward trend is apparent, whereas based on data available up to day 29, the average expectation of R t is < 1 from t = 27 days. Based on data available up to day 40 (20 days after the implementation of the control measures), the estimates indicate that the threshold value 1 is crossed at day 22, which is 2 days after control measures were implemented. With limited control measures, the observed changes are qualitatively the same, although slightly more time is required for R t estimates to decrease to < 1.

Figure 2
Average expectation of the temporal pattern of R t after implementation of control measures according to the day T of the last observation. A) Completely effective control measures. B) Limited control measures. Simulation values of R are also given: before day 20, R = 3; after day 20 R = 0 (A) and R = 0.7 (B). The gray zone indicates that R is < 1. Information that the average expectation of R has passed < 1 was obtained 9 (A) and 12 (B) days after control measures were implemented.

---

### Guidelines for the echocardiographic assessment of the right heart in adults: a report from the American Society of Echocardiography endorsed by the European association of echocardiography, a registered branch of the European Society of Cardiology, and the Canadian society of echocardiography [^111MaG3s]. Journal of the American Society of Echocardiography (2010). Medium credibility.

Table 6 diastolic function — Doppler and tissue Doppler reference values are provided with study counts and lower/mean/upper reference ranges, including: E (cm/s) 55 studies, n 2866, 35 (33–37), 54 (52–56), 73 (71–75); A (cm/s) 55 studies, n 3096, 21 (19–24), 40 (38–41), 58 (55–60); E/A ratio 56 studies, n 2994, 0.8 (0.7–0.9), 1.4 (1.4–1.5), 2.1 (2.0–2.2); deceleration time (ms) 25 studies, n 1284, 120 (105–134), 174 (163–186), 229 (214–243); E' (cm/s) 40 studies, n 1688, 8 (7–9), 14 (13–14), 20 (19–21); and E/E' ratio 3 studies, n 359, 2 (1–2), 4 (4–4), 6 (5–7). Age effects are noted: "average E/A ratio = 1.6 in the third decade of life, decreasing by 0.1 for every subsequent decade".

---

### Consensus statement of the Academy of Nutrition and Dietetics / American Society for Parenteral and Enteral Nutrition: indicators recommended for the identification and documentation of pediatric malnutrition (undernutrition) [^11113r6a]. Nutrition in Clinical Practice (2015). Medium credibility.

Adolescent basal metabolic rate (BMR) equations are provided for ages 10–18 years: Boys 10–18 y BMR = 16.6 x weight (kg) + 77 x height (m) + 572, and Girls 10–18 y BMR = 7.4 x weight (kg) + 42.7 x height (m) + 217; additional rows address energy estimation for children with developmental disabilities and note applicability to the pediatric critical care population.

---

### Longitudinal reference centiles for the gross motor function measure-66 in children and adolescents with cerebral palsy [^111GnAN6]. Developmental Medicine and Child Neurology (2025). Medium credibility.

Aim

To establish novel longitudinal reference values for the Gross Motor Function Measure-66 (GMFM-66) in children and adolescents with cerebral palsy aged 3 to 18years, to enable more accurate assessments of changes in motor function.

Method

This was a single-centre retrospective analysis of patients who participated in a rehabilitation programme between January 2006 and March 2022. The GMFM-66 was used to measure gross motor function. Paired GMFM-66 measurements from the follow-up phase of the rehabilitation programme were used to establish a reference centile for the change in GMFM-66 over a 6-month period using the lambda-mu-sigma method.

Results

Reference centiles for GMFM-66 changes (over a 6-month period; ± 1month) were created using 1190 longitudinal data pairs of GMFM-66 measurements (mean age 8years 4months [standard deviation 7years 11months] at start of follow-up), Gross Motor Function Classification System levels I to V. The z-scores for GMFM-66 change of a validation dataset by the new tool and the previously described method to quantify a change in GMFM-66 by individual effect size were highly correlated (Pearson's rank correlation coefficient 0.981 [95% confidence interval 0.979–0.984], p < 0.001) INTERPRETATION: The new reference values showed a high correlation with the previously published reference values, which were limited to an age range of 3 to 12years. The new reference values can be applied from an age of 3 to 18years. This facilitates the evaluation of medical treatment after a 6-month period also for children with cerebral palsy who are older than 12years.

---

### Roflumilast (Zoryve) [^116oLut2]. FDA (2024). Medium credibility.

The dosage of roflumilast TOP for treatment of plaque psoriasis in adults is 1 application(s) TOP daily (0.3% cream or foam)

---

### Guidance for incorporating FDA processes into the ACC / AHA clinical practice guideline methodology: a report of the American college of cardiology / American Heart Association joint committee on clinical practice guidelines [^1114T82P]. Journal of the American College of Cardiology (2025). High credibility.

Blood pressure thresholds and recommendations for treatment and follow-up — The algorithm categorizes normal BP, elevated BP, stage 1 hypertension, and stage 2 hypertension with associated actions and reassessment intervals. Normal BP (< 120/80 mm Hg) receives lifestyle therapy with reassess in 1 y, while elevated BP (120–129/ < 80 mm Hg) receives lifestyle therapy with reassess in 3–6 mo. Stage 1 Hypertension (130–139/80–89 mm Hg) begins with lifestyle therapy and reassess in 3–6 mo, then proceeds to the decision nodes "Stage 1 Hypertension? (130–139/80–89 mm Hg)" and "DM, CKD or increased short-term risk of CVD (10-year PREVENT risk ≥ 7.5%)?"; if yes, the pathway specifies BP-lowering medication AND reassess in 1 mo, followed by "BP goal met?" with "YES" leading to reassess in 3–6 mo and "NO" prompting to consider intensification of therapy while optimizing adherence. Stage 2 Hypertension (≥ 140/90 mm Hg) is displayed with lifestyle therapy, a left-hand directive states "Promote Lifestyle Therapy at All Thresholds", and the legend lists recommendation categories COR 1, COR 2a, COR 2b, COR 3: No Benefit, and COR 3: Harm; the figure notes it is "Based on the PREVENT calculator".

---

### ASNC imaging guidelines for nuclear cardiology procedures: standardized reporting of nuclear cardiology procedures [^114dBe5M]. Journal of Nuclear Cardiology (2017). Medium credibility.

ASNC standardized reporting — resting left ventricular diastolic function quantification is specified as the variable "Resting LV diastolic function — quantitative LV peak filling rate", entered as a numerical optional field with the response formatted as X.XX EDV/second; LV denotes left ventricular and EDV end-diastolic volume.

---

### 2019 methodology for creating expert consensus decision pathways: a report of the American college of cardiology [^114FME3X]. Journal of the American College of Cardiology (2019). High credibility.

Expert Consensus Decision Pathway timeline and scope — The timeline for development is more compressed than a Clinical Practice Guideline, usually in the range of 8 to 12 months, during which time the Work Group tracks emerging evidence and incorporates it into the pathway. Determination of the scope is a critical early step, and the Work Group should keep the scope wide enough to provide guidance on the most important clinical questions yet not so broad as to be unmanageable within the narrow timeframe typically allotted for content development.

---

### ACFAS scoring scale user guide [^115MutFk]. The Journal of Foot and Ankle Surgery (2005). Medium credibility.

ACFAS Scoring Scale Module 4: Ankle — function scoring totals Function (32 points) with Range of Motion (18 points) distributed as dorsiflexion, knee extended (7 points): 10–15˚ (7), 16–20˚ or 5–9˚ (4), > 20 or < 5˚ (0); plantarflexion (4 points): > 30˚ (4), 15–29˚ (2), < 15˚ (0); rearfoot (calcaneal inversion/eversion) (7 points): 25–45˚ (7), 10–24˚ (4), < 10˚ or > 45˚ (0); plus Balance (Rhomberg Test) (9 points): 15–20 seconds (9), 5–15 seconds (6), < 5 seconds (0); and Limp due to Ankle Pain (without shoes) (5 points): No (5), Yes (0).

---

### Does a first-case on-time-start initiative achieve its goal by starting the entire process earlier or by tightening the distribution of start times? [^115VTD7S]. British Journal of Anaesthesia (2018). Low credibility.

Background

We explored whether a previously successful initiative to improve first-case on-time starts succeeded because (i) preoperative steps started earlier (but the process did not necessarily improve) or (ii) the process was brought into better control.

Methods

We analysed 35 months of data comprising 28 882 first cases to calculate the difference of the time a patient entered the operating room (OR) vs the scheduled entry time. Median and inter-quartile range were used to evaluate changes in distribution parameters. A statistical process-control methodology was used to compare the differences in performance between the pre- and post-intervention phases.

Results

Post-intervention first cases entered the OR on average within 4 min [95% confidence interval (CI): 4–5 min] of the scheduled start time, as opposed to within 8 min (95% CI: 8–8 min) in the pre-intervention period. The median delay decreased from 5 min (95% CI: 5–5 min) to 2 min (95% CI: 2–2 min). The inter-quartile range of the difference between the scheduled start time and the first case in room time decreased from 13 min (95% CI: 13–13 min) to 10 min (95% CI: 9–10 min).

Conclusions

The reduction in inter-quartile range demonstrates that improvement in on-time starts resulted from the process being in better control. The start time of preoperative preparatory activities did not move earlier, which means that OR and preoperative staff do not need to arrive at work earlier. Improvements resulting from the process being in control were sustained.

---

### The snm procedure guideline for general imaging 6.0 [^1143n8HP]. SNMMI (2010). Medium credibility.

Gated imaging — electrocardiogram (ECG) gating is used to synchronize image acquisition with the patient's heart rate, and the number of frames per R-R interval should be no less than 16 for ejection fraction measurements and 32 for time-based measurements. For gated cardiac blood pool imaging, electronic zoom generally should be used to magnify the field of view to approximately 25 cm, a matrix size of 64 x 64 is sufficient, and typically a total of at least 5 million counts in the entire study will provide sufficient statistics for quantitative and functional image processing.

---

### Phase 0 / microdosing approaches: time for mainstream application in drug development? [^114uansU]. Nature Reviews: Drug Discovery (2020). High credibility.

Phase 0 approaches - which include microdosing - evaluate subtherapeutic exposures of new drugs in first-in-human studies known as exploratory clinical trials. Recent progress extends phase 0 benefits beyond assessment of pharmacokinetics to include understanding of mechanism of action and pharmacodynamics. Phase 0 approaches have the potential to improve preclinical candidate selection and enable safer, cheaper, quicker and more informed developmental decisions. Here, we discuss phase 0 methods and applications, highlight their advantages over traditional strategies and address concerns related to extrapolation and developmental timelines. Although challenges remain, we propose that phase 0 approaches be at least considered for application in most drug development scenarios.

---

### Fluctuation spectra of large random dynamical systems reveal hidden structure in ecological networks [^115Kki3z]. Nature Communications (2021). High credibility.

Computing the power spectral density

In the following, we use features of the bipartite interaction network. For instance, all nodes that are connected to e.g. node x i, will be prey nodes y j, and thus are not connected with each other (see Fig. 6). This allows us to write the following recursion formulas for the mean power spectral densities according to Eq. (42),

Recall that the top left entries of Ψ x and Ψ y deliver the mean power spectral densities for predators ϕ x and prey ϕ y respectively. For the bipartite model, the helping matrices χ i, χ i j (as defined in Eq. (23)) are given by,

Inserting and writing out Eq. (59) gives, where c x, c y are the number of connections per predator and prey species respectively. Analogous to Eq. (38) we now derive a system of equations and solve for r x, r y and ϕ x, ϕ y. In the main text we describe the features of the power spectral density deduced from this system of equations.

Interpreting the power spectral density in the context of temporal stability

For orientation, we here provide some interpretation of the power spectral density in the context of temporal stability. Essentially when we talk about temporal stability, we can be referring to one of two measures. The first is how far stochastic trajectories tend to stray from their equilibrium value over long time horizons. We refer to this as 'variability'. The second is how quickly population abundances tend to change over finite time horizons. We will characterise this by the 'temporal autocorrelation'.

The variability can be characterised by the variance in time-averaged trajectories around the mean. For a system such as Eq. (2), which we recall can be a linear approximation for a nonlinear system such as Eq. (1), we find that ξ is normally distributed with zero mean and a covariance matrix, Σ, that solves the following Lyapunov equation;

---

### Estimating the effectiveness of routine asymptomatic PCR testing at different frequencies for the detection of SARS-CoV-2 infections [^113TBmdy]. BMC Medicine (2021). Medium credibility.

Fig. 1
Testing and symptom data for the 27 individuals used in the analysis. Each point represents a symptom report and PCR test result. The border of the point is green if the PCR test result was positive and purple if it was negative. The inside of the point is red if the individual reported symptoms and white if they did not. Black crosses show the date of the initial negative serological test. Points are aligned along the x -axis by the timing of each participant's last asymptomatic report

We developed a Bayesian model to jointly infer both the likely infection time for each individual and the probability of a positive PCR test depending on the time since infection across all individuals. We used a likelihood function specifically for inferring parameters from censored data to derive a posterior distribution for the time of infection. This accounts for the fact that the true onset time is censored, i.e. symptom onset for each individual could have occurred anywhere between their last asymptomatic report and their first symptomatic report. Specifically, individual i has their likely infection time, T i, inferred based on the interval between their last asymptomatic report, and their first symptomatic report. The log-likelihood for the infection time for person i is as follows:

where F is the cumulative density function of the lognormal distribution for the incubation period of COVID-19 as estimated in Lauer et al. For a detailed description of the procedure used to arrive at the onset times from the censored data and list of the sources of uncertainty in our model, see Additional file 1: Section D.

For a given inferred infection time for person i, the relationship between the time since infection and a positive PCR test on person i, administered at time t n, i is given by a piecewise logistic regression model with a single breakpoint:

where C is the time of the breakpoint, x is the amount of time between infection and testing minus the value of the breakpoint, I (x) is a step function that equals 0 if x < 0 or equals 1 if x > 0, and the β terms define the regression coefficients fit across all tests and people (see Table 1 for parameter details).

---

### Identification of signal bias in the variable flip angle method by linear display of the algebraic ernst equation [^116ggzHA]. Magnetic Resonance in Medicine (2011). Low credibility.

A novel linear parameterization for the variable flip angle method for longitudinal relaxation time T(1) quantification from spoiled steady state MRI is derived from the half angle tangent transform, τ, of the flip angle. Plotting the signal S at coordinates x = Sτ and y = S/τ, respectively, establishes a line that renders signal amplitude and relaxation term separately as y-intercept and slope. This representation allows for estimation of the respective parameter from the experimental data. A comprehensive analysis of noise propagation is performed. Numerical results for efficient optimization of longitudinal relaxation time and proton density mapping experiments are derived. Appropriate scaling allows for a linear presentation of data that are acquired at different short pulse repetition times, TR < < T1 thus increasing flexibility in the data acquisition by removing the limitation of a single pulse repetition time. Signal bias, like due to slice-selective excitation or imperfect spoiling, can be readily identified by systematic deviations from the linear plot. The method is illustrated and validated by 3T experiments on phantoms and human brain.

---

### Cutting edge or blunt instrument: how to decide if a stepped wedge design is right for you [^112pSeFT]. BMJ Quality & Safety (2021). High credibility.

When might I consider doing a stepped wedge trial?

Research designs are shaped as much by practical constraints as by abstract schemes, and it is always a good idea to start with the constraints and work towards a design, rather than start with a design and try to fit it to constraints. These constraints will be unique to each research context, and box 1 lists some areas to think about. Still, there are some common features of settings where a stepped wedge trial might be considered as a possible design, and we now review these.

Box 1
Practical constraints on the design of a longitudinal cluster randomised trial

Are there limits on the time available to complete the evaluation, on the number of clusters, or on the number of participants (or the rate at which you can recruit participants) at each cluster? These constraints put limits on the overall scale of the evaluation, or force trade-offs between different design characteristics.
How will participants and their data be sampled in your study: as a series of cross-sectional surveys, as a continuous stream of incident cases, as a cohort followed over time, or some other way? Does the timescale divide into cycles, seasons or milestones that influence how you will sample participants and data?
Is there a limit on how many clusters can implement the intervention at the same time in the evaluation? If this is constrained by research resources (eg, if there are only enough trained research staff to implement the intervention one cluster at a time) then implementation must be staggered in some way.
If implementation is to be staggered, is there a minimum 'step length'? If the same team delivers the intervention in different clusters at different steps, then bear in mind it may take some time to get the intervention fully operational at a site, and the team will also need time to relocate from one cluster to the next.

---

### An analysis of multirules for monitoring assay quality control [^115b6P1B]. Laboratory Medicine (2020). Medium credibility.

Background

Multirules are often employed to monitor quality control (QC). The performance of multirules is usually determined by simulation and is difficult to predict. Previous studies have not provided computer code that would enable one to experiment with multirules. It would be helpful for analysts to have computer code to analyze rule performance.

Objective

To provide code to calculate power curves and to investigate certain properties of multirule QC.

Methods

We developed computer code in the R language to simulate multirule performance. Using simulation, we studied the incremental performance of each rule and determined the average run length and time to signal.

Results

We provide R code for simulating multirule performance. We also provide a Microsoft Excel spreadsheet with a tabulation of results that can be used to create power curves. We found that the R4S and 10x rules add very little power to a multirule set designed to detect shifts in the mean.

Conclusion

QC analysts should consider using a limited-rule set.

---

### Altered grid-like coding in early blind people [^115jyMn8]. Nature Communications (2024). High credibility.

Path integration analyses

As for the experimental design, analyses were conducted following. Participants' distance estimations were corrected using the standardization paths data to reduce the biases in the computation of the error not directly ascribable to the participants' path integration ability but rather to their tendency to underestimate or overestimate distances in real life. First, the participants' answers obtained at the beginning and the end of the experiment in the 'standardization paths' were averaged for each specific distance, 5 m and 10 m. Subsequently, we calculated the correction factor (C f) for both distances as follows:Where d standardize was the actual length of the path, and d response was the participants' answer. Second, the computed correction factors were multiplied by the participants' estimated distances during the task. Reported distances from 4 m to 7.5 m were corrected using the 5 m C f, and those greater than 7.5 m using the 10 m C f. Corrected distances (d corrected) and orientations (Ori) estimations of each stopping point were combined to calculate the x and y coordinates of the presumed starting position:Where x real and y real corresponded to the real coordinates of the starting position of each trial. In order to compute the path integration error of each stopping point, the Euclidean distance between the stopping point and the presumed starting point estimated at the previous stopping point was calculated (PI Error). For the first stopping point, the previous presumed starting position corresponded to the real starting position of the path. This procedure allowed us to calculate PI independently for each path segment, reducing the possible biases produced by a cumulative computation of it. Lastly, the calculated errors were averaged across stopping points and trials, and the performance was computed as follows:

---

### Limits on the computational expressivity of non-equilibrium biophysical processes [^116Sk959]. Nature Communications (2025). High credibility.

Improving expressivity by increasing input multiplicity

Biologically, F a can be interpreted, for example, as depending on the chemostatted activity of an enzyme (see the Methods). In biochemical kinetics, it is common for some species to be involved in multiple reactions simultaneously, making it plausible for F a to drive multiple edges. We find that allowing for input multiplicity improves classification expressivity, and one way this happens is by lifting the monotonicity constraint. We assume for simplicity that each of the D input variables, whereis the set of input labels, affects the same numberof edges. Setting M > 1 lifts the monotonicity constraint because the condition for π k (F i j) to be a monotonic function is that all other edge parameters are held fixed; with M > 1 this is no longer true since several edge parameters change simultaneously as an input is varied.

To better understand the gain in the decision boundary's flexibility allowed by setting M > 1, in the Supplementary Information we analyze the steady-state representation in the rational polynomial form of the matrix-tree expression, Eq. (2). Considering the case D = 1 and identifying turning points as roots of ∂ π i /∂ F a, we show that the maximum number R of such roots obeyswhich is a direct measure of the classifier's expressivity; see Fig. 3 E for an illustration and the Supplementary Information for a numerical verification up to M = 4. A proof of the scaling 2 M − 1 for rational polynomials with non-negative coefficients can be found in ref. Thus, once M > 1, π i is no longer subject to the monotonicity constraint and behaves like a non-negative rational polynomial of degree up to 2 M. Input multiplicity thus allows the non-equilibrium biological process to be more expressive and draw out decision boundaries that can classify more complex data structures. Indeed, returning to the previously failed classification with M = 1 (Fig. 3 C), we see that setting M = 2 allows the same network to now learn a decision boundary which successfully encloses the data assigned to class 1 (Fig. 3 F). This implies that classifying a finite band of input signal levels (like a band-pass filter) requires setting M > 1 along the corresponding input dimension. A recent development in synthetic biology has in fact shown in a specific example that drug binding to receptor molecules via two distinct binding pathways can be used to design band-pass-like responses to the drug (Fig. 3 G).

---

### Erratum: evolutionary history resolves global organization of root functional traits [^111jNHqm]. Nature (2018). Excellent credibility.

This corrects the article DOI: 10.1038/nature25783.

---

### Guidelines for neuroprognostication in adults with traumatic spinal cord injury [^114dM19G]. Neurocritical Care (2024). High credibility.

Functional outcome measures after traumatic spinal cord injury — assessment tools, scales, and timing are described: Functional outcome assessment is most often performed with the Functional Independence Measure (FIM) or Spinal Cord Independence Measure (SCIM). FIM scores range from 1 to 7; the FIM motor score is constructed from 13 subscales and ranges from 13 to 91, and functional independence is assumed once scores of 6 or higher are reached on all subscales. Scores are expected to improve in the first 3 months and plateau within 6–9 months, and in clinical studies FIM-based outcome assessments are typically performed at 1 year, with 6 month-scores acceptable when 1-year outcomes are unavailable.

---

### Prostate cancer, version 3.2026, NCCN clinical practice guidelines in oncology [^114BrSSn]. Journal of the National Comprehensive Cancer Network (2025). High credibility.

NCCN Prostate Cancer — principles of radiation therapy lists external beam radiation therapy (EBRT) regimen examples with dose–fraction schedules in columns labeled Definitive RT, Post-Treatment RT, and Advanced Disease. Conventional fractionation examples include 1.8–2 Gy x 37–45 fx and 1.8–2 Gy x 30–39 fx; moderate hypofractionation options include 3 Gy x 20 fx (preferred)^a, 2.7 Gy x 26 fx, 2.5 Gy x 28 fx, and 2.63–2.75 Gy x 20 fx with 2.5 Gy x 25 fx; ultra hypofractionation/stereotactic body radiotherapy (SBRT) examples include 9.5 Gy x 4 fx, 7.25–8 Gy x 5 fx, 6 Gy x 6 fx, 6.1 Gy x 7 fx, 9–10 Gy x 3 fx, 12 Gy x 2 fx, and 16–24 Gy x 1 fx. EBRT boost techniques include an EBRT with simultaneous integrated boost (See footnote b) and an EBRT with sequential SBRT boost of Prostate: 1.8 Gy x 23–28 fx with Boost: 6 Gy x 3 fx or 9.5 Gy x 2 fx. Symbol definitions are provided (✓ Preferred; ✱ Acceptable based on clinical and medical need; Regimens shaded gray are not recommended), and the note states All recommendations are category 2A unless otherwise indicated.

---

### Erratum: non-adaptive plasticity potentiates rapid adaptive evolution of gene expression in nature [^116vc433]. Nature (2018). Excellent credibility.

This corrects the article DOI: 10.1038/nature15256.

---

### Limits on the computational expressivity of non-equilibrium biophysical processes [^112nx4B9]. Nature Communications (2025). High credibility.

Discussion

We have explored the computational expressivity of classifiers implemented in trained non-equilibrium biochemical networks, which we model as Markov jump processes. An analytical solution for the steady states of these systems can be written in several equivalent ways, highlighting complementary interpretations of the classifier as computing a linear softmax operation using learnable, as computing a rational polynomial function with learnable scalar coefficients (Eq. (2)), and nonlinear feature vectors (Eq. (3)). The feature vectors and coefficients are themselves complicated functions of the tree weights of the physical network, and because of this dependency they are significantly constrained relative to abstract parametric classifiers having the same functional form as the matrix-tree expression. We identified several limitations to expressivity, including monotonic responses π k (F i j) and a reduction in degrees of freedom of the classifier function. We further showed that increasing input multiplicity (setting M > 1) helps mitigate these limitations, by creating additional turning points of π k (F i j) and allowing the number of degrees of freedom in the graph to saturate at 2 N e. With even modest input multiplicity, chemical reaction-based classifiers prove to be capable of solving difficult classification tasks, demonstrating non-linear information processing performance reminiscent of neural networks.

Key biological implications follow from the sensitive dependence of computational expressivity on the input multiplicity hyperparameter M, which we define as the number of edges driven by a single input variable. Biologically, M > 1 occurs when a single input variable, such as activity of an enzyme, temperature, or chemical potential gradient, simultaneously affects more than one chemical transition. Input multiplicity in a biochemical network may at first glance seem counterproductive because it decreases the network's modularity, but our results show that it serves to significantly expand a network's computational capabilities. In the context of cooperative binding, there is also a relationship between M and the Hill coefficient, which determines the sharpness of switch-like input responses. We hope to connect our general findings to specific biochemical systems in the future. For example, systems like the glycan code (Fig. 1 E) are known to involve promiscuous enzymes which attach sugar molecules to proteins, as well as high levels of cross-talk in the receptor-ligand interactions mediating cellular communication; our results suggest that these forms of high input multiplicity may play a crucial role in enabling efficient molecular information processing.