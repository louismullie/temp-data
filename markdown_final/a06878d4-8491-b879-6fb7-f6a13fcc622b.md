# Medical Question & Answer

**Sample ID**: a06878d4-8491-b879-6fb7-f6a13fcc622b
**Dataset Index**: 375525

---

## Question

Let's determine the integer solutions of the equation

3ⁿ = 2n² + 1

---

## Answer

> Let's see… What do we have here? The user is asking for the integer solutions of the equation 3^n = 2n^2 + 1. Let's break this down step-by-step. First, I need to think about testing small non-negative integers to find obvious solutions. Then, I should verify whether negative integers can work. Next, I will analyze growth rates to argue why larger integers cannot satisfy the equation. Finally, I will synthesize the complete set of integer solutions and double-check for any overlooked cases.

> Let me first confirm the base cases by plugging in small non-negative integers. For n = 0, the left side is 3^0 = 1 and the right side is 2(0)^2 + 1 = 1, so n = 0 is a solution. For n = 1, 3^1 = 3 and 2(1)^2 + 1 = 3, so n = 1 is also a solution. For n = 2, 3^2 = 9 and 2(2)^2 + 1 = 9, so n = 2 works as well. For n = 3, 3^3 = 27 versus 2(3)^2 + 1 = 19, so 27 > 19 and equality fails. For n = 4, 3^4 = 81 versus 2(4)^2 + 1 = 33, again 81 > 33, so equality fails. These checks give me three candidate solutions so far: 0, 1, and 2 [^notfound].

> Wait, let me verify whether negative integers could satisfy the equation. For n < 0, 3^n becomes a positive fraction less than 1, whereas 2n^2 + 1 is always a positive integer greater than or equal to 1, so the left side can never equal the right side for negative integers. Therefore, there are no negative integer solutions, and I can restrict my search to non-negative integers [^notfound].

> I will now examine the growth behavior to see if larger n can ever catch up. The exponential function 3^n grows much faster than the quadratic function 2n^2 + 1, and once 3^n surpasses 2n^2 + 1, it stays ahead. From the small-n tests, that crossover already happens by n = 3, where 27 > 19, and the gap widens as n increases, so there should be no solutions beyond n = 2. This growth-rate argument supports that 0, 1, and 2 are the only non-negative solutions [^notfound].

> Hold on, let's not jump to conclusions; I should double-check that I'm not missing a larger n where coincidence might occur. Let me consider the ratio 3^n / (2n^2 + 1). For n = 3 the ratio is 27/19 ≈ 1.42, for n = 4 it is 81/33 ≈ 2.45, and for n = 5 it is 243/51 ≈ 4.76, showing the ratio increases without bound as n grows, which confirms that the exponential dominates and no larger solutions exist. Thus, my earlier conclusion stands [^notfound].

> Putting this together, I should confirm the final answer. The complete set of integer solutions to 3^n = 2n^2 + 1 is n = 0, 1, and 2. I have verified these directly and ruled out negative integers and larger positive integers via growth-rate analysis, so I can be confident in this result [^notfound].

---

The integer solutions to (3^n = 2n^2 + 1) are **n = 0, 1, and 2**. For n ≥ 3, (3^n) grows faster than (2n^2 + 1), so there are no further solutions. For n < 0, (3^n) is a positive fraction less than 1, while (2n^2 + 1) is an integer ≥ 1, so there are no negative solutions.

---

## Verification of solutions

We first verify the candidate solutions by direct substitution:

| **N** | **Left-hand side (3^n)** | **Right-hand side (2n^2 + 1)** | **Equality** |
|-|-|-|-|
| 0 | 1 | 1 | True |
| 1 | 3 | 3 | True |
| 2 | 9 | 9 | True |
| 3 | 27 | 19 | False |
| 4 | 81 | 33 | False |

---

Thus, **n = 0, 1, and 2 are solutions**.

---

## Analysis for n ≥ 3

For n ≥ 3, we show that (3^n > 2n^2 + 1) by induction:

- **Base case (n = 3)**: (3^3 = 27 > 19 = 2(3)^2 + 1).
- **Inductive step**: Assume (3^k > 2k^2 + 1) for some k ≥ 3. Then:

3^(k+1) = 3 × 3^k > 3(2k² + 1) = 6k² + 3

We need to show (6k^2 + 3 > 2(k+1)^2 + 1 = 2k^2 + 4k + 3), which simplifies to (4k^2–4k > 0), or (4k(k - 1) > 0). This holds for k ≥ 2, and thus for k ≥ 3.

Therefore, **no solutions exist for n ≥ 3**.

---

## Analysis for n < 0

For n < 0, let n = -m where m is a positive integer. Then:

3ⁿ = 3^(−m) = [1/3^m] and 2n² + 1 = 2m² + 1

Since (\\frac{1}{3^m} < 1) and (2m^2 + 1 \\geq 3) for m ≥ 1, there are **no negative integer solutions**.

---

## Conclusion

The only integer solutions to (3^n = 2n^2 + 1) are **n = 0, 1, and 2**.

---

## References

### Harmonization of quality metrics and power calculation in multi-omic studies [^4134567c]. Nature Communications (2020). High credibility.

Sample size scenarios in MultiPower

In multi-omic studies, an assorted range of experimental designs can be found. All omic assays may be obtained on the same biological samples or individuals, which would result in identical replicates number for all data types. However, this is not always possible due to restrictions in cost or biological material, exclusion of low-quality samples, or distributed omic data generation. In these cases, sample size differs among omic types and yet the data are to be analyzed in an integrative fashion. MultiPower contemplates these two scenarios.

Under the first scenario, adding the constraint of equal sample size for all omics (x i = x for all I = 1,…, I) to the optimization problem in Eq. (1) results in a straightforward solution. First, the minimum sample size required to meet the constraint on the minimum power per omic (x i) is calculated and the initial estimation of x is set to x = max I { x i }. Next, the second constraint on the average power is evaluated for x. If true, the optimal sample size is x opt = x. Otherwise, x is increased until the constraint is met. Note that, under this sample size scenario, the cost per replicate does not influence the optimal solution x opt.

Under the second scenario, allowing different sample sizes for each omic, the optimization problem in Eq. (1) becomes a nonlinear integer programming problem, as the statistical power is a nonlinear function of x i. The optimization problem can be transformed into a 0–1 linear integer programming problem by defining the auxiliary variablesfor each omic i and each possible sample size n from 2 to a fixed maximum value n i max, wherewhen the sample size for omic i is n. The new linear integer programming problem can be formulated as follows:where f i (n) is the power for sample size n, given the parameters of omic i.

---

### A neural theory for counting memories [^b797eb11]. Nature Communications (2022). High credibility.

Theorem 2'

There is an absolute constant c for which the following holds. Suppose the neural count sketch sees n observations satisfying Assumption 1' with. Pick any 0 < δ < 1.
Suppose that m ≥ 2 k n and thatfor a positive integer f. Then with probability at least 1 − δ, when presented with a query x (i) with 0 ≤ f i ≤ f, the response of the neural count sketch will lie in the range f i ± 1.
Suppose that m ≥ 2 k / ϵ for some ϵ > 0 and that. Then with probability at least 1 − δ, when presented with a query x (i), the response of the neural count sketch will lie in the range f i ± ϵ n.

Note that the query x (i) need not belong to the original sequence of n observations, in which case f i = 0.

Theorem 5 gives bounds that are significantly more favorable for the 1-2-3-many sketch.

Theorem 5'

Suppose the 1-2-3-many sketch, with parameter β = 1, witnesses n observations that satisfy Assumption 1' with. Pick any 0 < δ < 1 and suppose that m ≥ 2 k N and. Then with probability at least 1 − δ, when presented with a query x (i), the response of the sketch will be e − r for some value r that is either f i or f i + 1 when rounded to the nearest integer.

Overall, these mathematical proofs provide bounds on how accurately stimuli can be tracked using the two neural count sketches.

The Drosophila mushroom body implements the anti-Hebbian count sketch

Here, we provide evidence supporting the "1-2-3-many" model from the olfactory system of the fruit fly, where circuit anatomy and physiology have been well-mapped at synaptic resolution. The evidence described below includes the neural architecture of stimulus encoding, the plasticity induced at the encoding-decoding synapse, and the response precision of the decoding (counting) neuron. The latter two we derive from a re-analysis of data detailing novelty detection mechanisms in the fruit fly mushroom body, where odor memories are stored.

---

### Estimating with confidence the risk of rare adverse events, including those with observed rates of zero [^766792a7]. Regional Anesthesia and Pain Medicine (2002). Low credibility.

Omission of a confidence interval (CI) associated with the risk of a serious complication can lead to inaccurate interpretation of risk data. The calculation of a CI for a risk or a single proportion typically uses the familiar Gaussian (normal) approximation. However, when the risk is small, "exact" methods or other special techniques should be used to avoid overshooting (risks that include values outside of [0,1]) and zero width interval degeneration. Computer programs and simple equations are available to construct CIs reasonably accurately. In the special case in which the complication has not occurred, the risk estimated with 95% confidence is no worse than 3/n, where n is the number of trials.

---

### Approximations to the expectations and variances of ratios of tree properties under the coalescent [^cf6c6a02]. G3 (2022). Medium credibility.

We have found that approximations for fixed n and in the limit asare quite accurate in predicting the expected values seen in coalescent simulations of the ratios (Fig. 2). For the variances, the approximations are generally less accurate, although in most cases, graphs of the approximations and simulated values have similar shape (Fig. 4). These approximations are obtained from a Taylor approximation for the variance of a ratio (equation 4), and higher-order approximations of this variance could potentially be applied by use of Taylor's theorem; as the order of the approximation increases, however, the complexity of the resulting formula also increases. For those variances for which the approximation and simulation are not close in Fig. 4, we advise caution in using the variances in settings in which a precise approximation is needed.

---

### Post hoc power analysis: is it an informative and meaningful analysis? [^7002a8fe]. General Psychiatry (2019). Medium credibility.

For power analysis, we want to determine the probability to reject the null H 0 in favour of the alternative H a. Given the type I error α, sample size n 1 and n 2, and H 0 and H a, we can calculate power for reject the null H 0:

Although similar in apperance, equation (3) is actually quite different from equation (2). equation (2) is generally used to compute p values, in which caseand s are all readily computed from the sample means and sample SD from the observed outcomes. As none of these quantities is available when performing power analysis for prospective studies, the power function in equation (3) is evaluated based on sample sizes, n 1 and n 2, the population mean difference δ and SD σ. Thus, unlike data analysis, all these parameters must be explicitly specified in order to compute power. Although results from similar studies may be used to help suggest and specify δ and σ, they are both conceptually and analytically different from their sample counterparts. Conceptually, δ and σ are determined by the entire study population, while δ and s are specific to a random sample from the study population. Analytically, δ and σ are population-level constants, whereas δ and s are random quantities and vary from sample to sample. The population parameters can be quite different from their sample counterparts.

In practice, we often set power at some prespecified levels and then perform power analysis to determine the minimum sample size to achieve the desired levels of power. We can use the power function equation (3) for this purpose as well. For example, if we want to determine sample sizes n 1 and n 2 to achieve, say, 0.8 power, we can solve for n 1 and n 2 in the following equation:

Note also that power functions can also be evaluated by replacing the mean difference δ and SD σ with the composite effect size d. Unlike δ, d is unit free and well interpreted, with d = 0.2, 0.5 and 0.8 representing small, medium and large effect size.

---

### Flexible categorization in perceptual decision making [^548f587f]. Nature Communications (2021). High credibility.

Model for n -choice decision making

To model a categorization task with n = 3 choices (Supplementary Fig. 5) we simulated a system of standard nonlinear coupled rate equations (see e.g. Equation (38) in):where ξ i is a Gaussian white noise process with amplitude σ for i = 1,2,3 and amplitude σ I for i = I, and with transfer function ϕ (x) = 0 for x < 0, ϕ (x) = x 2 for 0 ≤ x ≤ 1 andfor x > 1. The n = 4 case is a simple extension of these equations (the general system of rate equations for n > 2 can be found in). The parameters used for the simulations were s = 0.694, c = g = , τ = 20 ms, τ I = 10 ms, I I = 0, and σ I = 0. For n = 3 the inputs were taken as I 1 = I + 2∆ I /3, I² = I − ∆ I /3, I 3 = I − ∆ I /3, while for n = 4 they were I 1 = I + 3∆ I /4, I² = I − ∆ I /4, I 3 = I − ∆I /4, I 4 = I − ∆ I /4, with I = 2.25 and ∆ I = 0.03*s. For the top panels in Supplementary Fig. 5 the values of the noise strength where σ = 0.18 and σ = 0.13 for n = 3 and 4, respectively. The accuracy was calculated as the fraction of trials (N = 10,000) in which the highest firing rate at the end of the trial was r 1.

Reporting Summary

Further information on research design is available in the Nature Research Reporting Summary linked to this article.

---

### Using grid cells for navigation [^f5d9ba0e]. Neuron (2015). Low credibility.

Algorithmic Solution in 1D

The cyclical nature of the grid representation within each module i is such that an activity bump at phase p i implicitly represents an infinite set of "unwrapped" phases p i + 2 πn i, where n i can take any integer value, corresponding to an infinite set of distances s i (p i /2 π + n i) along that 1D axis that are separated by the scale s i of module i. Initially, we assume that all phases are zero at the current location a and the distance d to a goal location b must be inferred from the grid cell representation across modules at that location { p i (b)} = { p 1 (b), p 2 (b),…, p M (b)}. The grid representation of the goal location b is such that there is a set of unwrapped phases (one for each module) that explicitly represent the same distance — i.e. there is a set of integers { n i } for which:

Graphically, this coherent set of unwrapped phases across modules falls on a horizontal line when plotted against a y axis of represented distance (i.e. y = d; Figure 4 A) or, equivalently, on a straight line through the origin when plotted against a y axis of phase against inverse grid scale (Equation 2; Figure 4 B) — i.e. there is a set of integers { n i } for which:

This latter relationship is obtained by re-arranging Equation 1 and corresponds to the Fourier shift theorem (see discussion of the Algorithmic Solution in 2D below;). Thus, the distance d to a goal location represented by the set of module phases { p i } can be inferred by fitting a straight line through the origin on a plot of unwrapped phases p i + 2 πn i against inverse grid scale 1/ s i across modules (Figure 4 B). Moreover, this result can be generalized to any pair of arbitrary current and goal locations on that 1D axis, by replacing the absolute phase p i at the goal location with the phase difference Δp i between grid cell representations of current and goal locations in each module (Equation 3):

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^cbffc27e]. CDC (2011). Medium credibility.

Appendix G. Mathematical formula for sample size — sample size required to detect the difference of GM between two subpopulation groups defines a z test with null hypothesis GM1 = GM2 or GMmax/GMmin = 1 and alternative GM1 ≠ GM2 or GMmax / GMmin > 1, where GMmax = max(GM1, GM2) and GMmin = min(GM1, GM2). It considers power W (say 80% chance) to detect that one group's GM is at least k fold higher and gives the p-value as p = Prob(Z ≥ z) = 1 − φ−1(z) with Z ~ Normal (0,1). Given α (e.g., 0.05) and target ratio k, power is W = Prob (Z > φ−1(1 − α) − log10(k)/(S√(1/n1 + 1/n2))), where S is the expected standard deviation of log10(VL) in the population of interest; when n1 = n2 = n, W = Prob (Z > φ−1(1 − α) − log10(k)/(S√(2/n))). The required sample size is n = 2[(φ−1(1 − α) − φ−1(1 − W))S/log10(k)]^2, and tables are provided for W = 0.8 and W = 0.9 with α = 0.05.

---

### Approximations to the expectations and variances of ratios of tree properties under the coalescent [^8355e038]. G3 (2022). Medium credibility.

Properties of gene genealogies such as tree height (H), total branch length (L), total lengths of external (E) and internal (I) branches, mean length of basal branches (B), and the underlying coalescence times (T) can be used to study population-genetic processes and to develop statistical tests of population-genetic models. Uses of tree features in statistical tests often rely on predictions that depend on pairwise relationships among such features. For genealogies under the coalescent, we provide exact expressions for Taylor approximations to expected values and variances of ratios Xn/Yn, for all 15 pairs among the variables {Hn, Ln, En, In, Bn, Tk}, considering n leaves and 2 ≤ k ≤ n. For expected values of the ratios, the approximations match closely with empirical simulation-based values. The approximations to the variances are not as accurate, but they generally match simulations in their trends as n increases. Although En has expectation 2 and Hn has expectation 2 in the limit as n→∞, the approximation to the limiting expectation for En/Hn is not 1, instead equaling π2/3–2≈1.28987. The new approximations augment fundamental results in coalescent theory on the shapes of genealogical trees.

---

### A stochastic vs deterministic perspective on the timing of cellular events [^ea7bfa54]. Nature Communications (2024). High credibility.

To build intuition for the opposite case, that is, targets that are a small proportion of the steady-state mean, we again look to the extremes. If the target N is chosen such that N < r, that is, strictly below the burst size, then the expected waiting time to surpass this in the stochastic case is simply the expected waiting time to the first event, which is 1/ K; this is independent of N. But as N tends to 0, the deterministic solution also tends to 0, so that a small enough choice of N will place it faster than 1/ K; the precise boundary for this can be derived as N < K r (1 − e −1/ K) ≈ r. Note that this condition cannot hold for the simple birth–death process as r = 1 and N is discrete. In Fig. 2 D (bottom), we see that this difference extends to targets that are much higher than the burst size; here N = 40 and r = 10. The waiting time as predicted by the deterministic solution (blue vertical line) is indeed smaller than the MFPT to reach N (red vertical line). Comparing the trajectories in Fig. 2 D top and bottom, we see that the deterministic solution is faster than the stochastic solution when the target threshold is low, in agreement with our theoretical argument above. In Fig. 2 F we show how η scales with increasing K, for a low threshold value. The differences are most pronounced when the steady-state mean molecule numbers are small.

Here we considered a fixed burst size r, however, experimentally a geometric burst size distribution has been observed. We show that these results extend to the case where the burst size is geometrically distributed with mean b (Supplementary Information, Section 5).

---

### A game theoretic approach identifies conditions that foster vaccine-rich to vaccine-poor country donation of surplus vaccines [^124211e3]. Communications Medicine (2022). Medium credibility.

Numerical methods

To calculate the optimal solution, we calculated the expected cost to a given country i, given the set of strategies that all the countries adopt, we first calculate P 1 and P 2 (Eq. 1), and then we setand substitute the results in Eq. (2). In turn, we examine all possible sets of strategies, where in each set, each country can contribute a different amount (s i equals 0, 1, or 2 for each i). For each set, we calculate the total cost (Eq. (3)), and we find the set for which the total cost is minimized.

Next, we did the following to check whether the optimal solution is a Nash equilibrium. Without loss of generality, we examine whether country 1 can benefit from deviating if all the countries adopt the optimal solution. Specifically, country 1 has 2 possible deviations: toorif, and toorif. We first calculate, the expected cost to country 1 if it does not deviate (and the other countries adopt). Then, we calculateand, the expected cost to country 1 following each of its possible deviations. We conclude that the optimal solution is a Nash equilibrium if and only ifand.

In turn, we did the following to check whether the optimal solution is a self-enforcing international agreement. For every, we consider n signatories andnon-signatories, and we calculate the utility (minus the cost – Eq. (2)) of a signatory, and the utility of a non-signatory. (Note that, for each n). In turn, an agreement withsignatories is by definition the optimal solution. This solution is a self-enforcing international agreement if a country cannot benefit from opting-out when all N countries are signatories. Specifically, if a country remains a signatory, its utility is, while if it opts-out, its utility becomes. Therefore, the optimal solution is self-enforcing if and only if.

Statistics and reproducibility

This study is quantitative mathematical and computational. No statistical method is used. All results are fully reproducible: The methods are fully described in the Methods section and parameter values are given in the caption of each relevant figure.

Reporting summary

Further information on research design is available in the Nature Research Reporting Summary linked to this article.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^dee6cacc]. CDC (2011). Medium credibility.

Community viral load sample size calculations — sample size to detect differences in geometric mean (GM) viral load depends on power and standard deviation (S). The table specifies "α = 0.05 and W = 0.8", with S columns 1 to 1.5; for k = 3 the minimum sample sizes by S are 54, 66, 78, 92, 106, and 122, and "1.2 is the standard deviation of national VL data", so at S = 1.2 the needed sample size is 78.

---

### Post-hoc power analysis: a conceptually valid approach for power based on observed study data [^bb63137d]. General Psychiatry (2022). Medium credibility.

For power analysis, we want to determine the probability to reject the null H 0 in favour of the alternative H a under H a for the hypothesis in Equation 3. To compute power, we need to specify the H 0, H a, type I error α, and sample size n 1 and n 2, with n k denoting the sample size for group k (k = 1,2). Given these parameters, power is the probability of rejecting the null H 0 when the alternative H a is true:

where Pr (A|B) denotes the conditional probability of the occurrence of event A given event B, Z denotes a random variable following the standard normal distribution N (0,1), Z α/2 denotes the upper α/2 quantile of N (0,1), anddenotes the cumulative distribution function of the standard normal distribution N (0,1). If we condition on the null H 0 instead of H a in Equation 4, we obtain a type I error, as in Equation 5:

which is the probability of rejecting the null H 0 when the null is true. For power analysis, we generally set type I errors at α = 0.05, so that Z α/2 = Z 0.025 = 1.96.

In practice, we often set power at some prespecified levels and then perform power analysis to determine the minimum sample size to detect a prespecified effect size Δ with the desired level of power. For example, if we want to determine sample size n per group to achieve, say, 0.8 power with equal sample between two groups, we can obtain such minimum n by solving for n in the following equation:

When applying Equation 3 for a post-hoc power analysis in a study, we substitute the observed effect sizein place of the true effect size Δ. This observedis calculated based on the observed study data with sample size n 1 and n 2:

---

### Article 5. An introduction to estimation – 2: from z to T [^fb82b806]. Emergency Medicine Journal (2001). Low credibility.

Provided the sample size is large enough (that is, n greater than 100), the z statistic can be used to determine the confidence interval estimation of the population mean even when the sigma is not known. In these cases the estimation of the standard error of the mean is used. The z statistic is also valid when determining the population's proportion based upon a large sample. However, when dealing with smaller samples, the z statistic is replaced by the t statistic. This makes it possible to estimate, in a population with an unknown standard deviation: The probability of getting a sample mean greater than or equal to a particular value The value of a sample mean with a particular probability of occurring The probability of getting a sample mean between two particular values The confidence interval for the estimation of the population mean can also be determined using the t statistic.

---

### Odor-evoked inhibition of olfactory sensory neurons drives olfactory perception in drosophila [^1a0d9717]. Nature Communications (2017). Medium credibility.

Under steady-state conditions, that is, d P n /d t = 0 (n = 1, 2, 3, 4), and with the detailed balance condition α ′/ K A, ij = α / K I, ij satisfied, the steady-state probabilities of the four microscopic states can be obtained by requiring that the forward and backward fluxes be equal for each transition pair:

By solving the above equations, we obtain P 1, P 2, P 3, P 4. The total probability of receptors being in the active state (both ligand bound and unbound) iswhich makes the average activity of the OR/OSN

The above result is the same as Eq. 3 obtained from the effective free energy difference given by Eq. 2. To generalize to the case with multiple odors acting on the same OR/OSN, the energetic approach is much easier to use because the free energy difference is additive, see Eq. 2. Of course, the same result can be obtained from solving the steady state of the kinetic equations. Here, we briefly show the case for two odors (indexed as i, k), where there are four different active states due to the bound/unbound states of the two ligands. The total probability of being active is thenwhere P 1 is the probability of the inactive state without any odor binding. The above equation can be generalized to N odors, with 2 N different odor (ligand) binding combinations for the active state of the receptor. The corresponding active probability isand the average activity of the OSN j iswhich is exactly Eq. 3.

For the competitive binding interaction, that is, all ligands compete for the same binding sites, it is easy to follow the same analysis as above. Given N odors, there are 2(N + 1) microstates, and half of them correspond to active state. The corresponding active probability isand the average activity of the OSN j iswhich is exactly Eq. 6. Therefore, the energetic approach and the kinetic approach lead to the same results.

Statistics

All experiments were performed with experimental and control groups in parallel. Sample size was determined based upon preliminary experiments. Data were analyzed statistically using one-way ANOVA t test, presented as mean ± SEM, unless otherwise stated.

Data availability

All relevant data supporting the findings of this study are available from the corresponding author on request.

---

### Best (but oft-forgotten) practices: checking assumptions concerning regression residuals [^f2ebb771]. The American Journal of Clinical Nutrition (2015). Low credibility.

The residuals of a least squares regression model are defined as the observations minus the modeled values. For least squares regression to produce valid CIs and P values, the residuals must be independent, be normally distributed, and have a constant variance. If these assumptions are not satisfied, estimates can be biased and power can be reduced. However, there are ways to assess these assumptions and steps one can take if the assumptions are violated. Here, we discuss both assessment and appropriate responses to violation of assumptions.

---

### Giant five-photon absorption from multidimensional core-shell halide perovskite colloidal nanocrystals [^83687b88]. Nature Communications (2017). Medium credibility.

Multiphoton action cross-section spectra in perovskite NCs

Figure 2a shows a summary of the excitation wavelength dependence of the slopes (that is, orders of MPA processes) for these halide perovskite NCs spanning from 675 to 2,300 nm. The slopes of PL from the NCs are around 2 in the wavelength range of 675–1,000 nm, clearly indicating 2PA. As the excitation wavelength increases to 1,050–1,500 nm, the slopes increase to around 3 — revealing a switch of the excitation mechanism to 3PA. 4PA processes (with slopes around 4) dominate as the excitation wavelength is further increased to the range of 1,550–2,000 nm. In the long wavelength range 2,050–2,300 nm, the slopes are around 5, indicating the dominance of 5PA processes. Due to the NCs' size inhomogeneity, ησ n at the wavelength boundaries contains an admixture of contributions from both the lower- and higher-order MPA process (that is, 2/3PA, 3/4PA and 4/5PA and so on). Instead of a sharp transition at the wavelength boundaries (of 1,050, 1,550 and 2,050 nm), the slope deviates from the integer value by∼20%. Figure 2b–d shows a direct comparison of the NCs' MEPL brightness (that is, ησ n) for lower orders, n = 2, 3 and 4, over the wavelength range 675–2,000 nm. Similar to ησ 5, their spectral dependences exhibit a general overall decreasing trend with increasing wavelength. Their corresponding σ n values obtained through applying the calculation equations (see Supplementary Notes for details) and employing the σ 2 values at 800 nm measured by Z -scan as a standard are given in Figure 2b–d insets. In all these cases, the core–shell MAPbBr 3 /(OA) 2 PbBr 4 NCs possess∼6–8 times larger ησ n values than the core-only MAPbBr 3 NCs. The ησ n spectra suggest that the growth of the 2D (OA) 2 PbBr 4 shell over MAPbBr 3 NCs only enhances their MPA and PLQY, while hardly changing their spectral response.

---

### An experimental test of noncontextuality without unphysical idealizations [^b2fd1de5]. Nature Communications (2016). Medium credibility.

Preparations are represented by vectors of raw data specifying the relative frequencies of outcomes for each measurement, uncertainties on which are calculated assuming Poissonian uncertainty in the photon counts. For each run, the raw data is fit to a set of states and effects in a GPT in which three binary-outcome measurements are tomographically complete. This is done using a total weighted least-squares method. The average χ 2 over the 100 runs is 3.9 ± 0.3, agreeing with the expected value of 4, and indicating that the model fits the data well (see Supplementary Note 4, Supplementary Data 1 and 2, and Supplementary Software 1). The fit returns a 4 × 8 matrix that serves to define the 8 GPT states and 4 GPT effects, which are our estimates of the primary preparations and measurements. The column of this matrix associated to the t, b preparation, which we denote, specifies our estimate of the probabilities assigned by the primary preparationto outcome '0' of each of the primary measurements. The raw and primary data are compared in Fig. 3. The probabilities are indistinguishable on this scale. We plot the probabilities for P 1, P 2, and P 3 in Fig. 4a on a much finer scale. We then see that the primary data are within error of the raw data, as expected given the high quality of the fit to the GPT. However, the operational equivalences of equations (2) and (4) are not satisfied by our estimates of the primary preparations and measurements, illustrating the need for secondary procedures.

We define the six secondary preparations as probabilistic mixtures of the eight primaries: where theare the weights in the mixture. We maximizeover validsubject to the constraint of equation (4), that is,(a linear program). A high value of C P ensures that each of the six secondary preparations is close to its corresponding primary. Averaging over 100 runs, we find C P = 0.9969 ± 0.0001, close to the maximum of 1. An analogous linear program to select secondary measurements yields similar results. Supplementary Tables 3 and 4 display the weights that define each secondary preparation and measurement, averaged over 100 runs. Figure 3 also displays the outcome probabilities for the secondary procedures, confirming that they are close to ideal. Figure 4 demonstrates how our construction enforces the operational equivalences.

---

### Inferring replication timing and proliferation dynamics from single-cell DNA sequencing data [^1d071d02]. Nature Communications (2024). High credibility.

Equations for steps 2–3

Steps 2–3 have equations which differ from Step 1 since it must account for replicated bins and cannot solve for μ n analytically. The probability of each bin being replicated (ϕ n, m) is a function of the cell's time in S-phase (τ n), the locus's replication timing (ρ m), and the replication stochasticity term (α). Replication stochasticity (α) controls how closely cells follow the global RT profile by adjusting the temperature of a sigmoid function. The following equation corresponds to function block g :Equations corresponding to function block f differ from those in Step 1. The total CN (χ n, m) is double the somatic CN (X n, m) when a bin is replicated (Y n, m = 1), The GC rates (ω n, m) and negative binomial event counts (δ n, m) are computed the same as in Step 1 (Eq (2), Eq (4)). However, the expected read count uses total instead of somatic CN, Since CN is learned in Steps 2–3, the coverage/ploidy scaling term (μ n) must also be learned. We use a normal priorwhere the approximate total ploidy and total read counts are used to estimate the mean hyperparameters. Total ploidies for each cell are approximated using the CN prior concentrations (η) and times within S-phase (τ) to account for both somatic and replicated copies of DNA that are present. We fixed the standard deviation hyperparameters to always be 10x smaller than the means to ensure that μ n ≥ 0 despite use of a normal distribution (used for computational expediency)

---

### Experimental noise cutoff boosts inferability of transcriptional networks in large-scale gene-deletion studies [^65bff624]. Nature Communications (2018). Medium credibility.

Case N 0 = 1

We assume that the i -th node is the only unperturbed node in the network and hence set B il = 0 for all l. From Eq. (11) we obtain a unique solution for the i -th row of A relative to the diagonal element, A ii, with U j 1 the j -th element of the eigenvector that has the smallest eigenvalue. Note that the first term in the brackets vanishes as B il = 0 andis the only non-zero element of Σ 0. The important point is that any dependency on σ –which affects eigenvalues but not eigenfunctions–has dropped out, making this method asymptotically unbiased with respect to measurement noise. The fact that we can determine the elements of the i -th row of A only relative to a reference value, A ii, is rooted in fact that we have to determine the N parameters { A i 1,…, A ii,…, A iN } from N − 1 perturbations. As a consequence, the strengths of the links onto the target nodes cannot be compared directly if their restoring forces or degradation rates, A ii, are different. Generally, only relative values of A can be determined, as the average perturbation strength on node i cannot be disentangled from its restoring force A ii –a problem that is typically circumvented by definingfor all i. For the case that all nodes in the network are perturbed one-by-one, we can cycle through the network and remove the perturbations that act on the current receiver node, whereas keeping the perturbations on the remaining nodes. By computing the N corresponding covariance matrices and their eigenvectors, we can infer the complete network structure from Eq. (14) if the data quality is sufficiently high. Note that the method makes use of the fact that multi-node perturbations can be realised by superposition of single-node perturbations, which is a special property of linear models.

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^bb9b7f12]. Nature Communications (2025). High credibility.

After n refinement steps, we thus obtain a functiondefined at discrete points:which we extend to the entire interval [0, 1) by linear interpolation; see Fig. 5 d for an illustration. In practice we found that computations (specifically the risk computed by integrating) converge after about eight refinement steps.

This procedure has the important property that once a point is sampled, it does not change on further refinements:which follows from equation (67). Recall now that, as stated above, a process is self-consistent if "for small enough Δ Φ, the probability distribution at a point Φ [does] not depend on the level of refinement". Since equation (70) clearly satisfies that requirement, we see that the process obtained after infinitely many refinement steps is indeed self-consistent. We thus define the hierarchical beta (HB) process as

To complete the definition of, we need to specify how we choose the initial end pointsand. In our implementation, they are drawn from normal distributionswith Φ ∈ {0, 1}, where again c is determined via our proposed calibration procedure; this is simple and convenient, but otherwise arbitrary. We also need to explain how we choose the beta parameters α and β, which is the topic of the next subsection.

Choosing beta distribution parameters

All HB processes are monotone, continuous and self-consistent, but within this class there is still a lot of flexibility: since α and β are chosen independently for each subinterval, we can mouldinto a wide variety of statistical shapes. We use this flexibility to satisfy the two remaining desiderata: a) that realisationstrackover Φ ∈ [0, 1]; and b) that the variability ofbe proportional to. It is the goal of this subsection to give a precise mathematical meaning to those requirements.

Let x 1 ~ Beta(α, β) and x 2 = 1 − x 1. (The density function of a beta distribution is given in (24).) The mean and variance of x 1 areFor a given Φ, it may seem natural to select α and β by matchingtoandto. However both equations are tightly coupled, and we found that numerical solutions were unstable and unsatisfactory; in particular, it is not possible to make the variance large whenapproaches either 0 or 1 (otherwise the distribution of x 1 would exceed [0, 1]).

---

### A space-time tradeoff for implementing a function with master equation dynamics [^57fa10b0]. Nature Communications (2019). High credibility.

Master equations are commonly used to model the dynamics of physical systems, including systems that implement single-valued functions like a computer's update step. However, many such functions cannot be implemented by any master equation, even approximately, which raises the question of how they can occur in the real world. Here we show how any function over some "visible" states can be implemented with master equation dynamics-if the dynamics exploits additional, "hidden" states at intermediate times. We also show that any master equation implementing a function can be decomposed into a sequence of "hidden" timesteps, demarcated by changes in what state-to-state transitions have nonzero probability. In many real-world situations there is a cost both for more hidden states and for more hidden timesteps. Accordingly, we derive a "space-time" tradeoff between the number of hidden states and the number of hidden timesteps needed to implement any given function.

---

### Evolutionary dynamics of any multiplayer game on regular graphs [^10918f06]. Nature Communications (2024). High credibility.

Applying Eqs. (2) and (3) to Eq. (4), we can transform the expected accumulated payoff in the replicator equations into the expected single-game payoff, as shown in Eq. (13) in the Methods, which keeps the simplest irreducible computational complexity given the payoff structure a i ∣ k. In particular, we only need to calculate two types of quantities, andfor i, j = 1, 2,…, n, based on the given payoff structure a i ∣ k. The diagonal elements of these quantities coincide, as demonstrated in Eqs. (14) and (15) in the Methods. Therefore, for any given payoff structure a i ∣ k, there are at most (2 n − 1) n distinct quantities to calculate manually when determining the replicator equations. The computational complexity is thus O(n 2), square of the number of strategies, which can be solved within polynomial time. We also find that the computational complexity under the death-birth rule is O(n 3), cubic of the number of strategies, which can also be solved within polynomial time (Supplementary Information).

For specific payoff structures, the computational complexity can be further reduced. A common example is linear systems. In such systems, the payoff function includes at most linear terms in k 1, k 2,…, k n. This allows us to express the general payoff function as, where b i j represents the coefficient of the linear term and c i is the constant term for i, j = 1, 2,…, n. Applying this special payoff structure to Eq. (13) in the Methods, we can obtain a simplified form of the replicator equation for linear systems, Here, anddenote the mean payoff of i -players and all players in a well-mixed population, which can be directly calculated using the traditional replicator dynamics approach (Methods).

---

### Non-additivity of molecule-surface van der waals potentials from force measurements [^0dec3a03]. Nature Communications (2014). Medium credibility.

Weighted least-squares regression

We use a weighted least-squares regression, minimizing the quantity

when fitting the N data points within the fit interval j = 1,… N. Choosing the right weights w j is a non-trivial task.

In the case of a linear fit, the goodness of fit (gof) is best described by the reduced χ 2, defined as

where x i is the i th measured point andthe corresponding value of the fitted curve. Here the respective weighting function is the inverse of the variance σ 2 that is a measure of the statistical noise in the measured curve. While with this definition one obtains χ 2 = 1 for a perfect fit, there is no general rule of how close a reduced χ 2 should be to 1 for a good fit.

For a linear fit, the reduced χ 2 as defined above is a suitable gof criterion, because according to this definition, each data point of a linear data set has the same chance of contributing to the overall χ 2 value. However, in our case, we do not perform a linear fit. Rather, we fit a force law that is proportional to z −5 and moreover our experimental noise is practically constant over the whole fit interval. To gauge the quality of our fit to experimental data following this force law, we must ensure that the entire measured curve contributes to our gof criterion. Otherwise, experimental information would be lost, because any part of the measured data curve that does not contribute significantly to the gof criterion has no influence on the outcome of the fit. For this reason, we use the gof criterion in equation (8) with the weighting function

---

### Structural permeability of complex networks to control signals [^2ed83acc]. Nature Communications (2015). Medium credibility.

Our solution to problems 1 and 2 is based on the following sequence of steps. First, the graphof the network is augmented with additional nodes representing the external control signals to be injected into the network. These are initially connected by means of outgoing and inbounding edges to all nodes of the network. The construction of the augmented graphis concluded by adding self-loops to all nodes that do not already have one. The graphis then partitioned into disjoint cycles. Among all possible cycle partitions of, the one is selected that maximizes the number of edges ofentering nodes accessible from the drivers. This task is performed by solving an integer linear programme (ILP). The set of controllable nodesis given by the nodes of the cycle partition ofthat either have as inbounding edge an edge of, or an edge exiting the nodes that represent the input signals. As the additional edges allow to reduce directed paths to cycles, the optimality of our method is guaranteed by Hosoe's theorem.

An example of application of our method for problem 1 and M = 1 is shown in Fig. 2; the orange node in Fig. 2b represents the input signal, while the orange and blue edges are those added to enable the formation of a cycle partition. The nodes with inbounding black or orange edges are those encompassed in. A detailed description of our method for the solution of problems 1 and 2 is given in Supplementary Note 2, along with further details on its optimality.

Structural permeability of complex networks to control signals

By solving problem 1 for each value of M in the interval of integers [1, N] without restrictions on the admissible, target and untouchable nodes, we obtain the sequence of sets of optimal driver node Ω D (M) and the corresponding. Figure 3a portrays the sequencefor the Budding Yeast Protein Structure network (red) and the SciNet citation network (blue), see Supplementary Table 1 for more information on these networks. The question of which one has a greater propensity to be controlled is nontrivial, as, apparently, it varies depending on the value of M. Namely, in Fig. 3a, we observe that for small values of M, the red curve lies above the blue one, whereas the opposite is observed for M > 0.1 N.

---

### Experimental noise cutoff boosts inferability of transcriptional networks in large-scale gene-deletion studies [^981a9ed7]. Nature Communications (2018). Medium credibility.

Case N 0 > 1

If more than one node are not perturbed we get from Eq. (11)Non-unique solutions of Eq. (15) can arise if a given fraction of the variance of the receiver node i can be explained by more than one sender node, for example, when a perturbed node j targets two nodes with index i and l. In this case it is unclear from the node activity data whether i is affected directly by j or indirectly through l, or by a combination of both routes. If node l is not perturbed or only weakly perturbed, a statistical criterion is needed to decide about inferability or identifyability of the link j → i, which can be computed numerically as follows: To find out whether j transmits a significant amount of information to i that is not passing through l, we first remove node j from the observable nodes of the network but keep its perturbative effect on other nodes in the data set. We then determine the link strengths A ′ for the remaining network of size N − 1. To construct a possible realisation of A ′ we set in Eq. (15) the non-zero values of Σ 0 to unity and use W = U to arrive at the expressionwith U ′ determined from the sample covariance matrix with the j -th column and j -th row removed. Fixing W and Σ 0 to seemingly arbitrary values does not affect the result we are after. If l is the only unperturbed node besides i, then in the A ′ system l can now be treated as perturbed — as it may receive perturbations from the unobserved node j — and thus Eq. (14) applies. If l is part of many unperturbed nodes that are affected by j, then the knowledge how much each of these nodes contributes to the variance of the target node i (which is determined by W and Σ 0) is irrelevant as we are only interested in the total effect of the alternative routes on node i. Using the inferred link strength from Eq. (16) we can rewrite Eq. (2) as a two-node residual inference problem between j and i, where we obtain a lower bound for link strength from node j to i by using the variation of i that could not be explained by A ′. This concept is similar to computing partial correlations. Defining by, andthe 2 × 2 analogues to the full problem we obtainwiththe covariance matrix of the vectorand, using the scaled variances. Note that A ii < 0 for all i as these elements represent sufficiently strong restoring forces that ensure negative definiteness of A and that we havefrom Eq. (1) in the stationary case. An estimate for the minimum relative link strength from node j to node i can be calculated from Eq. (13) and is given byEq. (18) can be considered as an asymptotically unbiased response coefficient between node 1 as target node and node 2 as source node, as again any dependency on σ 2 has dropped out. An estimate for the maximum relative link strength from node j to node i follows from Eq. (18) with the off-diagonal elements of A ′ set to zero. We classify a link as non-inferable if there exists (i) a significant difference between the minimum und maximum estimated link strength and (ii) a minimum link strength that is not significantly different from noise.

---

### The making of the standard model [^0fc9a4ce]. Nature (2007). Excellent credibility.

A seemingly temporary solution to almost a century of questions has become one of physics' greatest successes.

---

### Thinking outside the curve, part I: modeling birthweight distribution [^b60d1510]. BMC Pregnancy and Childbirth (2010). Low credibility.

b. Simulation study on calibrating confidence intervals

For our second simulation study we generated 25 overlapping data sets of size 50,000 from design C in Table 3, the degree of overlap consistent with a population of 200,000. For each of various C between 2.0 and 5.0, we used Equation (7) to form confidence intervals for the mixture parameters p 1, p 2, p 3, p 4, μ 1, μ 2, μ 3, μ 4, σ 1, σ 2, σ 3, σ 4. We recorded how many of the mixture parameters were contained in their respective confidence intervals. This was repeated nine more times, and we tabulated how many of the 120 = 12 × 10 confidence intervals contained their targets. Confidence intervals were also formed using Equation (6) for comparative purposes. The above steps were repeated with overlapping data sets consistent with a population of 1,000,000 and with nonoverlapping data sets consistent with an effectively infinite population.

The results are summarized in Table 5. With an effectively infinite population, only 81.7% of the confidence intervals formed using Equation (6) contained their targets at C = 5.0. The confidence intervals formed using Equation (7) contained their targets 95.0% of the time at C = 2.5. The adjustment suggested by Equation (8) appears reasonable: φ = .05 = 50,000/1,000,000 and N rep = 25 yield C φ = 1.315 C 0, which accords with the 95.8% capture of mixture parameters at C = 3.5 ≈ 1.315 × 2.5 with a population of 1,000,000.

Table 5
Confidence Interval Coverage Probabilities in Simulation Studies

The row with " C " = 2 and "Population size" = 200,000 identifies the numbers and percentages of confidence intervals containing their targets of mixture parameters, based on 10 repetitions in each of which 25 samples of size 50000 were simulated from a 4-component normal mixture with 12 parameters; results under the heading of "Bias adjustment included" are based on Equation (7) with C = 2, results under the heading of "Bias adjustment omitted" are based on Equation (6) with C = 2, and the 25 samples of size 50000 had overlap consistent with a population size of 200,000. Other rows correspond to different choices of C and/or population sizes.

---

### Integer topological defects offer a methodology to quantify and classify active cell monolayers [^0ce989ed]. Nature Communications (2025). High credibility.

The linear theory can be refined by considering the anisotropic friction γ = γ 0 (I − ϵ Q), with the anisotropy coefficient 0 ≤ ϵ ≤ 1 representing larger friction along the direction perpendicular to cell alignment. However, a comparison of the measured θ v with θ v predicted from this extended simple theory showed that the theory failed to explain the inward cellular flows for all defect types (Fig. 3 i).

We modeled our system to understand the underlying mechanisms driving inward cellular flows, starting from a minimal particle-/cell-level dry active nematic model where induced patterns were attributed to some external field (Supplementary Information). Numerical simulation results of this model showed particle accumulation irrespective of types of + 1 defects, broadly agreeing with our experimental results. Confident in this model, we derived from it a nonlinear hydrodynamic theory using a Boltzmann–Ginzburg–Landau approach (Supplementary Information: Section 6). The theory showed the presence of two nonlinear active force terms in addition to the standard active force. Incorporating these terms and accounting for spatial density variations, the total active force replacing f was expressed asIn the steady state with ideal orientations, θ = θ p = ϕ + θ 0, eq. (2) can be written in polar coordinates aswhere. Our derivation showed that γ 2 < 0 and that γ 1 > 0 generally holds in most cases (Supplementary Information: Section 6). Therefore, the two nonlinear active forces pointed toward the defect cores, which contributed to inward flows and central cell accumulations (Fig. 4 a) (Notably, a Q ⋅ (∇ ⋅ Q) term was considered inin the context of active suspensions, where it was shown to stabilize the nematic order). Because our continuum theory included a flow term proportional to the density gradient, the net velocity of cellular flow was governed bywhere Γ = γ 0 (I − ϵ ρ Q), 0 ≤ ϵ ρ ≤ 1 and π 0 > 0.

---

### Expert panel on integrated guidelines for cardiovascular health and risk reduction in children and adolescents: summary report [^4c1faf55]. PES (2012). Medium credibility.

Quality criteria for assessment of individual randomized controlled trials — The table lists "Criteria Needed for 'Y' Selection" including study selection elements "Inclusion/exclusion criteria specified", "Criteria applied equally to study arms", and "Comparable patient characteristics", with details that "Health, demographics, and other characteristics of subjects/patients [are] described" and that the "Distribution of health, demographics, and other characteristics [is] similar across study arms at baseline". Methodologic safeguards include "Appropriate randomization" with the "Method of randomizing subjects to arms described" and "free from bias (e.g., random number generator)", "Allocation concealment" to prevent "foreknowledge of treatment assignment", and blinding of participants and assessors, including that "Patients or subjects [are] blinded to treatment as appropriate", "Provider or other treatment administrator [is] blinded", and "Data collectors/analysts or others with ability to affect results [are] blinded as appropriate". Additional items are "Low attrition rates" with a "Low rate of attrition for each arm" and "Withdrawals and reasons for withdrawal similar across arms", plus transparency items "Conflicts of interest" with "Sources of funding and investigators' affiliations described", and "Industry sponsorship beyond provision of drug or placebo", with notes that "Selection does not affect quality grade" and that "an asterisk will appear by quality grade if selection is 'Y'". Data value options shown include "• Y
- N
- NR/unknown".

---

### Developing a clinical prediction score: comparing prediction accuracy of integer scores to statistical regression models [^91dd14ad]. Anesthesia and Analgesia (2021). Medium credibility.

Researchers often convert prediction tools built on statistical regression models into integer scores and risk classification systems in the name of simplicity. However, this workflow discards useful information and reduces prediction accuracy. We, therefore, investigated the impact on prediction accuracy when researchers simplify a regression model into an integer score using a simulation study and an example clinical data set. Simulated independent training and test sets (n = 1000) were randomly generated such that a logistic regression model would perform at a specified target area under the receiver operating characteristic curve (AUC) of 0.7, 0.8, or 0.9. After fitting a logistic regression with continuous covariates to each data set, continuous variables were dichotomized using data-dependent cut points. A logistic regression was refit, and the coefficients were scaled and rounded to create an integer score. A risk classification system was built by stratifying integer scores into low-, intermediate-, and high-risk tertiles. Discrimination and calibration were assessed by calculating the AUC and index of prediction accuracy (IPA) for each model. The optimism in performance between the training set and test set was calculated for both AUC and IPA. The logistic regression model using the continuous form of covariates outperformed all other models. In the simulation study, converting the logistic regression model to an integer score and subsequent risk classification system incurred an average decrease of 0.057–0.094 in AUC, and an absolute 6.2%-17.5% in IPA. The largest decrease in both AUC and IPA occurred in the dichotomization step. The dichotomization and risk stratification steps also increased the optimism of the resulting models, such that they appeared to be able to predict better than they actually would on new data. In the clinical data set, converting the logistic regression with continuous covariates to an integer score incurred a decrease in externally validated AUC of 0.06 and a decrease in externally validated IPA of 13%. Converting a regression model to an integer score decreases model performance considerably. Therefore, we recommend developing a regression model that incorporates all available information to make the most accurate predictions possible, and using the unaltered regression model when making predictions for individual patients. In all cases, researchers should be mindful that they correctly validate the specific model that is intended for clinical use.

---

### Temperature-driven massless kane fermions in HgCdTe crystals [^952b432e]. Nature Communications (2016). Medium credibility.

The LL spectrum of massive or massless fermions in Hg 1− x Cd x Te has a more complex form:

Here, the LL index n runs over positive integers n = 1,2,… for the states in the electron and light-hole bands (ξ = ± 1). For the zero energy, flat heavy-hole band (ξ = 0), n runs over all non-negative integers, except 1: n = 0, 2, 3… The quantum number σ accounts for the Kramer's degeneracy lifted by the magnetic field B = (0, 0, B). The corresponding splitting can be viewed as the Zeeman (spin) splitting of LLs: E ξ, n,↑ (p z)− E ξ, n,↓ (p z). The non-parabolicity of the bands implies a strong dependence of this spin-splitting on p z, B and on LL and band indices.

Temperature-induced bandgap opening in narrow gap HgCdTe

Besides the linear behaviour of the absorption coefficient at zero magnetic field measured in both samples (see Supplementary Fig. 1 and Supplementary Note 2), the presence of pseudo-relativistic 3D fermions withis established at 2 K in the sample A by-like dependence of optical transitions and a spin splitting of LLs seen in Fig. 2a. The fitting of the two main lines based on equation (2) shows that they are related to the inter-band transitions between the heavy-hole band remaining at zero energy (ξ = 0) and the n = 1 spin-up and spin-down LLs with ξ = 1 (for details, see Supplementary Figs 2 and 3). The extracted bandgap value at 2 K equals = (5 ± 2) meV. Magneto-optical results obtained at temperatures from 57 to 120 K are shown in Fig. 2b–d. The bandgap, visualized by intersect of the inter-band transitions with the energy axis (shown by white arrows), increases with temperature (see also Supplementary Fig. 4). In addition to inter-band transitions (Δξ = 1), intra-band LLs transitions (Δξ = 0) are also observed and fitted with equation (2).

---

### Magnetostatic twists in room-temperature skyrmions explored by nitrogen-vacancy center spin texture reconstruction [^0550fa1f]. Nature Communications (2018). Medium credibility.

Topology of the solutions

In order to select the best candidate texture, we study the topology of the 2D vector field m (ρ, λ). For any 2D normalized vector field n (ρ), the topological number Q is defined as:Whenever n || z at the boundary, any continuous solution n (ρ) must have an integer Q value. Non-integer values of Q occur in the case of a discontinuity, which is energetically costly and unstable. Meanwhile, skyrmions are stable against local perturbations because of the large energetic cost preventing the skyrmion (Q = 1) from folding back into the ferromagnetic state (Q = 0). We therefore introduce continuity as a criterion for selecting physically allowed solutions. In Fig. 4b, we plot the absolute value of Q (λ) for each of the normalized vector fields n (ρ, λ), with n being the unit vector in the direction of m. The number Q can be visualized as the number of times the spin configuration n wraps around the unit sphere. To illustrate the value of Q, in the inset of Fig. 4b we plot the solid angle spanned by n while moving in the (x, y) plane. We obtain a value for Q approaching −1 as λ → 1. We therefore identify Néel or nearly-Néel solutions as the only ones compatible with the measured data.

---

### The optimal MR acquisition strategy for exponential decay constants estimation [^e2d26e09]. Magnetic Resonance Imaging (2008). Low credibility.

Estimating the relaxation constant of an exponentially decaying signal from experimental MR data is fundamental in diffusion tensor imaging, fractional anisotropy mapping, measurements of transverse relaxation rates and contrast agent uptake. The precision of such measurements depends on the choice of acquisition parameters made at the design stage of the experiments. In this report, chi(2) fitting of multipoint data is used to demonstrate that the most efficient acquisition strategy is a two-point scheme. We also conjecture that the smallest coefficient of variation of the decay constant achievable in any N-point experiment is 3.6 times larger than that in the image intensity obtained by averaging N acquisitions with minimal exponential weighting.

---

### Implications of improved representations of plant respiration in a changing climate [^e66eb849]. Nature Communications (2017). Medium credibility.

JULES modelling framework

The original JULES R d description, with Q 10 = 2.0, satisfies either R d = R d,25 Q 10 0.1(T l -25), or, with suppression at high and low temperatures, as (Eq. (18) of ref.) via additional denominator: R d = R d,25 [Q 10 0.1(T l -25)] / [(1 + exp(0.3(13.0- T l))) × (1 + exp(0.3(T l -36.0)))]. This latter form is our Standard JULES simulations, and has similarities to others modelling respiration as linear in GPP, with GPP itself a peaked Arrheniusfunction of T l. The JULES value of R d,25 is linear in the maximum carboxylation rate of Rubisco at 25 °C, V cmax,25 (μmol CO 2 m −2 s −1). Parameter f dr relates V cmax,25 to R d,25 as R d,25 = f dr × V cmax,25, where V cmax,25 satisfies V cmax,25 = 10 6 × n e × n l0. Quantity n l,0 is the prescribed mass-based PFT leaf nitrogen concentration (kg N (kg C) −1) and n e (mol CO 2 m −2 s −1 kg C (k; gN) −1) links V cmax,25 to leaf nitrogen concentration. Table 1 shows how these equations and parameters give Standard JULES R d,25 values.

---

### State estimation of a physical system with unknown governing equations [^794152e0]. Nature (2023). Excellent credibility.

State estimation is concerned with reconciling noisy observations of a physical system with the mathematical model believed to predict its behaviour for the purpose of inferring unmeasurable states and denoising measurable ones 1,2. Traditional state-estimation techniques rely on strong assumptions about the form of uncertainty in mathematical models, typically that it manifests as an additive stochastic perturbation or is parametric in nature 3. Here we present a reparametrization trick for stochastic variational inference with Markov Gaussian processes that enables an approximate Bayesian approach for state estimation in which the equations governing how the system evolves over time are partially or completely unknown. In contrast to classical state-estimation techniques, our method learns the missing terms in the mathematical model and a state estimate simultaneously from an approximate Bayesian perspective. This development enables the application of state-estimation methods to problems that have so far proved to be beyond reach. Finally, although we focus on state estimation, the advancements to stochastic variational inference made here are applicable to a broader class of problems in machine learning.

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^aeeaae45]. Nature Communications (2025). High credibility.

Fitting models to data is an important part of the practice of science. Advances in machine learning have made it possible to fit more-and more complex-models, but have also exacerbated a problem: when multiple models fit the data equally well, which one(s) should we pick? The answer depends entirely on the modelling goal. In the scientific context, the essential goal is replicability: if a model works well to describe one experiment, it should continue to do so when that experiment is replicated tomorrow, or in another laboratory. The selection criterion must therefore be robust to the variations inherent to the replication process. In this work we develop a nonparametric method for estimating uncertainty on a model's empirical risk when replications are non-stationary, thus ensuring that a model is only rejected when another is reproducibly better. We illustrate the method with two examples: one a more classical setting, where the models are structurally distinct, and a machine learning-inspired setting, where they differ only in the value of their parameters. We show how, in this context of replicability or "epistemic uncertainty", it compares favourably to existing model selection criteria, and has more satisfactory behaviour with large experimental datasets.

---

### Certified algorithms for equilibrium States of local quantum hamiltonians [^841257ea]. Nature Communications (2024). High credibility.

Predicting observables in equilibrium states is a central yet notoriously hard question in quantum many-body systems. In the physically relevant thermodynamic limit, certain mathematical formulations of this task have even been shown to result in undecidable problems. Using a finite-size scaling of algorithms devised for finite systems often fails due to the lack of certified convergence bounds for this limit. In this work, we design certified algorithms for computing expectation values of observables in the equilibrium states of local quantum Hamiltonians, both at zero and positive temperature. Importantly, our algorithms output rigorous lower and upper bounds on these values. This allows us to show that expectation values of local observables can be approximated in finite time, contrasting related undecidability results. When the Hamiltonian is commuting on a 2-dimensional lattice, we prove fast convergence of the hierarchy at high temperature and as a result for a desired precision ε, local observables can be approximated by a convex optimization program of quasi-polynomial size in 1/ε.

---

### A GPU-based computational framework that bridges neuron simulation and artificial intelligence [^49dda585]. Nature Communications (2023). High credibility.

Correctness proof for DHS

After applying DHS to a neural tree T = { V, E }, we get a partition P (V) = { V 1, V 2,… V n }, | V i | ≤ k, 1 ≤ i ≤ n. Nodes in the same subset V i will be computed in parallel, taking n steps to perform triangularization and back-substitution, respectively. We then demonstrate that the reordering of the computation in DHS will result in a result identical to the serial Hines method.

The partition P (V) obtained from DHS decides the computation order of all nodes in a neural tree. Below we demonstrate that the computation order determined by P (V) satisfies the correctness conditions. P (V) is obtained from the given neural tree T. Operations in DHS do not modify the tree topology and values of tree nodes (corresponding values in the linear equations), so the tree morphology and initial values of all nodes are not changed, which satisfies condition 1: the tree morphology and initial values of all nodes are identical to those in serial Hines method. In triangularization, nodes are processed from subset V 1 to V n. As shown in the implementation of DHS, all nodes in subset V i are selected from the candidate set Q, and a node can be put into Q only if all its child nodes have been processed. Thus the child nodes of all nodes in V i are in { V 1, V 2,… V i-1 }, meaning that a node is only computed after all its children have been processed, which satisfies condition 2: in triangularization, a node can be processed if and only if all its child nodes are already processed. In back-substitution, the computation order is the opposite of that in triangularization, i.e. from V n to V 1. As shown before, the child nodes of all nodes in V i are in { V 1, V 2,… V i-1 }, so parent nodes of nodes in V i are in { V i+1, V i+2,… V n }, which satisfies condition 3: in back-substitution, a node can be processed only if its parent node is already processed.

---

### How thoughtful experimental design can empower biologists in the omics era [^70aef70b]. Nature Communications (2025). High credibility.

A flexible but underused solution to this problem — power analysis — has existed for nearly a century. Power analysis is a method to calculate how many biological replicates are needed to detect a certain effect with a certain probability, if the effect exists (Fig. 2A). It has five components: (1) sample size, (2) the expected effect size, (3) the within-group variance, (4) false discovery rate, and (5) statistical power, or the probability that a false null hypothesis will be successfully rejected. By defining four of these, a researcher can calculate the fifth.

Usually, both the effect size and within-group variance are unknown because the data have not yet been collected. The choice of effect size for a power analysis is not always obvious: researchers must decide a priori what magnitude of effect should be considered biologically important. Acceptable solutions to this problem include expectations based on the results of small pilot experiments, using values from comparable published studies or meta-analyses, and reasoning from first principles. For example, a biologist planning to test for differential gene transcription may define the minimum interesting effect size as a 2-fold change in transcript abundance, based on a published study showing that transcripts stochastically fluctuate up to 1.5-fold in a similar system. In this scenario, the stochastic 1.5-fold fluctuations in transcript abundance suggest a reasonable within-group variance for the power analysis. As another example, a bioengineer may only be interested in mutations that increase cellulolytic enzyme activity by at least 0.3 IU/mL relative to the wild type, because that is known to be the minimum necessary for a newly designed bioreactor. To determine the within-group variance for a power analysis, the bioengineer could conduct a small pilot study to measure enzyme activity in wild-type colonies. As a final example, if a preliminary PerMANOVA for a very small amount of metabolomics data (say, N = 2 per group) from a pilot study estimated that R 2 = 0.41, then a researcher could use that value as the target effect size for which he/she aims to obtain statistical support.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^b1513dd5]. CDC (2011). Medium credibility.

Table 1b — Minimum sample size for detecting geometric mean (GM) ratio k with α = 0.05 and W = 0.9 — presents required sample sizes by standard deviation (S) strata and defines parameters; α is significance level, W is power, GM is geometric mean, and S is standard deviation, noting that 1.2 is the standard deviation of national viral load (VL) data. Each jurisdiction will need to assess the standard deviation of their local VL data and then determine the appropriate sample size needed to assess VL, and if the sample size is inadequate to meet the recommended case inclusion criterion, an alternate method may need to be used, such as combining multiple years of data. For k = 3, sample sizes across S = 1, 1.1, 1.2, 1.3, 1.4, 1.5 are 75, 91, 108, 127, 147, 169, and Table 1b uses power = 90%; jurisdictions may also explore differences in means of viral loads, including categorical differences in the proportion with undetectable or very low VL.

---

### 2019 ASCCP risk-based management consensus guidelines: methods for risk estimation, recommended management, and validation [^c44c6ea6]. Journal of Lower Genital Tract Disease (2020). High credibility.

Risk-based management uncertainty — confidence interval and decision confidence score: Confidence intervals for each risk estimate were calculated using a normal approximation or exact methods based on the binomial distribution, and a "decision confidence score" is defined that combines the uncertainty in the statistical precision and how close the risk estimates fall to the clinical action thresholds; mathematical details on estimating the recommendation confidence scores are presented in Appendix Part D (http://links.lww.com/LGT/A160).

---

### A doubly stochastic renewal framework for partitioning spiking variability [^cdd89138]. Nature Communications (2025). High credibility.

Estimation from data

In partitioning equation Eq. (5), the spike-count mean E[N T] and variance Var(N T) can be measured directly from data, whereas ϕ and the firing rate variance Var(λ T) are unknown. Thus, to partition variability in spike data, we first need to estimate ϕ. We devise an estimation method for ϕ, which we call the DSR method, based on our assumption that the firing rate changes smoothly and is approximately constant within a bin. We apply Eq. (5) to spike counts measured in two bin sizes, T and 2 T, to yield two equations, which we solve to obtain a quadratic equation for ϕ (Methods):Here, the spike-count mean and variance for each bin size E[N T], E[N 2 T], Var(N T), Var(N 2 T) are measured directly from the spike data, and ϕ DSR is the only unknown variable. Thus, we can solve Eq. (6) to estimate ϕ DSR from data. Then, we can separate the firing rate and point process variance using the estimated ϕ DSR in Eq. (5).

We consider two criteria for selecting the bin size T used to estimate spiking irregularity. On one hand, we require T > 1/E[λ] to ensure that the partitioning Eq. (5) holds. On the other hand, the bin size should be as small as possible, given the assumption that the firing rate remains constant within each bin. To satisfy both conditions, we set T = 2/E[λ] for each neuron in all analyses (Methods). We confirmed that our estimation method remains robust across a broad range of bin sizes (Supplementary Note 1.2, Supplementary Figs. 1–3).

---

### Smaller sample sizes for phase II trials based on exact tests with actual error rates by trading-off their nominal levels of significance and power [^cbb77402]. British Journal of Cancer (2012). Low credibility.

Background

Sample sizes for single-stage phase II clinical trials in the literature are often based on exact (binomial) tests with levels of significance (alpha (α) < 5% and power > 80%). This is because there is not always a sample size where α and power are exactly equal to 5% and 80%, respectively. Consequently, the opportunity to trade-off small amounts of α and power for savings in sample sizes may be lost.

Methods

Sample-size tables are presented for single-stage phase II trials based on exact tests with actual levels of significance and power. Trade-off in small amounts of α and power allows the researcher to select from several possible designs with potentially smaller sample sizes compared with existing approaches. We provide SAS macro coding and an R function, which for a given treatment difference, allow researchers to examine all possible sample sizes for specified differences are provided.

Results

In a single-arm study with P(0) (standard treatment) = 10% and P(1) (new treatment) = 20%, and specified α = 5% and power = 80%, the A'Hern approach yields n = 78 (exact α = 4.53%, power = 80.81%). However, by relaxing α to 5.67% and power to 77.7%, a sample size of 65 can be used (a saving of 13 patients).

Interpretation

The approach we describe is especially useful for trials in rare disorders, or for proof-of-concept studies, where it is important to minimise the trial duration and financial costs, particularly in single-arm cancer trials commonly associated with expensive treatment options.

---

### Mathematical modeling is more than fitting equations [^994275ea]. The American Psychologist (2014). Low credibility.

Comments on the comments made by Brown et al. (see record 2013-24609-001). The article by Brown et al. regarding the Fredrickson and Losada (see record 2005-11834-001) article discussed the use of differential equations in science and repeated our earlier observation (Luoma, Hämäläinen, & Saarinen, 2008) that there is lack of justification for the use of the Lorenz equations in the latter article. In this comment we want to point out that Brown et al. presented a very narrow view on mathematical modeling in behavioral research. We describe how the conceptual use of mathematical models is essential in many fields.

---

### Memristive bellman solver for decision-making [^f8d1530d]. Nature Communications (2025). High credibility.

Challenges and solutions for realizing a memristive Bellman solver

As shown in Fig. 1a, dynamic programming, an idea that breaks down a complex problem into simple ones, has been widely applied in various dynamic optimization fields such as reinforcement learning, auto-driving and path planning etc. The dynamic optimization problem aims to find the optimal decision from the initial state to the end state. The core foundation is to solve Bellman equation to find the most valuable decision. Figure 1b illustrates a scene with three states (S 1, S 2, S 3) where each state can transition to any other state (including itself) and S 1 set as Start and S 3 set as End. The decision could be made by the optimized value of each state, which could be obtained by solving the Bellman equation. It should be noted that, here the presentation form of Bellman equation, which is the backward induction form, is different from that of standard reinforcement learning, but their mathematical essence is the same (see Supplementary Note 1). Moreover, the action space is implicitly incorporated into the state transition probabilities. Figure 1c (left panel) shows the action implicit backward induction Bellman equation for optimizing each state value in Fig. 1b, which features an iterative solving process. It should be noted that in the action implicit backward induction Bellman equation (without time dimension), V (S n) and V (S n -1) represents the current state value and the previous state value, respectively. The current state and the previous state can be any one in the state space S. That is to say, in Fig. 1c (left panel), the green-marked V (S 1), V (S 2) and V (S 3) can represent the previous values of states S 1, S 2, and S 3 respectively (because the ∑ has been unfolded). However, the current state value marked in red is uncertain. It can be any one of the states S 1, S 2, and S 3. Therefore, although formally it seems that the equation can be compatible with MCIM. However, in the actual operation process, there is no time dimension, and we cannot determine how the current output correspond to next input. Therefore, the action implicit backward induction Bellman equation cannot be compatible with MCIM. To facilitate the Bellman solving process with efficient MCIM operation principle, the time dimension has been introduced. The introduced time dimension could transform the iterative solution process into a recurrent dot product process (Fig. 1c, right panel) to reduce the computation complexity (reduce iteration numbers) (see Supplementary Note 2 and Methods), and without influencing the convergency properties of Bellman equation solving process. The value of current state (V (S t)) could be obtained by the dot product operation between the sum of current state's reward and value of previous state multiplied by an attenuation constant (R (S t)+ γ V (S t – 1)) and the state transition probabilities (. The dot product operation can be implemented by the memristor array efficiently, by mapping the R (S t)+ γ V (S t – 1) as input and mapping theas conductances (weights).

---

### A rational approach to the clinical use of cumulative effective dose estimates [^0571c08e]. AJR: American Journal of Roentgenology (2011). Low credibility.

Objective

This article will address ongoing efforts to track cumulative dose estimates of patient radiation exposure that have the potential to transform radiation protection research.

Conclusion

There is no clear consensus on how to use this information clinically. A rational consideration of this question reveals that until the linear no-threshold model is supplanted, cumulative dose estimates are of little clinical relevance and never constitute a logical reason to avoid an imaging evaluation that is otherwise medically indicated.

---

### Minimum epistasis interpolation for sequence-function relationships [^02f80925]. Nature Communications (2020). High credibility.

Thus, to generalize our solution for the two-locus bi-allelic case with one missing genotype to larger sequence spaces and arbitrary geometric arrangements of the missing data, we want to find the value of f that matches our observed phenotypes where available, but otherwise minimizes. To do this, we note thatis non-negative, since the ϵ 2 for each face is non-negative, and that the formula for the ϵ 2 of each face is a second-degree polynomial and thus so is. As a result, our constrained minimization problem is in fact a positive semi-definite quadratic minimization problem with an equality constraint, a form of problem that has an analytical solutionbased on solving a single set of coupled linear equations (see "Methods").

In particular, if we write the set of known genotypes as B and the minimum epistasis interpolation solution as, we first assignfor i ∈ B to satisfy the constraint that our solution is equal to the observed phenotypic value when available. Then, for each i ∉ B, the minimum epistasis reconstruction of the sequence–function relationship is given by settingto the solution of the following α l − ∣ B ∣ equations (one equation for each i ∉ B, see "Methods"):where the values of c (i, j) depend only on the Hamming distance between genotypes i and j and are given by:

The above calculation comes down to solving a set of α l − ∣ B ∣ linear equations, and hence scales cubically with the number of unobserved genotypes, α l − ∣ B ∣. While this approach can readily be applied to moderately sized sequence spaces (e.g. α l less than a million), the exponential dependence of the number of possible genotypes on the sequence length l makes this straight-forward approach impractical for longer sequences. Nonetheless, we can show that the minimization problem can in fact be kernelized to remove this exponential dependence on l, so that ultimately the computational complexity scales linearly in l and cubically in the number of observed genotypes ∣ B ∣ (see Supplementary Methods Proposition 2). Moreover, these equations have a unique solution if and only if the least squares fit of the corresponding nonepistatic model has a unique solution (see Supplementary Methods Proposition 1).

---

### Drawing statistical conclusions from experiments with multiple quantitative measurements per subject [^70dff523]. Radiotherapy and Oncology (2020). Medium credibility.

In experiments with multiple quantitative measurements per subject, for example measurements on multiple lesions per patient, the additional measurements on the same patient provide limited additional information. Treating these measurements as independent observations will produce biased estimators for standard deviations and confidence intervals, and increases the risk of false positives in statistical tests. The problem can be remedied in a simple way by first taking the average of all observations of each specific patient, and then doing all further calculations only on the list of these patient means. A more sophisticated statistical modeling of the experiment, for example in a linear mixed model, is only required if (i) there is a large imbalance in the number of observations per patient or (ii) there is a specific interest in actually identifying the various sources of variation in the experiment.

---

### Hydrocodone bitartrate (hysingla ER) [^0c03a0bc]. FDA (2023). Medium credibility.

Warnings and precautions regarding the use of hydrocodone bitartrate ER PO (also known as Hysingla ER):
- **Adrenal insufficiency**: use caution in patients taking the drug for a prolonged period (> 1 month).
- **Central sleep apnea**: use caution in patients taking higher doses.
- **Decreased serum hydrocodone level**: use caution in patients taking CYP3A4 inducers (such as rifampin, carbamazepine, or phenytoin) or discontinuing CYP3A4 inhibitors (such as macrolide antibiotics, azole antifungals, or protease inhibitors).
- **Erectile dysfunction, infertility**: use caution in patients taking the drug for a prolonged period.
- **Exacerbation of increased ICP**: use caution in patients with increased ICP, brain tumor, or head injury.
- **Growth suppression**: use caution in patients with chronic corticosteroid therapy. Monitor growth regularly in pediatric patients receiving chronic corticosteroid therapy. Reassess the need for hydrocodone regularly and adjust the corticosteroid therapy as appropriate.
- **Hypotension, syncope**: use caution in patients with reduced blood volume or taking other CNS depressants. Monitor BP after initiation and dose titration. Avoid use in patients with circulatory shock.
- **Mask symptoms of head injury**: use caution in patients with head injury. Avoid use in patients with impaired consciousness or coma.
- **Opioid overdose**: use caution in patients taking CNS depressants or with a history of opioid use disorder or prior opioid overdose. Consider prescribing naloxone based on the patient's risk factors for overdose.
- **Opioid withdrawal syndrome**: do not discontinue abruptly in patients physically dependent on opioids.
- **Opioid withdrawal syndrome**: use caution in patients taking CYP3A4 inducers (such as rifampin, carbamazepine, or phenytoin) or discontinuing CYP3A4 inhibitors (such as macrolide antibiotics, azole antifungals, or protease inhibitors).
- **Opioid withdrawal syndrome**: use extreme caution in patients taking mixed agonist/antagonist analgesics (such as pentazocine, nalbuphine, or butorphanol) or partial agonist analgesics (such as buprenorphine).
- **Prolonged QT interval**: use caution in patients with congestive HF, bradyarrhythmia, electrolyte abnormalities, or taking drugs prolonging QT interval. Avoid use in patients with congenital long QT syndrome. Do not exceed 90 mg BID in patients developing QT prolongation.
- **Seizure**: use caution in patients with seizure disorder.
- **Serotonin syndrome**: use caution in patients taking serotonergic drugs.
- **Serotonin syndrome**: use extreme caution in patients taking MAOIs or within 14 days of stopping treatment.
- **Somnolence**: use extreme caution in patients performing activities requiring mental alertness, such as driving or operating machinery.
- **Sphincter of Oddi dysfunction**: use caution in patients with biliary tract disease and acute pancreatitis.

---

### Gravitationally redshifted absorption lines in the X-ray burst spectra of a neutron star [^b4654ece]. Nature (2002). Excellent credibility.

The fundamental properties of neutron stars provide a direct test of the equation of state of cold nuclear matter, a relationship between pressure and density that is determined by the physics of the strong interactions between the particles that constitute the star. The most straightforward method of determining these properties is by measuring the gravitational redshift of spectral lines produced in the neutron star photosphere. The equation of state implies a mass-radius relation, while a measurement of the gravitational redshift at the surface of a neutron star provides a direct constraint on the mass-to-radius ratio. Here we report the discovery of significant absorption lines in the spectra of 28 bursts of the low-mass X-ray binary EXO0748-676. We identify the most significant features with the Fe XXVI and XXV n = 2–3 and O VIII n = 1–2 transitions, all with a redshift of z = 0.35, identical within small uncertainties for the respective transitions. For an astrophysically plausible range of masses (M approximately 1.3–2.0 solar masses; refs 2–5), this value is completely consistent with models of neutron stars composed of normal nuclear matter, while it excludes some models in which the neutron stars are made of more exotic matter.

---

### GlycoGenius: a streamlined high-throughput glycan composition identification tool [^9c791268]. Nature Communications (2025). High credibility.

To fail the monoisotopic check, a spectrum peak must fulfill two criteria. The first criterion is that another spectrum peak must exist with an m / z value offset negatively by the mass of the hydrogen atom divided by an integer representing the possible charge states (i.e. −1.0074, −0.5037, −0.3328, etc.). For the second criterion, the intensity of this other peak (from the first criterion) must exceed a given threshold (i.e. if the peak is small enough, it will be ignored). For the second criterion, the intensity ratio between the first two peaks in the postulated isotopic envelope is evaluated. For this purpose, the average mass and the theoretical isotopic envelope of peptides, glycans, and lipids with an increasing number of building blocks were calculated (similar to a simplified Averaginemodel, used for peptides, but our model also considers different molecule classes). A linear interpolation and regression were performed to establish the relationship between the mass of these compounds and the relative intensity of the second isotopic envelope peak (see Source data, Table 6). Using Eq. 1, GG evaluates whether two given peaks are the monoisotopic and first isotopologue peaks of the same isotopic envelope.

To verify whether the isotopic envelope has the correct charge state, GG checks for the existence of a peak with m / z offset positively from the monoisotopic peak by the mass of the hydrogen atom divided by an integer representing the possible charge states. If such a peak is found, it is checked whether its intensity is high enough for it to qualify as the second peak of the isotopic distribution using Eq. 1. To fail a charge state check, the tested peak must meet three criteria: the positively offset peak must be found; it must be of high enough intensity; and must match a charge state other than the putative glycan charge state.

If the picked m / z value is confirmed to be monoisotopic and with the correct charge state, the PPM error of the monoisotopic peak is calculated, and the quality of the remaining isotopologues is assessed. The ratio between the theoretical intensity and the measured intensity of each isotopic envelope peak is scaled using a logistic function (Eq. 2) with the optimized steepness parameter (k value) corresponding to 10, and the midpoint set to 0.5.

---

### Double-layer geodesic and gradient-index lenses [^f9d05ba5]. Nature Communications (2022). High credibility.

Fig. 2
Double-layer lenses with foci at infinity.

The Generalized Maxwell Fish Eye profile can be used as a double-layer lens, with one homogeneous layer and foci at infinity. We show four examples, with ray turning angle Δ ϕ of π plus a π /4; b π /2; c π; and d 3 π /2. When M is an odd integer, the lens provides the functionality of an ordinary Eaton lens; when M is an even integer, it acts as an invisible lens where the rays travel in loops in the lens. Note that in Fig. 2 and the following figures the mirror coupling into lower waveguide is indicated as a red solid line, and the color of the rays (blue/green) indicates in which waveguide the propagation occurs (upper inhomogeneous index/lower homogeneous index).

If one focus lies on the rim of the lens r s = 1, and the other at infinity r i = ∞ we have a double-layer lens with a function of an ordinary Luneburg lens or beam divider, depending on the value of Δ ϕ. In this case, the half turning angle (13) takes the form

Inserting this expression in (14) and carrying out the integral over L, the corresponding expression for the turning point as a function ofisSubstitutinginto (29) we obtain an implicit equation for the refractive index as a function of radiuswhere α = M /(M + 1/2) and β = (M − 1)/(M + 1/2). Examples of the numerical solution to this equation are shown in the density plots within Fig. (3). We can also find the shape of the equivalent geodesic lensAgain, this can be treated as a special case of the general solution in Eq. (23), thus obtained by letting r s = 1, r i = ∞ in Eqs. (21), (22). In general, we must numerically evaluate the refractive index (30) and the surface height. However, in the special case of M = 1, the implicit Eq. (30) reduces to a quadratic equation n −4/3 − n −2/3 /2 − r 2 /2 = 0 in n −2/3, which has the solution (taking the root where n = 1 at r = 1)

---

### A practical guide for understanding confidence intervals and P values [^c0a7d4a0]. Otolaryngology — Head and Neck Surgery (2009). Low credibility.

The 95 percent confidence interval about the mean demarcates the range of values in which the mean would fall if many samples from the universal parent population were taken. In other words, if the same observation, experiment, or trial were done over and over with a different sample of subjects, but with the same characteristics as the original sample, 95 percent of the means from those repeated measures would fall within this range. This gives a measure of how confident we are in the original mean. It tells us not only whether the results are statistically significant because the CI falls totally on one side or the other of the no difference marker (0 if continuous variables; 1 if proportions), but also the actual values so that we might determine if the data seem clinically important. In contrast, the P value tells us only whether the results are statistically significant, without translating that information into values relative to the variable that was measured. Consequently, the CI is a better choice to describe the results of observations, experiments, or trials.

---

### Single-photon absorption and emission from a natural photosynthetic complex [^c6cebde3]. Nature (2023). Excellent credibility.

Fig. 3
Probability distribution of the number of heralds per heralded fluorescent photon.

a, Schematic of the raw signal time trace showing how the number of heralds between successive heralded fluorescent detections is counted. b, c, Experimental, theoretical and simulated probability distributions. b, The experimentally measured probability distributions from the same raw data used to extract the conditional g (2) (t = 0) of heralded fluorescent photons in Fig. 2c constructed from a total of 1,668,407 heralded fluorescent detections and a total of 17,773,649,622 herald detections, with 5-h integration time. Note here that the value P (N = 0) ≈ 4.8 × 10 −6 is non-zero due to the eight events of two heralded fluorescent photons conditioned on a herald detection that also gave rise to the small non-zero g (2) (t = 0) value of the heralded fluorescence in Fig. 2c. The solid red curves show the prediction of the analytical stochastic model of equation (1) using the measured experimental parameters. c, Simulated probability distributions using a numerical Monte Carlo model (Supplementary Information Section VII) with experimental parameters from b shown together with the theoretical prediction of equation (1). The insets in b and c zoom into the first 50 bins, showing the data points together with error bars representing Poisson s.d. (normalized by the total counts of all bins). d, The s.d. of P (N = 1) as a function of the integration time showing that the Poisson counting noise (defined asnormalized by the total counts of all bins) can be reduced by longer integration time and thus, more total events. The inset zooms into a simulated distribution with 1.8 × 10 6 -s integration time (100 times longer than the experimental and simulated data in b and c) and shows much reduced noise (Supplementary Information Fig. 8). Error bars represent Poisson s.d.

---

### Direction-averaged diffusion-weighted MRI signal using different axisymmetric B-tensor encoding schemes [^fea18ab2]. Magnetic Resonance in Medicine (2020). Medium credibility.

2.1 Linear, planar and STE

In LTE, and assuming stick‐like geometry, in Equation (1), therefore. The sensitivity of MR to axon radius would alter thescaling 36 because there will be a perpendicular diffusivity and the exponent in Equation (1) would not equal zero, and thus the exponential term outside of the error function would not equal unity.

In PTE, andhas the following form:For large b‐values; therefore, the diffusion signal can be approximated by the following equation (see Appendix A):where! denotes the double factorial and N depends on thevalue (Figure 1 and Table 1).

Figure 1
The approximated signal over the original PTE signal, for different N values

Table 1
The minimum number of terms for reconstructing the PTE signal for different error threshold values

For large b‐values, the extra‐axonal signal decays exponentially faster than the intra‐axonal compartment, and can be neglected.

The asymptotic expansion of erfi(x) in Equation A2 (see Appendix A) is valid when x → ∞, but large values ofwould suppress the signal to immeasurable levels, and therefore there are practical bounds on the value ofthat can be achieved. Therefore, we compared the original signal in Equation 2 and the approximated signal using Equation 3 for different values of N and(Figure 1 and Table 1). We use a normalized error to compare the original (Equation 2) and the approximated signal (Equation 3):where S is the original signal obtained from Equation 2 andis the approximated signal from Equation 3.

In STE,… For large b‐values, both intra‐ and extra‐axonal signals decay exponentially fast, and both of them are negligible. Therefore, the STE does not provide a considerable signal for large b‐values in a two‐compartment model.

2.2 General case of axisymmetric B‐tensor

Here, we consider the general case of an axisymmetric b‐tensorto cover all b‐tensor shapes between(PTE) to(LTE).

---

### A faster and more accurate algorithm for calculating population genetics statistics requiring sums of stirling numbers of the first kind [^74ba87a4]. G3 (2020). Medium credibility.

Remarks on computing

When computing the quantitydefined in (5), numerical instability may happen whenis close to 1. In that case, the computation ofsuffers from cancellation of digits. For example, take. Then, andbecomes about 6.6561 when using the first relation in (9). However, when we calculateand use the second relation, then we obtain the more reliable result.

We conclude that, when, it is better to switch and obtainfrom the sum in (8) andusing the second relation in (9). A simple criterion to decide about this can be based on using the saddle point(see Remark 1 below).

A second point is numerical overflow when n is large, becauserapidly becomes large when m is small with respect to n. For example, when, we haveTherefore, it is convenient to scale the Stirling number in the form. In addition, the Pochhammer termin front of the sum in (2) will also be large with n; we have.

We can write the sum in (2) in the formLeading to a corresponding modification in the recurrence relation in (7) for the scaled Stirling numbers:To control overflow, we can consider the ratioThis function satisfiesif. For small values of n we can use recursion in the formFor large values of n and all, we can use a representation based on asymptotic forms of the gamma function.

It should be observed that using the recursions in (7) and (12) is a rather tedious process when n is large. For example, when we use it to obtainfor all, we need all previouswithfor all. A table look-up forin floating point form may be a solution. When n is large enough, the algorithm mentioned in evaluates each needed Stirling number by using the asymptotic approximation derived in.

Data availability

Code implementing the new estimator for Fu'sin R is available at.

---

### Recommendations for a standardized pulmonary function report. An official American Thoracic Society technical statement [^d6d8fcbc]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Spirometry reporting specifies that numerical values are given for the FEV1, the FVC, and the FEV1/FVC ratio; the latter should be reported as a decimal fraction and the space for percent predicted value left blank, and if bronchodilators are given the LLN column need not be repeated with absolute and percent change given only for FEV1 and FVC. Other numerical values such as the forced inspiratory flow at 75% of FVC (FEF75%) and FEF25–75% are not recommended for routine use. Graph requirements include that for the volume–time curve the volume scale should be at least 10 mm/L, the time scale at least 20 mm/s, and 1 second prior to the start of expiration should be displayed; on the flow–volume plot the flow display should be at least 5 l/min/L/s, and the ratio of flow to volume should be 2 L/s to 1 L, and linear and log scales where values are plotted as z-scores relative to the predicted value (z = 0) give an intuitive sense of severity.

---

### Exponential concentration in quantum kernel methods [^4f8432da]. Nature Communications (2024). High credibility.

Proposition 2

Assume that the fidelity quantum kernelexponentially concentrates towards some exponentially small value as per Definition 1. Suppose anshot SWAP test is used to estimate the Gram matrix for a training datasetof size N s. Then, with probability exponentially close to 1 (i.e. probability at leastsuch thatfor some c > 1), the estimate of the Gram matrixis statistically indistinguishable from the matrixwhose diagonal elements are 1 and off-diagonal elements are instances ofwhere eachtakes either + 1 or − 1 with equal probability. We note thatdoes not contain any information about the input data.

We refer the readers to Supplementary Note II for an introduction to some preliminary tools for a hypothesis testing and Supplementary Note III A 2 for further technical details regarding the SWAP test, which includes formal definitions of statistical indistinguishability (i.e. Definition 2 for distributions and Definition 3 for outputs), and a proof of the proposition.

Although statistical estimates of the kernel behave differently depending on the choice of measurement strategy, they are both in effect independent of the input data for large n. Thus training with this estimated Gram matrix leads to a model whose predictions are independent of the input training data. We present numerical simulations to support our theoretical findings in Supplementary Note III A 3.

Crucially, this conclusion applies generally beyond kernel ridge regression to other kernel methods including both supervised learning tasks and unsupervised learning tasks. As a concrete example, we consider the optimal solution for kernel ridge regression in the presence of exponential concentration.

Corollary 1

Consider a kernel ridge regression task with a squared loss function and regularization λ using the same assumptions as Proposition 1. Denote y as a vector with its i th elements equal to y i.
For the Loschmidt Echo test, the optimal parameters are found to be

with probability at least 1 − δ withfor some b > 1.

For a test data point, the model prediction is 0 with probability at leastsuch thatfor some.
2 For the SWAP test, the optimal parameters are statistically indistinguishable from the vector

with probability at leastwithfor some. Here, is a data-independent random matrix whose diagonal elements are 1 and off-diagonal elements are instances ofin Eq. (14).

In addition, with probability exponentially close to 1, the model prediction on unseen data is statistically indistinguishable from the data-independent random variables that result from measuring.

---

### A heuristic information cluster search approach for precise functional brain mapping [^c09736ed]. Human Brain Mapping (2020). Medium credibility.

And the spectral quality for the arriving feature v i can be measured by a feature‐level spectral score defined below:

Therefore, given the selected feature subset IC, the new arriving feature v n will be admitted if its inclusion improves the discriminative ability of the feature subset (calculated via Equation (6)), that is Equation (1).

2.5 Redundancy analysis

By performing an intragroup pruning before further expanding the cluster IC ∪ v n, the optimal group of voxels V opt ∈ { IC ∪ v n } can be selected. This process further increases the precision of the information map by removing residual redundancies as a high quality set of features should not only be individually relevant, but also should not be redundant with respect to each other.

For this analysis, we consider the interaction between the voxels within the clusters to be able to capture the complex structures among groups of voxels rather than only assessing their individual predictive power. Therefore, the redundancy score of each feature is calculated as its mutual information with the rest of the features in the group which is formulated below (Brown, 2009).where I denotes the mutual information between two vectors, X is the set of features, and n is the number of features. A large value of J represents high redundancy, that is, low quality of the feature. Also, the mutual information I is calculated by the following formula:where x denotes the feature values and z represents the class labels.

2.6 An efficient implementation

As discussed in the previous section, the proposed approach performs a set of calculations in every step of the search to investigate every voxel at the vicinity of the detected feature clusters. Therefore, a large fraction of the calculations is repeated due to the overlap among regions, that is, a voxel can be visited multiple times. This significantly affects the analysis time and can make search over large regions impractical. We apply this improvement by breaking down the calculations for every voxel and storing them at hash tables to be later used in the search process. At every step of the search, the algorithm first checks if the fragment of calculation encountered during the search already exists in the tables. If so, it directly uses the previously calculated values recursively to save time; otherwise, it performs the necessary calculation and stores it in the table for future use. This top‐down approach significantly increases the search time, making precise information detection on voxel‐level resolutions over large search spaces possible. Time complexity of ICS based on our experimentation setup is discussed in the discussion section.

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^0ec15444]. Nature Communications (2025). High credibility.

The advantage of a probability likeis that it makes explicit which kinds of replication variations are involved in computing the tail probability; the interpretation of equation (33) is thus clear. However it can only be computed after generating a large number of synthetic datasets, which requires a known and parametrisable data-generating process; on its own therefore, cannot be used directly as a criterion for comparing models. By choosing c such that the criteria are correlated, we transfer the interpretability of onto. Moreover, if we can find such a c, then we have de facto validatedfor the set of simulated experiments described by Ω.

Since defining an epistemic distribution involves making many arbitrary choices, one may want to define multiple distributions Ω 1, Ω 2,… to ensure that results are not sensitive to a particular choice of Ω. Equation (34) can easily be generalised to account for this, following a similar logic as equation (15), in which case it becomes

We found that an effective way to verify equation (34) is by plottingagainst, where values ofare obtained by averaging comparison outcomes, conditioned on the value ofbeing within an interval (this is explained more precisely in the Methods). We thus obtain a histogram ofagainst, which works best when the size of bins is adjusted so that they have similar statistical power. We illustrate this in Fig. 6, where histograms are shown as curves to facilitate interpretation. These curves are drawn against the "overconfident regions" (where equation (34) is violated), depicted in red or yellow: we look therefore for values of c which as much as possible stay within the white regions. The visual representation makes it easier to judge the extent to which small violations of equation (34) can be tolerated. An extended version of this figure, where all 48 conditions are tested, is provided as Supplementary Fig. 1.

---

### Oligomerization-mediated autoinhibition and cofactor binding of a plant NLR [^66d67007]. Nature (2024). Excellent credibility.

The parameters for the helical symmetry were determined through the analysis of the power spectrum of 2D class averages using RELION 3.08, following a similar approach used for determining the helical parameters of L7–TIR complexes. The parameters were determined to be about 65 Å in the helical rise and about ± 55° in the helical rotation. Specifically, the analysis showed that when segments were extracted using 256 pixel boxes, the reciprocal layer line analysis of the 2D averages indicated the presence of a 512/52 × 1.1 pixel repeat pattern along the helical axis. This indicated that the helix had a minimal helical rise of 10.83 Å or its integer n times of values. On the basis of the knowledge of the presence of a C 3 symmetry, we tested three, six and nine times of 10.83 Å and determined that about 65 Å was the solution (n = 3 and 9 yielded irregular densities). There was also an obvious 140 Å helical pitch (2D pattern repeated every 140 Å along the helical axis), which indicated that for every 140 Å linear translation along the axis, the same molecular would rotate back to a same helical rotational angle. With an extra C 3 symmetry, this indicated that for every 140 Å along the helical axis, every molecule rotated 120° or 120′ integer times. We then tested ± 120/(140/65) = ± 55° as the candidate helical rotation values. Because our calculation was based on 2D images, this helical rotation value could be both positive and negative values.

---

### Connectome-driven neural inventory of a complete visual system [^72267670]. Nature (2025). Excellent credibility.

Defining column centre lines and ROIs

Column ROIs subdivide a neuropil ROI and are based on the centre lines of columns (pins) (Fig. 3). The basic idea of a column pin is that it is a smooth, slightly bent line that goes through the centre of synapses of specific neurons assigned to the same hexagonal coordinate (as described in previous sections). More formally, a pin is a list of 3D points (pin points, called ColumnPin in neuPrint) that start at the top of a neuropil ROI and end at the bottom.

Starting from a neuron to hexagonal coordinate assignment in the medulla (15 cell types, as described in the section 'Assigning neurons to medulla hexagonal coordinates'), we constructed pin α corresponding to hexagonal coordinates α according the following eight steps:
For all neurons assigned to the hexagonal coordinate α, we obtained the 3D positions r i of their presynapses and postsynapses in the medulla, where i runs from 1 to the number of synapses.
We identified the axes of biggest variation of r i using principal component analysis (PCA). We fixed the sign of the axis of biggest variation (PC1) such that PC1 points from the top to the bottom of the medulla ROI.
We computed the Euclidean distances l i of r i from their meanin the PC2–PC3 plane, and dropped the r i for which l i < f lat σ lat, whereis the root mean squared distance of all r i. The factor f lat = 2 was chosen manually such that the r i with l i ≥ f lat σf lat looked like they were part of a different column.
For the remaining r i, we defined their normalized projection onto PC1 as t i such that t i = –1 corresponded to the top and t i = 1 to the bottom of the medulla ROI. To clarify, this PC1 was computed in step (2), using all r i.
For those r i with t i < 0.1, we computed a new PC1 and found its intersection with the top of the medulla ROI. We denoted this 3D point as r top. If there were fewer than N PC points with t i < 0.1, then we took the points with the N PC smallest t i values instead to compute the new PC1. We chose the integer N PC = 800 manually to ensure a robust estimation of r top. In an analogous computation, using the points with t i > 0.2 or the points with the N PC largest t i values and the intersection of their newly computed PC1 with the bottom of the medulla ROI, defined r bottom.
We defined a list L ′ α, for which the first element was r top, the last element was r bottom and intermediate elements were determined by rank ordering the r i by t i. We defined another list L α for which the first element was r top, the last element was r bottom and the intermediate elements were the averages of the elements in L ′ α with their N avg neighbours. The integer N avg = 260 was determined manually such that the elements in L α formed approximately a smooth line. This line was often helical and typically did not go through the centre of the r i.
To create a line that went through the centre of the r i, we found the shortest sublist O α of L α, for which the first element was r top, the last element was r bottom and the 3D Euclidean distance between neighbouring points was at most d max. We defined d max as the 3D Euclidean distance between r top and r bottom divided by N tang. We selected the integer N tang = 7 such that O α corresponded to a line that went approximately through the centre of the r i. Note that the minimal length of O α is N tang + 1.
For each of the three spatial coordinates of points in O α, we used a spline interpolation (a piecewise cubic hermite interpolating polynomial (PCHIP)) to uniformly sample O α with N samp points. This list of length N samp defined pin α. We manually determined N samp = 121 by checking whether the synapse distributions along a pin looked approximately smooth and did not change much with small changes in N samp.

---

### KDIGO 2022 clinical practice guideline for diabetes management in chronic kidney disease [^bfaebbd4]. Kidney International (2022). High credibility.

Figure 5 — assessment of kidney function in glomerular disease — uses ml/min per 1.73 m^2 units and notes that the correction coefficient for race in GFR estimating equations is controversial and discussions are ongoing, referring readers to the KDIGO CKD guideline for more information.

---

### KDIGO 2024 clinical practice guideline for the evaluation and management of chronic kidney disease [^b40fce01]. Kidney International (2024). High credibility.

Selection of GFR estimating equations — use of validated eGFR: We recommend using a validated GFR estimating equation to derive GFR from serum filtration markers (eGFR) rather than relying on the serum filtration markers alone (1D). Practice Point 1.2.4.1 advises to use the same equation within geographical regions (as defined locally [e.g., continent, country, and region] and as large as possible), noting that within such regions, equations may differ for adults and children. The recommendation places a high value on using an estimating equation validated in the population of interest, and the key points are to use an equation validated and most suited to the population of interest; there is potential for harm if people get different eGFR values when receiving care in different settings, and there is benefit to clinical care, research, and public health with the use of validated equations such that decisions, research findings, and public policy are informed by accurate estimates of CKD.

---

### A general derivation and quantification of the third law of thermodynamics [^43738480]. Nature Communications (2017). Medium credibility.

The most accepted version of the third law of thermodynamics, the unattainability principle, states that any process cannot reach absolute zero temperature in a finite number of steps and within a finite time. Here, we provide a derivation of the principle that applies to arbitrary cooling processes, even those exploiting the laws of quantum mechanics or involving an infinite-dimensional reservoir. We quantify the resources needed to cool a system to any temperature, and translate these resources into the minimal time or number of steps, by considering the notion of a thermal machine that obeys similar restrictions to universal computers. We generally find that the obtainable temperature can scale as an inverse power of the cooling time. Our results also clarify the connection between two versions of the third law (the unattainability principle and the heat theorem), and place ultimate bounds on the speed at which information can be erased.

---

### Modeling uncertainty in single-copy assays for HIV [^c02085f8]. Journal of Clinical Microbiology (2012). Low credibility.

We present a simple computational model of measurement accuracy for single-copy sensitivity assays (SCA) of HIV RNA that was developed from first principles. The model shows that the SCA is significantly right-skewed. Measured virus concentrations of 1 and 10 virions/ml had overlapping 95% confidence intervals and were statistically indistinguishable.

---

### A DNA-based system for selecting and displaying the combined result of two input variables [^afa284ca]. Nature Communications (2015). Medium credibility.

Oligonucleotide-based technologies for biosensing or bio-regulation produce huge amounts of rich high-dimensional information. There is a consequent need for flexible means to combine diverse pieces of such information to form useful derivative outputs, and to display those immediately. Here we demonstrate this capability in a DNA-based system that takes two input numbers, represented in DNA strands, and returns the result of their multiplication, writing this as a number in a display. Unlike a conventional calculator, this system operates by selecting the result from a library of solutions rather than through logic operations. The multiplicative example demonstrated here illustrates a much more general capability — to generate a unique output for any distinct pair of DNA inputs. The system thereby functions as a lookup table and could be a key component in future, more powerful data-processing systems for diagnostics and sensing.

---

### Limits on the computational expressivity of non-equilibrium biophysical processes [^37daae40]. Nature Communications (2025). High credibility.

Fig. 2
The matrix-tree theorem.

A Computing the steady-state occupancy π 1 by summing weights over directed spanning trees. Directed spanning trees are subgraphs containing all graph nodes but no cycles, with edges oriented toward a root node. In each directed spanning tree, the input forces make a positive, negative, or zero contribution to the tree weight. The structural vectorsare shown below each tree; these quantities enter into Equation (3) below. B Schematic illustration of the high-dimensional space of feature vectors ψ (i; θ) and χ (i, F). The depicted arrangement of vectors could solve a binary classification problem.

We define the input multiplicityas the number of edges affected per input variable, which we assume to be the same for each input. To focus on the functional way in which the input driving enters the steady-state probabilities, the driving contributions can be factored out in the algebraic expressions for the numerator and denominator of Equation (1). This has been previously been used to make analytical progress for M = D = 1 in, for example, refs. –. This equivalent formulation of Eq. (1) suggests that steady states of Markov jump processes implement a rational polynomial function of exponentiated input variables. Defining, we rewrite the matrix-tree expression for π i for general D and M We use the multi-index, whereis the set of D input labels and each componentof the multi-index runs over the values, to enumerate themonomials. These monomials y μ (F) in Equation (2) combinatorially depend on the different mixtures μ of input driving, representing a net total μ a of signed contributions from the input force F a, μ b such contributions for F b, and so on for each input. The coefficients, which are functions of the parameters θ, are the sums of weights over all directed spanning trees rooted at node i which have the corresponding mixture μ of signed input contributions. The monomial coefficientsthus represent learnable amplitudes of each polynomial basis function y μ (F). The coefficients in the denominator are defined as. Classification will be successful if, for F ρ drawn from class ρ, the coefficientsand monomials y μ (F ρ) are large for the same μ. In the subsequent sections of the paper and in the Supplementary Information we use the formulation in Equation (2) to show how the classification ability of a non-equilibrium Markov processes may be systematically modulated.

---

### Resolving discrepancies between chimeric and multiplicative measures of higher-order epistasis [^8eb47c09]. Nature Communications (2025). High credibility.

Multivariate Bernoulli distribution

The three parametrizations we derived for the bivariate Bernoulli distribution extend to the multivariate Bernoulli distribution. Suppose that (X 1,…, X L) ∈ {0, 1} L is distributed according to a multivariate Bernoulli distribution. Then the distribution P (X) of the random variables X is uniquely specified by one of the three following parametrizations.
General parameters: These are 2 L non-negative valuessatisfyingFor example if L = 3, then p 010 = P (X 1 = 0, X 2 = 1, X 3 = 0) and p 110 = P (X 1 = 1, X 2 = 1, X 3 = 0). Note that since, only 2 L − 1 valuesare necessary to define the distribution.
Natural/canonical parameters: These are 2 L real numberssatisfyingSimilar to the general parameters p i, only 2 L − 1 values β S are necessary to uniquely define the distribution. Typically, the parameter, often called a normalizing constant or a partition function of the distribution, is left unspecified. As noted in the bivariate setting, equation (23) shows that the multivariate Bernoulli is an exponential family distribution with 2 L − 1 sufficient statistics of the form ∏ i ∈ S X i for subsets S with ∣ S ∣ > 0. Moreover, by rewriting (23) aswe observe that the natural parameters β correspond to interaction coefficients in a log-linear regression model with response variables p. For example, the natural parameter β 12 is the coefficient of the interaction term x 1 x 2.
Moments/mean parameters: These are 2 L real numberssatisfyingFor example if L = 3, then μ 13 = E [X 1 X 3] while μ 12 = E [X 1 X 2]. The mean parametersare sufficient statistics for the multivariate Bernoulli distribution, as seen in the exponential family form (23) of the multivariate Bernoulli distribution.

---

### Semiparametric estimation of the relationship between ROC operating points and the test-result scale: application to the proper binormal model [^9a9d14d8]. Academic Radiology (2011). Low credibility.

Rationale and Objectives

Semiparametric methods provide smooth and continuous receiver operating characteristic (ROC) curve fits to ordinal test results and require only that the data follow some unknown monotonic transformation of the model's assumed distributions. The quantitative relationship between cutoff settings or individual test-result values on the data scale and points on the estimated ROC curve is lost in this procedure, however. To recover that relationship in a principled way, we propose a new algorithm for "proper" ROC curves and illustrate it by use of the proper binormal model.

Materials and Methods

Several authors have proposed the use of multinomial distributions to fit semiparametric ROC curves by maximum-likelihood estimation. The resulting approach requires nuisance parameters that specify interval probabilities associated with the data, which are used subsequently as a basis for estimating values of the curve parameters of primary interest. In the method described here, we employ those "nuisance" parameters to recover the relationship between any ordinal test-result scale and true-positive fraction, false-positive fraction, and likelihood ratio. Computer simulations based on the proper binormal model were used to evaluate our approach in estimating those relationships and to assess the coverage of its confidence intervals for realistically sized datasets.

Results

In our simulations, the method reliably estimated simple relationships between test-result values and the several ROC quantities.

Conclusion

The proposed approach provides an effective and reliable semiparametric method with which to estimate the relationship between cutoff settings or individual test-result values and corresponding points on the ROC curve.

---

### Confidence reports in decision-making with multiple alternatives violate the Bayesian confidence hypothesis [^b245f51e]. Nature Communications (2020). High credibility.

Inference on a given trial. We assume that the observer knows the mean and standard deviation of each category based on the exemplar dots, and that the observer assumes that the three categories have equal probabilities. The posterior probability of category C given the measurement x is then p (C | x) ∝ p (x | C) = N (x; m C, (σ s 2 + σ 2) I). Instead of the true posterior p (C | x), the observer makes the decisions based on q (C | x), a noisy version of the posterior probability. We obtain a noisy posterior q (C | x) by drawing from a Dirichlet distribution. The Dirichlet distribution is a generalization of the beta distribution. Just like the beta distribution is a continuous distribution over the probability parameter of a Bernoulli random variable, the Dirichlet distribution is a distribution over a vector that represents the probabilities of any number of categories. The Dirichlet distribution is parameterized as

Γ represents the gamma function. p is a vector consisting of the three posterior probabilities, p = (p 1, p 2, p 3) = (p (C = 1 | x), p (C = 2 | x), p (C = 3 | x)). q is a vector consisting of the three posterior probabilities perturbed by decision noise, q = (q 1, q 2, q 3) = (q (C = 1 | x), q (C = 2 | x), q (C = 3 | x)). The expected value of q is p. The concentration parameter α is a scalar whose inverse determines the magnitude of the decision noise; as α increases, the variance of q decreases. To make a category decision, the observer chooses the category that maximizes the posterior probability:

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^6517cbfd]. Annals of the American Thoracic Society (2023). High credibility.

Basic stereological measurements — first-order parameters and probes identify that "First-order parameters are volume (three-dimensional [3D]), surface (two-dimensional [2D]), length (one-dimensional [1D]), and number (zero-dimensional [0D])". Measurements "are performed using geometric test probes, such as points (0D), lines (1D), planes (2D), or volumes (3D)". These probes create countable events in the image, and "Raw counts provide ratios… that are multiplied by the reference space volume to obtain absolute measures for the lung or subcompartment".

---

### Double-layer geodesic and gradient-index lenses [^b603811c]. Nature Communications (2022). High credibility.

As a final example, we consider a class of index profiles where both foci lie on the rim of the lens r s = r i = 1. As a single-layer problem, the solution would be the generalized Maxwell Fish Eye lens. However as we have already seen, the generalized Maxwell Fish Eye acts as a double-layer lens with foci at infinity. In this case, the half turning angle equalswhich compared to (28), only differs by a factor of two in front of thefunction. We can therefore straightforwardly find the turning point and the surface shape as beforewhich in the same manner as the previous case, yields the same implicit equation for the refractive index (30), but with the modified powers α = M /(M + 1) and β = (M − 1)/(M + 1). Example refractive index profiles are shown as density plots in Fig. 4. The equivalent geodesic lens can also be found as, which is a special case of Eq. (23) with r s = r i = 1 in Eqs. (21), (22). Again, the general index profiles and surface heights must be obtained numerically. However, for the particular case of M = 2 the implicit Eq. (30) reduces to a quadratic equation n −4/3 − n −2/3 r 1/3 /2 − r 5/3 /2 = 0 and we can solve for the index profile analytically (taking the root where n = 1 at r = 1)

---

### A solution to the collective action problem in between-group conflict with within-group inequality [^d7f4cac0]. Nature Communications (2014). Medium credibility.

Generalizations

To check the robustness of our conclusions, we have also studied two modifications of the basic model. In the first version, the probability of group success was specified as

where β is a positive parameter measuring the 'decisiveness' of the group strength: β > 1 implies that stronger groups will get disproportionately large shares of the reward. Increasing β increases both the individual and group efforts (as expected), but it results in no other qualitative differences (Fig. 5).

In another modification, we assumed that group-mates share the reward equally (v i = 1/ n) but differ with respect to the cost coefficient c i. This may be the case if some individuals are stronger than others, so that the same amount of effort can be less costly for them in fitness terms. In our simulations, we used costs c i that increased linearly with rank i. Specifically, we used n equally spaced values of c i from (1− d)/2 to (1+ d)/2 with five different values of d: 0.05 (small differences in costs), 0.1, 0.2, 0.4, 0.8 (large differences in costs). Our major conclusions remain qualitatively valid, with low-cost individuals playing the role of high-valuation group members in the basic model (Fig. 6). Note that the model with differences in costs is mathematically equivalent to a model with a constant cost c but with differences in fighting abilities s i and the total group effort defined as

In this model, individuals with larger fighting abilities s i will, ceteris paribus, contribute more to X. Therefore, our conclusions are applicable to the case of within-group variation in strengths. That is, lower-cost or higher-strength individuals are predicted to contribute more effort than their group-mates, and they can have lower reproductive success as a result.

---

### Linkage analysis and map construction in genetic populations of clonal F1 and double cross [^037e8eff]. G3 (2015). Low credibility.

Recombination frequency estimation in scenario 1 in clonal F 1 progenies

We begin with the most ideal situation where locus 1 has four identifiable genotypes A 1 C 1, A 1 D 1, B 1 C 1, and B 1 D 1, and locus 2 has four identifiable genotypes A 2 C 2, A 2 D 2, B 2 C 2, and B 2 D 2. The first row and first column ofshow the four female and male gametes and their frequencies, from which we can easily derive theoretical frequencies of the 16 identifiable genotypes at the two linked loci. For convenience, the 16 genotypes were rearranged in Table 2, and sample sizes of the 16 genotypes were represented by n 1, n 2,…, and n 16. Based on theoretical frequencies and sample sizes in Table 2, the likelihood function (L) and logarithm likelihood (log L) can be constructed in Equation (1).

Table 2
Theoretical frequencies of the 16 identifiable genotypes in the clonal F 1 population at two linked loci

where C is a constant independent of the unknown recombination frequencies. The maximum likelihood estimates (MLE) of recombination frequencies can be calculated either by solving the likelihood equation (i.e.) or by some approximate algorithms when there is no analytic solution to the likelihood equation. From Equation (1), MLE of r F and r M can be directly calculated from Equation (2).where n i is the observed sample size for the i th genotype (Table 2), n i: j is the sum of n i to n j, and n is the total sample size (i.e. n = n 1:16).

Define the estimate of the combined recombination frequency r in Equation (3). It can be easily seen that the estimate thus defined in Equation (3) is always less than 0.5. In addition, it can be proved that the estimate in Equation (3) is also MLE of r, when directly calculated from its likelihood function.

---

### How to deal with interval-censored data practically while assessing the progression-free survival: a step-by-step guide using SAS and R software [^75145878]. Clinical Cancer Research (2016). Low credibility.

We describe how to estimate progression-free survival while dealing with interval-censored data in the setting of clinical trials in oncology. Three procedures with SAS and R statistical software are described: one allowing for a nonparametric maximum likelihood estimation of the survival curve using the EM-ICM (Expectation and Maximization-Iterative Convex Minorant) algorithm as described by Wellner and Zhan in 1997; a sensitivity analysis procedure in which the progression time is assigned (i) at the midpoint, (ii) at the upper limit (reflecting the standard analysis when the progression time is assigned at the first radiologic exam showing progressive disease), or (iii) at the lower limit of the censoring interval; and finally, two multiple imputations are described considering a uniform or the nonparametric maximum likelihood estimation (NPMLE) distribution. Clin Cancer Res; 22(23); 5629–35. ©2016 AACR.

---

### A machine learning automated recommendation tool for synthetic biology [^9f17695c]. Nature Communications (2020). High credibility.

In order to address (iii), as well as to avoid entrapment in local optima and search the phase space more effectively, we choose to solve the optimization problem through sampling. More specifically, we draw samples from a target distribution defined aswherecan be interpreted as the uniform "prior" on the set, andas the 'likelihood' term of the target distribution. Sampling from π implies optimization of the function G (but not reversely), since the modes of the distribution π correspond to the optima of G. As we did before, we resort to MCMC for sampling. The target distribution is not necessarily differentiable and may well be complex. For example, if it displays more than one mode, as is often the case in practice, there is a risk that a Markov chain gets trapped in one of them. In order to make the chain explore all areas of high probability one can "flatten/melt down" the roughness of the distribution by tempering. For this purpose, we use the parallel-tempering algorithmfor optimization of the objective function through sampling, in which multiple chains at different temperatures are used for exploration of the target distribution (Fig. 3).

Choosing recommendations for the next cycle is done by selecting from the sampled inputs. After drawing a certain number of samples from π (x) we need to choose recommendations for the next cycle, making sure that they are sufficiently different from each other as well as from the input experimental data. To do so, first we find a sample with optimal G (x) (note that G (x) values are already calculated and stored). We only accept this sample as a recommendation if there is at least one feature whose value is different by at least a factor γ (e.g. 20% difference, γ = 0.2) from the values of that feature in all data points. Otherwise, we find the next optimal sample and check the same condition. This procedure is repeated until the desired number of recommendations are collected, and the condition involving γ is satisfied for all previously collected recommendations and all data points. In case all draws are exhausted without collecting the sufficient number of recommendations, we decrease the factor γ and repeat the procedure from the beginning. Pseudo code for this algorithm can be found in Algorithm 1 in Supplementary Information. The probability of success for these recommendations is computed as indicated in the "Success probability calculation" section in Supplementary Information.

---

### Analysis by categorizing or dichotomizing continuous variables is inadvisable: an example from the natural history of unruptured aneurysms [^18317801]. AJNR: American Journal of Neuroradiology (2011). Low credibility.

In medical research analyses, continuous variables are often converted into categoric variables by grouping values into ≥ 2 categories. The simplicity achieved by creating ≥ 2 artificial groups has a cost: Grouping may create rather than avoid problems. In particular, dichotomization leads to a considerable loss of power and incomplete correction for confounding factors. The use of data-derived "optimal" cut-points can lead to serious bias and should at least be tested on independent observations to assess their validity. Both problems are illustrated by the way the results of a registry on unruptured intracranial aneurysms are commonly used. Extreme caution should restrict the application of such results to clinical decision-making. Categorization of continuous data, especially dichotomization, is unnecessary for statistical analysis. Continuous explanatory variables should be left alone in statistical models.

---

### On the noise in "augmented T1-weighted steady state magnetic resonance imaging" [^5e296aaa]. NMR in Biomedicine (2023). Medium credibility.

Recently, Ye and colleagues proposed a method for "augmented T1-weighted imaging" (aT 1 W). The key operation is a complex division of gradient-echo (GRE) images obtained with different flip angles. Ye and colleagues provide an equation for the standard deviation of the obtained aT 1 W signal. Here, we show that this equation leads to wrong values of the standard deviation of such an aT 1 W signal. This is demonstrated by Monte Carlo simulations. The derivation of the equation provided by Ye and colleagues is shown to be erroneous. The error consists of a wrong handling of random variables and their standard deviations and of the wrong assumption of correlated noise in independently acquired GRE images. Instead, the probability distribution obtained with the aT 1 W-method should have been carefully analyzed, perhaps on the basis of previous literature on ratio distributions and their normal approximations.

---

### Experimental noise cutoff boosts inferability of transcriptional networks in large-scale gene-deletion studies [^7d1c5482]. Nature Communications (2018). Medium credibility.

Limits of network inferability

To estimate the upper bound of links that can be inferred from knockout screens, we consider a directed but not necessarily acyclic network of N nodes, with node activities as observables and a predefined subset of nodes that are perturbed independently by external forces. The perturbed nodes are randomly distributed within the network and we denote by q the fraction of nodes that are perturbed. We assume that an arbitrarily large set of perturbation experiments can be generated, with the freedom to tune the perturbation strength for each node independently. We further assume that other perturbative sources and measurement noise are absent. Calculation of the expected fraction of inferable links, F (q), can be carried out by a simple counting procedure (Figs 1a and 2a), assuming that links can be represented by noiseless, linear functions with non-zero slope. Under these conditions, a directed link between source and target node is inferable–or equivalently its link strength is identifiable–if it is not possible to fully reconstruct the activity state of the source node from the node activities of the remaining network. Consequently, a link is inferable if a part of the variation of the target node can be only explained by the source node, given that a link between them exists, and implies non-zero partial correlation between source and target node. To allow detection of arbitrarily small partial correlations, we make sure that there exists a finite fraction of experiments for each target node, where the target node is not perturbed (Online Methods and Supplementary Note 1). If, for example, only one node in the network is perturbed that targets multiple other nodes, its node activity can be fully reconstructed by any of its targets, resulting in zero partial correlation coefficients, which implies that none of the directed links can be inferred (Fig. 1a, right network). In contrast, if two out of three nodes are perturbed, all links targeting the unperturbed node are inferable (Fig. 1a, left network). In addition, nodes that have been identified as targets of the current target node can be removed prior to inference. This is because an existing link from the actual target node excludes them from transmitting information back to it, as we exclude bidirectional links from our analysis. This makes the network (Fig. 1a, left network) fully inferable, as the link between the remaining two perturbed nodes can be inferred by collecting experiments for which the target node is unperturbed. We emphasise that our approach to network inferability does not account for a priori known restrictions on the network topology, as in the case of directed acyclic graphs. Such constraints can strongly increase the inferability of directed links.

---

### Principled approach to the selection of the embedding dimension of networks [^e899ba67]. Nature Communications (2021). High credibility.

If we repeat the same analysis using the LINE algorithm, although the rate of decay of the function L (d) is not identical for the two embedding algorithms, we find analogous behavior with L (d) smoothly decaying toward an asymptotic value as d increases (Fig. 1 a, b). However, a faster decay doesn't necessarily imply that the corresponding algorithm is able to represent more efficiently the network than the other algorithm. The reference embeddings used in the definition of L (d) are in fact algorithm dependent, and the quantitative comparison between the L (d) functions of two different algorithms may be not informative.

Similar observations can be made when we perform the geometric embedding of synthetic networks. In Fig. 2, we display L (d) for networks generated according to the SB model. We still observe a saturation of the function L (d). Graph clustering reaches maximal performance at slightly smaller d values than those necessary for the saturation of L (d), and slowly decreases afterwards. The drop of performance for values of the embedding dimension that are too large has been already observed in other papers, see for example refs. Results based on LE embedding provide similar insights (Supplementary Figs. 3–6): L (d) saturates quickly as d increases; the performance in the graph clustering task is optimal before the saturation point of L (d); further, the saturation point is compatible with the presence of a significant gap in the sorted Laplacian eigenvalues.

Fig. 2
Geometric embedding of synthetic networks.

a Normalized loss as a function of the embedding dimension in SB networks. We generate graph instances composed of N = 256 nodes and C = 16 groups with p i n = 0.2 and p o u t = 0.02. Embedding is performed using node2vec. The blue curve in panel a is the best fit curve of Eq. (2) (, and R 2 = 5.93 × 10 −3) with the data points. b Test of performance in recovering the ground-truth community structure in SB graphs as a function of the dimension of the embedding. We measure the recovery accuracy with normalized mutual information (NMI). Symbols refer to average values of NMI computed over 10 instances of the SB model; the shaded region identifies the range of values corresponding to one standard deviation away from the mean.

---

### Quantifying entanglement in a 68-billion-dimensional quantum state space [^b339b52f]. Nature Communications (2019). High credibility.

Adaptive sampling algorithm

For each configuration, experimental data are stored in nodes in a quad-tree decomposition of P whose levels describe increasingly fine detail. The i th node corresponds to a square area ofat locationwith span. Nodes are sampled by placing the corresponding, one-dimensional local patterns on the DMDs and generating a coincidence histogram during acquisition time T a = 0.5 s. Coincidences C i are counted within a 1-ns coincidence window centered on the coincidence peak; accidental coincidences A i are counted in a 1-ns window displaced 2 ns from the coincidence window. Coincidence and accidental values are appended to a list each time the node is sampled. The estimated count rate, whereis a calibrated, relative fiber coupling efficiency. Optionally, A i can be subtracted from C i for accidental removal. Uncertainty is computed by assuming Poissonian counting statistics for C i and A i and applying standard, algebraic propagation of error through the calculation of the entanglement quantity (Eq. (3)).

The data collection algorithm consists of a partitioning phase followed by an iterative phase. During partitioning, the algorithm repeatedly iterates through a scan-list of leaves of the tree. Node i is considered stable when sgn(αR T − R i) is known to at least β standard deviations of certainty, where splitting threshold α (0 ≤ α ≤ 1) and stability criterion β are user-chosen heuristics. Stable nodes are no longer measured. If a node is stable and R i ≥ αR T, the node is split into four equal-sized sub-quadrants, which are initially unstable and added to the scan-list. Optionally, a maximum resolution (maximum tree depth) may be set.

The transition to the iterative phase occurs when the percentage of unstable leaves is < Γ, a user-chosen parameter. At this point, stability is ignored and all leaf nodes are scanned repeatedly and guaranteed to have the same total acquisition time. Various final stopping criteria can be used; we chose a fixed total run time. Note that heuristic parameters α, β, and γ may be changed during operation if desired. For the data shown in this manuscript, α = 0.002, β = 2, and Γ = 0.15 with a 30-h runtime.

The probability distributionis computed by uniformly distributing the estimated count rate (with or without accidental subtraction) from each leaf node across its constituent elements in, followed by normalization.

---

### Ion-induced nucleation of pure biogenic particles [^d4badc27]. Nature (2016). Excellent credibility.

The HOM concentration in equation (4) is determined from its production and loss rates:where MT represents total monoterpenes. The IUPACreaction rate constants (in cm 3 per molecule per second) for oxidation of α-pinene by ozone and hydroxyl radicals are, respectively:where T (in K) is the temperature (the α-pinene+O 3 rate constant is updated on the IUPAC website at). The HOM yields in each ozone–monoterpene and hydroxyl–monoterpene reaction areand, respectively. The parameter k HOM is the HOM loss rate or, equivalently, the atmospheric condensation sink, CS (in s −1). The condensation sink is determined assuming the diffusion characteristics of a typical α-pinene oxidation product (see appendix A1 of ref.). Assuming steady-state in equation (5), the HOM concentration becomes:where the HOM yield from ozonolysis is, and from reaction with the hydroxyl radical is(Extended Data Fig. 2). The HOM yield from ozonolysis is determined from CLOUD measurements in the presence of a hydroxyl scavenger (0.1% H 2). The HOM yield from reaction with hydroxyl radicals is determined from CLOUD measurements in the absence of ozone, and where photolysed HONO provides the OH· source. Therefore, the experimental measurement of hydroxyl-initiated oxidation is made in the presence of NO x, as occurs in the atmosphere.

The small-ion concentration in equation (4) is calculated from the steady-state solution of the ion balance equation:where q (in cm −3 s −1) is the ion-pair production rate and α is the ion–ion recombination coefficient (in cm 3 s −1). The factor of 2 in equation (4) accounts for nucleation from positive and negative ions. For the CLOUD GCR data, q = 1.7 cm −3 s −1. Terrestrial radioactivity such as radon contributes additional ionization in the boundary layer over land masses. The ion loss rate, k i, is due to the condensation sink, CS, and ion-induced nucleation:where J iin /(2[n ±]) is given by equation (4) and the steady-state concentration of small ions is, from equation (8):From equations (8) and (9), J iin saturates at 2 q at high nucleation rates (see Fig. 3).

---

### Principled approach to the selection of the embedding dimension of networks [^0e29bd7c]. Nature Communications (2021). High credibility.

There could be several ways of finding the solution of Eq. (1). Here, given the smoothness of the empirically observed loss functions, we opted for a parametric approach. Specifically, we found thatwhere s and α are fitting parameters, represents well the data in various combinations of embedding methods and embedded networks. Equation (2) is meant to describe the mathematical behavior of the loss function only in the range d ∈ [1, d r). In particular in the definition of Eq. (2), L ∞ appears as the value of the normalized loss function in the limit of infinitely large embedding dimensions. However, such a limit has no physical meaning and is used only to facilitate the mathematical description of the empirical loss function in the regime d ≪ d r. Best fits, obtained by minimizing the mean-squared error, of the function of Eq. (2) are shown in Figs. 1 a, b and 2 a. We performed a systematic analysis on a corpus of 135 real-world networks. We found that best estimatesof the asymptotic value L ∞ for the normalized embedding loss function are generally close to zero (Supplementary Table 1). Best estimatesof the factor s, regulating the rate of the power-law decay toward L ∞, seem very similar to each other, irrespective of the network that is actually embedded. Measured values ofindicate only a mild dependence on the underlying network (see Fig. 3). For uncorrelated clouds of points in d dimension, the central limit theorem allows us to predict that L (d) ~ 1/ d 1/2. Our best estimatesof the decay exponent α are generally greater than those expected for uncorrelated clouds of data points, indicating that the embedding algorithm correctly retains network structural information even in high-dimensional space. In systematic analyses performed on random networks constructed using either the ER and the BA models, we find that the size of the network is not an important factor in determining the plateau value L ∞. The decay exponent α and the rate s are positively correlated with density of the embedded network, but their values become constant in the limit of large network sizes (see Fig. 4).

---

### Recommendations for a standardized pulmonary function report. An official American Thoracic Society technical statement [^345386c9]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Selecting and reporting reference values — procedural guidance states that PFT laboratories must select appropriate reference values generated from high-quality data from a large sample of healthy asymptomatic individuals who have never smoked or had other respiratory illness or significant exposures, reports should identify the source of reference values, any change in selected equations should be noted and prior percent predicted values recalculated if possible, pediatric–adult equation discontinuity is discouraged, and extrapolation beyond the age range should not be done during growth, increases uncertainty in the elderly, and must be noted in technician comments.

---

### Practical guide for interpreting and reporting cardiac PET measurements of myocardial blood flow: an information statement from the American Society of Nuclear Cardiology, and the Society of Nuclear Medicine and Molecular Imaging [^b4a2d4ce]. Journal of Nuclear Cardiology (2021). High credibility.

Compartment model — placement of ROIs and time-activity curves includes reviewing myocardial ROI placement in each frame, correcting motion (particularly in the blood pool phase) and interpreting any study with patient motion with caution, and recognizing that dynamic curves show the blood pool and myocardial activities, the model fit, blood pool spillover, and the partial-volume-corrected myocardial curve; the blood pool and myocardial activity curves should start at zero with at least one background frame prior to radiotracer infusion and have single peaks, rest and stress blood pool curves should have similar shapes and peaks with stress peaks somewhat lower, uptake and myocardial blood flow (MBF) maps should have similar regional distributions with homogeneous goodness-of-fit parameter polar maps showing high values for the R^2 map and low values for the χ^2 map, and blood pool spillover polar maps used for partial-volume correction should be carefully examined because asymmetric spillover distribution may suggest patient motion or improper contours.

---

### Modelling insights into the COVID-19 pandemic [^de45a0e2]. Paediatric Respiratory Reviews (2020). Medium credibility.

Coronavirus disease 2019 (COVID-19) is a newly emerged infectious disease caused by the severe acute respiratory syndrome coronavirus-2 (SARS-CoV-2) that was declared a pandemic by the World Health Organization on 11th March, 2020. Response to this ongoing pandemic requires extensive collaboration across the scientific community in an attempt to contain its impact and limit further transmission. Mathematical modelling has been at the forefront of these response efforts by: (1) providing initial estimates of the SARS-CoV-2 reproduction rate, R 0 (of approximately 2–3); (2) updating these estimates following the implementation of various interventions (with significantly reduced, often sub-critical, transmission rates); (3) assessing the potential for global spread before significant case numbers had been reported internationally; and (4) quantifying the expected disease severity and burden of COVID-19, indicating that the likely true infection rate is often orders of magnitude greater than estimates based on confirmed case counts alone. In this review, we highlight the critical role played by mathematical modelling to understand COVID-19 thus far, the challenges posed by data availability and uncertainty, and the continuing utility of modelling-based approaches to guide decision making and inform the public health response. †Unless otherwise stated, all bracketed error margins correspond to the 95% credible interval (CrI) for reported estimates.

---

### Primary glioblastoma cells for precision medicine: a quantitative portrait of genomic (in) stability during the first 30 passages [^9ad9424a]. Neuro-Oncology (2018). Low credibility.

Mathematical Model of Subpopulation Differential Growth Rate

To quantify cell growth rates, we used the exponential model, in which n (t) is the number of cells as a given time point, Δ t is a time interval (in days), and r is the growth rate parameter (in doublings per day). From this basic assumption, we first derived equations to (i) estimate growth ratesin the unit doublings/day, and (ii) estimate corresponding of doubling times (t D) in the unit hours/doubling, described in the Supplementary Methods. Next, we derived a technique to estimate the difference in growth ratefrom alteration fraction data as follows. Consider a mixture of 2 populations of cells, where n1(t) is the number of cells of type 1 at time t and n2(t) is the number of cells of type 2 at time t. The proportion of cells of type 2 at an arbitrary starting time point t0 is defined as

ie, the fraction of type 2 cells of the total pool of cells. Assume that cells grow exponentially from timeThe number of cells of types 1 and 2 are given by

where r is the doubling rate in 1/hours and Δ r is the difference in doubling rate between population 2 and population 1. At timethe proportion of cells in population 2 is given by

Substitution of equations 1 and 2 into equation 3, followed by simplification and solving for Δ r (details in Supplementary equations S1) gives the result in equation 4 (Results, below).

Solving equations 2 and 3 for Δ r gives the result in equation 1. Note that the time unit t is arbitrary, where passage is used, implying that Δ r calculated using equation 1 is the differential cell growth per passage is exp(Δ r).

---

### Longitudinal changes in the frequency of mosaic chromosome Y loss in peripheral blood cells of aging men varies profoundly between individuals [^8dfa0582]. European Journal of Human Genetics (2020). Medium credibility.

Transformation of mLRRY-values into percentage of cells with LOY

We developed a formula for conversion of mLRRY-values estimated from SNP-array data into the unit LOY (%) by taking advantage of LOY-estimates from pairwise studied DNA samples. First we determined that the percentage of LOY estimated in samples studied by WGS and ddPCR (n = 26) yielded essentially identical results. The readouts from these technologies could therefore be used as reference points for establishing a relationship between mLRRY and LOY (%). The latter was performed in parallel and independently using the data generated from pairwise studied samples using ddPCR and SNP-array (n = 121) as well as from samples analysed using WGS and SNP-array (n = 26) (Supplementary Fig. 3). Specifically, the mLRRY-values were first antiloged (2 mLRRY) and correlated with data generated from the same samples by WGS (n = 26) or ddPCR (n = 121). Calculation of power equations resulted in the relationships y = 0.9242*(2^ x)^1.7703 and y = 0.9695*(2^ x)^1.8779 (describing the relationships between level of LOY estimated by mLRRY and percentage of LOY estimated by WGS or ddPCR, respectively). The constants in these relationships were rounded to the nearest integer and used to adjust the antilog mLRRY, resulting in two equations: (1) percent of cells with a Y chromosome = 100*2 2*mLRRY and (2) LOY (%) = 100*(1−2 2*mLRRY).

Statistical analyses

Calculations of Pearson's coefficient of determination (R 2) and linear regression models were performed using R. Standardised beta-values (β) was calculated using the R library lm.beta.

---

### 2013 ACC / AHA guideline on the assessment of cardiovascular risk: a report of the American college of cardiology / American Heart Association task force on practice guidelines [^d0e143d1]. Journal of the American College of Cardiology (2014). Medium credibility.

Table 5 — Distribution of estimated 10-year risk of a first hard atherosclerotic cardiovascular disease (ASCVD) event in the CVD-free, nonpregnant US population aged 40 to 79 years is organized by predicted 10-year risk categories < 2.5%, 2.5%–4.9%, 5.0%–7.4%, 7.5%–9.9%, 10.0%–14.9%, 15.0%–19.9%, and ≥ 20.0%. In the overall population, the percentages (95% CI) are 33.4 (31.2–35.5), 21.0 (19.4–22.7), 12.7 (11.4–14.0), 7.4 (6.5–8.3), 8.9 (8.1–9.6), 6.3 (5.6–7.1), and 10.2 (9.5–11.0), with corresponding n of 33 534 000, 21 151 000, 12 766 000, 7 470 000, 8 940 000, 6 380 000, and 10 300 000. Among men, the percentages (95% CI) are 17.4 (15.2–19.7), 22.7 (20.3–25.1), 15.6 (13.8–17.4), 10.1 (8.5–11.6), 12.1 (10.7–13.5), 8.8 (7.4–10.2), and 13.3 (12.1–14.4), with n of 8 386 000, 10 950 000, 7 511 000, 4 847 000, 5 849 000, 4 248 000, and 6 388 000. Among women, the percentages (95% CI) are 48.0 (44.8–51.3), 19.5 (17.3–21.6), 10.0 (8.3–11.8), 5.0 (3.8–6.2), 5.9 (5.1–6.7), 4.1 (3.4–4.7), and 7.5 (6.5–8.4), with n of 25 148 000, 10 200 000, 5 256 000, 2 622 000, 3 091 000, 2 131 000, and 3 912 000. These estimates were derived by applying the Pooled Cohort Equations to NHANES 2007–2010 with N = 5367 and weighting to 100 542 000 US population.

---

### Experimental demonstration of quantum advantage for NP verification with limited information [^d2e46774]. Nature Communications (2021). High credibility.

In order to explain the importance of our result let us first go back to the classical case and describe a possible scheme for verification. Since we know that in case the formula is not satisfiable then for any assignment at least a constant δ fraction of the clauses are not satisfied, then for verification it suffices for Arthur to pick a random clause, obtain the values of the four variables and check whether the clause is satisfied or not. By repeating this for a small constant number of clauses, Arthur can verify with high probability whether the instance is satisfiable or not, and moreover, the information Arthur receives about the solution is very small (just the value of the variables in a few clauses). We can also see this protocol in a slightly modified version, which will be closer to our quantum verification protocol based on Sampling Matching. Instead of having Arthur pick uniformly at random a small number of clauses out of all possible clauses to verify, we can assume that Arthur picks each clause with some probability so in the end the expected number of clauses he picks is the same number as in the initial protocol.

There is of course a well-known issue in these schemes. Once Merlin knows which clause Arthur wants to test, he can easily adapt the values of the variables to make this clause satisfiable. Arthur cannot force Merlin to be consistent across the different clauses, namely to keep the same value for each variable in the different clauses. One way to remedy this would be by having Merlin send the entire assignment to Arthur (which is the usual verification protocol), but in this case Arthur gets all the information about the classical solution. Another solution is through interactive computational zero-knowledge proofs, where one uses cryptographic primitives, i.e. bit commitment, in order to force the behavior of Merlin, but such schemes necessitate communication between Arthur and Merlin and only offer computational security.

---

### 2019 ASCCP risk-based management consensus guidelines for abnormal cervical cancer screening tests and cancer precursors [^14f849d0]. Journal of Lower Genital Tract Disease (2020). High credibility.

ASCCP risk-based management — risk estimation methodology: For each combination of past and current screening results, CIN 3+ risk was estimated using prevalence–incidence mixture models that jointly model prevalent CIN 3+ at the current test via logistic regression and incident CIN 3+ at subsequent testing via proportional hazards, with the models designed to handle verification bias and interval censoring and to avoid rigid distributional assumptions (e.g. Weibull).

---

### Iterative decomposition of water and fat with echo asymmetry and least-squares estimation (IDEAL): application with fast spin-echo imaging [^f4b854f3]. Magnetic Resonance in Medicine (2005). Low credibility.

Chemical shift based methods are often used to achieve uniform water-fat separation that is insensitive to Bo inhomogeneities. Many spin-echo (SE) or fast SE (FSE) approaches acquire three echoes shifted symmetrically about the SE, creating time-dependent phase shifts caused by water-fat chemical shift. This work demonstrates that symmetrically acquired echoes cause artifacts that degrade image quality. According to theory, the noise performance of any water-fat separation method is dependent on the proportion of water and fat within a voxel, and the position of echoes relative to the SE. To address this problem, we propose a method termed "iterative decomposition of water and fat with echo asymmetric and least-squares estimation" (IDEAL). This technique combines asymmetrically acquired echoes with an iterative least-squares decomposition algorithm to maximize noise performance. Theoretical calculations predict that the optimal echo combination occurs when the relative phase of the echoes is separated by 2pi/3, with the middle echo centered at pi/2+pik (k = any integer), i.e., (-pi/6+pik, pi/2+pik, 7pi/6+pik). Only with these echo combinations can noise performance reach the maximum possible and be independent of the proportion of water and fat. Close agreement between theoretical and experimental results obtained from an oil-water phantom was observed, demonstrating that the iterative least-squares decomposition method is an efficient estimator.

---

### Analog optical computer for AI inference and combinatorial optimization [^d9ce3cb7]. Nature (2025). Excellent credibility.

Inference and export to the AOC

Once training has completed, the weight matrix W is quantized to signed 9-bit integers usingwith the rounded and clamped matrix on the right-hand side being the quantized weight matrix W Q. Whenever we report AOC-DT results, we report results obtained with the quantized matrix.

Exporting trained models to the AOC requires several further steps. First, the model inputs x and the bias term b need to be condensed into a single vector b AOC = b + x followed by clamp to ensure the values fit into the dynamic range of the AOC device (Supplementary Information section D). Second, as the optical matrix multiplication is implemented using SLMs, elements of the weight matrix are bounded by one such that all quantization-related factors disappear. However, the original maximum element of the matrix max(W) needs to be re-injected, which we achieve via the β gain in equation (2), approximately restoring the original matrix W.

The quantized matrix is split into positive and negative parts, and each part is displayed on its respective SLM.

AOC sampling and workflow

Each classification instance (that is, MNIST or Fashion-MNIST test image) is run once on the AOC, and the fixed point is sampled at the point marked in Extended Data Fig. 3 after a short 2.5 - μs cooldown window after the switch is closed, as shown in Extended Data Fig. 5a, b. The sampling window extends over 40 samples at 6.25 MHz, corresponding to 6.4 μs. This ensures that the search of fixed points for the equilibrium models happens entirely in the analog domain. Once sampled, we digitally project the vector into the output space. For classification, the input is projected from 784 to 16 dimensions, the output is projected from 16 to 10 classes. The label is then determined by the index of the largest element in the output vector (argument-max). For regression tasks, the IP and OP layers transform a scalar to 16 dimensions and back, respectively. The MSE results in Fig. 2c were obtained by averaging over 11 repeats for each input. This means that we restart the solution process 11 times, including the sampling window, and average the resulting latent fixed-point vectors. Importantly, the solve-to-solve variability appears to be centred close to the curve produced by the AOC-DT, enabling us to average this variability out (Supplementary Fig. 6).

---