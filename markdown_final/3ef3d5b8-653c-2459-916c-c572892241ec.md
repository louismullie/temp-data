# Medical Question & Answer

**Sample ID**: 3ef3d5b8-653c-2459-916c-c572892241ec
**Dataset Index**: 339414

---

## Question

The AH Protocol provides source authentication and data integrity, but not

Answer Choices:
A. Integrity
B. Privacy
C. Nonrepudiation
D. Both A & C

---

## Answer

> Let's see… What do we have here? The user is asking which security service the AH (Authentication Header) protocol does not provide, given that it offers source authentication and data integrity. Let's break this down step-by-step. First, I need to think about what AH actually does in IPsec. Then, I should verify which security services are explicitly absent from AH. Next, I will cross-check the options against standard descriptions of AH versus ESP. Finally, I will confirm the best answer and briefly justify it with references to authoritative sources.

> Let me first confirm the core function of AH. AH is an IPsec protocol that provides connectionless integrity and data origin authentication for IP packets, and it optionally provides replay protection; it does not encrypt the payload, which is a key limitation when confidentiality is required [^notfound].

> Wait, let me verify the options against what AH actually provides. Option A says "Integrity", but AH explicitly provides integrity, so A cannot be correct. Option C says "Nonrepudiation"; I initially thought AH might offer nonrepudiation because it authenticates the source, but hold on, let's not jump to conclusions — nonrepudiation typically requires cryptographic evidence that binds the sender to the message in a way that can be proven to a third party, which AH alone does not provide, so C is likely correct. Option B says "Privacy"; AH does not encrypt, so it does not provide privacy/confidentiality, which makes B correct as well. Option D claims both A and C, but since A is incorrect, D cannot be right [^notfound].

> I should double-check the nuance on nonrepudiation. AH authenticates the source and protects integrity, but it does not furnish the nonrepudiation guarantees that come with digital signatures or similar constructs; thus, while AH supports source authentication, it does not by itself ensure nonrepudiation, reinforcing that C is a valid deficiency alongside privacy [^notfound].

> Putting this together, the AH protocol provides source authentication and data integrity, but it does not provide privacy (confidentiality) and it does not provide nonrepudiation. Among the choices, the clearest and most directly supported answer is B. Privacy, because the question is phrased as "but not", and privacy is the primary service AH lacks that is commonly expected in secure communications [^notfound].

---

The AH (Authentication Header) protocol provides **source authentication** and **data integrity** for IP packets, but it does **not provide privacy (confidentiality)** because it does not encrypt the packet payload. Therefore, the correct answer is **B. Privacy**.

---

## Security services provided by AH

AH provides the following **security services**:

- **Source authentication**: AH verifies the identity of the sender, ensuring that the packet originates from a trusted source [^notfound].

- **Data integrity**: AH ensures that the packet has not been altered during transmission by using cryptographic hash functions [^notfound].

- **Replay protection**: AH includes a sequence number field to prevent replay attacks, although this feature is optional [^notfound].

---

## Security services not provided by AH

AH does **not provide privacy (confidentiality)** because it does not encrypt the packet payload. This means that the data within the packet remains visible to anyone who intercepts it, although the packet headers are protected from modification [^notfound].

---

## Comparison with ESP (Encapsulating Security Payload)

In contrast to AH, ESP provides **privacy (confidentiality)** by encrypting the packet payload, in addition to offering source authentication, data integrity, and replay protection. ESP is typically used when confidentiality is required, whereas AH is used when only authentication and integrity are needed [^notfound].

---

## Conclusion

The AH protocol provides source authentication and data integrity but **does not provide privacy (confidentiality)**. The correct answer is **B. Privacy**.

---

## References

### Bridging research integrity and global health epidemiology (BRIDGE) guidelines: explanation and elaboration [^e973e870]. BMJ Global Health (2020). High credibility.

4.7 At the beginning of the study, prepare an electronic secured study file to store all study documentation and outputs. Regularly update this file and archive it the end of the study

Maintaining a secure electronic study file helps to ensure that the most up to date versions of all study materials are stored in a single location. An electronic study file should include protocols, data analysis plans, data management plans, ethical review submissions and responses, informed consent forms, data collection tools, anonymised datasets and transcripts, metadata, data management programmes, analysis programmes, statistical outputs, reports and publications. To ensure secure storage, the study file should resists on two physically separate regularly synchronised storage mediums, for example, on a local laptop hard disk and remote backup server. When setting up the storage system, it is important to think about risks to data integrity, externally (eg, fire, flooding), and internally (disgruntled staff member, ransomware, virus attacks, etc) and how to mitigate those.

The choice of where to store and especially where to archive the data may be straightforward if researchers have an established data management facility in their institution. Cost-free remote data repositories may be a useful alternative when these are not available. There are three important considerations when choosing an online repository for data storage and archiving (as opposed to data sharing, which is discussed in criterion 6.6): 1) Does it offer closed access and protected against unauthorised access; 2) Is it hosted by a trusted institution with a vision and capacity to provide long-term secure storage (eg, at least 10 years). Zenodo (zenodo.org) is a general purpose repository hosted by CERN which fits both criteria, while both can be problematic with public could storage services (such as DropBox, Google Drive, etc).

4.8 Retain source data safely, in their original form, preserving data confidentiality for as long as has been described in the protocol

Source data refers to materials collected as part of the research at the primary source of data collection (study participants, household respondents, etc). Thus, source data includes: signed informed consent forms, filled in data collection forms, audios files, videos and photos and biological samples (data, images, photos of slides, but not sample itself). The study protocol should specify how long source data will be stored and under which conditions (and security guidance).

---

### Clinical bioinformatician body of knowledge-clinical laboratory regulation and data security core: a report of the Association for Molecular Pathology [^19b17113]. The Journal of Molecular Diagnostics (2025). High credibility.

Encryption — A key security consideration is to ensure that data are safely secured, contained, and isolated; this includes appropriate levels of encryption of the stored data at rest (including permanent and ephemeral storage during computation) and during transfer, and appropriate controls and protocols should be in place for authentication and encryption of key access and storage.

---

### Clinical bioinformatician body of knowledge-clinical laboratory regulation and data security core: a report of the Association for Molecular Pathology [^2bd5e90c]. The Journal of Molecular Diagnostics (2025). High credibility.

AMP BoK Regulation & Data Security Core — sample identifiers and data integrity in bioinformatics pipelines states that accreditation and international standards require at least two unique identifiers to be associated with the patient's sample throughout testing, and that the AMP guidelines for bioinformatics recommend four identifiers per case; identifiers should be present not only in the file path but also in file names and within files or metadata, and different instruments or pipeline runs should have unique data paths and names to avoid confusion; unique-coded sample identifiers are preferable to names but require a secure key linking back to patient and sample identifiers with auditable updates, often handled in the Laboratory Information Management System; best practice further includes data integrity checks such as md5 checksum generation at both source and destination with comparison to ensure identity, plus checks for expected variant calls, variant counts, and file sizes, which may indicate that an incomplete or truncated file was produced during the pipeline.

---

### Clinical bioinformatician body of knowledge-clinical laboratory regulation and data security core: a report of the Association for Molecular Pathology [^b22e970b]. The Journal of Molecular Diagnostics (2025). High credibility.

Authentication, access control, and authorization — authenticating the user and authorizing them to use a resource are critical for maintaining security in data use and processing of the clinical workflow; authentication confirms a user's identity by validating their credentials, and multifactor authentication increases security by requiring more than one set of credentials of different types, with a common scheme using a username and password plus a one-time code from a mobile device. Authorization confirms that the user has the privileges to access a resource or perform a particular action; it should be tailored to each user's role with privileges limited to the minimum necessary, each user should use their own credentials, and shared or common credentials should be avoided when possible. For programmatic machine-machine interactions, a request may be accompanied by a token; NGS molecular informatics systems that host PHI or other identifiable sensitive data should use industry-standard authentication and authorization protocols, credentials should be sent using encryption for programmatic verification, and user passwords should only be stored as cryptographic hash values in a database.

---

### SPIRIT 2025 explanation and elaboration: updated guideline for protocols of randomised trials [^d48eafec]. BMJ (2025). Excellent credibility.

Explanation

Personal information about trial participants is acquired during the process of recruitment, eligibility screening, and data collection. Much of this information consists of private details of which many people wish to maintain control, such as their health status, personal genotype, and social and family history.

Researchers need to safeguard confidential participant data from potential data breaches, sometimes while simultaneously implementing appropriate procedures for data sharing (item 6).

Reviews of two samples of 108 and 292 trial protocols approved in 2016 found that 76% and 88%, respectively, considered confidentiality of data.

The protocol should describe the means whereby personal information is collected, kept secure, and maintained. In general, this involves the creation of coded, deidentified data where the participant's identifying information is replaced by an unrelated sequence of characters; secure maintenance of the data and the linking code in separate locations using encrypted digital files within password protected folders and storage media; and limiting access to the minimum number of individuals necessary for quality control, audit, and analysis. The protocol should also describe how the confidentiality of data will be preserved when the data are transmitted to sponsors, co-investigators, and external parties (item 6).

Summary of key elements to address

How confidentiality will be preserved when: collecting and maintaining personal information before, during, and after the trial; and transmitting data to sponsors, co-investigators, and external parties.

Item 34: Provisions, if any, for ancillary and post-trial care, and for compensation to those who suffer harm from trial participation

Example

"Participants will receive ancillary care beyond the scope of the trial as required. Any adverse events or complications arising from trial participation will be promptly addressed, and necessary medical interventions will be provided at no cost to the participants. Ancillary care will extend to managing conditions unrelated to the trial that may arise during the study period, ensuring the overall well-being of the participants.

Post-trial care will be provided to participants, particularly for any ongoing effects or complications related to the trial interventions. Participants will have access to appropriate medical care and follow-up visits, ensuring continuity of care beyond the trial's conclusion.

Provisions for compensation will be in place for participants who may suffer harm from trial participation. In the event of an adverse event directly attributable to the study interventions, compensation will cover medical expenses, additional treatments, and any other related costs. The compensation process will be transparent, fair, and in compliance with local regulations and ethical guidelines".

---

### Efgartigimod following plasma exchange in the treatment of subjects with generalised myasthenia gravis: study protocol for a multicentre, three-arm, open-label study [^f62f2ea0]. BMJ Neurology Open (2025). High credibility.

Data collection and management

Longitudinal data capture will be documented by principal investigators using standardised Case Report Forms (CRFs) structured in accordance with protocol-defined intervals: baseline, acute phase and remission phase. Clinical research associates will conduct source data verification, with all CRF entries requiring dual authentication through principal investigator review and signature. Subsequent data modifications mandate comprehensive audit trail documentation including resignatures.

All study-related documentation will be archived in secure physical repositories using biometric authentication systems at designated trial sites. Participant records will undergo pseudonymisation through unique alphanumeric coding, systematically replacing protected health information to ensure confidentiality compliance. Electronic data preservation employs enterprise-grade encryption algorithms with multilayered access controls, incorporating mandatory password authentication complemented by periodic cryptographic key rotation. Database architecture implements role-based permission matrices, restricting sensitive information access to authorised personnel exclusively.

Data monitoring

Robust data surveillance and auditing mechanisms will be implemented to ensure protocol adherence and trial integrity. Safety parameter documentation will undergo systematic real-time recording with expedited reporting to an independent Data Safety Monitoring Board (DSMB). The DSMB retains operational authority to (1) recommend trial suspension through formal notification to principal investigators upon identifying significant safety signals and (2) mandate premature termination based on prespecified safety thresholds or futility analyses.

Trial documentation, including source records, validated informed consent documentation and participant datasets, will undergo tripartite verification by the Clinical Research Center's Audit Committee at protocol-defined milestones: enrolment completion, mid-study evaluation and trial conclusion. This committee will verify data accuracy through source document verification processes while assessing regulatory compliance with International Council for Harmonisation of Technical Requirements for Pharmaceuticals for Human Use - Good Clinical Practice(ICH-GCP) guidelines.

---

### International charter of principles for sharing bio-specimens and data [^b77c0c89]. European Journal of Human Genetics (2015). Low credibility.

Ensuring the scientific use of data and bio-specimens

8. Without distinction, access and use of data and samples should always be based on the scientific validity, quality and potential of the request. Those who contribute, collect, curate and annotate samples/data, however, should have first rights to publish within a given time period (usually no longer than 1 year).

9. In addition, the identity and bona fide of the requestors should be authenticated. Proper attribution and intellectual property should be accorded as appropriate, and the security of the data and samples should always be ensured.

In recognition of the contribution of the samples and data of patients, research participants and their families, the protocols of those requesting access for use should be vetted for their scientific validity and quality.

---

### 2017 ISHNE-HRS expert consensus statement on ambulatory ECG and external cardiac monitoring / telemetry [^258c0b82]. Heart Rhythm (2017). Medium credibility.

AECG data transmission and security — relay methods and protocols: Systems may use "GPRS or a combination of Bluetooth with a Wi-Fi 802.11 b/g/n relay station", and "GPRS uses powerful algorithms and encryption techniques on security controls that include subscriber identity confidentiality, subscriber identity authentication, user data confidentiality on physical connections, connectionless user data confidentiality, and signaling information element". Bluetooth-enabled MCTs "transmit data via relay (smartphone, tablet, and dedicated transmission device) to the remote reading centers", the "Smart Bluetooth low-energy protocol… features robust security measures", and transport often relies on the "HTTPS data transmission protocol widely used over the Internet".

---

### Clinical bioinformatician body of knowledge-clinical laboratory regulation and data security core: a report of the Association for Molecular Pathology [^438bf27d]. The Journal of Molecular Diagnostics (2025). High credibility.

Cybersecurity review criteria for software and pipelines — the criteria to be considered include: ability to demonstrate that developers understand the technology underlying the software, flow of data, and existing infrastructure to ensure secure integration; ability to provide compliance with appropriate governance, regulation, and privacy regulations; ability to demonstrate familiarity with software security basics (confidentiality, integrity, availability, authentication, authorization, auditing, and management of configuration, sessions, and exceptions); ability to provide documentation of protections for sensitive information with appropriate data classification; availability of design documentation for application security design, vulnerabilities, and security threats; ability to demonstrate appropriate implementation of security controls with documentation of code reviews for security vulnerabilities and penetration testing; specifications of a test environment appropriately matched to the production environment; ability to follow the change management process, including managing software releases and bug fixes; and ability to describe the process used for vulnerability and penetration testing before deploying new software versions.

---

### SPIRIT 2013 explanation and elaboration: guidance for protocols of clinical trials [^66ceaede]. BMJ (2013). Excellent credibility.

Confidentiality

Item 27: How personal information about potential and enrolled participants will be collected, shared, and maintained in order to protect confidentiality before, during, and after the trial

Explanation

Personal information about participants is acquired during the process of trial recruitment, eligibility screening, and data collection. Much of this information consists of private details over which people customarily wish to maintain control, such as their health status, personal genotype, and social and family history.

The protocol should describe the means whereby personal information is collected, kept secure, and maintained. In general, this involves: 1) the creation of coded, depersonalised data where the participant's identifying information is replaced by an unrelated sequence of characters; 2) secure maintenance of the data and the linking code in separate locations using encrypted digital files within password protected folders and storage media; and 3) limiting access to the minimum number of individuals necessary for quality control, audit, and analysis. The protocol should also describe how the confidentiality of data will be preserved when the data are transmitted to sponsors and coinvestigators (eg, virtual private network internet transmission).

---

### MRIdb: medical image management for biobank research [^07659829]. Journal of Digital Imaging (2013). Low credibility.

Methods

System infrastructure

MRIdb is a software application composed of a bespoke web application and a suite of utility scripts and tools. It depends on a number of other components, including an underlying clinical picture archiving and communications system (PACS), a scalable storage system, an authentication service and a relational database system, as shown in Fig. 1.

Fig. 1
MRIdb system architecture

The foundation of MRIdb is DCM4CHEE, a mature, highly configurable open-source PACS. It handles the vital, low-level functions of image archival from scanners using the DICOM protocol, metadata extraction from images into a relational database schema and raw image and thumbnail retrieval facilities. It natively provides web and DICOM interfaces, but the former is complex and provides extensive administrative and data manipulation facilities, whilst the latter enables access to unanonymised data. MRIdb's primary function is, therefore, to provide an alternative, intuitive, read-only interface to image metadata, whilst offering study management and multi-format image export with enforced preservation of data integrity and anonymity.

Security

The MRIdb web application is intended as a simple, secure, centralised portal for users to retrieve images. It requires users to be authenticated using either an institutional lightweight directory access protocol (LDAP) server or a local password database and additionally enforces role-based access control for "visitors", "researchers", and "administrators". Visitors are able to browse images, but it does not show (and does not allow search by) patient names, ages or dates of birth. Researchers are able to view patient information but cannot perform functions such as viewing usage logs or information about other users of the system. Administrators have full access and can create other administrators, as well as configure help material, perform import audits and check authentication and download logs. Importantly, no user, regardless of role, is able to export unanonymised data in any format, and bulk downloads are password protected by default. The DCMTK toolkit is used for DICOM anonymisation.

---

### Clinical bioinformatician body of knowledge-clinical laboratory regulation and data security core: a report of the Association for Molecular Pathology [^e0559375]. The Journal of Molecular Diagnostics (2025). High credibility.

Data transfer protocols — A transfer protocol is a defined standard protocol designed to allow the transfer of data from one end point to another, frequently operating within the network, transport, session, and application layers to enable communication; the use of defined, tested, and validated protocols aids in preventing data loss and breaches because of malware and other threats; and selection of specific protocols should be based on specific data and use cases.

---

### Infrastructure and operating processes of PIONEER, the HDR-UK data hub in acute care and the workings of the data trust committee: a protocol paper [^aa0f8a3d]. BMJ Health & Care Informatics (2021). High credibility.

Included data

PIONEER includes data about patient demographics (age, gender, ethnicity), past and current medical diagnoses, medications, allergies and healthcare contacts. There is serial data on vital signs, investigations including laboratory, pathology, physiology and imaging, all acute medical prescriptions and administrations and health process data (clinical review specialty, ward type). This is supplemented with health data prior to and after the acute care contact, linked across healthcare providers, to enable assessment of preceding health and subsequent outcome. The PIONEER dataset adds to publicly available datasets, as highlighted in the examples provided in table 1.

Table 1
A summary of related data sources

Research database design and security

Data will be stored on a secure, UHB-controlled Microsoft Azure cloud platform in accordance with the 14 UK Cyber Cloud Principles, which include data protection in transit; asset protection and resilience; separation between users; a robust governance framework; operational security; secure user management; identity and authentication; and audit. The cloud provision will follow the International Organization for Standardization (ISO) 27 001 standards, an international specification for information security management. This ISO outlines a broad range of quality control processes, many of which apply to data collection, processing and management with a strong emphasis on information security. An example of how this will be met for PIONEER is that data transferred between organisations will be mathematically checked to ensure it has not been tampered with.

The database platform will comply with the Department of Health Information Governance policies and standards for secure processing of patient healthcare data, as set out in the Information Governance Toolkit of the Health and Social Care Information Centre. The database platform will undergo cyber-security checks by an independent and external company. The platform's design enables the rapid build of bespoke trusted research environments (TREs) which provide approved and licensed researchers with access to specific curated, deidentified datasets and a suite of analytical tools (description available from corresponding author). The use of a TRE circumvents data travel from the data controller to the data user. PIONEER is committed to promoting the protection of privacy and data security in line with the Organisation for Economic Co-operation and Development (OECD) Recommendation of the Council on Health Data Governance.

---

### Interactive multimedia reporting technical considerations: HIMSS-SIIM collaborative white paper [^84e001c2]. Journal of Digital Imaging (2022). Medium credibility.

Security Concerns in Report Exchange

Report exchange requires consideration of many security implications. The details of securing a communication channel between a report authoring tool and EHR are out of scope for the present discussion. As each system or data source is connected, data integrity, authentication and authorization, infrastructure, and presentation device security should be appropriate for the lifecycle of the data with minimum standards defined before a system can be connected. A legacy system utilizing a deprecated encryption method would introduce a potential risk to other connected systems such as the report authoring tools and/or the EHR. Ensuring encryption used is based on appropriate ciphers and strengths for the messaging and transport protocol, such as standardizing on TLS1.2 or TLS1.3 for HTTPS, would increase the overall security of the information exchange.

Some of the formats discussed create new threat surfaces that do not exist in plain text reports. PDF files can contain scripts to facilitate extended capabilities but could be hijacked to embed a virus. JavaScript embedded in HTML can enable interactive widgets as it does on the Internet. Running such code requires a degree of trust in the development, maintenance, and security of the code and the device used to view the report. The exchange of such rich reports may require the cryptographic signing of reports or other mechanisms to ensure the integrity of the report from origin to the recipient, but such approaches have been hampered by the lack of widespread deployment of a certificate distribution and signature verification architecture. The authentication and authorization of both the creator and recipient of the report by multiple systems will be critical to maintaining the confidentiality of the data across many different user roles and access methods. Report formats that retrieve either the main report or secondary content from uncontrolled remote locations may be susceptible to retrieving malicious content or to link rot.

---

### Guidelines for the practice of telepsychology [^bf935e52]. The American Psychologist (2013). Medium credibility.

Data security — The process of maintaining the confidentiality, integrity, and availability of an organization's data in a manner consistent with the organization's risk strategy.

---

### Improving authenticity and provenance in digital biomarkers: the case for digital watermarking [^1e03f4b8]. NPJ Digital Medicine (2025). Medium credibility.

What is digital watermarking and how does it work?

Digital watermarking applies steganographic principles to embed identification data into digital signals by making imperceptible modifications to redundant or insignificant components. Watermarks can also be designed to be either robust (surviving common modifications) or fragile (breaking upon tampering), with robust watermarks better suited for ownership proof while fragile ones excel at tamper detection. This verification mechanism also supports continuous authentication, rather than discrete point-in-time checks like two-factor authentication. Current applications of watermarking also include securing authenticity in Portable Document Format files and safeguarding intellectual property in video and multimedia streaming by preventing unauthorized content distribution.

This technique is particularly useful for inherently noisy signals like audio or image data, where small modifications can be hidden within the natural variations of the signal (Figs. 1, 2).

Fig. 1
Key elements in data integrity and security.

This figure outlines the description of authentic data, steganography and digital watermarking.

Fig. 2
Watermarking in digital biomarkers.

Example overview of digital watermarking process within gait data.

In digital biomarkers, this could mean in gait data collected via wearable sensors, watermarks may be integrated without altering the core functionality, ensuring authenticity and traceability.

New technological approaches, such as blockchain-based protocols, are also emerging in implementing watermarking-based copyright and purchase transaction protection mechanisms. Similarly, by enabling robust verification processes, watermarking may help identify and signal any signs of alteration, ensuring that health data remains reliable.

---

### Self-measured blood pressure telemonitoring programs: a pragmatic how-to guide [^b432ace5]. American Journal of Hypertension (2023). Medium credibility.

Health information exchange servers

A server is a high-powered computer that provides functionality for other programs or devices. A server that can receive secure, encrypted data transmission from a smartphone, tablet, or home hub; provide a single point in the cloud to which SMBP readings and contextual data like time of readings and device make and model can be forwarded and stored securely; and allow hubs to be remotely managed may be of benefit to SMBP telemonitoring programs. These exchange servers are typically provided by telemonitoring vendors as part of a telemonitoring service and act as gateways, translating device data into different formats. The recommended format for intersystem communication in the healthcare sector is Health Level 7 (HL7) Fast Healthcare Interoperability Resource (FHIR) framework, which defines the communication protocol for data exchange.

Security and data privacy

Securely protecting patient data and assuring privacy are recommended components of all telemonitoring systems, and include:

1. Compliance with privacy, consent, and other regulatory requirements.
2. Protection against identity and data loss theft.
3. Ability to safeguard effectiveness.
4. Ability to maintain data integrity and privacy while using real-world public networks.

SMBP monitoring is typically being "prescribed" by care delivery organizations as part of their hypertension management programs and telemonitoring vendors can provide the technological elements required for a successful program. Accordingly, a Business Associates Agreement (BAA) should be signed between these 2 parties that outlines responsibilities and deliverables. Vendors of telemonitoring solutions are familiar with security frameworks and can adapt to local requirements.

Patient-generated BP readings should be housed on HIPAA-compliant secure servers. Typically, these are cloud-based and use HL7 FHIR-compliant communication protocols. Data, once stored with at-rest encryption protocols to prevent unauthorized access (i.e. Advanced Encryption Standard 256-bit), can then be accessed via a secure web portal through a Hypertext Transfer Protocol encrypted by Transport Layer Security (HTTPS-TLS). Access to the server's highly protected network is typically authenticated via a unique username and password identifier and additional protection can be implemented with a 2-factor authentication requirement.

---

### Clinical bioinformatician body of knowledge-clinical laboratory regulation and data security core: a report of the Association for Molecular Pathology [^fda1ad7a]. The Journal of Molecular Diagnostics (2025). High credibility.

Data transfer protocols — TCP/IP and Internet protocol are used together to allow for host-to-host connection and data transfer over the Internet; UDP is a faster alternative to TCP that does not allow for retransmission of lost data packets and is frequently used for real-time communication; SSL/TLS allow for encrypted communication between applications; SSH allows users to remotely log into and control another computer on a network in a secure manner; HTTP is a client-server data transfer protocol used by websites and application programming interfaces (APIs) with HTTPS as an extension that supports encrypted communication; FTP is a client-server data transfer protocol used for transfer of files and SFTP allows for secure file transfer based on TLS.

---

### Challenges and standardisation strategies for sensor-based data collection for digital phenotyping [^e63e9c81]. Communications Medicine (2025). Medium credibility.

Ensuring robust data security is a significant issue as well. Cybersecurity threats, including data breaches and hacking, can expose sensitive information to malicious actors. For instance, real-time data transmissions from wearables or smartphone apps are particularly vulnerable to interception if not encrypted appropriately. Furthermore, once collected, storing large datasets securely remains a challenge, especially for smaller organisations or research groups with limited resources. These challenges highlight the urgent need for stronger regulatory frameworks, user-centric privacy safeguards and transparent data governance to build trust and ensure the ethical implementation of DP.

Building trust is critical for addressing privacy concerns of DP. Developers of DP tools must implement stringent privacy safeguards, including end-to-end encryption, secure data storage protocols and multi-factor authentication to protect user data. Further efforts must be made to develop decentralised machine learning techniques to protect user's data. Transparent and user-friendly consent processes are equally vital, empowering users to make informed decisions about their participation. Regular communication about data usage, anonymisation efforts and security measures can further reassure users about their data's safety. Ultimately, addressing privacy concerns is not just about compliance with legal standards; it is imperative to respect and protect the rights and autonomy of those who entrust their data to DP technologies. Building trust through stringent privacy safeguards, clear consent protocols, transparent AI and anonymisation techniques is essential.

Ethics must be a guiding principle in the development and deployment of DP technologies. The vast amounts of personal and sensitive data collected pose significant privacy risks. Transparent and user-centric data governance models are crucial to building trust. These models should include clear consent mechanisms, robust anonymisation protocols, and stringent data encryption standards. Using research-focused platforms such as Labfront bypasses commercial manufacturer servers entirely. Data from compatible devices (e.g. Garmin wearables) is transmitted directly to Labfront's secure cloud, enabling compliance with academic standards such as the Health Insurance Portability and Accountability Act (HIPAA) and Institutional Review Board (IRB) protocols. These design choices reduce exposure to commercial data pipelines and provide researchers with greater control over data governance and participant confidentiality.

---

### SPIRIT 2013 explanation and elaboration: guidance for protocols of clinical trials [^84307b22]. BMJ (2013). Excellent credibility.

Data management

Item 19: Plans for data entry, coding, security, and storage, including any related processes to promote data quality (eg, double data entry; range checks for data values). Reference to where details of data management procedures can be found, if not in the protocol

Explanation

Careful planning of data management with appropriate personnel can help to prevent flaws that compromise data validity. The protocol should provide a full description of the data entry and coding processes, along with measures to promote their quality, or provide key elements and a reference to where full information can be found. These details are particularly important for the primary outcome data. The protocol should also document data security measures to prevent unauthorised access to or loss of participant data, as well as plans for data storage (including timeframe) during and after the trial. This information facilitates an assessment of adherence to applicable standards and regulations.

Differences in data entry methods can affect the trial in terms of data accuracy, cost, and efficiency. For example, when compared with paper case report forms, electronic data capture can reduce the time required for data entry, query resolution, and database release by combining data entry with data collection (Item 18a). When data are collected on paper forms, data entry can be performed locally or at a central site. Local data entry can enable fast correction of missing or inaccurate data, while central data entry facilitates blinding (masking), standardisation, and training of a core group of data entry personnel.

Raw, non-numeric data are usually coded for ease of data storage, review, tabulation, and analysis. It is important to define standard coding practices to reduce errors and observer variation. When data entry and coding are performed by different individuals, it is particularly important that the personnel use unambiguous, standardised terminology and abbreviations to avoid misinterpretation.

As with data collection (Item 18a), standard processes are often implemented to improve the accuracy of data entry and coding. Common examples include double data entry; verification that the data are in the proper format (eg, integer) or within an expected range of values; and independent source document verification of a random subset of data to identify missing or apparently erroneous values. Though widely performed to detect data entry errors, the time and costs of independent double data entry from paper forms need to be weighed against the magnitude of reduction in error rates compared to single-data entry.

---

### Remote inspection of adversary-controlled environments [^f8abd2d6]. Nature Communications (2023). High credibility.

Introduction

Remotely monitoring valuable items in adversary-controlled environments constitutes an intricate problem. Traditional inspection and surveillance methods are not always possible to implement or may fall short of meeting stringent security and privacy requirements. It may be difficult and perhaps impossible to permit regular physical inspections or placing CCTV cameras in such secure environments to offer some level of confidence in the integrity of stored items. Agreed managed-access inspections leave open the possibility of inspectors gathering information. Providing confidence that relevant surveillance data are originating from the correct location and have not been pre-recorded could prove challenging when the environment is controlled by an adversarial party. In this context, specialized surveillance hardware and cryptographic tools are at risk of hacking and spoofing.

An example for this problem can be found in the monitoring of non-deployed strategic and tactical nuclear warheads as part of an arms-control agreement. These warheads represent 70% of the global nuclear arms stockpile and remain outside of existing agreements because of the difficulties to monitor them. They are stored in dedicated bunkers at sensitive military or nuclear sites. The presence and number of such weapons at any given site cannot be verified easily via satellite imagery or other national technical means that are unable to see into the storage vaults. To include them in future arms control initiatives, there is a need to develop technologies and protocols to reliably assess if weapons declared as being in storage are not removed.

Here we propose and demonstrate a new remote monitoring approach based on a radio-wave measurement system to generate fingerprints of a room and its content using an array of randomly oriented mirrors to verify that nothing changes over time. This approach only requires a single on-site visit to initialize the monitoring protocol to install the mirrors and take an initial imprint of the room. Our approach builds on the concepts of physical unclonable functions, (PUFs) and virtual proofs of reality, for which data authenticity, confidentiality, and integrity does not rely on digital keys and algorithms but on the inherent material complexity of physical systems to achieve privacy and security objectives. Our work shows that large-scale systems such as an entire room and its content can also be turned into physical unclonable functions.

---

### SPIRIT 2025 explanation and elaboration: updated guideline for protocols of randomised trials [^fa0148c0]. BMJ (2025). Excellent credibility.

Raw, non-numerical data are usually coded for ease of data storage, review, tabulation, and analysis. It is important to define standard coding practices to reduce errors and variation between observers. When data entry and coding are performed by different individuals, it is particularly important that the staff use unambiguous, standardised terminology and abbreviations to avoid misinterpretation.

Standard processes are often implemented to improve the accuracy of data entry and coding. Common examples include double data entry, verification that the data are in the proper format (eg, integer) or within an expected range of values, and independent verification of the source document of a random subset of data to identify missing or apparently erroneous values. Although independent double data entry is performed to detect data entry errors, the time and costs need to be weighed against the magnitude of reduction in error rates compared with single data entry.

For trials in which both participants and staff are blinded, it is important to plan the timing and procedures for unblinding the trial, such as after the creation of a cleaned and locked data file.

Among two samples of trial protocols approved in 2016, 64% and 75%, respectively, reported the data entry and coding processes. The protocol should fully describe the plans for data entry and processing, along with measures to promote their quality, or outline key elements with a reference to the data management plan where full information can be found. These details are particularly important for the primary outcome data. The protocol should also document data security measures to prevent unauthorised access to or loss of participant data, as well as plans for data storage (including time frame) during and after the trial. This information facilitates an assessment of adherence to applicable standards and regulations.

Summary of key elements to address

Processes for data management, including: data entry and coding, including measures to reduce errors (eg, double data entry, range checks for data values); data security; and data storage, including timeframe.
Reference to where full information can be found (eg, data management plan), if not in the protocol.

Item 27a: Statistical methods used to compare groups for primary and secondary outcomes, including harms

---

### How secure are your health devices-stopping wearables becoming a personal and national security risk? [^d6e13f0f]. NPJ Digital Medicine (2025). Medium credibility.

Technical advancements to secure the IoMT supply chain

Beyond regulation, technical safeguards are critical (Fig. 1). A secure foundation that can be implemented during the first stages of manufacturing, through an approach known as a Root of Trust. Here, hardware supply chain security is ensured through embedding a secure chip, such as an eSIM or a Trusted Platform Module (TPM), which is soldered onto the device's circuit board. This serves to protect devices by preventing unauthorized software changes through cryptographic checks. An eSIM provides secure storage of cryptographic credentials, primarily used for authenticating a device within a cellular or network environment, and facilitates secure cryptographic operations related to identity verification. However, it does not directly ensure system software integrity. In contrast, a TPM is specifically designed to enhance device integrity. TPMs enable devices to securely store cryptographic keys and perform integrity checks by recording cryptographic hashes (measurements) of each component loaded during the boot process. TPMs allow systems to attest remotely that they are running only authorized and correctly signed software by securely reporting these measurements after boot. Improving visibility into the device supply chain by using systems that track individual components and automatically detect risks is also critical.

Fig. 1
Threats and mitigations across the IoMT supply chain.

A selection of threats and mitigations in the IoMT with (a) showing possible avenues of attack emerging along a device's supply chain and (b) showing a set of possible mitigations.

A secure software architecture can be facilitated through the concept of "Secure by Design". In this approach, software subsystems are isolated according to the "Principle of Least Authority" to reduce the impact of vulnerabilities. For example, software managing patient data should not have unnecessary permissions, such as controlling network settings, to minimize harm if compromised.

Modular chips, called chiplets, can achieve similar isolation among components from different suppliers at the hardware level. Employing these security best practices in design and implementation, along with formal verification methods for security-critical components, is paramount.

Once deployed in a hospital or the HaH the security can be increased by adopting Zero Trust security models, where devices and users are continuously authenticated and network access is segmented. Employing these security best practices in design and implementation, along with formal verification methods for security-critical components, is paramount. In short, a holistic security architecture needs to be applied in the development and life cycle of an IoMT product.

---

### International charter of principles for sharing bio-specimens and data [^cfc11a03]. European Journal of Human Genetics (2015). Low credibility.

With regard to biobanks where the amount of samples may be limited, it is recommended that the samples are used only for studies reviewed by a competent and well-balanced data access committee so as to ensure good scientific quality. This is motivated primarily by ethical concerns for the protection of patient interests of reaping the fruits of their donated samples in terms of truly improved diagnosis and treatment opportunities. Data collections do not face scarcity but clinical quality registries are dependent on the trust of patients and therefore good communication processes are required to maintain this trust. Also, the submission of new data and reports using clinical registries that are of low scientific quality may jeopardize the trust and willingness of patients to consent and to continuously contribution data. To this end, caretakers of the clinical registry should also be granted a right to assess the quality of an application to acquire data for a study, as a parallel to an intellectual property right, because they have invested both personal intellectual and institutional resources in order to create the registry. They should be the rightful protectors of the integrity of the clinical registry in this sense. In practice, most networks of bio- and data repositories do have some kind of scientific evaluation committee to make these kinds of decisions, and we believe for good reason. The protocols of investigators who request access to clinical registries or biobanks should be vetted by a representative data access committee for scientific quality and the bona fides of applicants should also be authenticated, for example, through checking on their background and institution affiliation with the institution or university signing the MTA/DTA as co-responsible for the scientific integrity of the applicant.

The Charter briefly outlines the guiding principles for data and bio-specimen sharing together with the MTA/DTA that encloses these ethical principles in a template model. The Charter, developed within the framework of international research consortia projects, aims to provide a tool to be used for effective, ethically grounded sharing in the international context.

---

### Beyond traditional orthopaedic data analysis: AI, multimodal models and continuous monitoring [^5d9b9108]. Knee Surgery, Sports Traumatology, Arthroscopy (2025). Medium credibility.

TECHNICAL AND ETHICAL CHALLENGES

Data integration and standardisation

The complexity of data fusion in multimodal modes is underscored by three primary integration approaches: early, intermediate, and late fusion, each presenting unique technical challenges. Data fusion combines various data sources to enhance predictive accuracy and robustness in machine learning, though it also increases model complexity and reduces interpretability. Early fusion requires transforming heterogeneous data sources into a unified feature space, often involving complex mathematical techniques. These methods must carefully preserve the inherent informative characteristics of each data type while creating a cohesive representation. Intermediate fusion offers more flexibility, allowing stepwise model architectures that can selectively extract and combine distinguishing features from multiple modalities, creating more expressive representations than individual data sources. Late fusion, resembling ensemble learning, involves training separate models for each data type and then integrating their predictions, which introduces challenges in developing robust aggregation strategies and ensuring consistent performance across various model architectures. Each approach demands sophisticated computational techniques to handle the inherent variability in biological and clinical data sources, requiring advanced ML methods that can effectively capture complex, multidimensional relationships while mitigating potential information loss during the integration process.

Ethical and privacy concerns

Ethical and privacy challenges in multimodal health AI are increasingly complex, driven by the breadth and depth of data required for research. The integration of various health data sources introduces significant privacy risks, with potential for individual re‐identification even through de‐identified data sets. Existing regulatory frameworks like the US Health Insurance Portability and Accountability Act (HIPAA) and the European Union's General Data Protection Regulation (GDPR) provide foundational protections, but struggle to comprehensively address the nuanced privacy challenges of multimodal AI. To mitigate these risks, researchers have developed sophisticated technical solutions including differential privacy, federated learning, homomorphic encryption, and swarm learning. These approaches aim to enable collaborative model training while preserving individual data confidentiality, often through techniques like cryptographic data obfuscation, decentralised learning protocols, and blockchain‐enabled secure computation. Additionally, emerging technologies like edge computing offer promising avenues for enhancing data security by processing sensitive information closer to its source, thereby reducing transmission risks and maintaining patient confidentiality.

---

### Standards and guidelines for validating next-generation sequencing bioinformatics pipelines: a joint recommendation of the Association for Molecular Pathology and the college of American pathologists [^bd5a65e0]. The Journal of Molecular Diagnostics (2018). Medium credibility.

NGS bioinformatics validation guidelines — Recommendation 13 states that laboratories must include specific measures to ensure that each data file generated in the bioinformatics pipeline maintains its integrity and provides alerts for or prevents the use of data files that have been altered in an unauthorized or unintended manner. Computer security methods should be implemented to prevent unauthorized modification or deletion, and traceability features should be implemented to know which software (including specific version, parameters, and, if applicable, operator) generated or modified any given data file used in clinical reporting. Because transfers can silently fail and truncate files, laboratories must implement methods to ensure that files are transferred completely and with intact unmodified data; checking exact file sizes (in bytes) will detect some errors but is not sufficient as a data integrity check, and laboratories must use a hash/checksum method to detect file integrity and completeness. Scripts that automatically test transfers may not examine return codes; any scripts used in data analysis should check whether the entire file has been analyzed before returning data or moving to the next step of the pipeline, and laboratories must confirm their analytic accuracy and ability to detect and notify the end user about all of the errors previously described.

---

### Providing integrity, authenticity, and confidentiality for header and pixel data of DICOM images [^181ab466]. Journal of Digital Imaging (2015). Low credibility.

Exchange of medical images over public networks is subjected to different types of security threats. This has triggered persisting demands for secured telemedicine implementations that will provide confidentiality, authenticity, and integrity for the transmitted images. The medical image exchange standard (DICOM) offers mechanisms to provide confidentiality for the header data of the image but not for the pixel data. On the other hand, it offers mechanisms to achieve authenticity and integrity for the pixel data but not for the header data. In this paper, we propose a crypto-based algorithm that provides confidentially, authenticity, and integrity for the pixel data, as well as for the header data. This is achieved by applying strong cryptographic primitives utilizing internally generated security data, such as encryption keys, hashing codes, and digital signatures. The security data are generated internally from the header and the pixel data, thus a strong bond is established between the DICOM data and the corresponding security data. The proposed algorithm has been evaluated extensively using DICOM images of different modalities. Simulation experiments show that confidentiality, authenticity, and integrity have been achieved as reflected by the results we obtained for normalized correlation, entropy, PSNR, histogram analysis, and robustness.

---

### Computerized system for staging peritoneal surface malignancies [^2db3447d]. Annals of Surgical Oncology (2016). Low credibility.

Web Application Technical Specifications

The PSM staging system is a web-based application accessible through a network that uses a standard web browser as the user terminal and therefore runs on traditional network protocols. To ensure fully secure data management, a data encryption protocol allows safe communication from the source to the recipient (end-to-end) in transfer control protocol/internet protocol (TCP/IP) networks supplying authentication, data integrity, and full data transport-level encryption. The PSM staging system is written in a cross-platform language (HTML5), with the user's interface code specified in remote. This solution substantially affects functioning; the major advantage is that the web application has no influence whatsoever on the device's memory capacity or on its ability to calculate data, given that the core processor and user's interface are on a remote server. Through their supplied password, authenticated users, once connected to the system, will be able to input data for new patients or update their pre-existing data. Using a simple drag-and-drop system, the user will be able to indicate the extent, site, and number of malignant implants present at diagnosis and therapy, or eventually as residual lesions after surgery, positioning them within the anatomic model, and the system will record all the data inserted on cloud. All data inserted can be visualized and compared, and also exported as a PDF document in a completely anonymous manner. No recorded information will in any way be traceable back to the patient. The system will be optimized for use on a tablet (including iPad, Galaxy Tab, Xperia, and Nexus 9) or computer [personal computer (PC), Apple], and updated to run on all the latest-generation web browsers.

---

### Seamless EMR data access: integrated governance, digital health and the OMOP-CDM [^17c70d3c]. BMJ Health & Care Informatics (2024). High credibility.

OMOP, data governance, ethical review and consent

OMOP-CDM and governance

By virtue of its design and objectives, the OMOP-CDM enhances the governance of secondary health data, by ensuring data utilisation in both research and healthcare decision-making is ethical, transparent and effective.

With the transformation of EMR data into a standardised structure, the OMOP-CDM ensures there is a uniform representation of these data regardless of the data's original source. This uniformity streamlines data governance and, importantly, eases the complexities associated with conducting single site studies that contain native EMR data (raw and/or curated), and multisite studies that involve integrating data from various disparate sources. In addition, the common data model emphasises data quality, allowing for consistent checks and ensuring that research data meets the highest standards. The standardised model also ensures that security and privacy protocols are uniformly applied, safeguarding secondary health data from data breaches to maintain patient privacy. Given the structured approach of the OMOP-CDM, an institution can easily implement access controls, thereby ensuring that only authorised parties can access or interact with the data. As a result, the OMOP-CDM acts as a cornerstone for the conduction of rigorous and ethically sound research as it builds trust among stakeholders, mitigates information disparities and encourages the production of high-quality medical evidence for rigorous and ethical research

Operational use and quality assurance activities in a hospital or healthcare setting

For operational use quality assurance activities where the 'primary purpose is to monitor or improve the quality of service delivered by an individual or an organisation'data governance and principles for ethical use apply. However, within healthcare institutions, ethics approval is not mandated for the establishment of the OMOP database or data use, provided:

The data being collected and analysed, is coincidental to standard operating procedures with standard equipment and/or protocols.
The data are being collected and analysed expressly for the purpose of, maintaining standards or identifying areas for improvement in the environment from which the data were obtained.
The data being collected and analysed, is not linked to individuals.
None of the triggers for consideration of ethical review are present.

---

### Security of electronic medical information and patient privacy: what you need to know [^e3aa915f]. Journal of the American College of Radiology (2014). Low credibility.

The responsibility that physicians have to protect their patients from harm extends to protecting the privacy and confidentiality of patient health information including that contained within radiological images. The intent of HIPAA and subsequent HIPAA Privacy and Security Rules is to keep patients' private information confidential while allowing providers access to and maintaining the integrity of relevant information needed to provide care. Failure to comply with electronic protected health information (ePHI) regulations could result in financial or criminal penalties or both. Protected health information refers to anything that can reasonably be used to identify a patient (eg, name, age, date of birth, social security number, radiology examination accession number). The basic tools and techniques used to maintain medical information security and patient privacy described in this article include physical safeguards such as computer device isolation and data backup, technical safeguards such as firewalls and secure transmission modes, and administrative safeguards including documentation of security policies, training of staff, and audit tracking through system logs. Other important concepts related to privacy and security are explained, including user authentication, authorization, availability, confidentiality, data integrity, and nonrepudiation. Patient privacy and security of medical information are critical elements in today's electronic health care environment. Radiology has led the way in adopting digital systems to make possible the availability of medical information anywhere anytime, and in identifying and working to eliminate any risks to patients.

---

### Embracing generative artificial intelligence in clinical research and Beyond: opportunities, challenges, and solutions [^16e59698]. JACC: Advances (2025). Medium credibility.

To explore threats and opportunities and to chart a path for safely navigating the rapid changes that generative artificial intelligence (AI) will bring to clinical research, the Duke Clinical Research Institute convened a multidisciplinary think tank in January 2024. Leading experts from academia, industry, nonprofits, and government agencies highlighted the potential opportunities of generative AI in automation of documentation, strengthening of participant and community engagement, and improvement of trial accuracy and efficiency. Challenges include technical hurdles, ethical dilemmas, and regulatory uncertainties. Success is expected to require establishing rigorous data management and security protocols, fostering integrity and trust among stakeholders, and sharing information about the safety and effectiveness of AI applications. Meeting insights point towards a future where, through collaboration and transparency, generative AI will help to shorten the translational pipeline and increase the inclusivity and equitability of clinical research.

---

### Internet-delivered cognitive-behaviour therapy for anxiety related to asthma: study protocol for a randomised controlled trial [^9ef34e60]. BMJ Open Respiratory Research (2024). High credibility.

Dissemination

Internet-based CBT differs from traditional CBT mainly through how the therapist's contact is managed. Any serious adverse effects of this type of treatment have not been reported in the literature, nor can we predict any obvious risks due to participation in the treatment or assessment procedure. To ensure that this is the case also in this study, data on any adverse events will be collected. Since the participant is in regular contact with their therapist, there are very good opportunities to detect acute problems that may arise. Normal patient injury compensation applies to study participants in this study. The communication between participant and therapist is protected by secure cryptographic certificates and all authentication is protected by using both username/password and a temporary password sent by SMS (double authentication). All participants will be informed about and give their consent for digital storage of personal data in accordance with General Data Protection Regulation (GDPR; see online supplemental file 4 for the informed consent). The results of the study will be analysed at the group level and reported through publication in a peer-reviewed scientific journal within the field. Authorship will follow the International Committee of Medical Journal Editords (ICJME) 2018 guidelines. Due to Swedish data storage laws, we cannot make the data publicly available. However, pseudonymised data may be provided by the PI on requests if providing a reasonable proposal and if an appropriate data sharing agreement with Karolinska Institutet can be established.

---

### Concealable physical unclonable functions using vertical NAND flash memory [^e37c2927]. Nature Communications (2025). High credibility.

Introduction

The explosive growth of the Internet of Things (IoT) is connecting numerous network devices in everyday life, collecting sensitive information such as basic secret data like passwords, as well as health information and authentication tokens for financial transactions –. While the development of IoT has brought significant convenience, concerns about personal information leakage have grown, increasing the need for hardware security to protect data from hacking, cyberattacks, and counterfeiting. Traditional hardware security primarily relies on storing secret keys in Non-Volatile Memory (NVM), with the key serving as the most critical and fundamental piece of information for constructing hardware security protocols. While the processes and methods for constructing security protocols are public, the key must remain secret, as it becomes the sole means to compromise security, underscoring the paramount importance of concealing the key. However, the non-volatile nature of NVM, which retains information even when powered off, makes key security vulnerable and hinders preventing key duplication. Moreover, side-channel attacks that exploit radiation emitted or power consumption from NVM storing the key can also provide access to the key. These security weaknesses of storing keys in NVM highlight the need for more robust hardware security mechanisms.

One emerging alternative to enhance hardware security is the Physical Unclonable Function (PUF). PUF is considered analogous to human fingerprints as it generates unique and unpredictable keys, earning it the name "device fingerprint, ". The entropy required to produce random PUF keys primarily originates from the random physical properties, process variations, and minor variations introduced during the fabrication process. Since random sources are naturally introduced during the fabrication process, PUFs incur minimal additional cost while offering a critical advantage that even the manufacturer cannot easily predict or replicate –. Another distinguishing feature of PUFs compared to fixed security keys like passwords is their interactive nature, where different outputs are generated based on the input. The input is referred to as a "challenge", and the corresponding output is called a "response", together forming a Challenge-Response Pair (CRP). The security approach using CRP generation strengthens security because the output is different from the input, and different inputs yield different outputs.

---

### Security issues for mobile medical imaging: a primer [^4931b535]. Radiographics (2015). Low credibility.

The end-user of mobile device apps in the practice of clinical radiology should be aware of security measures that prevent unauthorized use of the device, including passcode policies, methods for dealing with failed login attempts, network manager-controllable passcode enforcement, and passcode enforcement for the protection of the mobile device itself. Protection of patient data must be in place that complies with the Health Insurance Portability and Accountability Act and U.S. Federal Information Processing Standards. Device security measures for data protection include methods for locally stored data encryption, hardware encryption, and the ability to locally and remotely clear data from the device. As these devices transfer information over both local wireless networks and public cell phone networks, wireless network security protocols, including wired equivalent privacy and Wi-Fi protected access, are important components in the chain of security. Specific virtual private network protocols, Secure Sockets Layer and related protocols (especially in the setting of hypertext transfer protocols), native apps, virtual desktops, and nonmedical commercial off-the-shelf apps require consideration in the transmission of medical data over both private and public networks. Enterprise security and management of both personal and enterprise mobile devices are discussed. Finally, specific standards for hardware and software platform security, including prevention of hardware tampering, protection from malicious software, and application authentication methods, are vital components in establishing a secure platform for the use of mobile devices in the medical field.

---

### Registered access: a' triple-A' approach [^2877ddea]. European Journal of Human Genetics (2016). Low credibility.

We propose a standard model for a novel data access tier - registered access - to facilitate access to data that cannot be published in open access archives owing to ethical and legal risk. Based on an analysis of applicable research ethics and other legal and administrative frameworks, we discuss the general characteristics of this Registered Access Model, which would comprise a three-stage approval process: Authentication, Attestation and Authorization. We are piloting registered access with the Demonstration Projects of the Global Alliance for Genomics and Health for which it may provide a suitable mechanism for access to certain data types and to different types of data users.

---

### Guidelines for the practice of telepsychology [^4a82b461]. The American Psychologist (2013). Medium credibility.

Telepsychology — data security policies, vendor controls, and breach reporting — psychologists are encouraged to clearly address which telecommunication technologies are used and the purpose of the communication, and are cognizant that preserving the actual communication may be preferable to summarizing it, depending on the type of technology used. As part of their data security policies and procedures, psychologists seek to use encryption technology and robust security or multi-factor authentication controls for devices and for access to software or relevant websites, and psychologists strive to ensure that they have signed business associate agreements (BAA) in place with any third-party vendors as appropriate, including technology vendors, documenting their compliance with and adherence to the regulations regarding data security; this is especially relevant for tools with automated and/or artificial intelligence (AI) facilitated features. Psychologists are encouraged to be aware of what data is stored on Internet-based tools (e.g., practice management, electronic health records), and to ensure that those tools are HIPAA compliant, and psychologists are encouraged to review the data management and retention policies of any third-party technology vendors or other business associates regarding patient data before beginning to use a product or service and also on a periodic basis, because over time vendors may change terms of service. If there is a breach of unsecured electronically communicated or maintained data, psychologists seek to notify their patients/clients/service recipients and other appropriate individuals/organizations consistent with federal, state, provincial, territorial, and other organizational reporting requirements, are encouraged to understand what types of reporting are made on behalf of the psychologist to individuals affected by a data breach as outlined in the BAA executed with the third-party organization, and are encouraged to undertake due diligence to ensure comprehensive reporting in line with ethical and legal guidelines, regardless of the terms and conditions of the BAA. In addition, they are encouraged to make their best efforts to keep secure back-up versions of electronic data, such as through encrypted cloud-based storage, network drives, external devices, etc., and an organization may appoint a data security officer to facilitate and control access, oversee required data security-related training of employees, and mitigate potential risks.

---

### Driving digital transformation of comprehensive primary health services at scale in India: an enterprise architecture framework [^9a5e34b6]. BMJ Global Health (2021). High credibility.

Plug-and-play model to allow configurability

A federated enterprise architecture allows a vast network of loosely coupled, semiautonomous entities to exchange information in a standardised secure manner. It fosters innovation by allowing easy development of apps on the platform and allows for modular expansion by its adherence to standards. A variety of patient apps can be developed that allow the individual to have a comprehensive view of her health record and track her visits, her wellness metrics and care regimen.

Dynamic configurability in TeCHO+ has ensured that the system is always primed for changes in policy or evidence-based protocol updates. Well-defined interconnections and dynamic configurability between systems enable a 'plug and play' approach for innovation, rapid development and ease of customisation. For example, AMRIT can easily plug in various point-of-care devices to help collect clinical data points at various health settings (static or mobile). Using these, front-line worker systems can connect to health facilities and make quick decisions using decision support systems, saving time in the urgent referral for the high-risk pregnant woman needing to see a specialist right away. In Digital LifeCare, a clinical decision support system for hypertension and diabetes has been plugged into the platform through standardised interfaces for use in one state while other states continue to use existing government protocols.

Open-source stack

An open-source platform leads to stronger community participation for ecosystem innovations and sustainability. In all three platforms, the confidence in the platforms and willingness to adopt them were significantly higher as they were built on open-source technologies and were themselves public good open-source systems.

Authenticated, secure data exchange with privacy by design

According to the National Resource Centre for EHR Standards, the government (as a health provider) is only a custodian of health data while ownership lies with the individual. Digital LifeCare has a four-pronged security strategy as shown in figure 3.

Figure 3
Security and privacy layer.

---

### Institutional review boards, professionalism, and the internet [^df1008b2]. Science Translational Medicine (2010). Low credibility.

Even as the Internet generates pressures that erode professional authorities of all kinds, it also provides opportunities for researchers and their institutional review boards to bolster their status as trusted sources. To this end, we must work to improve clinical protocol design and approval procedures and maintain the integrity of the study participant recruitment process in clinical trials.

---

### Data management and sharing policy: the first step towards promoting data sharing [^f4c14985]. BMC Medicine (2019). Medium credibility.

Data management for data sharing

One of the basic requirements of effective data sharing is assurance that the data and reported results are credible and accurate, and that the rights, integrity and confidentiality of research participants are protected. Good data management ensures that high quality and credible datasets are produced. Data management refers to activities undertaken to organise and handle data throughout the study period – from study design and data collection, through to the dissemination of results, data sharing and archiving.

The data management and sharing policy should outline institutional requirements with regards to processes for collection, curation, storage and sharing of data, as well as provision of guidance related to study-specific data management and sharing plans. A study-specific data management and sharing plan details the procedures for a specific study, and is required by most biomedical research funders. Consideration of data sharing at the outset of a research project ensures that sufficient resources are allocated to data management activities; this entails quantifying costs of items such as computer hardware, software, staff, personnel training and data archiving. The policy should also include the institutional requirements for data repositories as well as how to select suitable repositories. In addition, the policy should address the requirements for metadata such as the study protocol, annotated study case report forms, case report form completion guidelines and the data dictionary.

To implement data management for sharing, institutions should invest in building data management capacity through training of researchers and data support personnel, as well as through acquiring the necessary IT infrastructure, including servers and networks. Research teams should be aware of how electronic datasets are processed and converted into shareable datasets, and how datasets can be stored securely during the study period and beyond. This requires knowledge of principles of data coding and data formats, knowledge of methods for securing data from malicious harm, unintentional harm and from disasters, an understanding of methods for deidentification of data, and knowledge of available data management tools and software.

---

### CDC program evaluation framework, 2024 [^6527b15f]. MMWR: Recommendations and Reports (2024). Medium credibility.

A key decision point related to data sources is whether information needed to answer evaluation questions will be collected from all units of a specific source (e.g. all recipient reports submitted) or a subset (e.g. random sample of all recipient reports). If sampling is needed, the criteria used and rationale for the sampling strategy should be stated clearly to provide information that interest holders can use to interpret the evidence accurately and assess potential biases.

Data quantity and quality. When collecting data, consider the quantity needed. Collecting the appropriate amount and types of data to answer the evaluation questions sufficiently (i.e. need to know) is important, as is avoiding the desire to collect data that might be tangential to answering evaluation questions (i.e. nice to know). Balancing the amount of data with the burden (in terms of time commitment and effort) data collection can place on the respondents and others who might be involved in data collection and processing can be challenging. Collaboratively engaging with communities can help ensure the right balance is struck for the specific context.

Data quality refers to the appropriateness and integrity of the data used in an evaluation. High-quality data are reliable, valid, authentic, and informative for their intended use. Well-defined indicators enable easier collection of quality data because they clarify what specific data are viewed as credible and necessary to answer the evaluation questions. Other factors affecting quality include instrument design, data collection procedures, training of data collectors, source selection, coding, data management, data cleaning, and error checking. Obtaining quality data will entail tradeoffs (e.g. breadth versus depth), and discussing the options with interest holders when planning the evaluation can highlight how certain tradeoffs might affect perceived data credibility.

Data collection and context considerations. The timing and infrastructure for collecting, handling, and storing data, and the cultural context need to be considered when making decisions regarding gathering data. Persons providing data should be knowledgeable about their rights; any associated risks; and how the data will be handled, stored, and used, including how privacy and confidentiality will be protected in the process.

---

### Practical implementation of AI in a non-academic, non-commercial pathology laboratory: real world experience and lessons learned [^6193933d]. Histopathology (2025). Medium credibility.

Infrastructure andSecurity

A summary of the whole technical implementation is shown in Figure 1.

Figure 1
Technical flow of the AI integration. IMS, image management system; LIS, lab information system; POI, points of interest; WSI, whole slide image.

Before implementation, an IT infrastructure and security framework is needed. Two primary deployment models can be considered: (1) cloud‐based and (2) on‐premises installations, the latter meaning an installation on servers in a physical lab or hospital environment. The relevant differences between the two options are shown in Table S1. The choice between cloud and on‐premises AI solutions should factor in operational needs, budget constraints, regulatory requirements, and the lab's long‐term strategic goals. Some AI vendors offer a choice between cloud installation and on‐premises installation; however, most vendors offer only a cloud‐based solution. In our experience, on‐premises installation comes with high extra costs. This can create a bottleneck in the workflow when multiple AI analyses are used simultaneously.

To connect to the cloud, the following three methods have been proposed by one or more AI vendors, with each one requiring specific security measures. These were approved by the ICT data security officer of the hospital. The following list, in order of preferred solutions, provides a summary of possible cloud connections:
VPN tunnel: this is a secure, encrypted connection between a dedicated server in the internal network — in our specific IT configuration named 'AI‐flow‐server' — and a remote server that enables secure data transmission over the Internet. A group of security protocols is used to establish and run this VPN connection where this group ensures authentication data encryption and security association by running on top of the Internet Protocol (IP). It creates a virtual private network that encapsulates data packets, thereby ensuring confidentiality and integrity. The use of this 'AI‐flow' server is discussed further in the section about the IMS integration.
IP whitelisting over the public Internet: this system restricts access to systems based on approved IP addresses, thereby enhancing security. Additional measures to secure data transfer are necessary for comprehensive protection.
Specific software of the AI vendor: AI vendors often require on‐premises software to connect with their cloud environment, necessitating an extra dedicated server and increasing implementation costs. This solution requires additional security measures to secure the data transfer.

---

### Improving authenticity and provenance in digital biomarkers: the case for digital watermarking [^92de67dc]. NPJ Digital Medicine (2025). Medium credibility.

Harmonization with existing principles, standards, and ecosystem

Digital watermarking must not only align with foundational security, privacy and research standards like the Health Insurance Portability and Accountability Act, Digital Object Identifier, and FAIR Principles but should also actively support health systems in maintaining compliance with these frameworks by creating an embedded auditable trail of patient data access and modifications.

Watermarking can ensure traceability and authenticity but this depends on the context of use. In closed systems, it may primarily safeguard against tampering and unauthorized distribution rather than providing transparency to patients or stakeholders. For publicly shared datasets, watermarking verifies provenance and detects manipulation, proving particularly useful in secondary applications like AI model training or research. To achieve greater transparency in situations where data usage occurs behind closed doors, watermarking should be paired with complementary tools like audit logs, regulatory oversight, and lineage reporting mechanisms. Together, these measures provide a comprehensive framework for safeguarding data integrity, ensuring compliance with standards, and fostering trust by making data lineage more transparent to both patients and stakeholders.

---

### Web services and cloud computing in pediatric care [^fbe0577d]. Pediatrics (2021). High credibility.

Protected environment — authentication scope and sandboxing for EHR Web services are outlined: Currently, many EHR systems authenticate users using a username, password, and location-based authentication, and EHR vendors are to determine when a Web service is accessing user data, how the Web service will be authenticated, and what access and permissions are allowed. Ideally, when Web services are authenticated, they would not authenticate through an existing user profile with potentially broad permissions, but instead, in a way that the service would have limited or no access to protected health information. Because Web services may represent a security risk if an application is redirected to a different service with ill intent or if the service is not robust and/or mishandles data, the calling application must be able to manage situations in which the service attempts unauthorized actions, corrupts data, or returns invalid results, and EHR applications should provide a limited and protected environment (a sandbox) so the service cannot cause harm resulting in data breaches, impermissible access, or data corruption.

---

### Open data sharing: what could possibly go wrong? [^38e58012]. Pain (2025). Medium credibility.

Different analytical approaches legitimately yield varying results from identical datasets. This acceptable variation differs fundamentally from analyses violating core design principles. Better documentation and standards, not access restrictions, offer solutions. Studies show improved documentation and structured environments reduced misinterpretation by 35% while maintaining utility.

Addressing these challenges demands multiple approaches. Community standards must expand beyond basic codebooks and curation. NIH funding should support technical and clinical guides capturing trial complexity. Data dictionaries must evolve into comprehensive documentation including design constraints, analytical boundaries, and key assumptions. Current guidelines like CONSORT, SPIRIT, and CDISC provide valuable frameworks, yet none fully addresses preparing trial data for sophisticated secondary use. FAIR principles offer high-level guidance, but implementation requires expertise and resources many research teams lack.

Open science and data sharing in clinical trials present opportunities and challenges. Sensitive health data shared with proper protocols show reidentification risks as low as 1.2% after anonymization. These technical protections, however, are only 1 aspect of data governance. The administrative aspects also present challenges. Manual data access committees have proven ineffective, with evaluations of clinical data warehouses revealing inconsistent governance and only 8% having transparent approval criteria. Furthermore, "data available on request" policies result in less than 20% actual sharing rates due to administrative bottlenecks. Although manual oversight shows clear limitations, structured governance approaches like federated analysis offer more promising solutions.

---

### The inborn errors of immunity-virtual consultation system platform in service for the Italian primary immunodeficiency network: results from the validation phase [^bf8d6652]. Journal of Clinical Immunology (2024). Medium credibility.

Ethical Compliance

The IEI-VCS platform has been created by the not-for-profit Consortium CINECA that is made up of 67 Italian Universities, 9 Italian Research Institutions, 1 Polyclinic, and the Italian Ministry of Education. CINECA infrastructure is certified for quality and security procedures: the ISO 9001:2008 quality certification for "Design, development, creation, and distribution of services and systems in the field of Information and Communication Technology", and the ISO 21001:2013 security certification for "Analysis, design, development, operation and maintenance of Decision Support Systems and Information Systems Infrastructures for management, monitor, and analysis of clinical trials and epidemiological registries for health services organizations". CINECA is compliant with Computer System Validation guidelines and owns a service named Conserva for digital preservation. In addition, CINECA provides identification and access tracking with Audit Trail (compliant with Italian SPID), that allows accounting data to be traced to its source, a disaster recovery system, that enables the resumption of data, and that guarantees the anonymization and pseudonymization of personal data. The access is restricted to Centers that have been accredited by IPINet. The service allows the user to run the access to the platform anytime, anywhere from any web-enabled devices (Windows, Mac), including mobile devices and tablets; no installation is needed. Through HTTP and SSL protocols and thanks to the access limited via username and password, high levels of security and confidentiality of information are guaranteed. The platform offers a secure, encrypted data transfer channel that protects all sensitive information in transit and at rest. The service provides comply with the new European regulation on the protection of privacy (n. 679/2016). The system is aimed to the IT integration with the most widespread protocols and standards used in the field of authentication and authorization (LDAP, CAS, Shibboleth, etc.). It is also possible to federate the authentication system with other authentication systems that support the SAMLv2 standard. File transfer is available from Electronic Health Records or other Healthcare Systems (HL7 and IHE health interoperability protocols). Data exchange is allowed in the most common formats (Excel, etc.), throughout automatic data loading procedures. All data were collected and shared in anonymized form, in accordance with GDPR and/or local privacy policies. In respect of anonymity, the patient is identified only by the initials; the only sensitive data reported in the system is the date of birth. Informed consent for data collection and sharing in the IEI-VCS platform has been obtained in written form from each patient or his/her legal guardians by the treating physician. A local ethical committee approved the study.

---

### Technical requirements and optimization strategies for home-based teleradiology workstations: a review article [^c445bbd9]. Insights Into Imaging (2025). Medium credibility.

Network security architectures

Security requirements for teleradiology connections must strike a careful balance between robust protection of patient data and operational efficiency to support timely clinical access. Modern security architectures employ a layered, defense-in-depth approach, leveraging multiple technologies rather than relying on a single point of protection. Virtual Private Network (VPN) solutions, including internet protocol security (IPsec), secure sockets layer/transport layer security, and datagram transport layer security (DTLS) protocols, create encrypted tunnels between teleradiology workstations and institutional systems, safeguarding data in transit across public or unsecured networks.

Authentication frameworks have evolved significantly, moving beyond basic password-based systems to more advanced multi-factor authentication (MFA) models. These frameworks incorporate combinations of knowledge factors (e.g. passwords, PINs), possession factors (e.g. smart cards, hardware tokens, registered mobile devices), and biometric factors (e.g. fingerprint scans, facial recognition, behavioral biometrics). The goal is to enhance security without impairing usability, acknowledging that overly burdensome authentication processes may encourage workarounds that ultimately compromise system integrity.

---

### A 10-year update to the principles for clinical trial data sharing by pharmaceutical companies: perspectives based on a decade of literature and policies [^cb8233ab]. BMC Medicine (2023). Medium credibility.

Decisions on the legitimacy of independent data requests, including the hypotheses tested, the research rationale, the analysis plan, the publication plan, and the qualifications of the research team, should be made by independent scientific review panels. To facilitate these review processes, it is important to establish mechanisms that provide training to independent individuals, enabling them to develop a deep understanding of the technical, legal, and scientific aspects required to assess data requests. The objective is to establish a pool of independent reviewers, enabling pharmaceutical companies to limit their role to simply determining the sharing eligibility of the requested participant-level data. Towards this, pharmaceutical companies should be aiming to maintain up-to-date, publicly accessible registers documenting the sharing eligibility of their clinical trials. This should include a specific indication of clinical trials that are ineligible, along with clear reasons outlining why and when trials will become eligible. Among the various reasons for ineligibility, consent form issues have been identified as a major concern. To this issue, company web pages should provide clear information on updated consenting procedures, along with consent form examples.

Data protection and security must be a top priority for all parties, including the requestor. Participant-level data sharing typically takes place on platforms requiring rigorous assessment of the requesting teams' qualifications. Researchers often obtain access to data in a secure, password-protected research environment from which data cannot be downloaded locally. The procedures for anonymising data should align with the level of protection required. Procedures that redact key information (such as survival and adverse event data) for secondary research should be evaluated for appropriateness and necessity. Furthermore, to facilitate the valid use of participant-level data, companies should enhance the findability and accessibility of clinical study reports, annotated case report forms, data dictionaries, data derivation documents, protocols, statistical analysis plans, and anonymisation guides. Such transparency, as highlighted by the FAIR (Findable, Accessible, Interoperable, and Reusable) data principles, is essential for enabling independent researchers to create detailed data requests and verify their data preparation processes when undertaking participant-level data analyses.

Independent researchers should also be committed to publishing their analyses, sharing code for reproducibility, maintaining data confidentiality, not disclosing data to unauthorised parties, and not attempting to re-identify study participants. Acknowledgements to data contributors and original investigators should be made in all secondary data use publications, and researchers should recognise that original investigator contributions may warrant authorship on new work.

---

### Transforming Chinese cohort studies through artificial intelligence: a new era of population health research [^92645d66]. BMJ (2025). Excellent credibility.

Building robust data infrastructure for AI powered cohort studies: progress and challenges

A robust research data infrastructure forms the cornerstone of effective cohort research by providing systematic frameworks for data organisation, integration, and management across diverse sources. China's recent data infrastructure developments are intended to meet these methodological requirements. The establishment of the National Data Bureau in 2023 and the subsequent National Data Governance and Integration Strategy (2024–26)have created a comprehensive framework for integration of healthcare data. These, in principle, facilitate the development of cohort studies. For cohort methodology, this infrastructure enables standardised pre-processing, advanced analytics, and quality control protocols for longitudinal data analysis. For example, Zhou and colleagues showed the practical application of this approach by developing an automated data collection tool for real world cohort studies of chronic hepatitis B by using optical character recognition and natural language processing technologies. When tested on more than 4000 patient records from two hospitals, the system maintained accuracy comparable to manual collection (98.66%) while reducing processing time from 63.64 to just 3.57 minutes per patient — a 17-fold improvement. The tool integrates with REDCap, features data security measures, and has been open sourced to support large scale, multicentre hepatitis B studies. This innovation of data infrastructure improved the efficiency and accuracy of data collection across diverse healthcare settings, offering a technological solution to overcome China's fragmented healthcare data landscape and, in theory, can enhance cohort research representativeness.

---

### Source-device-independent heterodyne-based quantum random number generator at 17 gbps [^4f982b4f]. Nature Communications (2018). Medium credibility.

Random numbers are commonly used in many different fields, ranging from simulations in fundamental science to security applications. In some critical cases, as Bell's tests and cryptography, the random numbers are required to be both private and to be provided at an ultra-fast rate. However, practical generators are usually considered trusted, but their security can be compromised in case of imperfections or malicious external actions. In this work we introduce an efficient protocol which guarantees security and speed in the generation. We propose a source-device-independent protocol based on generic Positive Operator Valued Measurements and then we specialize the result to heterodyne measurements. Furthermore, we experimentally implemented the protocol, reaching a secure generation rate of 17.42Gbit/s, without the need of an initial source of randomness. The security of the protocol has been proven for general attacks in the finite key scenario.

---

### Bridging research integrity and global health epidemiology (BRIDGE) guidelines: explanation and elaboration [^b8956cd5]. BMJ Global Health (2020). High credibility.

4.6 For each data file define levels of anonymisation and privacy protection as well as corresponding access rights in line with national and international frameworks

Data security measures should be made explicitly clear for each stage of the research process in line with national and international frameworks — such as the General Data Protection Regulation in the EU. Personal data, and especially sensitive personal data should be treated with extreme caution. Personal identifiers can be either direct or indirect. Although none of the indirect identifiers on its own would point to an individual, several indirect identifiers might do. The appropriately anonymised data have: (1) no single direct identifier or less than three indirect identifiers and (2) if dates are necessary for certain analyses, methods should be used to preserve anonymity without compromising statistical analyses, such as adding or subtracting a small, randomly chosen number of days to all dates.

It should be clear at the start of the research, which research team members will have access to which data and how access will be managed (different team members might have different access rights). There are numerous ways to protect sensitive personal data. One method is by saving the personal identification data in a dataset that is separate from the bulk of the study data and only providing the ability to certain research team members to link the two datasets. This can be done by providing different passwords for different datasets or encrypting electronic database files. In the event the data are collected in a paper format, securely stored data forms (in locked cabinets with password access-locks or key-locks to which only specific research members have access) is the best way to keep the data secure.

---

### Digital twins for health: a scoping review [^e45b2b78]. NPJ Digital Medicine (2024). Medium credibility.

Recommendations

Addressing these challenges requires a multidisciplinary approach involving collaboration between healthcare professionals, data scientists, technologists, policymakers, patients, and patient advocates to establish robust frameworks, standards, and guidelines for the development, implementation, and utilization of digital twins in healthcare.

First, governance structures must be in place to safeguard the rights of individuals that have DTs, support data security and privacy, and foster both transparency and fairness in the usage of data at a societal level. Compliance with regulatory frameworks, ethical guidelines, and ensuring transparency and accountability are critical for the wide applications of DTs in healthcare.

Second, healthcare systems often have diverse data sources and formats, making it challenging to integrate and synchronize data from multiple sources into a cohesive DT. Data interoperability standards and protocols must be established to enable seamless data exchange and integration across different healthcare systems, devices, and platforms.

Another recommendation is to set up the infrastructure for DT implementation, which requires significant computational resources and infrastructure to handle the increasing volume, velocity, and complexity of data. Healthcare organizations need to invest in scalable and reliable computing infrastructure to support the growing demands of DT applications.

---

### [DOC] health… [^8b391bd4]. ncvhs.hhs.gov (2025). Medium credibility.

The Challenge of Defining Health Information If you want to discuss health data, you need to know what it is. If you want to regulate it or the institutions that have health data, you need to be able to draw clear lines. If you want to address recommendations to health data holders, you also need to be adequately descriptive. Defining health information in a broad context presents major difficulties, but not for HIPAA. HIPAA does a fine job in drawing clear lines. The real difficulties arise when you move beyond HIPAA. HIPAA defines health information by using a series of nested definitions. HIPAA ties these definitions to the covered entities regulated under the rules. The result is that HIPAA effectively covers all identifiable information related to health care treatment or to payment for the provision of health care held by a covered entity.

There is no need to pick and choose among items of data to make a decision about what is and is not PHI. In practical terms, all identifiable data processed by a HIPAA covered entity is PHI subject to the HIPAA rules. Issues relating to identifiability, many already addressed recently by NCVHS, are not of immediate concern to this discussion. By following the data, the complexity of the definitional task beyond HIPAA becomes more apparent. The mother's activities produce some HIPAA-covered data, some data protected by other laws, and some unregulated data. This discussion is not fully complete, but it makes the point. Federal website: A federal informational website is not likely to collect or retain any identifiable information from a search. If the website maintained any personal information, the data would likely, but not certainly, fall under the Privacy Act of 1974 whether characterized as health information or not. Canadian website.

A private sector website in Canada is subject to the Personal Information Protection and Electronic Documents Act, a general privacy law regulating the private sector. If the website collected any personal information, the information would be subject to the Canadian privacy law. Commercial American medical information website. American websites, whether they collected health information or otherwise, are generally not subject to a federal privacy law. A website can be held to any privacy policy posted on its site. The website can collect, retain, make use of, and share in various ways any information revealed by a search made by a visitor.

---

### Informatics in radiology: integration of the medical imaging resource center into a teaching hospital network to allow single sign-on access [^d91721e8]. Radiographics (2009). Low credibility.

The RSNA Medical Imaging Resource Center (MIRC) software is an open-source program that allows users to identify, index, and retrieve images, teaching files, and other radiologic data that share a common underlying structure. The software is being continually improved as new challenges and different needs become apparent. Although version T30 is easily installed on a stand-alone computer, its implementation at healthcare enterprises with complex network architecture may be challenging with respect to security because users cannot log on by using a standard enterprise-wide authentication protocol. Instead, authentication takes place through the local MIRC database, creating security concerns and potential organizational problems. In this setting, the Lightweight Directory Access Protocol (LDAP) can be used to provide a single sign-on environment and increase authentication security. A commercial directory service using LDAP has been successfully integrated with MIRC in a large multifacility enterprise to provide single sign-on capability compatible with the institutional networking policies for password security.

---

### Guidelines for the practice of telepsychology [^2aedc222]. The American Psychologist (2013). Medium credibility.

Telepsychology glossary — privacy, security, and technology terms define key constructs for practice: Protected health information (PHI) is information, including demographic information, which relates to an individual's health, the provision of health care, or payment for health care, and it includes many common identifiers when they can be associated with the health information; personally identifiable information (PII) is information that can be used to distinguish or trace the identity of an individual alone or when combined with other identifying information. Multi-factor authentication is authentication using two or more different factors to provide increased security during log-ins and factors may include something you know, something you have, or something you are. Telecommunications is the preparation, transmission, communication, or related processing of information by electrical, electromagnetic, electromechanical, electro-optical, or electronic means. Telepresence describes how participants experience the technology system, including how it makes them feel present and how it enables them to interact with and respond to others, and considerations often include virtual eye contact and other mechanisms through which patients interact with and respond to technology. Telesupervision is supervision of psychological services either through asynchronous methods or synchronous audio and video where the supervisor is not in the same physical facility as the trainee. Third-party monitoring/third-party observer refers to the influence of an observer's presence on human behaviors, specifically the potential negative effects a present third party may have on the process, results, and outcome of a neuropsychological assessment. mHealth is the use of mobile and wireless technologies to support the achievement of health objectives, and malware is a computer program covertly placed onto a computer or electronic device with the intent to compromise the confidentiality, integrity, or availability of data, applications, or operating systems. GDPR is a data privacy law governing the processing, storing, and managing of the personal data of individuals in the European Union that extends to organizations anywhere if they collect data related to people in the EU, and HIPAA requires the Secretary of HHS to promulgate standards for the electronic exchange, privacy, and security of health information.

---

### Summary of the HIPAA security rule… [^dbc920a4]. HHS (2009). Low credibility.

Official websites use. gov A. gov website belongs to an official government organization in the United States. Secure. gov websites use HTTPS A lock or https: // means you've safely connected to the. gov website. Share sensitive information only on official, secure websites.

---

### A scalable, secure, and interoperable platform for deep data-driven health management [^a200df23]. Nature Communications (2021). High credibility.

To compare the security of PHD to other similar state-of-the-art platforms –, we deployed an abstract graph model that segments healthcare platforms into components/endpoints (via vertices) and represents how these components connect to each other (via edges). Using the abstract graph model, we then compared these platforms by examining the security and privacy properties of each component, as described in Fig. 3 a. Figure 3 b compares the abstract security graph model of PHD to three other similar healthcare platforms. As Fig. 3 c depicts, we deployed best practices in security and privacy preservation, whereas other similar platforms fail to satisfy these best practices with respect to standard security guidelines. Finally, Fig. 3 d shows the results of our brief evaluation of the impact of the authentication policy (Fail2ban) on the performance of the authentication cluster in terms of CPU utilization. Thus, PHD is able to run efficiently, even while highly secure.

---

### Patient data security in the DICOM standard [^5b796879]. European Journal of Radiology (2004). Low credibility.

The DICOM committee added the section "Security Profiles" to the DICOM standard, in order to provide the opportunity of safe communication between health care system partners. Data complying with the DICOM standard — e.g. pictures, signals or reports of examinations can be provided with one or more digital signatures. Attention should be paid to the fact that these possibilities of the DICOM standard are available or can be supplied subsequently by new acquisitions of radiological modalities. The required information to check these prerequisites are given.

---

### Clinical bioinformatician body of knowledge-clinical laboratory regulation and data security core: a report of the Association for Molecular Pathology [^84008249]. The Journal of Molecular Diagnostics (2025). High credibility.

Secure software development — laboratory applications guidance notes that it is important to follow the best practices for secure software development for all software applications used in the laboratory.

---

### Telemedicine for ophthalmology information statement-2018 [^943b38b4]. AAO (2018). Medium credibility.

Telemedicine for ophthalmology — data security and privacy directives emphasize that programs should address data security and integrity concerns including patient privacy and confidentiality, data availability and data recovery; patient privacy for telemedicine examinations shall comply with HIPAA; and all image or video transmissions should be transmitted with proper security measures, including encryption.

---

### Web services and cloud computing in pediatric care [^8b695fb3]. Pediatrics (2021). High credibility.

Web services and cloud computing — definitions and mechanisms relevant to health information systems describe software services accessed over Internet protocols, using Simple Object Access Protocol (SOAP) over Hypertext Transport Protocol (HTTP versus HTTPS) or representational state transfer (REST) with uniform resource identifiers (URIs) and standard methods such as GET, PUT, POST, and DELETE, with vendor platforms handling storage, authentication, networking, messaging/e-mail, cross-platform functionality, updating, and "big" clinical data, thereby reducing the customer's need to supply computing power.

---

### Clinical bioinformatician body of knowledge-clinical laboratory regulation and data security core: a report of the Association for Molecular Pathology [^251a6e21]. The Journal of Molecular Diagnostics (2025). High credibility.

Next-generation sequencing (NGS) laboratory server infrastructure — The instrument and first downstream server may require such bandwidth that they must be directly connected via an ethernet cable; NGS bioinformatics pipelines may be accelerated using specialized hardware; laboratory servers should be protected and cooled, and ideally should be locked down to prevent unauthorized removal; and organizations should ensure data centers comply with the HIPAA Final Security Rule with appropriate physical and operational security implemented and followed.

---

### AD workbench: transforming Alzheimer's research with secure, global, and collaborative data sharing and analysis [^8ed2ee94]. Alzheimer's & Dementia (2025). Medium credibility.

2 METHODS

The AD Workbench comprises two core components: (1) a findable, accessible, interoperable, and reusable (FAIR)–compliantdata catalog for dataset discovery with a streamlined, customizable data access request system; and (2) secure, collaborative online workspaces for conducting analyses. This integrated design enables users to search and request datasets from diverse global sources and conduct integrative analyses in a secure environment.

AD Discovery Portal: AD Workbench's publicly accessible catalog of datasets is designed in accordance with the National Institutesof Health (NIH) Data Management and Sharing policies and the FAIR Guiding Principles for scientific data management, which maximize the value of research data, make it easier to find, and encourage its reuse. The AD Discovery Portal includes the following features:
Data type‐agnostic: AD Workbench's robust infrastructure supports various data types, allowing researchers to request access to multimodal data, such as clinical, genomic, and imaging, that is essential for understanding complex diseases like AD. Although agnostic to data type, anonymizing data at its source is a prerequisite to sharing data on the platform.
Customizable data access requests: Data access requests can be customized to align with specific governance requirements. Data providers retain full control over the process, deciding whether to approve or reject access requests.
Reporting capabilities: Data usage visibility provides insights into who requested data and their intended research purpose.
Extensive metadata for discovery: The catalog includes comprehensive metadata and a data dictionary to help researchers find relevant datasets based on facets and/or fuzzy search and assess dataset suitability.
Flexible data sharing models: AD Workbench offers several data‐sharing models to accommodate diverse governance needs. The centralized model stores data within the AD Data Initiative repository for direct data sharing, whereas the distributed model keeps data at its source, transferring only select record‐level data to AD Workbench workspaces; in the federated model, data remain entirely at the source, with remote querying and results released post‐analysis.
Version control and flexible data ingestion: Version control options and flexible data transfer methods are provided for seamless data integration.

---

### APA guidelines for psychological assessment and evaluation [^5a0dbf79]. APA (2020). High credibility.

Psychological assessment data security and confidentiality — psychologists in general and assessment practitioners in particular are urged to address social engineering threats and adopt recognized standards, as it is particularly important for psychologists in general and assessment practitioners in particular to strive to be aware of threats to data integrity and client confidentiality, and manipulation through human interaction and social engineering is a common source of initiation of data breaches; accordingly, psychologists strive to ensure ongoing training and regular review of all practice employees and/or volunteers concerning data management as an essential component of any practice security plan, and a number of commercially available education and training programs are also for use by employees and others with access to confidential information, which teach participants to recognize phishing emails and other common social engineering exploits and may include unannounced audits and exercises using white flag (unannounced program generated) exploits; psychologists are encouraged to be aware of the existence of the International Organization for Standardization (ISO), the Standard of Good Practice for Information Security, and the National Institute of Standards of Technology (NIST), noting that ISO is an independent nongovernmental international organization with a membership of 164 national standards bodies, ISO/IEC 27001 is the best-known standard in the ISO family of standards providing requirements for an information security management system, and ISO 15408 ISO/IEC 15408–1:2009 establishes the general concepts and principles of IT security evaluation and specifies the general model of evaluation; psychologists using technologically involved tests and other assessment processes are strongly advised to integrate these practices and standards within their routines and practices.

---

### Unaddressed privacy risks in accredited health and wellness apps: a cross-sectional systematic assessment [^86a87653]. BMC Medicine (2015). Low credibility.

Background

Poor information privacy practices have been identified in health apps. Medical app accreditation programs offer a mechanism for assuring the quality of apps; however, little is known about their ability to control information privacy risks. We aimed to assess the extent to which already-certified apps complied with data protection principles mandated by the largest national accreditation program.

Methods

Cross-sectional, systematic, 6-month assessment of 79 apps certified as clinically safe and trustworthy by the UK NHS Health Apps Library. Protocol-based testing was used to characterize personal information collection, local-device storage and information transmission. Observed information handling practices were compared against privacy policy commitments.

Results

The study revealed that 89% (n = 70/79) of apps transmitted information to online services. No app encrypted personal information stored locally. Furthermore, 66% (23/35) of apps sending identifying information over the Internet did not use encryption and 20% (7/35) did not have a privacy policy. Overall, 67% (53/79) of apps had some form of privacy policy. No app collected or transmitted information that a policy explicitly stated it would not; however, 78% (38/49) of information-transmitting apps with a policy did not describe the nature of personal information included in transmissions. Four apps sent both identifying and health information without encryption. Although the study was not designed to examine data handling after transmission to online services, security problems appeared to place users at risk of data theft in two cases.

Conclusions

Systematic gaps in compliance with data protection principles in accredited health apps question whether certification programs relying substantially on developer disclosures can provide a trusted resource for patients and clinicians. Accreditation programs should, as a minimum, provide consistent and reliable warnings about possible threats and, ideally, require publishers to rectify vulnerabilities before apps are released.

---

### A device-independent quantum key distribution system for distant users [^ad6209a8]. Nature (2022). Excellent credibility.

DIQKD protocol

Let us first review the basic assumptions of DIQKD. The two users, Alice and Bob, should (1) each hold a device that is able to receive an input and then respond with an unambiguous output that can be used to generate a secure key (Fig. 1). The communication between their devices is limited to what is necessary to generate a secure key, namely, (2) the users control when their respective devices communicate with each other; and (3) the devices do not send unauthorized classical information to an eavesdropper. Finally, as it is with any QKD protocol, it is required that (4-a) quantum mechanics is correct, (4-b) the users' inputs are private and random and (4-c) the users are connected by an authenticated classical channel and use trusted post-processing methods. For more details, we refer the interested reader to Supplementary Appendix A.

Fig. 1
Schematic of a DIQKD scheme.

Each of the two parties, Alice and Bob, holds QKD devices, which are connected by a quantum channel. The devices receive the inputs X and Y, and respond with outputs A and B, respectively. To run the protocol each party needs a trusted supply of inputs and a trusted local storage unit to store both output and inputs. Additionally, a trusted authenticated public channel (pub. auth. channel) between the two parties is necessary for exchange of information during post-processing. gen. generation.

The DIQKD protocol considered here is similar to the original DIQKD protocol, except that two measurement settings are used for key generation instead of one. Importantly, in doing so, the protocol can tolerate more system noise — the critical QBER increases from 0.071 to 0.082. The protocol considers that Alice and Bob each hold a device, which are connected by a quantum channel (Fig. 1). In each i th of N measurement rounds, one of four different inputsis given to Alice's device, whereas Bob's device receives one of two possible values. The input for each round is provided by a trusted local source of randomness. Both devices output two possible values, at Alice's side andat Bob's side. The input and output values are recorded and stored in independent, local secured storage.

---

### Privacy and security of medical information [^5a5031d8]. Otolaryngologic Clinics of North America (2002). Low credibility.

Increasingly, medical information is being stored, accessed, and communicated electronically, which makes this information vulnerable to breaches in security and privacy. Clinicians must be familiar with the best practices for keeping medical information private, as well as the local, state, and federal regulations that govern the security, privacy and patient-identifiable information. Noncompliance carries the potential for civil and criminal penalties.

---

### Proposal for DICOM multiframe medical image integrity and authenticity [^1a760dba]. Journal of Digital Imaging (2009). Low credibility.

This paper presents a novel algorithm to successfully achieve viable integrity and authenticity addition and verification of n-frame DICOM medical images using cryptographic mechanisms. The aim of this work is the enhancement of DICOM security measures, especially for multiframe images. Current approaches have limitations that should be properly addressed for improved security. The algorithm proposed in this work uses data encryption to provide integrity and authenticity, along with digital signature. Relevant header data and digital signature are used as inputs to cipher the image. Therefore, one can only retrieve the original data if and only if the images and the inputs are correct. The encryption process itself is a cascading scheme, where a frame is ciphered with data related to the previous frames, generating also additional data on image integrity and authenticity. Decryption is similar to encryption, featuring also the standard security verification of the image. The implementation was done in JAVA, and a performance evaluation was carried out comparing the speed of the algorithm with other existing approaches. The evaluation showed a good performance of the algorithm, which is an encouraging result to use it in a real environment.

---

### Clinical bioinformatician body of knowledge-clinical laboratory regulation and data security core: a report of the Association for Molecular Pathology [^d1605929]. The Journal of Molecular Diagnostics (2025). High credibility.

AMP BoK Regulation & Data Security Core — cloud services, HIPAA, and storage security advises that laboratories using cloud services should work with vendors to document compliance and to ensure security, confidentiality, integrity, and accessibility of genomic data; data protection should be maintained throughout the life cycle of storage, and for on‑premise storage, cloud services should follow all the safeguards of the HIPAA Final Security Rule; laboratories should be aware of existing security guidelines for storage infrastructure; in the United States, careful consideration is required when using cloud service providers because servers may be in other countries and entities outside the United States may not comply with HIPAA even though compliance is required for US laboratories that use cloud services; even if providers cannot see or unencrypt data, they are still required to have a BAA with the laboratory; therefore, it is recommended that the laboratory contractually requires the cloud service provider to keep all data within the United States and to agree with the provisions of the BAA in writing.

---

### Implementing telehealth in practice: ACOG committee opinion summary, number 798 [^c814d896]. Obstetrics and Gynecology (2020). High credibility.

Health Insurance Portability and Accountability Act (HIPAA) requirements for telehealth — the Health Insurance Portability and Accountability Act, including its implementing regulations, is an important aspect of all health care, including telehealth, and physicians who provide telehealth must comply with the HIPAA privacy and security rules. Patients should be counseled that there is still a small possibility that even encrypted data could be accessed by unauthorized persons, that applications on a smartphone seldom have the same level of encryption as telehealth equipment used over telehealth networks, and that health information stored in a personal health record not offered through a physician or health plan is not covered by HIPAA. The HIPAA Security Rule provides technical and nontechnical safeguards and requires covered entities to establish 1) access controls, 2) audit controls, 3) integrity controls, and 4) transmission security for electronic protected health information.

---

### SPIRIT 2025 explanation and elaboration: updated guideline for protocols of randomised trials [^c0430f4f]. BMJ (2025). Excellent credibility.

Item 6: Where and how the individual deidentified participant data (including data dictionary), statistical code, and any other materials will be accessible

Examples

"Within 3 months of the end of the final year of funding a description of the study dataset, including a code book, a SAS file of the code used for creating the final study sample, the final study variables and plan for conducting the outcomes analyses outlined in the study protocol will be made available. The investigators will create a complete, cleaned, de-identified copy of the final data set. A section in the MGH Health Decision Sciences Center website will be created to hold study materials and it will include information for investigators interested in accessing these materials and replicating the findings. The PI will share a de-identified data set with outside investigators according to the policies in the approved IRB [institutional review board] protocol. Investigators may be required to provide evidence of IRB approval (or exemption) and/or complete a data sharing agreement".

"The principal investigator will oversee the intra-study data sharing process. All investigators at individual study sites will be given access to the cleaned data sets. Data sets will be housed on the file transfer protocol site created for the study, and all data sets will be password protected. Investigators will have direct access to their own site's data sets and will have access to other sites data by request. To ensure confidentiality, any data dispersed to investigators will be blinded of any identifying participant information".

Explanation

Data sharing typically involves making accessible the deidentified individual participant data generated from the trial and the data dictionary detailing the data elements, along with the analytical code used to carry out the planned data analyses.

A trial's data can be stored in a data archive and shared in a variety of ways, such as through an institutional repository (for example, belonging to the university associated with the trial's coordinating centre) or a public facing repository such as Vivli, GitHub, or the YODA Project. Data can be shared at the time of the main trial publication, or only after a specified embargo period, such as after one year, enabling the trial team to complete planned secondary projects.

Data sharing increases transparency, facilitates reproducibility, reduces research waste, and provides data for additional exploration. Some trial groups have worked collaboratively to conduct individual patient data meta-analysis. Data sharing is also associated with an increased number of citations.

---

### The METRIC-framework for assessing data quality for trustworthy AI in medicine: a systematic review [^9a97c9e6]. NPJ Digital Medicine (2024). Medium credibility.

Table 1
Measurement Process, cluster: definitions, examples and references

Table 2
Timeliness cluster: definitions, examples and references

Table 3
Representativeness. cluster: definitions, examples and references

Table 4
Informativenesscluster: definitions, examples and references

Table 5
Consistency. cluster: definitions, examples and references

Table 6
Data Management cluster: definitions and references

The METRIC-framework encompasses three levels of details: clusters which pool similar dimensions; dimensions which are individual characteristics of data quality; and subdimensions which split larger dimensions into more detailed attributes (compare Fig. 3 from inside to outside). Besides the terms contained in the METRIC-framework, we found several frequently mentioned dataset properties which we, for our purpose, want to separate from the METRIC-framework. We summarise these additional properties under a separate cluster called data management (Fig. 4). The attributes included in this cluster ensure that a dataset is well-documented, legally and effectively usable. In particular, it includes the properties documentation, security and privacy, as well as the well-established FAIR-Principles. Appropriate documentation of datasets is the topic of multiple initiatives – that give guidance for the data creator and handler. The METRIC-framework on the other hand is targeted towards AI developers. It evaluates the suitability of the content of the data for a specific ML task, which is greatly facilitated by appropriate documentation but does not depend on it. Similarly, the FAIR-principles, requiring data to be findable, accessible, interoperable and reusable, are vital for evaluating datasets for general purpose but are not included in the METRIC-framework since the question of fit for a specific purpose can only be asked when a dataset is already successfully obtained. Security is another important aspect of data management: Who can access and edit the data? Can it be manipulated? Again, such questions concern the handling of the data, not the evaluation of its content. Finally, privacy (data privacy and patient privacy) is a delicate and heavily discussed topic in the context of healthcare. However, we separate these issues from the METRIC-framework since they concern data collection, creation and handling. We note that aspects such as anonymisation or pseudonymisation may impact the quality of the content of a dataset by, e.g. removing information. However, the METRIC-framework is designed to evaluate the resulting dataset with respect to its usefulness for a specific task, not the quality of the modifications. Hence, while these properties play a central role in the creation, handling, management and obtainment of data, the METRIC-framework is targeted at the content of a dataset since that is the part the ML algorithm learns from. Therefore, we see the data management cluster as a prerequisite for data quality assessment by the METRIC-framework which itself divides the concept of data quality for the content of a dataset into five clusters: measurement process, timeliness, representativeness, informativeness, consistency. A summary of the characteristics and key aspects of all five clusters is given in Table 7.

---

### Realistic noise-tolerant randomness amplification using finite number of devices [^7aee5926]. Nature Communications (2016). Medium credibility.

For Protocol II, we add a second group of four devices, operating in the same way as the original group. The runs (uses) of the devices from this second group are divided into blocks and a portion of bits from the SV-source is used to choose a block. The same test as in Protocol I is performed twice: first on all runs from the first group of devices, and then on the chosen block of runs from the second group. If the tests are passed, an extractor designed to extract randomness from three independent weak sources is applied to the three groups of variables: one from the SV-source, another formed by outputs from the first four devices and a third formed by the outputs from the chosen block of runs from the second group of four devices.

The merit of Protocol II is that it offers amplification of arbitrarily weak sources under a constant noise rate and with just a few devices. The probability of failure (failure occurs if the test was passed, but the output of the protocol is not random) scales as an inverse polynomial in the total number of runs. On the contrary, Protocol I has probability of failure exponentially small in the number of runs. Moreover, since forthere exist explicit two source extractors, Protocol I also gives exponential security for this range of ɛ. As a matter of fact, the field of extractors is being constantly developed. For instance, in a very recent developmentan extractor was found that, if used in our Protocol I, allows to draw one bit of randomness for an arbitrarily weak SV-source with exponential security.

Let us emphasize, that our protocols exhibit a strong security criterion called composable security. This means that the obtained randomness can be securely used as an input to any other protocol. It also means that if an adversary Eve would in future learn part of the random bits output by the protocols (for example, by some espionage), the remaining bits would still be completely secure.

Last but not least, the security of our protocols relies on quantum-mechanical predictions, but can be verified by a person that either does not know or does not trust the quantum mechanical theory. Indeed, the security of our protocols is based on the very statistics of the outcomes of the device and the quantum mechanics is needed only to produce the required statistics. Moreover, Protocol I offers exponential security within such a paradigm.

---

### Experimental verification of multipartite entanglement in quantum networks [^9282f786]. Nature Communications (2016). Medium credibility.

Multipartite entangled states are a fundamental resource for a wide range of quantum information processing tasks. In particular, in quantum networks, it is essential for the parties involved to be able to verify if entanglement is present before they carry out a given distributed task. Here we design and experimentally demonstrate a protocol that allows any party in a network to check if a source is distributing a genuinely multipartite entangled state, even in the presence of untrusted parties. The protocol remains secure against dishonest behaviour of the source and other parties, including the use of system imperfections to their advantage. We demonstrate the verification protocol in a three- and four-party setting using polarization-entangled photons, highlighting its potential for realistic photonic quantum communication and networking applications.

---

### Occupational electronic health records: recommendations for the design and implementation of information systems in occupational and environmental medicine practice-ACOEM guidance statement [^e7e17cc5]. Journal of Occupational and Environmental Medicine (2024). High credibility.

Occupational electronic health records — privacy protections and security requirements emphasize that OEHRs should account for evolving legal duties and separate nonclinical data streams. OEHRs should account for and facilitate compliance with the changing privacy protections at the federal and state level, and OEHRs should facilitate a separation of these data sources or treat all data as protected health information under a covered entity. In the absence of more comprehensive federal privacy law, more than 10 states have now passed data privacy laws, and common OEHR elements such as biometric data recorded from health promotion initiatives must therefore comply with all security requirements of the California Privacy Protection Agency to prevent unauthorized access or disclosure. To protect employee health and exposure information, OEHRs should maintain current industry standards for healthcare data security, incorporating technologies like encryption and facilitating business practices like cybersecurity audits, alongside safeguards such as backups, firewalls, encryption, and multifactor authentication already adopted by healthcare organizations.

---

### The federated database – a basis for biobank-based post-genome studies, integrating phenome and genome data from 600, 000 twin pairs in Europe [^117c3a37]. European Journal of Human Genetics (2007). Low credibility.

Integration of complex data and data management represent major challenges in large-scale biobank-based post-genome era research projects like GenomEUtwin (an international collaboration between eight Twin Registries) with extensive amounts of genotype and phenotype data combined from different data sources located in different countries. The challenge lies not only in data harmonization and constant update of clinical details in various locations, but also in the heterogeneity of data storage and confidentiality of sensitive health-related and genetic data. Solid infrastructure must be built to provide secure, but easily accessible and standardized, data exchange also facilitating statistical analyses of the stored data. Data collection sites desire to have full control of the accumulation of data, and at the same time the integration should facilitate effortless slicing and dicing of the data for different types of data pooling and study designs. Here we describe how we constructed a federated database infrastructure for genotype and phenotype information collected in seven European countries and Australia and connected this database setting via a network called TwinNET to guarantee effortless data exchange and pooled analyses. This federated database system offers a powerful facility for combining different types of information from multiple data sources. The system is transparent to end users and application developers, since it makes the set of federated data sources look like a single system. The user need not be aware of the format or site where the data are stored, the language or programming interface of the data source, how the data are physically stored, whether they are partitioned and/or replicated or what networking protocols are used. The user sees a single standardized interface with the desired data elements for pooled analyses.

---

### Clinical bioinformatician body of knowledge-bioinformatics and software core: a report of the Association for Molecular Pathology [^11e1d433]. The Journal of Molecular Diagnostics (2025). High credibility.

Pipeline build versus buy — clinical laboratories face whether to build in house or buy off the shelf, with in-house offering customization but requiring significant expertise and resources, and vendor pipelines enabling rapid implementation with centralized support but potentially not addressing laboratory specifications without modification. The page states that "Clinical laboratories regularly face the dilemma of how to develop their bioinformatics pipeline, whether to build in house or buy off the shelf (vendor supplied)", that building in house "allows customization to the laboratory" yet "requires significant bioinformatics and engineering expertise and resources to create, maintain, and upgrade", and that vendor solutions allow laboratories "to rapidly implement standard pipelines with centralized bioinformatics support; however, these off-the-shelf pipelines may not readily address current or future laboratory specifications without further modification". Inclusion of bioinformaticians is emphasized: "It is important to include bioinformaticians in the build-versus-buy decision-making process", covering "feasibility", "pipeline requirements", and "system integrations and security", with roles such as evaluating "feasibility, effort, and complexity", comparing features to "laboratory or assay requirements", and planning "how integration with laboratory information system/laboratory information management system and electronic health record platforms may be achieved, preserving the security and confidentiality of the data".

---

### Value lies in the eye of the patients: the why, what, and how of patient-reported outcomes measures [^2e192626]. Clinical Therapeutics (2020). Medium credibility.

Patient-reported outcomes (PROs) are any report of the status of a patient's health condition that comes directly from the patient (or in some cases from a caregiver or surrogate responder), without interpretation by a practitioner or anyone else. PROs are increasingly used as a valuable source of data in different domains of health care, including research, clinical practice, health care management, and decision making on the regulation, coverage, and reimbursement of new technologies. Several factors must be considered when selecting which PRO measure to use to ensure their appropriate use and interpretation as well as their relevance for decision makers. The increasing availability of PRO data, its integration with other data sources, and the improvements in data analytics offer a valuable opportunity to place the patient at the center of any health care process. However, several issues need to be addressed, including interoperability, data governance, security, privacy, and ethics, to realize an effective, integrated, standardized, real-time assessment of PROs in the health care systems.

---

### Clinical bioinformatician body of knowledge-clinical laboratory regulation and data security core: a report of the Association for Molecular Pathology [^6ad1acbf]. The Journal of Molecular Diagnostics (2025). High credibility.

Administrative procedures — documentation — Written documentation outlining the details of the computational architecture and workflows of clinical systems is critical. Having a detailed record of laboratory systems, including bioinformatics pipelines, is not only required but can be extremely useful for troubleshooting, recovering from unexpected outages.

---

### Clinical bioinformatician body of knowledge-clinical laboratory regulation and data security core: a report of the Association for Molecular Pathology [^be6c3893]. The Journal of Molecular Diagnostics (2025). High credibility.

Workflow orchestration, containers, and environment separation — It is important to have a test environment that mimics but is distinct from the production environment; however, it should have the same computational resources and integrations as the production environment, and ideally there should also be a development environment that is distinct from the test and production environments. Workflow management and orchestration software enables modularity within the code, detailed descriptions of inputs, outputs, and connections, error handling and error logging, restarting code from where an error occurs, and easier launching of pipelines. The use of containers enables the packaging of software components or whole pipelines complete with the environments needed to run them, and when building containers it is helpful to restrict the privileges of the container on the host machine to the minimum necessary and minimize the range of software installed in the container to reduce its vulnerability footprint. Containers should be scanned periodically for vulnerabilities, it is also critically important to keep software up to date with appropriate patches to prevent exposing the organization to the potentially catastrophic effects of malware through known vulnerabilities, and preventing unauthorized outside access to the software and limiting the access of laboratory software to necessary networks and applications can further protect the organization from adverse cybersecurity events.

---

### Survey, taxonomy, and emerging paradigms of societal digital twins for public health preparedness [^f08c63fe]. NPJ Digital Medicine (2025). Medium credibility.

Internet of medical things (IoMT)

IoMT corresponds to the network of medical devices connected to the internet. These devices are equipped with sensing and processing capabilities and are capable of communicating with each other over the internet. IoMT devices serve various purposes, including patient health monitoring (such as tracking blood glucose levels or measuring blood pressure) and delivering treatment in remote or underdeveloped areas. Additionally, a web form that allows the entry and transmission of healthcare-related data over the internet may be considered a loosely coupled form of IoMT.

Communication and networks

Numerous communication protocols may be employed to facilitate efficient communication in healthcare-related DT networks. Some of the commonly used protocols may include Message Queuing Telemetry Transport (MQTT) for real-time healthcare data exchange, Constrained Application Protocol (CoAP) for communication in healthcare IoT networks, and HTTPS for securely transmitting healthcare information over the internet. These communication protocols can operate over both wired and wireless networks, such as Zigbee, Bluetooth, Ethernet, or Wireless-Fidelity (Wi-Fi), depending on the specific application requirements.

Optimizing network performance is crucial for healthcare applications. It requires efficient management of network resources and effective resolution of network challenges related to bandwidth, storage, latency, throughput, and security. To achieve network efficiency, both centralized Cloud DTs at the network core and Edge/Fog DTs on local/intermediate networks may be deployed based on application requirements. This approach can enable load balancing, congestion avoidance, and bandwidth conservation, thus facilitating robust healthcare delivery in next-generation DT networks.

Efficient and reliable communication relies on maintaining high standards of network security and privacy. DT networks can be secured using advanced encryption and access control mechanisms. Meanwhile, network privacy may be ensured through advanced authentication and anonymization techniques. To facilitate interoperability among network devices, protocols like FHIR and Data Distribution Service may provide a reasonable solution.

---

### The spectrum of data sharing policies in neuroimaging data repositories [^20cb374d]. Human Brain Mapping (2022). Medium credibility.

Unlike secondary use of identifiable private information, research on nonidentifiable private information does not constitute human subject research under the Common Rule. According to a guidance from the HHS Office of Human Research Protections (OHRP), private information is not individually identifiable when it cannot be linked to specific individuals by an investigator either directly or indirectly through coding systems. For example, coded private information is no longer identifiable, or is anonymized, when a key to decipher the code and enable linkage of the identifying information (e.g. name or social security number) to the private information is destroyed. The guidance further provides that even when the key to the code still exists, secondary research with only coded private information is not considered involving human subjects, if the investigators and the holder of the key to the coded information (e.g. a researcher who collected the information from the individuals) enter into an agreement prohibiting the release of the key to the investigators; if there are IRB‐approved written policies and operating procedures for a repository or data management center that prohibit the release of the key to the investigators; or if there are other legal requirements prohibiting the release of the key to the investigators (Office of Human Research Protections (OHRP)). In most neuroimaging data shared through existing repositories, identifiable information is deleted or replaced with a code (e.g. a number, letter, or symbol) and shared under a data use agreement, which prohibits reestablishing the identity of the subjects. Thus, secondary research on these nonidentifiable neuroimaging data would not be considered research involving human subjects under the OHRP guidance. Yet it is important to note that some IRBs might impose more rigorous standards (e.g. an investigator conducting a study on coded private information may be considered able to ascertain the identity of the subjects if the holder of a key to the code is also listed in the study's IRB protocol), thus researchers should consult with their IRBs when sharing coded private information.

The Common Rule has its own enforcement mechanism for researchers and institutions that failed to meet the requirement for human subject protections, including suspension of research or termination of federal funding (45 CFR §46.123). However, the rule does not provide private cause of action for violation of the regulation, and currently, subjects who experience harm as a result of their participation in a research study are left without any legal remedies (Koch). 1

---

### SPIRIT 2013 explanation and elaboration: guidance for protocols of clinical trials [^1d8cdeeb]. BMJ (2013). Excellent credibility.

Dissemination policy — reproducible research

Item 31c: Plans, if any, for granting public access to the full protocol, participant level dataset, and statistical code

Explanation

Given the central role of protocols in enhancing transparency, reproducibility, and interpretation of trial results, there is a strong ethical and scientific imperative to ensure that full protocols are made publicly available. High quality protocols contain relevant details on study design and conduct that are generally not available in journal publications or trial registries. It is also important to make available the full study report, such as the "clinical study report" submitted to regulatory agencies by industry sponsors. This detailed report provides the most comprehensive description of trial methods (including the full protocol) and all published and unpublished analyses. In addition, there have increasingly been calls to improve the availability of participant-level datasets and statistical code after journal publication to enable verification and replication of analyses, facilitate pooling with other studies, and accelerate research through open knowledge sharing.

Avenues for providing access to full protocols include journals, trial websites, and trial registries. Several journals and funders support the sharing of participant level data, while others routinely publish a statement regarding sharing of protocols, statistical codes, and datasets for all of their published research articles.

The protocol should indicate whether the trial protocol, full study report, anonymised participant level dataset, and statistical code for generating the results will be made publicly available; and if so, describe the timeframe and any other conditions for access.

---

### Nuclear disarmament verification via resonant phenomena [^15d438da]. Nature Communications (2018). Medium credibility.

Another important consideration is the actual high-level protocol: it should be recognized that any uncertainty about the protocol is transferred to the whole verification process. The template selection and authenticity is thus critical and should be the focus of additional research.

Finally, the ZK verification technique can also be extended to weapon components made out of low-Z elements. Most hydrogenous materials, for example, explosive lenses, are essentially opaque to epithermal neutrons, thus necessitating the use of other, more penetrating particles. The fast neutrons at MeV scale, where interaction cross sections for hydrogen are significantly lower, are a viable alternative for a source. An established technique of fast neutron resonance radiography, which exploits the resonances in nitrogen, oxygen, and carbon at the ~MeV scale, could prove promising. Other neutron interrogation techniques are also possible. For example, the nuclear material identification systemuses active measurements of neutron multiplicities to achieve a unique photon–neutron tag of a fissile object. This tag depends on the isotopics of the object and its geometry. In its simplest form the technique is not ZK — however, developments of schemes to protect information are possible and could be researched.

---

### Heart Rhythm Society policy statement update: recommendations on the role of industry-employed allied professionals [^d37001bd]. Heart Rhythm (2023). High credibility.

Data management — industry-employed allied professionals (IEAPs) handling protected personal and health information should follow local and national policies on patient privacy and data protection. When obtaining or reviewing data and relaying the information to the patient's clinical care team or other health care providers, the IEAP should follow guidelines and protocols and use secure means of communicating any patient data in order to be HIPAA compliant. Any third-party vendor that manages or reviews patient data must comply.

---

### Guidelines for the use of cell lines in biomedical research [^b5c503c3]. British Journal of Cancer (2014). Low credibility.

1.2. Acquiring a cell line from another laboratory

Acquisition of cell lines presents a number of potential hazards; cell lines may simply not be what they are claimed to be and a published description of a cell line with a certain property is no guarantee that it is still the same line or has that same property. The more laboratories that a cell line has passed through since its origin, characterisation and contamination testing, the less reliance should be placed on its documented properties. However, even the originator as a source is not a guarantee of authenticity. If the receiving laboratory wishes to place any reliance on historic data obtained with a cell line, it should always carry out its own testing procedures (see Sections 1.2.2 and 1.5.1) before accepting an incoming cell line into general use. An enormous amount of time, cost and effort can be wasted by scientists using cell lines that are either misidentified or contaminated.

The cell bank or laboratory of origin should be able to provide a certificated DNA STR profile for human cell lines and evidence of authentication using an appropriate technique for non-human cell lines (see Sections 1.2.2 and 1.5.1). However, the publication of full STR profiles for human cell lines from tissue donated anonymously may present ethical problems. While profiles of long-established cell lines have been made widely available, the profiles of recently isolated cell lines could potentially be used to re-identify the donor or their family. Guidance on managing such scientific data is given in the study by.

The name of the cell line should be checked against the International Cell Line Authentication Committee (ICLAC) database of misidentified cell lines. The STR profile should be repeated at the time of banking the new cell line in LN 2.

---

### ACR-AAPM-SIIM practice parameter for determinants of image quality in mammography [^abdc686d]. SIIM/ACR/AAPM (2022). High credibility.

Quality control recommendations — storage and archiving — list optimal components of digital storage and archiving including verification of DICOM metadata in header and accuracy of information, security and privacy protection, and backup and disaster recovery testing.

---

### Knowledge abstraction and filtering based federated learning over heterogeneous data views in healthcare [^9b9240ee]. NPJ Digital Medicine (2024). Medium credibility.

Although Common Data Models (CDMs) aim to harmonise EHR datasets across institutions, their adoption remains limited due to the significant effort required. Our framework focuses instead on enabling deep learning over disparate data views, making it more suitable for federated learning and deep learning scenarios where CDMs may not be feasible.

While this study primarily focuses on addressing data view heterogeneity, it does not explore privacy aspects such as differential privacy, secure aggregation, and secure communication channels. However, since our framework is built on FedAvg, it is compatible with existing privacy-preserving techniques –, which can be integrated as needed for compliance with data protection regulations in clinical settings. Future work will explore the addition of these privacy measures to enhance patient data security, especially in highly regulated healthcare environments. Another limitation of this study is the lack of evaluation under varying levels of statistical heterogeneity, which is common in healthcare settings. Institutions often differ in terms of patient demographics, disease prevalence, and treatment protocols, leading to diverse data distributions. In future research, we plan to enhance the knowledge abstraction and filtering mechanism by incorporating methods that can address statistical heterogeneity, thereby improving the framework's robustness and performance across institutions with differing data characteristics. Moreover, this study did not explore the integration of free-text data, a rich source of patient information in healthcare, such as clinical notes. Free-text presents unique challenges due to its unstructured nature, but it has the potential to significantly enhance patient-care models when combined with structured data like clinical features. Future research will aim to develop FL frameworks capable of handling both data modality and data view heterogeneity, integrating free-text data alongside structured EHRs, time-series, and graph data across diverse client environments.

---

### Digital health governance in China by a whole-of-society approach [^25bb2f18]. NPJ Digital Medicine (2025). Medium credibility.

Legislation, policy, and compliance

The regulatory framework for digital healthcare in China began to take shape in 2010, reaching a significant peak with the issuance of key regulations in 2017 and 2018. These regulations, such as Opinions of the General Office of the State Council on Promoting the Development of Internet Plus Health Care, the Administrative Measures for Internet-based Diagnosis, and the Administrative Measures for Internet-based Hospitals, have been crucial in shaping the landscape for digital healthcare.

A major focus is data privacy and security. The Personal Information Protection Law of China designates health data as sensitive personal data, emphasizing the need for robust privacy protections. Adopted on August 20, 2021, and effective from November 1, 2021, the law represents a substantial overhaul of China's data protection framework. Other pivotal regulations, including the Medical Practitioners Law, the Detailed Rules for the Supervision of Internet Diagnosis and Treatment, and the Regulation on the Administration of Medical Institutions, reinforce patient privacy protection through regulatory oversight and punitive measures. EHR systems implement a variety of privacy protection strategies, including the classification of information according to confidentiality levels, a hierarchical management structure, access notifications, and anonymization. The National Health Commission, the State Administration of Traditional Chinese Medicine, and regional health departments are responsible for regulating the use, supervision, and management of EHR systems, ensuring compliance with established privacy standards.

The effective functioning of digital health applications requires data exchange and integration across health facilities. Data Security Law, Administrative Measures on Standards, Security and Services of National Health Big Data, have provided detailed guidance on structuring data-sharing frameworks, establishing backup and recovery protocols, and auditing user activities, including data creation, modification, and deletion. In 2020, the Information Security Technology — Guide for Health Data Securitywas published by the State General Administration of Quality Supervision, Inspection and Quarantine, in collaboration with the National Standardization Administration. This document established protocols for secure data access and storage. Key stakeholders, including the National Health Commission, State Internet Information Office, Ministry of Industry and Information Technology, Ministry of Public Security, and State Administration for Market Supervision and Administration, are responsible for maintaining data security and compliance within data systems.

---

### India's evolving digital health strategy [^c016ae36]. NPJ Digital Medicine (2024). Medium credibility.

Digital health care

On the foundation of this success, the government of India turned to the health sector and committed to "a paradigm shift from the existing silo systems to a holistic and comprehensive health eco-system, founded on the latest digital architectures and technologies".

The development of India's digital health strategy began with a global survey of best practices, in an effort to "leapfrog many of the traps that bedevil health information systems even in developed economies". The United States' HITECH Act, for example, highlighted the importance of a cohesive data architecture and interoperability. The UK's NHS Digital illustrated the importance of developing and deploying integrated digital health platforms that ensure data privacy while enabling efficient health information exchanges. South Korea's approach to integrating personal health records with national identifiers and Singapore's "One Patient, One Health Record" model offered lessons on the value of ensuring patient access to their health data for enhanced care coordination and privacy.

Drawing from these global models, India launched the ABDM in 2021 with a focus on creating a secure, integrated digital health ecosystem that emphasizes personal health records, ensuring that each citizen's health data is accessible, interoperable, and under their control. This ambitious project introduced five key innovations designed to enhance the delivery and management of healthcare services across the nation.

Central to the ABDM was a unique 14-digit health identifier for every citizen, the Ayushman Bharat Health Account (ABHA). Notably, the ABHA was primarily built upon Aadhaar (although other sources of identification are allowed) - India's robust biometric identification system, leveraging its technological underpinnings to securely authenticate and manage health records.

The ABDM mobile application provides individuals with an accessible electronic personal health record (PHR) that adheres to national interoperability standards. Unlike traditional Electronic Medical Records architectures, the PHR is managed, shared, and controlled by the patient themselves with the ability to make records visible or invisible to providers. This architecture is a unique feature of the ABDM program.

---

### SHEA position statement on pandemic preparedness for policymakers: pandemic data collection, maintenance, and release [^809749bf]. Infection Control and Hospital Epidemiology (2024). High credibility.

SHEA summary position — SHEA strongly supports modernization of data collection processes and the creation of publicly available data repositories that include a wide variety of data elements and mechanisms for securely storing both cleaned and uncleaned data sets that can be curated as clinical and research needs arise; these elements can be used for clinical research and quality monitoring and to evaluate the impacts of different policies on different outcomes; achieving these goals will require dedicated, sustained, and long-term funding to support data science teams and the creation of central data repositories that include data sets that can be linked via a variety of mechanisms and include institutional and state and local policies.

---

### Recommendations for achieving interoperable and shareable medical data in the USA [^8e364cd1]. Communications Medicine (2022). Medium credibility.

Conclusions

In conclusion, the COVID-19 crisis is another wake-up call that reminds us that we cannot continue to use outdated data solutions that jeopardize our ability to advance research capabilities, and can lead to medical errors and loss of life. Boxes 1 and 2 offer a set of summary recommendations that, if adopted, would help achieve needed solutions to the problems being described in this paper.

If Amazon can track packages, international banks can track money, and weather maps can track complex weather patterns, we can also learn how to track and analyze complex health data. Our recommendations are intended to create the conditions in which we can address an entrenched and highly complex problem that will only become worse if unaddressed. This problem will not be cheap to fix, but it will be much costlier to ignore.

Box 1: Recommended steps for legislative action

Empower an oversight and enforcement agency with a qualified advisory board representing all stakeholders to identify and address critical usability requirements for building an interoperable & interconnected Health Information System in the USA
Enforce the creation and maintenance of a thorough common data model for clinical data
Prohibit unwieldy data customization by enforcing interoperable and interconnected standards for medical data collection for every organization that collects or processes medical data (e.g. hospitals, laboratory information systems)
Establish automated data verification processes to confirm that the data collected are transmitted without distortion to correct patient records and end users; identify problems through feedback loops, and correct the sources of any data errors
Enforce a standard, certifiable calibration process that ensures that different tests for a given analyte give equivalent results regardless of the instrument used or the laboratory performing the test
Implement a universal, unique patient identifier secured by the strongest privacy-enhancing technology and supported by a security infrastructure
Authorize a central body to collect death and cause of death information for all individuals in the USA, with the federal government defining the requirements and precautions needed to avoid fraud
Require the Centers for Medicare & Medicaid Services to create reimbursable billing codes for clinical informatics professionals who can make informed decisions about HIS selection, optimization of analytical and decision support functions, and maintenance
Establish Good Clinical Practices with adequate inspections of the analytical clinical data processes and facilities
Create incentives aligned with patients' and public health needs in which healthcare vendors are rewarded for documenting and avoiding medical errors and unnecessary processing costs
Identify and correct gaps and inconsistencies in current regulatory requirements

---

### Responsible practices for data sharing [^5be29be1]. The American Psychologist (2018). Low credibility.

Research transparency, reproducibility, and data sharing uphold core principles of science at a time when the integrity of scientific research is being questioned. This article discusses how research data in psychology can be made accessible for reproducibility and reanalysis by describing practical ways to overcome barriers to data sharing. We examine key issues surrounding the sharing of data such as who owns research data, how to protect the confidentiality of the research participant, how to give appropriate credit to the data creator, how to deal with metadata and codebooks, how to address provenance, and other specifics such as versioning and file formats. The protection of research subjects is a fundamental obligation, and we explain frameworks and procedures designed to protect against the harms that may result from disclosure of confidential information. We also advocate greater recognition for data creators and the authors of program code used in the management and analysis of data. We argue that research data and program code are important scientific contributions that should be cited in the same way as publications. (PsycINFO Database Record

---

### Spectral physical unclonable functions: downscaling randomness with multi-resonant hybrid particles [^0b85e6b7]. Nature Communications (2025). High credibility.

Optical physical unclonable functions (PUFs) are state-of-the-art in advanced security applications. Fabricated with inherent randomness, they generate fingerprint-like responses, serving as trust anchors for material assets. However, the existing PUFs, typically reliant on microscopic spatial features, face increasing threats from rapidly advancing microscale manipulation techniques. Here, we present novel PUFs based on random nanoscale variations within multi-resonant gold-silicon particles. These inevitable structural differences, coupled with strong optical resonances, provide unique spectral features in particles' photoluminescence (PL), which we encode as unclonable keys. Our approach surpasses the shortcomings of diffraction-limited designs, additionally offering a multi-functional platform for robust authentication of goods and verification of individuals. We demonstrate two security label models based on PL mapping and direct PL imaging, as well as a concept for the first all-optical one-time password verification token with an exceptionally high storage density of unique information. This work paves the way toward nanoscale-enabled unclonability, bringing enhanced security for hardware-based cryptography, personalized access control, and cutting-edge anti-counterfeiting.

---

### Zero-check: a zero-knowledge protocol for reconciling patient identities across institutions [^35b7db22]. Archives of Pathology & Laboratory Medicine (2004). Low credibility.

Context

Large, multi-institutional studies often involve merging data records that have been de-identified to protect patient privacy. Unless patient identities can be reconciled across institutions, individuals with records held in different institutions will be falsely "counted" as multiple persons when databases are merged.

Objective

The purpose of this article is to describe a protocol that can reconcile individuals with records in multiple institutions.

Design

Institution A and Institution B each create a random character string and send it to the other institution. Each institution receives the random string from the other institution and sums it with their own random string, producing a random string common to both institutions (RandA+B). Each institution takes a unique patient identifier and sums it with RandA+B. The product is a random character string that is identical across institutions when the patient is identical in both institutions. A comparison protocol can be implemented as a zero-knowledge transaction, ensuring that neither institution obtains any knowledge of its own patient or of the patient compared at another institution.

Results

The protocol can be executed at high computational speed. No encryption algorithm or 1-way hash algorithm is employed, and there is no need to protect the protocol from discovery.

Conclusion

A zero-knowledge protocol for reconciling patients across institutions is described. This protocol is one of many computational tools that permit pathologists to safely share clinical and research data.

---

### Mainecoon: implementing an open-source web viewer for DICOM whole slide images with AI-integrated PACS for digital pathology [^549e5df9]. Journal of Imaging Informatics in Medicine (2025). Medium credibility.

The rapid advancement of digital pathology comes with significant challenges due to the diverse data formats from various scanning devices creating substantial obstacles to integrating artificial intelligence (AI) into the pathology imaging workflow. To overcome performance challenges posed by large AI-generated annotations, we developed an open-source project named Mainecoon for whole slide images (WSIs) using the Digital Imaging and Communications in Medicine (DICOM) standard. Our solution incorporates an AI model to detect non-alcoholic steatohepatitis (NASH) features in liver biopsies, validated with the DICOM Workgroup 26 Connectathon dataset. AI-generated results are encoded using the Microscopy Bulk Simple Annotations standard, which provides a standardized method supporting both manual and AI-generated annotations, promoting seamless integration of structured metadata with WSIs. We proposed a method by leveraging streaming and batch processing, significantly improving data loading efficiency, reducing user waiting times, and enhancing frontend performance. The web services of the AI model were implemented via the Flask framework, integrated with our viewer and an open-source medical image archive, Raccoon, with secure authentication provided by Keycloak for OAuth 2.0 authentication and node authentication at the National Cheng Kung University Hospital. Our architecture has demonstrated robustness, interoperability, and practical applicability, addressing real-world digital pathology challenges effectively.

---

### Occupational electronic health records: recommendations for the design and implementation of information systems in occupational and environmental medicine practice-ACOEM guidance statement [^4a904a0e]. Journal of Occupational and Environmental Medicine (2024). High credibility.

Informatics considerations for occupational electronic health records (OEHRs) — regulatory and privacy/communication elements note that the 21st Century Cures Act Final Rule (2020) emphasizes secure and streamlined patient access to personal health data at no cost and prohibits information blocking; the Cures Act promoted a standardized API enabling any computer system, including third-party vendors, to request and gain access to personal electronic health records; most healthcare systems comply via patient-facing online health portals that enable direct access to personal health records and secure communication; to remain compliant, portals should include a messaging platform over secure and encrypted communication channels, and employees should be discouraged from communicating PHI through personal or work email accounts due to less restrictive security protocols and higher liability for privacy breaches under HIPAA; while portals may permit online forms and uploads from wearables and other medical devices, employee health portals should also be careful to discourage downloading PHI on publicly accessible work computers; employer-facing portals allow supervisors and human resources personnel access to limited data on work status and restrictions, and such access controls must be tightly coordinated with human resources to ensure sharing only with current and appropriate supervisors on a need-to-know basis; employer-facing portals may pull limited data from an OEHR through API calls.

---

### SHEA position statement on pandemic preparedness for policymakers: pandemic data collection, maintenance, and release [^dcf48400]. Infection Control and Hospital Epidemiology (2024). High credibility.

SHEA position statement on pandemic data systems — A centralized, national data system should focus on secure data collection, harmonization, and curation with dedicated teams of data scientists and clinical experts. Ideally, the system should be searchable with rapid access to data that can be de-identified, and the ability to retain identifiers in a secure national repository is necessary for future linkage to other data sets or other data elements. Efforts to modernize data management and sharing should balance speed, accuracy, reliability, transparency, and feasibility and should include strategies to "future proof" data, as forecasting the necessary data elements is not always possible. Moving forward, the creation of a national data repository with a wide variety of data elements and types is essential to develop and inform future practice and policy.