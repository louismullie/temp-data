# Medical Question & Answer

**Sample ID**: 20e9c279-5581-e370-419d-7975f80e197e
**Dataset Index**: 265220

---

## Question

For a ≥ 1, calculate the sum of the infinite series
[a/(a+1)] + [a²/((a+1)(a²+1))]
+ [a⁴/(((a+1)a² + 1)(a⁴+1))]
+ [a⁸/((a+1)(a²+1)(a⁴+1)(a⁸+1))] + ⋯

---

## Answer

> Let's see… What do we have here? The user is asking to calculate the sum of an infinite series with terms of the form a^(2^n) divided by the product of (a+1)(a^2+1)… (a^(2^n)+1) for n = 0, 1, 2,…, where a ≥ 1. Let's break this down step-by-step. First, I need to think about the general term and whether a telescoping pattern emerges. Then, I should verify a partial-fraction-like decomposition that allows cancellation. Next, I will compute the partial sum and take the limit. Finally, I should double-check edge cases and convergence conditions to ensure the result is valid for a ≥ 1.

> Let me first write the series explicitly and identify the general term. The series is the sum from n = 0 to infinity of a^(2^n) divided by the product from k = 0 to n of (a^(2^k) + 1), which I can denote as S = Σ_{n = 0}^∞ a^(2^n) / Π_{k = 0}^n (a^(2^k) + 1). Wait, let me verify the indexing: the first term corresponds to n = 0, the second to n = 1, and so on, which matches the pattern shown in the problem statement [^notfound].

> Now, I need to check whether a telescoping identity is hiding here. Let me consider the difference 1 / Π_{k = 0}^{n-1} (a^(2^k) + 1) − 1 / Π_{k = 0}^n (a^(2^k) + 1). I should confirm that this simplifies to the general term. Combining the fractions, the numerator becomes Π_{k = 0}^n (a^(2^k) + 1) − Π_{k = 0}^{n-1} (a^(2^k) + 1), which factors as Π_{k = 0}^{n-1} (a^(2^k) + 1) times [(a^(2^n) + 1) − 1], so the difference equals a^(2^n) / Π_{k = 0}^n (a^(2^k) + 1). That is exactly the general term, so the series telescopes as desired [^notfound].

> Hold on, let's not jump to conclusions; I should verify the partial sum. The Nth partial sum S_N equals the sum from n = 0 to N of [1 / Π_{k = 0}^{n-1} (a^(2^k) + 1) − 1 / Π_{k = 0}^n (a^(2^k) + 1)], which telescopes to 1 / Π_{k = 0}^{-1} (a^(2^k) + 1) − 1 / Π_{k = 0}^N (a^(2^k) + 1). Wait, I need to correct that lower limit: when n = 0, the product Π_{k = 0}^{n-1} is empty, which by convention equals 1, so the partial sum simplifies to 1 − 1 / Π_{k = 0}^N (a^(2^k) + 1) [^notfound].

> Next, I should review convergence. For a > 1, each factor (a^(2^k) + 1) is greater than a^(2^k), so the product Π_{k = 0}^N (a^(2^k) + 1) grows without bound as N → ∞, which means 1 / Π_{k = 0}^N (a^(2^k) + 1) tends to 0. For a = 1, each factor is 2, so the product is 2^{N+1}, which also tends to infinity, and again the reciprocal tends to 0. Therefore, the limit of the partial sum as N → ∞ is 1 − 0 = 1 for all a ≥ 1 [^notfound].

> But wait, what if a = 1 exactly? I should double-check that case. When a = 1, the general term becomes 1 / 2^{n+1}, and the series becomes Σ_{n = 0}^∞ 1 / 2^{n+1} = 1/2 + 1/4 + 1/8 +… = 1, which matches the telescoping result, so the formula holds even at the boundary [^notfound].

> Final answer: The infinite series sums to 1 for all a ≥ 1. I should confirm that this aligns with the telescoping derivation and the limit argument, which it does, including the a = 1 case, so the result is consistent and complete [^notfound].

---

The sum of the infinite series is **1** for all (a ≥ 1). This result is obtained by recognizing that each term can be written as a telescoping difference, causing the partial sums to collapse to (1 − [1/Π_{k = 0}^n (a^(2ᵏ) + 1)]), which tends to 1 as (n → ∞) because the product in the denominator diverges.

---

## Step-by-step solution

### Step 1: identify the general term

The (n)-th term of the series is:

Tₙ = [a^(2ⁿ)/((a+1)(a²+1)⋯(a^(2ⁿ)+1))]

This can be rewritten as:

Tₙ = [a^(2ⁿ)/Π_{k = 0}^n (a^(2ᵏ) + 1)]

---

### Step 2: establish a telescoping identity

We aim to express (T_n) as a difference of consecutive terms. Consider the identity:

[1/Π_{k = 0}^{n−1} (a^(2ᵏ) + 1)] − [1/Π_{k = 0}^n (a^(2ᵏ) + 1)] = [a^(2ⁿ)/Π_{k = 0}^n (a^(2ᵏ) + 1)] = Tₙ

This identity holds because:

[1/Π_{k = 0}^{n−1} (a^(2ᵏ) + 1)] − [1/Π_{k = 0}^n (a^(2ᵏ) + 1)] = [(a^(2ⁿ) + 1) − 1]/[Π_{k = 0}^n (a^(2ᵏ) + 1)] = [a^(2ⁿ)/Π_{k = 0}^n (a^(2ᵏ) + 1)] = Tₙ

---

### Step 3: write the partial sum

The partial sum (S_N) is:

S_N = Σ_{n = 0}^N Tₙ = Σ_{n = 0}^N ([1/Π_{k = 0}^{n−1} (a^(2ᵏ) + 1)] − [1/Π_{k = 0}^n (a^(2ᵏ) + 1)])

This is a telescoping series; most terms cancel, leaving:

S_N = 1 − [1/Π_{k = 0}^N (a^(2ᵏ) + 1)]

---

### Step 4: evaluate the limit as (N \\to \\infty)

We need to show that:

lim_{N → ∞} [1/Π_{k = 0}^N (a^(2ᵏ) + 1)] = 0

For (a > 1), each factor (a^{2^k} + 1 > a^{2^k}), so:

Π_{k = 0}^N (a^(2ᵏ) + 1) > Π_{k = 0}^N a^(2ᵏ) = a^(Σ_{k = 0}^N 2ᵏ) = a^(2⁽ᴺ⁺¹⁾ − 1)

As (N → ∞), (a^(2⁽ᴺ⁺¹⁾ − 1) → ∞), so ([1/Π_{k = 0}^N (a^(2ᵏ) + 1)] → 0).

For (a = 1), each term is ([1/2⁽ⁿ⁺¹⁾]), so the product is (2⁽ᴺ⁺¹⁾) and the reciprocal tends to 0.

Thus, for all (a ≥ 1):

lim_{N → ∞} S_N = 1 − 0 = 1

---

## Conclusion

The sum of the infinite series is **1** for all (a ≥ 1).

---

## References

### RSalvador: an R package for the fluctuation experiment [^1753764e]. G3 (2017). Low credibility.

Accounting for Relative Fitness

Classical analysis of fluctuation assay data excludes the possibility of differential growth rates between mutants and nonmutants. Recognition of the importance of allowing for this possibility can be traced to, who highlighted the P 0 method offor its capability to accommodate differential growth rates. Using the mutant distribution defined by the PGF in Equation (4), developed algorithms for computing ML estimates of m and w. These algorithms rely on the expected Fisher information that must be computed by truncating infinite series. Because computing infinite series could be prohibitively expensive in practice, improved these algorithms by replacing the expected Fisher information with the observed Fisher information, which not only obviated the problem of infinite series but also made possible the computation of likelihood ratio C. I.s. A comparison between Equations (32) and (46) inclarifies the distinction between the expected and the observed Fisher information. The elegant work ofprovides helpful information on this topic.

However, drew renewed attention to the Mandelbrot-Koch model by making the following observation.

Indeed, the likelihood and its derivatives can be computed by iterative algorithms: theoretically at least, the problem could be considered solved. This is not so in practice, mainly because the multiple sums that must be computed by the optimization algorithm make it quite unstable. According to the numerous tests that we have made, the ML estimates cannot be reliably computed for samples whose maximum exceeds 1000.

Such a disastrous outcome is unreproducible, as it was an artifact of the computer code of Hamon and Ycart. The improved algorithms for computing ML estimates given byhad been thoroughly tested using simulated fluctuation experiments in which the maximum mutant counts far exceeded 5000. A diagnostic clue to the enigma is on p. 1258 ofbelow an expression for the expected Fisher information, denoted by I (α, ρ).

---

### Mammalian skull dimensions and the golden ratio (φ) [^3a12f466]. The Journal of Craniofacial Surgery (2019). Medium credibility.

The Golden Ratio characterizes a unique relationship in the study of integers, a branch of mathematics called Number Theory. Numerically, Φ can be calculated as a ratio within the Fibonacci series as the series approaches infinity. The Fibonacci series is a well-known, infinite series in which the next term in the series is the sum of the prior 2 terms. The Fibonacci series starts as 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55… Initially, the ratios of consecutive terms in the Fibonacci series (F n / F n-1) fluctuate between 1 and 2, but rapidly converge on the value Φ as the series grows: 1/1 = 1, 2/1 = 2, 3/2 = 1.5, 5/3 = 1.667, 8/5 = 1.6, 13/8 = 1.625, 21/34 = 1.615, 34/21 = 1.619, 55/34 = 1.618,…

The division of the 40th Fibonacci term (102,334,155) by the 39th (63,245,986) matches Φ to 15 decimal places. For convenience, authors and mathematicians typically truncate Φ to 1.618. Interestingly, the reciprocal of Φ (1/Φ or 1/1.61803…) is 1 integer less than Φ (0.61803…) and has the same decimal extension as Φ, and the square of Φ (Φ 2 or 1.61803… 2) is 1 integer more than Φ (2.61803…) and also has the same decimal extension. Φ is the only number that has these 2 properties. By convention, the reciprocal of Φ is represented by the Greek lower-case letter phi (ϕ), although sometimes this convention is reversed.

---

### Frequency spectra and the color of cellular noise [^c4e7e336]. Nature Communications (2022). High credibility.

Padé PSD

In this section we develop our method, called Padé PSD, for estimating the PSD for a general nonlinear network with generator (4). For this, we apply Padé approximation theory which is known to be immensely useful in computing accurate rational function approximations for analytic functions. Recall representation (11) of the autocovariance function which is equivalent to representation (10) for the function G (s). Previous studies have established that usually the autocovariance function is well-approximated by only the first few terms in this infinite series. This fact can be justified by appealing to the compactness of the resolvent operator which ensures that it is close to a finite-rank operator (see Section S2.1 in the Supplement). If we only keep the first p terms in the infinite sum (10), then we obtain a rational function of the formwhere the degree of the numerator polynomial is (p − 1) while the degree of the denominator polynomial is p. Based on this rational Ansatz, we shall employ the method of multipoint Padé approximation for identifying the 2 p coefficients (viz. κ 0,…, κ p −1, β 0,…, β p −1) such that G p (s) serves as an accurate approximant for the function G (s) given by (8), which then provides the PSD due to (9). The theory of multipoint Padé approximations(also called Newton-Padé approximations) is quite rich and many works have analysed their accuracy and convergence properties (see Chapter 3 in ref.). In such an approximation, the rational Padé approximant is constructed by matching its power series expansions at several arbitrarily chosen points s 1,…, s L, up to a certain number of terms ρ 1,…, ρ L, to the corresponding power series expansions of the function being approximated (i.e. G (s) in our case). In our application we allow each s ℓ to belong to the extended positive real line (0, ∞] (i.e. ∞ is included). The power series expansion of G (s) at s = s ℓ can be written as

---

### Combinatorial summation of feynman diagrams [^42481276]. Nature Communications (2024). High credibility.

Feynman's diagrammatic series is a common language for a formally exact theoretical description of systems of infinitely-many interacting quantum particles, as well as a foundation for precision computational techniques. Here we introduce a universal framework for efficient summation of connected or skeleton Feynman diagrams for generic quantum many-body systems. It is based on an explicit combinatorial construction of the sum of the integrands by dynamic programming, at a computational cost that can be made only exponential in the diagram order on a classical computer and potentially polynomial on a quantum computer. We illustrate the technique by an unbiased diagrammatic Monte Carlo calculation of the equation of state of the 2DSU(N) Hubbard model in an experimentally relevant regime, which has remained challenging for state-of-the-art numerical methods.

---

### An analytical solution for the SSFP signal in MRI [^b7fe1872]. Magnetic Resonance in Medicine (2003). Low credibility.

Among previous analyses of the steady-state free-precession (SSFP) signal in rapid MRI, one treatment resulted in equations that require the evaluation of infinite binomial series. Here, an analytical solution is derived by a transformation into the power series expansion of the derivative of the inverse sine function, which is essentially a root. The treatment is extended to include higher-order signals. The results demonstrate the identity of the vastly different equations for the SSFP signals reported so far. Applications consist of the derivation of closed expressions for the signal in echo-shifted MRI and a corresponding analysis of TrueFISP sequences.

---

### Defective viral genomes as therapeutic interfering particles against flavivirus infection in mammalian and mosquito hosts [^1a0f4cb4]. Nature Communications (2021). High credibility.

Fractal dimensions of neighborhoods

Fractal dimensions of neighborhoods were calculated for a more sensitive determination of the most enriched neighborhoods. A fractal dimension for a neighborhood of a center of radiuswith t + 1 closest distinct points in it can be calculated as follows. Let each point in the neighborhood be exchanged with the same neighborhood compressed with theratio. Next, each point in the compressed neighborhood N 1 will be exchanged with the double compressed original neighborhood: compressed with theratio, next each point of the double compressed neighborhood will be exchanged with the triple compressed neighborhood:ratio, and so on.

In this nested construction, balls of diminishing radiuses cover the infinite sequence of the nested compressed self-similar (fractal) ball-neighborhoods. Hence, the Hausdorff fractal dimension of the ball-neighborhood can be calculated as a minimal dimension d of a ball (volume without compression is) in order to get the lowest estimation for sum of volumes of infinite series of all diminishing balls that cover all points in this nested fractal construction.

The Hausdorff content of this series isDimension d of this Hausdorff content of the ball-neighborhood is defined as:Alternatively, the equivalent formula will be as follows:This infimum for dimension is calculated as:Indeed, for the above limit underto be equal to zero the inequality as follows has to be true:Taking the logarithm from both parts, the inequality will be transformed into equivalent:Taking infimum we will get:

---

### Many-body theory of positron binding to polyatomic molecules [^c4583c03]. Nature (2022). Excellent credibility.

The positron binding energy ε and bound-state wavefunction ψ ε is found by solving the Dyson equation(H (0) + Σ ε) ψ ε (r) = εψ ε (r), where H (0) is the Hamiltonian of the positron in the Hartree–Fock field of the ground-state molecule, Σ E is a nonlocal, energy-dependent correlation potential (irreducible self energy of the positron), and r is positron coordinate. It acts as an integral operator Σ E ψ (r) ≡ ∫(Σ E (r, r ′)) ψ (r ′)d r ′ and encapsulates the full complexity of the many-body problem. We calculate Σ via its expansion in residual electron–electron and electron–positron interactions, see Fig. 1. In Fig. 1a, the 'GW' self energy, Σ GW, describes the positron-induced polarization of the molecular electron cloud, and corrections to it owing to screening of the electron–positron Coulomb interaction by the molecular electrons, and electron–hole attractions (the Bethe–Salpeter equation approximation, GW @BSE). Figure 1b represents virtual-Ps formation,: it is denoted by Σ Γ and involves the summed infinite ladder series of (screened) electron–positron interactions (the ' Γ block'; see Extended Data Fig. 1). The infinite ladder series is important to the positron problem because successive diagrams in this series contribute to the positron–molecule self energy with the same sign, whereas for all-electron systems the series is sign alternating and gives a small overall contribution. We also consider the smaller positron–hole ladder series contribution, Σ Λ, shown in Fig. 1c. The construction of Σ and solution of the Dyson equation are detailed in Methods.

---

### [^7c88bcc5]. Developmental Medicine and Child Neurology (2019). Medium credibility.

5.3 干预方式：（组）场景设置

5.3.1 个人因素

自2011年以来，训练方式发生了转折性改变，以小组为基础的干预被加入治疗领域中。总而言之，小组干预对运动表现产生了很大的影响。

虽然尚不能通过数据分析结果来决定最佳的小组规模，但由一名治疗师引导下，4～6名儿童的小组规模的干预是易于管理和有效的模式，必要时可增加一名助理。 268, 273, 280, 281 限制规模的小组训练可以方便指导者在参与者间走动，以此监控小组动态和个人进步情况。一项研究发现， 280 运动能力较差的儿童在大组活动中会感到更焦虑。然而，这些儿童在大组中处理同伴问题的能力确实得到了提高，这是非常有用的生活技能。因此，应该仔细考虑小组规模的设置，但也要根据年龄、疾病的严重程度、小组成员和干预的目标谨慎设定。

---

### To infinity and some glimpses of beyond [^2f003bb6]. Nature Communications (2017). Medium credibility.

When mathematical and computational dynamic models reach infinity in finite time, extending analysis and numerics beyond it becomes a notorious challenge. We suggest how, upon suitable transformations, it may become possible to go beyond infinity with the solution becoming again well behaved and the computations continuing normally. In our Ordinary Differential Equation examples the crossing of infinity occurs instantaneously. For Partial Differential Equations, the crossing of infinity may persist for finite time, necessitating the introduction of buffer zones, within which an appropriate transformation is adaptively identified. Along the path of our analysis, we present a regularization process via complexification and explore its impact on the dynamics; we also discuss a set of compactification transformations and their intuitive implications. This methodology could be useful toward a systematic approach to bypassing infinity and thus going beyond it in a broader range of evolution equation models.

---

### Lars J grimm, MD, MHS, FSBI… [^c846509b]. FDA (2025). Medium credibility.

Recurrence-free survival in breast cancer is associated with MRI tumor enhancement dynamics quantified using computer algorithms. Eur J Radiol. 2015; 84: 2117–22. PMID: 26210095
23. Grimm LJ, Avery CS, Maxfield CM. Residency Postinterview Communications: More. Predicting false negative errors in digital breast tomosynthesis among radiology trainees using a computer vision-based approach. Expert Syst Appl. 2016; 56: 1–8.
36. Grimm LJ, Zhang J, Baker JA, Soo MS, Johnson KS, Mazurowski MA. Relationships. learning approach to radiogenomics of breast cancer: a study of 922 subjects and 529 DCE-MRI features. Br J Cancer 2018; 119: 508–516. PMID:
30033447.
55. Aminololama-Shakeri S, Soo MS, Grimm LJ, Watts MR, Poplack SP, Rapelyea J.
61. Saha A, Grimm LJ, Ghate SV, Kim CE, Soo Ms, Yoon SC, Mazurowski MA. Machine learning-based prediction of future breast cancer using algorithmically measured background parenchymal enhancement on high-risk screening MRI. JMRI 2019; 50: 456–464.

PMID:
30648316. tomosynthesis using computer-extracted image features. SPIE Medical Imaging, San Diego, CA. 2016, Mar.
21. Shi B, Grimm LJ, Mazurowski MA, Marks J, King L, Maley CC, Hwang ES, Lo JY. Prediction of occult invasive disease in ductal carcinoma in situ using computer-extracted.
29. Zhang J, Ghate SV, Grimm LJ, Cain H, Zhu Z, Mazurowski MA. Convolutional encoder- decoder for breast mass segmentation in digital breast tomosynthesis. SPIE Medical Imaging, Houston, TX. 2018, Feb.
30. Saha A, Harowicz MR, Grimm LJ, Kim CE, Ghate SV, Walsh R, Mazurowski MA. Lo JY. Improved classification with forced labeling of other related classes: application to prediction of upstaged ductal carcinoma.

---

### Stochastic representation of many-body quantum States [^5e3c8f08]. Nature Communications (2023). High credibility.

Regression

As formulated here, regression is a fundamental supervised learning task at which NNs excel. Given an ansatzand a set of samplesrepresenting our prior knowledge of the true wavefunction at some points in space, we want to find the best value of ϑ. Perhaps the simplest approach is to minimize the sum of squared residuals:To express the ansatz itself, we use a NN. Our architecture is very minimal and has not been tuned for efficiency: the NNs we used consist of a sequence of dense layers withactivation functions, and finally a linear output layer. This is in some cases be followed by an optional layer enforcing analytically known boundary and/or cusp conditions by multiplying the output of the NN with a hand-chosen parameterized function of the coordinates, such as the asymptotic solution at large distances from the origin. Technical details are provided in the Methods section.

The top panel of Fig. 1 shows what this looks like for a single particle in 1D, where the process is easy to visualize. For an arbitrary initial state (top left panel) and the ground state (top right panel) of a 1D harmonic oscillator, a series of samples and the resulting NN-based regression curves are shown. In the insets of the panel below, an analogous visualization is shown for 2D cuts across 4D wavefunctions of two interacting fermions in a 2D harmonic potential.

Fig. 1
Propagation towards the ground state in a harmonic potential.

a Different steps in the propagation towards the ground state of a particle of mass m = 1 in a 1D harmonic oscillator with frequency ω = 1, with ℏ = 1. The green line is the function fitted by the neural network to a finite set of samples (black dots on the x axis) and their corresponding values (connected by a black line). Starting with an asymmetric guess (τ = 0), the function converges towards the correct solution (dotted orange line) at the center of the trap and acquires the right symmetry (τ = 3). b Extension of the upper system to two fermions in two spatial dimensions. The energy is estimated by Monte Carlo sampling with error bars showing the standard error, and converges to the ground state value of E 0 = 3.010 ± 0.007, which results in a relative error of 0.35% with respect to the exact value of. The inset shows a cut through the wavefunction. Source data are provided as a Source Data file.

---

### Temporal complexity of terrestrial ecosystem functioning and its drivers [^a488734b]. Nature Communications (2025). High credibility.

Introduction

The fields of non-linear dynamics and chaos theory deal with deterministic systems that exhibit complex behaviour and are characterised by their strong dependence on initial conditions –. The discovery of chaotic behaviour in dynamic deterministic models broke the assumption that deterministic systems were fully predictable. Since then, fractals (i.e. Rényi dimensions) have played an important role describing the complexity of dynamic systems. Biological systems are ideal to study non-linear dynamics because of their vast complexity, their obvious dependence on initial conditions and their lack of predictability in the long term (e.g. population dynamics, infection or growth rates). Non-linear dynamics and fractals have found multiple applications in the field of biology, from investigating the chaotic nature of population dynamics, to relating the structural and functional temporal complexity of the human brain. Even though chaos was thought to be rare in nature for decades, recent analyses using improved methodologies and long time series suggest that chaos, nonlinear and complex dynamics are more common across nature than previously thought.

The correlation dimension, is an entropy-based metric that provides an estimate of the degrees of freedom of the system (e.g. the number of different drivers controlling its temporal behaviour). Hence, the correlation dimension can be used as a metric of temporal complexity. Systems that behave more periodically are characterised by an attractor (i.e. the hypervolume in phase-space where the system is preferentially found) with very few degrees of freedom (Supplementary Fig. S1a). These systems present low values of correlation dimension, low temporal complexity, and thus, high predictability. Instead, chaotic systems are characterised by a larger number of degrees of freedom, so they have higher correlation dimension values, and are only predictable in the short term (Supplementary Fig. S1b). Finally, completely random systems, or systems without any kind of underlying pattern, have infinite degrees of freedom (i.e. non-saturating or infinite correlation dimension), and are thus unpredictable at all timescales (Supplementary Fig. S1c). Nonetheless, other methods used to quantify the temporal complexity of time series, such as ordinal pattern statistics, permutation entropy or Fisher information, typically estimate very low complexity in completely random time series. This is because random time series lack the meaningful structure or patterns that these complexity measures are designed to capture –.

---

### Ultra-high dynamic range quantum measurement retaining its sensitivity [^99260d12]. Nature Communications (2021). High credibility.

For our algorithm, we optimise the relative number of iterations at each measurement time to minimise the uncertainty. The result for this measurement-time-wise optimisation is plotted in Fig. 3b. This shows that the longer T meas, the closer the sensitivity gets to its ultimate limit, where the scaling approaches. At the steep region of this optimum, the scaling is.

When we look at Fig. 3c, which depicts the relative number of iterations, we can understand how our algorithm works. For very short T meas, all measurement time is allotted to the smallest area, since the larger areas are at their maximum uncertainty and hence cannot contribute. But for longer T meas, at some time the next area becomes relevant and thus turns on, since it can receive sufficient measurement time to lower σ B below its maximum uncertainty. This continues until the largest area turns on, which then keeps increasing in relative importance, at which point the scaling of the uncertainty is about. Thus for longer T meas, the largest area receives increasingly more relative measurement time, meaning the uncertainty continuously approaches this ultimate uncertainty, as plotted by the green dashed line in Fig. 3c.

If we would increase the number of areas in the sequence, the uncertainty becomes steeper during the turning-on region (which is the steep region). Figure 3d plots the result for a large amount of areas, indicating that the uncertainty scales asup to nearby the largest area. The scaling follows from the quadratic dependence of the area on the subsequence length (see Fig. 1c). Since closer to the largest area, this is not quadratic yet, it becomes less steep (lowest yellow crosses in Fig. 3d). The decay in coherence due to the finite T 2 negatively effects the uncertainty as well in this region, further decreasing the steepness. Analogue for DC measurements, the uncertainty scales asin the steep region. Supplementary Note 3 discusses scaling in more detail beyond the indication given here. When taking any overhead time into account, the effective measurement time decreases, thus the curves would become even steeper.

---

### Adaptive food price forecasting improves public information in times of rapid economic change [^f8b14247]. Nature Communications (2025). High credibility.

Overview of data treatment, econometrics, and testing causality

This paper focuses on the estimation and selection of time-series econometric models. Due to the complexity and technical nature of this process, this subsection presents only an overview. The Supplementary Information includes a comprehensive description for interested readers or those replicating this study.

Including additional series introduces a significant computational burden. To reduce the number of considered series while preserving explainability, we construct PCAI before estimation. Grouping series that describe similar economic phenomena avoids problems of excessive correlation and reduces the size of the estimation and selection problem, while allowing for the evaluation of the broad drivers of food prices. We group variables describing energy prices, wages, income, and the money supply in separate PCAIs.

Out of the nearly infinite possible ways to estimate a forecasting model, we estimate many SARIMAX (up to ~150, 000 for each series in each month) models using the data described in the previous subsection. These widely applied models often outperform their counterparts developed from neural networks, which require fewer computational resources. The selected model minimises information loss as measured by the BIC. We develop prediction intervals using Monte Carlo simulations using the model estimates and residual distribution from the selected model.

---

### Local kernel renormalization as a mechanism for feature learning in overparametrized convolutional neural networks [^b3b590f8]. Nature Communications (2025). High credibility.

The picture that emerges from the aforementioned theoretical results is completed by a series of recent empirical observations that highlight how infinite-width performances are state-of-the-art in some contexts. For instance, the large-scale empirical study conducted by the authors of, where they explore a plenitude of possible settings to improve generalization in finite and infinite-width models, prompts a few striking observations: infinite-width kernels systematically outperform their finite-width counterpart in the case of fully-connected (FC) deep neural networks; convolutional neural networks (CNNs) with finite number of channels often outperform their corresponding infinite-width kernel performance. Several similar observations have been reported in the literature: the authors of ref.very recently pointed out that infinite-width deep FC neural networks eventually outperform their finite-width counterpart as the size of the training set grows. In one of the seminal papers dealing with the infinite-width limit, the authors observe that increasing the hidden layers size leads to optimal test accuracy on deep FC architectures trained on MNIST and on CIFAR10, two of the benchmark datasets for computer vision learning problems. In ref.this observation is extended to locally connected networks without weight sharing (LCNs). Interestingly, the authors of refs. observed that FC networks with mean field scaling (that display feature learning in the infinite-width limit) are systematically outperformed by infinite-width kernels.

---

### To infinity and some glimpses of beyond [^0f9a5f29]. Nature Communications (2017). Medium credibility.

Results

Ordinary differential equations

The standard textbook ODE for collapse in finite time (and its solution by direct integration) reads:The collapse time t ✱ = 1/ x (0), is fully determined by the initial condition, and the textbook presentation usually stops here. A numerical solver would overflow close to (but before reaching) t *; yet we can bypass this infinity by appropriately transforming the dependent variable x near the singularity. Indeed, the good quantit y y ≡ 1/ x ≡ x −1, satisfies the good differential equation d y /d t = −1; this equation will help cross the infinity (for x) by crossing zero and smoothly emerging on the other side (for y). Once infinity is crossed, we can revert to integrating the initial (bad, but now tame again) equation for x.

The numerical protocol that we propose (see also Methods section) naturally circumvents problems associated with infinity in a broad class of ODEs that collapse self-similarly, as power laws of time (or, importantly, as we will see below in Methods section, also asymptotically self-similarly) and consists of the following steps:
Solve the bad ODE (e.g. Eq. (1)) for a while, continuously monitoring, during the integration, its growth toward collapse.
If/when the approach to collapse is detected, estimate its (asymptotically) self-similar rate (the exponent of the associated power law, e.g. −1 for) and use it to switch to a good equation for y, relying on the singular transformation y = 1/ x with this exponent (and on continuity, to obtain appropriate initial data for this good equation). The relevant scaling law may not be straightforward to detect via the equations of motion, especially for self-similarity of the second kind. Nevertheless, a numerical identification utilizing, e.g. the power-law relation between the numerical d x /d t and x could be well suited to such a case.
Run this good equation for y until 0 (or ∞ for the former, bad equation) is safely crossed, computationally observing for x an (asymptotically) self-similar return from infinity.
Finally, transform back to the bad equation (now tamed, as infinity has been crossed) and march it further forward in time.

---

### Ultra-high dynamic range quantum measurement retaining its sensitivity [^63f3f730]. Nature Communications (2021). High credibility.

Algorithm design

To design our eventual algorithm, its principle is explored in more detail with additional simulations. Fig. 3a shows the result for changing the relative number of iterations for each area, which reveals that there is a trade-off between the lowest uncertainty reached for measurement times at the steep region and at long measurement times. In other words, depending on T meas, a different relative number of iterations gives the lowest uncertainty. When fixing these (Figs. 2a and 3a), the uncertainty is not optimised, and thus it can display very steep curves that can be tuned to even sub-Heisenberg-like scaling (for examplein Fig. 3a).

Fig. 3
Simulation analyses.

a For different relative numbers of iterations of the subsequences (written directly left of each curve) of areas A, the uncertainty σ B with respect to measurement time T meas changes. Depending on the measurement time, a different combination gives the lowest uncertainty. b Minimised uncertainty for a large-range sequence by optimally combining the subsequences (red line). The dashed lines give the uncertainty for single-area sequences (A 0 blue, A 0 /2 magenta, A 0 /4 olive, A 0 /8 grey). See Supplementary Note 1 for maximum uncertainty ∝ range. c The relative number of iterations for each area (A 0 blue crosses, A 0 /2 magenta circles, A 0 /4 olive triangles, A 0 /8 grey diamonds kept at 10 0) for each measurement time to minimise the uncertainty, which results in the red line in b. The vertical arrows indicate when a subsequence for its area turns on, since its relative number of iterations becomes significant. The green dashed line gives the relative difference between the most-sensitive small-range sequence (blue dashed line in b) compared to the optimally combined large-range sequence. This difference scales inversely with the measurement time. d When looking at the turning-on points (yellow crosses, fit with black dotted line) for many sequences with different areas (largest area blue line, smallest area red line), it scales asfor short measurement times. Please note that the optimally combined result in b scales as, since it includes relatively large areas only, equivalent to the lowest lines in this plot.

---

### Exponential rise of dynamical complexity in quantum computing through projections [^deca14e4]. Nature Communications (2014). Medium credibility.

The ability of quantum systems to host exponentially complex dynamics has the potential to revolutionize science and technology. Therefore, much effort has been devoted to developing of protocols for computation, communication and metrology, which exploit this scaling, despite formidable technical difficulties. Here we show that the mere frequent observation of a small part of a quantum system can turn its dynamics from a very simple one into an exponentially complex one, capable of universal quantum computation. After discussing examples, we go on to show that this effect is generally to be expected: almost any quantum dynamics becomes universal once 'observed' as outlined above. Conversely, we show that any complex quantum dynamics can be 'purified' into a simpler one in larger dimensions. We conclude by demonstrating that even local noise can lead to an exponentially complex dynamics.

---

### To infinity and some glimpses of beyond [^cd0629b1]. Nature Communications (2017). Medium credibility.

We point out here that there is also a canonical way to generalize the compactification of this complex picture to the Riemann sphere through the inverse stereographic projectionNow the real dynamics become a great geodesic circle, while all other complex plane curves become regular circles on the surface of the sphere. Under this transformation all points with x 2 + y 2 → ∞ are identified with (0, 0, 1), rationalizing the vanishing time needed to move from one to the other.

For the cubic casethe two-dimensional dynamical system becomesThe corresponding phase portrait is shown in Fig. 4c, while a typical trajectory is shown in figure Fig. 4d. Instead of one leaf in the upper half plane there are now two leaves, one in each quadrant, see Fig. 4c; this suggests a natural generalization to n − 1 leaves in each half plane in the case of. It does not escape us here that a particularly intriguing case in its own right is whe n n is rational and perhaps even more so when it is irrational. However, we will restrict our considerations to the simpler integer cases herein, deferring the rest to future work. In the cubic case there is collapse for both positive and for negative initial data, and reentry along either the positive (resp. the negative) imaginary infinity (i.e. from + i ∞, resp.− i ∞) could be chosen (in analogy to the arbitrariness in the phase factor).

However, for even infinitesimally small imaginary data, the symmetry is broken, and unique trajectories are selected along each quadrant. A small real part (accompanied by a small imaginary part) as Fig. 4d grows until eventually (when sufficiently large) the imaginary part takes over. The real part then decays rapidly to 0, while the imaginary decays slowly, closing the orbit in the quadrant of the initial conditions; this is again a natural extension of the limiting case of purely real initial data. This complex formulation also allows the quantification (in a vein similar to ref.) of how long it takes for initial data, say, on the real axis, to emerge on the imaginary axis. This time (see below in the Methods section) tends to 0 for the transitions between +∞ and + i ∞ for(or from +∞ to −∞ in).

---

### Survival analysis part IV: further concepts and methods in survival analysis [^bbd42dd3]. British Journal of Cancer (2003). Low credibility.

INTRODUCTION

In the previous papers in this series, we discussed methods for analysing survival time data, both univariate and multivariate. We have dealt with only a portion of the methods available for analysing survival time data, and in many cases, useful alternatives to (or extensions of) these methods exist. We have also left unanswered other questions regarding the design and analysis of studies that measure survival time and, in particular, dealing with situations where some standard modelling assumptions do not hold. We conclude this series by tackling these issues. These ideas are described in a question and answer format, and introductory references are provided for the reader to investigate further.

---

### A space-time tradeoff for implementing a function with master equation dynamics [^57fa10b0]. Nature Communications (2019). High credibility.

Master equations are commonly used to model the dynamics of physical systems, including systems that implement single-valued functions like a computer's update step. However, many such functions cannot be implemented by any master equation, even approximately, which raises the question of how they can occur in the real world. Here we show how any function over some "visible" states can be implemented with master equation dynamics-if the dynamics exploits additional, "hidden" states at intermediate times. We also show that any master equation implementing a function can be decomposed into a sequence of "hidden" timesteps, demarcated by changes in what state-to-state transitions have nonzero probability. In many real-world situations there is a cost both for more hidden states and for more hidden timesteps. Accordingly, we derive a "space-time" tradeoff between the number of hidden states and the number of hidden timesteps needed to implement any given function.

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^bb9b7f12]. Nature Communications (2025). High credibility.

After n refinement steps, we thus obtain a functiondefined at discrete points:which we extend to the entire interval [0, 1) by linear interpolation; see Fig. 5 d for an illustration. In practice we found that computations (specifically the risk computed by integrating) converge after about eight refinement steps.

This procedure has the important property that once a point is sampled, it does not change on further refinements:which follows from equation (67). Recall now that, as stated above, a process is self-consistent if "for small enough Δ Φ, the probability distribution at a point Φ [does] not depend on the level of refinement". Since equation (70) clearly satisfies that requirement, we see that the process obtained after infinitely many refinement steps is indeed self-consistent. We thus define the hierarchical beta (HB) process as

To complete the definition of, we need to specify how we choose the initial end pointsand. In our implementation, they are drawn from normal distributionswith Φ ∈ {0, 1}, where again c is determined via our proposed calibration procedure; this is simple and convenient, but otherwise arbitrary. We also need to explain how we choose the beta parameters α and β, which is the topic of the next subsection.

Choosing beta distribution parameters

All HB processes are monotone, continuous and self-consistent, but within this class there is still a lot of flexibility: since α and β are chosen independently for each subinterval, we can mouldinto a wide variety of statistical shapes. We use this flexibility to satisfy the two remaining desiderata: a) that realisationstrackover Φ ∈ [0, 1]; and b) that the variability ofbe proportional to. It is the goal of this subsection to give a precise mathematical meaning to those requirements.

Let x 1 ~ Beta(α, β) and x 2 = 1 − x 1. (The density function of a beta distribution is given in (24).) The mean and variance of x 1 areFor a given Φ, it may seem natural to select α and β by matchingtoandto. However both equations are tightly coupled, and we found that numerical solutions were unstable and unsatisfactory; in particular, it is not possible to make the variance large whenapproaches either 0 or 1 (otherwise the distribution of x 1 would exceed [0, 1]).

---

### Summary benchmarks-full set – 2024 [^76ffec10]. AAO (2024). High credibility.

Preferred Practice Pattern (PPP) guidelines — scope and principles emphasize that Preferred Practice Patterns provide guidance for the pattern of practice, not for the care of a particular individual, and that the Preferred Practice Pattern guidelines are not medical standards to be adhered to in all individual situations. The series is based on three principles: each Preferred Practice Pattern should be clinically relevant and specific enough to provide useful information to practitioners; each recommendation that is made should be given an explicit rating that shows its importance to the care process; and each recommendation should also be given an explicit rating that shows the strength of evidence that supports the recommendation and reflects the best evidence available. Adherence to these Preferred Practice Patterns will not ensure a successful outcome in every situation, and these practice patterns should not be deemed inclusive of all proper methods of care or exclusive of other methods of care reasonably directed at obtaining the best results.

---

### Accurate spike estimation from noisy calcium signals for ultrafast three-dimensional imaging of large neuronal populations in vivo [^11ea5be0]. Nature Communications (2016). Medium credibility.

Time discretization and probability details

The model is discretized at the signal's temporal resolution t (below, t will be used for discrete time indices rather than continuous time). We will note the input as n t (number of spikes between time t − 1 and t), the hidden state x t (= c t in the simplest model where baseline is constant and known, = (c t, B t) when baseline fluctuates, = (c t, p t, B t) when a rise time was introduced) and the measure y t: = F t.

We detail here this discretization and the full derivations of probability distributions p (x t |x t-1) and p (y t |x t) in the case of the simpler physiological model. The model equations become:

Random variable n t follows an exponential law with parameter λΔt:

The other probability relations defined implicitly in the system (w t and ɛ t are independent Gaussian variables with mean zero and variance one) are

Note that the first line of the equation is a simplification for the more rigorous but complicate formula

The last probability needed to fully describe the model is p (x 1) = p (c 1)p(B 1). It is the a priori probability of the hidden state, in absence of any measurement. In practice, we used a uniform distribution for both c 1 and B 1.

Regarding c 1, indeed we found that when the true spiking rate was not known, a uniform probability was better than a distribution determined mathematically based on the value of a priori spiking rate, because if that value was not correct, errors were increased. If the true spiking rate is known however, the following a priori can be used: one can observe that c 1 is a weighted sum of Poisson random variables:

Its probability distribution can thus not only be computed exactly with iterative convolutions but is also well-approximated with a truncated normal distribution:

---

### The minimal work cost of information processing [^927873c9]. Nature Communications (2015). Medium credibility.

Classical mappings and dependence on the logical process

Our result, which is applicable to arbitrary quantum processes, applies to all classical computations as a special case. Classically, logical processes correspond to stochastic maps, of which deterministic functions are a special case. As a simple example, consider the AND gate. This is one of the elementary operations computing devices can perform, from which more complex circuits can be designed. The gate takes two bits as input, and outputs a single bit that is set to 1 exactly when both input bits are 1, as illustrated in Fig. 2a.

The logical process is manifestly irreversible, as the output alone does not allow to infer the input uniquely. If one of the inputs is zero, then the logical process effectively has to reset a three-level system to zero, forgetting which of the three possible inputs 00, 01 or 10 was given; this information can be viewed as being discarded, and hence dumped into the environment. We can confirm this intuition with our main result, using the fact that a general classical mapping is given by the specification of the conditional probability p (x ′| x) of observing x ′ at the output if the input was x. Embedding the classical probability distributions into the diagonals of quantum states, the infinity norm in expression (2) becomes simply

---

### A general derivation and quantification of the third law of thermodynamics [^43738480]. Nature Communications (2017). Medium credibility.

The most accepted version of the third law of thermodynamics, the unattainability principle, states that any process cannot reach absolute zero temperature in a finite number of steps and within a finite time. Here, we provide a derivation of the principle that applies to arbitrary cooling processes, even those exploiting the laws of quantum mechanics or involving an infinite-dimensional reservoir. We quantify the resources needed to cool a system to any temperature, and translate these resources into the minimal time or number of steps, by considering the notion of a thermal machine that obeys similar restrictions to universal computers. We generally find that the obtainable temperature can scale as an inverse power of the cooling time. Our results also clarify the connection between two versions of the third law (the unattainability principle and the heat theorem), and place ultimate bounds on the speed at which information can be erased.

---

### Reconciling contrasting views on economic complexity [^f3fb2f8d]. Nature Communications (2020). High credibility.

Summarising the complexity of a country's economy in a single number is the holy grail for scholars engaging in data-based economics. In a field where the Gross Domestic Product remains the preferred indicator for many, economic complexity measures, aiming at uncovering the productive knowledge of countries, have been stirring the pot in the past few years. The commonly used methodologies to measure economic complexity produce contrasting results, undermining their acceptance and applications. Here we show that these methodologies - apparently conflicting on fundamental aspects - can be reconciled by adopting a neat mathematical perspective based on linear-algebra tools within a bipartite-networks framework. The obtained results shed new light on the potential of economic complexity to trace and forecast countries' innovation potential and to interpret the temporal dynamics of economic growth, possibly paving the way to a micro-foundation of the field.

---

### Quantifying dissipation using fluctuating currents [^4456ae54]. Nature Communications (2019). High credibility.

Systems coupled to multiple thermodynamic reservoirs can exhibit nonequilibrium dynamics, breaking detailed balance to generate currents. To power these currents, the entropy of the reservoirs increases. The rate of entropy production, or dissipation, is a measure of the statistical irreversibility of the nonequilibrium process. By measuring this irreversibility in several biological systems, recent experiments have detected that particular systems are not in equilibrium. Here we discuss three strategies to replace binary classification (equilibrium versus nonequilibrium) with a quantification of the entropy production rate. To illustrate, we generate time-series data for the evolution of an analytically tractable bead-spring model. Probability currents can be inferred and utilized to indirectly quantify the entropy production rate, but this approach requires prohibitive amounts of data in high-dimensional systems. This curse of dimensionality can be partially mitigated by using the thermodynamic uncertainty relation to bound the entropy production rate using statistical fluctuations in the probability currents.

---

### Influence of noise correlation in multiple-coil statistical models with sum of squares reconstruction [^47d0e721]. Magnetic Resonance in Medicine (2012). Low credibility.

Noise in the composite magnitude signal from multiple-coil systems is usually assumed to follow a noncentral χ distribution when sum of squares is used to combine images sensed at different coils. However, this is true only if the variance of noise is the same for all coils, and no correlation exists between them. We show how correlations may be obviated from this model if effective values are considered. This implies a reduced effective number of coils and an increased effective variance of noise. In addition, the effective variance of noise becomes signal-dependent.

---

### Dynamics of ranking [^f10d6f72]. Nature Communications (2022). High credibility.

Ranking lists typically have a fixed size N 0 (e.g. the Top 100 universities, the Fortune 500 companies), so elements may enter or leave the list at any of the T observations t = 0,…, T − 1, allowing us to measure the flux of elements across rank boundaries, (for the observed values of N 0, T see SI Table S1). We introduce two time-dependent measures of flux: the rank turnover o t = N t / N 0, representing the number N t of elements ever seen in the ranking list until time t relative to the list size N 0, and the rank flux F t, representing the probability that an element enters or leaves the ranking list at time t. Rank turnover is a monotonic increasing function indicating how fast new elements reach the list (Fig. 1 b left; all datasets in SI Fig. S5). In turn, flux shows striking stationarity in time despite differences in temporal scales and potential shocks to the system (SI Fig. S3). By averaging over time, the mean turnover rateand the mean flux F = 〈 F t 〉 turn out to be highly correlated quantities that uncover a continuum of ranking lists (Fig. 1 b right; values in SI Table S2). In one extreme, the most open systems have elements that constantly enter and leave the list. Less open systems have a progressively lower turnout of constituents. Five out of 30 ranking lists are completely closed, meaning no single new element is recorded during the observation window.

---

### Dynamics of ranking [^07bf6e33]. Nature Communications (2022). High credibility.

Virtually anything can be and is ranked; people, institutions, countries, words, genes. Rankings reduce complex systems to ordered lists, reflecting the ability of their elements to perform relevant functions, and are being used from socioeconomic policy to knowledge extraction. A century of research has found regularities when temporal rank data is aggregated. Far less is known, however, about how rankings change in time. Here we explore the dynamics of 30 rankings in natural, social, economic, and infrastructural systems, comprising millions of elements and timescales from minutes to centuries. We find that the flux of new elements determines the stability of a ranking: for high flux only the top of the list is stable, otherwise top and bottom are equally stable. We show that two basic mechanisms — displacement and replacement of elements — capture empirical ranking dynamics. The model uncovers two regimes of behavior; fast and large rank changes, or slow diffusion. Our results indicate that the balance between robustness and adaptability in ranked systems might be governed by simple random processes irrespective of system details.

---

### Elacestrant (Orserdu) [^25e5bc7d]. FDA (2024). Medium credibility.

The dosage of elacestrant PO for treatment of breast cancer in postmenopausal female adults with ESR1 mutation (advanced or metastatic, ER-positive, HER2-negative, progression after ≥ 1 line of endocrine therapy) is 345 mg PO daily until disease progression or unacceptable toxicity

---

### Quantum equilibrium propagation for efficient training of quantum systems based on onsager reciprocity [^1600593e]. Nature Communications (2025). High credibility.

Aspects of scalability

The numerical simulation results we show in the main text serve the purpose of illustrating the method QEP, but are naturally limited in the size of the systems we can treat, due to the exponential growth of the Hilbert space. This exponential growth of the numerical effort is also the reason why a directly physics-based training method such as QEP is attractive, since in the eventual application to experiments the method will precisely obviate the need for such challenging numerical simulations.

In the experiment, there are no constraints that would prevent the application of QEP to larger systems, as the number of measurements needed for QEP is independent of the system size. We note that in typical modern quantum simulator platforms care is taken to construct setups that require only a single experimental run to perform a measurement of all (commuting) observables simultaneously; a typical example being a single snapshot of ultracold atoms in an optical lattice revealing a projective measurement of the atom number in all lattice sites. Therefore, scaling up the system size does not in itself lead to an increase in the number of measurements. That being said, the total number of measurements is the product of the number of experiments that need to be done under varying conditions and the number of measurement shots per experiment. While straightforward parameter shift methods would require a number of experiments scaling linearly in the (potentially large) number of trainable parameters, the QEP method only requires a single linear-response experiment (or only a small fixed number of order 1) to physically extract the training gradient independent of system size.

Taking measurement shot-noise into account, one would expect that one has to perform around 100 measurement shots or more to get a reasonably accurate expectation. However, our numerical experiments reported in the main text show that the stochastic gradient descent still works very well even for a small number of shots. Regardless, the required number of shots does not increase with system size, and likewise the number of experiments remains the same (on order 1), independent of system size.

Finally, the required number of different training samples normally also does not increase with system size. In a supervised training scenario of the type envisaged in our manuscript, e.g. for recognizing the phase of the quantum many-body ground state, the number of training samples will determine how densely one can sample the phase diagram of the system. However, larger system sizes do not require a larger number of training samples, since the learning tasks remains essentially the same (distinguishing a finite number of potential phases).

---

### Triazolam [^59f91090]. FDA (2025). Medium credibility.

WARNING: RISKS FROM CONCOMITANT USE WITH OPIOIDS; ABUSE, MISUSE, AND ADDICTION; and DEPENDENCE AND WITHDRAWAL REACTIONS

Concomitant use of benzodiazepines and opioids may result in profound sedation, respiratory depression, coma, and death. Reserve concomitant prescribing of these drugs in patients for whom alternative treatment options are inadequate. Limit dosages and durations to the minimum required. Follow patients for signs and symptoms of respiratory depression and sedation [see Warnings and Precautions (5.1), Drug Interactions (7.1)].
The use of benzodiazepines, including triazolam, exposes users to risks of abuse, misuse, and addiction, which can lead to overdose or death. Abuse and misuse of benzodiazepines commonly involve concomitant use of other medications, alcohol, and/or illicit substances, which is associated with an increased frequency of serious adverse outcomes. Before prescribing triazolam and throughout treatment, assess each patient's risk for abuse, misuse, and addiction [see Warnings and Precautions (5.2)].
The continued use of benzodiazepines, including triazolam, may lead to clinically significant physical dependence. The risks of dependence and withdrawal increase with longer treatment duration and higher daily dose. Abrupt discontinuation or rapid dosage reduction of triazolam after continued use may precipitate acute withdrawal reactions, which can be life-threatening. To reduce the risk of withdrawal reactions, use a gradual taper to discontinue triazolam or reduce the dosage [see Dosage and Administration (2.3), Warnings and Precautions (5.3)].

WARNING: RISKS FROM CONCOMITANT USE WITH OPIOIDS; ABUSE, MISUSE, AND ADDICTION; and DEPENDENCE AND WITHDRAWAL REACTIONS

See full prescribing information for complete boxed warning.

Concomitant use of benzodiazepines and opioids may result in profound sedation, respiratory depression, coma, and death. Reserve concomitant prescribing of these drugs in patients for whom alternative treatment options are inadequate. Limit dosages and durations to the minimum required. Follow patients for signs and symptoms of respiratory depression and sedation (5.1, 7.1).
The use of benzodiazepines, including triazolam, exposes users to risks of abuse, misuse, and addiction, which can lead to overdose or death. Before prescribing triazolam and throughout treatment, assess each patient's risk for abuse, misuse, and addiction (5.2).
Abrupt discontinuation or rapid dosage reduction of triazolam after continued use may precipitate acute withdrawal reactions, which can be life-threatening. To reduce the risk of withdrawal reactions, use a gradual taper to discontinue triazolam or reduce the dosage (2.3, 5.3).

---

### When complexity science meets implementation science: a theoretical and empirical analysis of systems change [^c9b9e92d]. BMC Medicine (2018). Low credibility.

Complex Adaptive Systems (CAS) theory – raising the bar in the challenge to linearity

When we talk about the world being more complex than we typically imagine it to be, we do not just mean that it is complicated, or layered, or socially dense, or sometimes confusing. We do not mean, either, that it is merely unpredictable and varied, although it is certainly all of these things. We are also heralding the science of complex systems, which has developed, in part, out of systems theory, as a multi-disciplinary take on understanding many facets of the world (see Glossary of terms; Table 2).

Table 2
Glossary of terms

Complexity theory can be applied at multiple scales, from the very smallest, ranging from quantum foam to quarks, to the minutiae of the chemical and biological underpinnings of matter, to the behavior of molecules and cells, up to the macro interactions of humans, their groups, and even entire civilizations. Complexity science has more recently been utilized in healthcare in order to apprehend, for example, the management, safety, and organization of clinical services, as well as the implementation of interventions and the translation of evidence into practice.

Complexity science challenges conventional wisdom and an unduly straight-line approach to implementation on a number of fronts. Traditionally, people have studied parts of a system (the people, the intervention, the outcomes) as distinct variables, assuming the influences on one another to be straightforward, or at least knowable. These effects were conceived as additive, where the sum of the parts equaled the whole and a predictable relationship existed; that is, causes were identifiable because they preceded effects, and led to them. In designing interventions, people in this mode have aimed for reduction and control, removing the influence of, or controlling, 'extraneous' or 'confounding' variables. Researchers and implementers then inferred the ability to generalize findings derived from this approach across contexts. Thus, an effect observed through well-controlled experimentation in one environment would be assumed to occur similarly in other situations; this may have worked in some cases, but by no means always.

---

### Quantum simulation of exact electron dynamics can be more efficient than classical mean-field methods [^a0f15b14]. Nature Communications (2023). High credibility.

Quantum algorithms for simulating electronic ground states are slower than popular classical mean-field algorithms such as Hartree-Fock and density functional theory but offer higher accuracy. Accordingly, quantum computers have been predominantly regarded as competitors to only the most accurate and costly classical methods for treating electron correlation. However, here we tighten bounds showing that certain first-quantized quantum algorithms enable exact time evolution of electronic systems with exponentially less space and polynomially fewer operations in basis set size than conventional real-time time-dependent Hartree-Fock and density functional theory. Although the need to sample observables in the quantum algorithm reduces the speedup, we show that one can estimate all elements of the k-particle reduced density matrix with a number of samples scaling only polylogarithmically in basis set size. We also introduce a more efficient quantum algorithm for first-quantized mean-field state preparation that is likely cheaper than the cost of time evolution. We conclude that quantum speedup is most pronounced for finite-temperature simulations and suggest several practically important electron dynamics problems with potential quantum advantage.

---

### A' periodic table' for protein structures [^84158fb9]. Nature (2002). Excellent credibility.

Current structural genomics programs aim systematically to determine the structures of all proteins coded in both human and other genomes, providing a complete picture of the number and variety of protein structures that exist. In the past, estimates have been made on the basis of the incomplete sample of structures currently known. These estimates have varied greatly (between 1,000 and 10,000; see for example refs 1 and 2), partly because of limited sample size but also owing to the difficulties of distinguishing one structure from another. This distinction is usually topological, based on the fold of the protein; however, in strict topological terms (neglecting to consider intra-chain cross-links), protein chains are open strings and hence are all identical. To avoid this trivial result, topologies are determined by considering secondary links in the form of intra-chain hydrogen bonds (secondary structure) and tertiary links formed by the packing of secondary structures. However, small additions to or loss of structure can make large changes to these perceived topologies and such subjective solutions are neither robust nor amenable to automation. Here I formalize both secondary and tertiary links to allow the rigorous and automatic definition of protein topology.

---

### Spontaneous ordering of identical materials into a triboelectric series [^f893a3e7]. Nature (2025). Excellent credibility.

When two insulating, neutral materials are contacted and separated, they exchange electrical charge 1. Experiments have long suggested that this 'contact electrification' is transitive, with different materials ordering into 'triboelectric series' based on the sign of charge acquired 2. At the same time, the effect is plagued by unpredictability, preventing consensus on the mechanism and casting doubt on the rhyme and reason that series imply 3. Here we expose an unanticipated connection between the unpredictability and order in contact electrification: nominally identical materials initially exchange charge randomly and intransitively, but-over repeated experiments-order into triboelectric series. We find that this evolution is driven by the act of contact itself-samples with more contacts in their history charge negatively to ones with fewer contacts. Capturing this 'contact bias' in a minimal model, we recreate both the initial randomness and ultimate order in numerical simulations and use it experimentally to force the appearance of a triboelectric series of our choosing. With a set of surface-sensitive techniques to search for the underlying alterations contact creates, we only find evidence of nanoscale morphological changes, pointing to a mechanism strongly coupled with mechanics. Our results highlight the centrality of contact history in contact electrification and suggest that focusing on the unpredictability that has long plagued the effect may hold the key to understanding it.

---

### Graphs [^c6c6d5a7]. Chest (2006). Low credibility.

Two rules of good graphs are presented and explicated.

---

### CORRECTION [^a0bf88a0]. Annals of Family Medicine (2017). Low credibility.

[This corrects the article on p. 14 in vol. 15.].

---

### Fundamental limits to learning closed-form mathematical models from data [^a345300f]. Nature Communications (2023). High credibility.

Given a finite and noisy dataset generated with a closed-form mathematical model, when is it possible to learn the true generating model from the data alone? This is the question we investigate here. We show that this model-learning problem displays a transition from a low-noise phase in which the true model can be learned, to a phase in which the observation noise is too high for the true model to be learned by any method. Both in the low-noise phase and in the high-noise phase, probabilistic model selection leads to optimal generalization to unseen data. This is in contrast to standard machine learning approaches, including artificial neural networks, which in this particular problem are limited, in the low-noise phase, by their ability to interpolate. In the transition region between the learnable and unlearnable phases, generalization is hard for all approaches including probabilistic model selection.

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^990cb6a1]. Annals of the American Thoracic Society (2023). High credibility.

Ventilation–perfusion heterogeneity metrics — modeling description — states that regional V_A/Q heterogeneity can be described with simple functional models corresponding to a few compartments and a narrow unimodal V_A/Q distribution; small-length scale heterogeneity develops in disease and under mechanical ventilation, resulting in heterogeneous V_A/Q distributions; and metrics include global heterogeneity and the second moments (log scale) of ventilation (LogSDV) and perfusion (LogSDQ) versus V_A/Q ratios distributions.

---

### The extricable links between health, wealth, and profits [^97b3255d]. BMJ (2018). Excellent credibility.

How then can governments, citizens, and societies begin to make progress on regulating large private stakeholders in global health? International collective action, global regulatory frameworks, and other efforts to tackle key risk factors for poor health form the third theme of our series. As tobacco control is most often pointed to as a success, we will analyse lessons from the Framework Convention on Tobacco Control and consider where new instruments and approaches are necessary.

Our broad vision for this series is to reignite the debate on investing in health and healthcare systems, with a focus on non-communicable diseases. Policy makers can no longer point to a lack of evidence on the links between health and wealth, ill health and losses to the economy, and the influence of commercial determinants of health. We invite contributions to this series that are relevant to our three themes. As the commissioned papers in this series will show, a solid evidence base drawn from public health, epidemiology, economics, and political science already exists on how to move forward and create a world focused on healthier lives instead of corporate profits.

---

### Inferring time derivatives including cell growth rates using gaussian processes [^b5444dae]. Nature Communications (2016). Medium credibility.

Methods

Using a Gaussian process to fit time-series data

In the following, we will denote a Gaussian distribution with mean μ and covariance matrix Σ as(μ, Σ) and use the notation of Rasmussen and Williamsas much as possible.

Prior probability

For n data points y i at inputs x i (each x i is a time for a growth curve), we denote the underlying latent function as f (x). We define a covariance matrix k (x, x ′), which has an explicit dependence on hyperparameters θ, and obeys

where the expectations are taken over the distribution of latent functions (samples of f (x)).

We interpret equation (1) as giving the prior probability distribution of the latent functions f (X), where were we use X to denote the inputs x i, such that

where K (X, X) is the n × n matrix with components k (x i, x j). With f denoting [f (x 1). f (x n)], this prior probability can be written as

noting the dependence of k (x, x ′; θ) on the hyperparameters θ.

---

### Experimental simulation of closed timelike curves [^7ca5d5d2]. Nature Communications (2014). Medium credibility.

Closed timelike curves are among the most controversial features of modern physics. As legitimate solutions to Einstein's field equations, they allow for time travel, which instinctively seems paradoxical. However, in the quantum regime these paradoxes can be resolved, leaving closed timelike curves consistent with relativity. The study of these systems therefore provides valuable insight into nonlinearities and the emergence of causal structures in quantum mechanics — essential for any formulation of a quantum theory of gravity. Here we experimentally simulate the nonlinear behaviour of a qubit interacting unitarily with an older version of itself, addressing some of the fascinating effects that arise in systems traversing a closed timelike curve. These include perfect discrimination of non-orthogonal states and, most intriguingly, the ability to distinguish nominally equivalent ways of preparing pure quantum states. Finally, we examine the dependence of these effects on the initial qubit state, the form of the unitary interaction and the influence of decoherence.

---

### Quantum physics in connected worlds [^764252bb]. Nature Communications (2022). High credibility.

Theoretical research into many-body quantum systems has mostly focused on regular structures which have a small, simple unit cell and where a vanishingly small fraction of the pairs of the constituents directly interact. Motivated by advances in control over the pairwise interactions in many-body simulators, we determine the fate of spin systems on more general, arbitrary graphs. Placing the minimum possible constraints on the underlying graph, we prove how, with certainty in the thermodynamic limit, such systems behave like a single collective spin. We thus understand the emergence of complex many-body physics as dependent on 'exceptional', geometrically constrained structures such as the low-dimensional, regular ones found in nature. Within the space of dense graphs we identify hitherto unknown exceptions via their inhomogeneity and observe how complexity is heralded in these systems by entanglement and highly non-uniform correlation functions. Our work paves the way for the discovery and exploitation of a whole class of geometries which can host uniquely complex phases of matter.

---

### Clinical practice guidelines: childhood hearing screening [^350f9400]. AAA (2011). Medium credibility.

Methodology — levels of evidence for individual screening studies: The US Preventative Services Task Force rates study quality for screening studies as Level I randomized controlled trial, Level II non-randomized controlled trial, Level III cohort or case–control study, Level IV ecological or descriptive studies (e.g. international pattern time series), and Level V opinions of respected authorities based on clinical experience, descriptive studies or reports of expert committees.

---

### Practical guidance for the evaluation and management of drug hypersensitivity: specific drugs [^86cbdb46]. The Journal of Allergy and Clinical Immunology: In Practice (2020). High credibility.

Omalizumab — desensitization protocol provides a timed series at 0 min 0.0625 mg (1/100, 0.05 mL), 30 min 0.625 mg (1/100, 0.5 mL), 60 min 1.25 mg (1/10, 0.1 mL), 90 min 2.5 mg (1/10, 0.2 mL), 120 min 5 mg (1/10, 0.4 mL), 150 min 10 mg (1/10, 0.8 mL), 180 min 20 mg (Undiluted, 0.16 mL), 210 min 37–40 mg (Undiluted, 0.30–0.32 mL), 240 min 37–55 mg (Undiluted, 0.30–0.44 mL), and 270 min 37–55 mg (Undiluted, 0.30–0.44 mL); footnotes state the dose of omalizumab to which patients were desensitized was lower than their target dose and subsequent injections were administered weekly or biweekly, and that these larger dose steps depend on the total dose to be administered.

---

### Learning noise-induced transitions by multi-scaling reservoir computing [^9b089041]. Nature Communications (2024). High credibility.

In the training phase, we generated a series consisting of 40000 δ t. The training set is t ∈ [0, 40], and the predicting set is t ∈ [40, 80]. Figure 4 b displays the training set. Following the method in our framework, the deterministic part is reconstructed as shown in Fig. 4 c. A proper set of hyperparameters is listed in Table 1 (Example 3). We observe that the generated time series starting from the ten initial points converge to two potential wells, where the time series has rotational dynamics. In the predicting phase, we perform rolling prediction within t ∈ [40, 80] (Fig. 4 d).

In the evaluation, we predict 100 replicates of the time series, and compare them with 100 replicates simulated from Eqs. (14) and (15). Figure 4 e presents histograms of the number of transitions for the 100 predicted and test time series, in t ∈ [40, 80]. Figure 4 f presents histograms of transition time for the 100 predicted and test time series, in t ∈ [40, 80]. The results demonstrate that, for the 2D bistable non-gradient system, the present method accurately learns the dynamics and yields precise estimations on the number of transitions and transition time.

---

### Bayesian tomography of high-dimensional on-chip biphoton frequency combs with randomized measurements [^b26a4537]. Nature Communications (2022). High credibility.

In addition to the parameters forming ρ, the scale factor K must also be suitably parameterized. Following ref. we find it convenient to write K = K 0 (1 + σ z), where K 0 and σ are hyperparameters defined separate of the inference process, and z is taken to follow a standard normal distribution, leading to a normal prior on K of mean K 0 and standard deviation K 0 σ. We take σ = 0.1 and K 0 equal to the sum of the counts in all d 2 bins for the first JSI measurement (r = 1), where the absence of modulation ensures that all initial photon flux remains in measured bins, i.e. This provides an effectively uniform prior, since a fractional deviation of 0.1 is much larger than the maximum amount of fractional uncertaintyexpected from statistical noise at our total count numbers; the use of a normal distribution simplifies the sampling process.

The total parameter set can therefore be expressed as the vector, with the prior distribution

We note that this parameterization entails a total of 4 d 4 + 1 independent real numbers (2 d 4 complex parameters for ρ, one real parameter for K) — noticeably higher than the minimum of d 4 − 1 required to uniquely describe a density matrix. Nevertheless, this ρ (y) parameterization is to our knowledge the only existing constructive method to produce Bures-distributed states, and is straightforward to implement given its reliance on independent normal parameters only.

Following Bayes' rule, the posterior distribution becomeswhereis a constant such that ∫ d x π (x) = 1. We have adopted this notation for Bayes' theorem — rather than the more traditional — to emphasize the functional dependencies on x, which are all that must be accounted for in the sampling algorithm below. From π (x), the Bayesian mean estimator f B of any quantity (scalar, vector, or matrix) expressible as a function of x can be estimated aswhere, in lieu of direct integration, S samples { x (1),…, x (S) } are obtained from the distribution π (x) through Markov chain Monte Carlo (MCMC) techniques, as described below.

---

### On the noise in "augmented T1-weighted steady state magnetic resonance imaging" [^5e296aaa]. NMR in Biomedicine (2023). Medium credibility.

Recently, Ye and colleagues proposed a method for "augmented T1-weighted imaging" (aT 1 W). The key operation is a complex division of gradient-echo (GRE) images obtained with different flip angles. Ye and colleagues provide an equation for the standard deviation of the obtained aT 1 W signal. Here, we show that this equation leads to wrong values of the standard deviation of such an aT 1 W signal. This is demonstrated by Monte Carlo simulations. The derivation of the equation provided by Ye and colleagues is shown to be erroneous. The error consists of a wrong handling of random variables and their standard deviations and of the wrong assumption of correlated noise in independently acquired GRE images. Instead, the probability distribution obtained with the aT 1 W-method should have been carefully analyzed, perhaps on the basis of previous literature on ratio distributions and their normal approximations.

---

### An exact form for the magnetic field density of States for a dipole [^2c861285]. Magnetic Resonance Imaging (2001). Low credibility.

We present an analytical form for the density of states for a magnetic dipole in the center of a spherical voxel. This analytic form is then used to evaluate the signal decay as a function of echo time for different volume fractions and susceptibilities. The decay can be considered exponential only in a limited interval of time. Otherwise, it has a quadratic dependence on time for short echo times and an oscillatory decaying behavior for long echo times.

---

### Statistical modeling of RNA structure profiling experiments enables parsimonious reconstruction of structure landscapes [^88ac8573]. Nature Communications (2018). Medium credibility.

Structure selection and relative abundance estimation

Prior to feeding the linear model with candidate structures and sequencing reads, we screen for inconsistencies between these two distinct inputs and remove structures that are markedly inconsistent with the data. Such dimensionality reduction serves to alleviate potential model overfitting and improve performance. Notably, the linear system in truncation mode has a relatively small number of patterns compared to mutation mode, which number is often substantially smaller than the number of structures considered, resulting in an under-determined system and the risk of overfitting. Pre-filtering of inconsistent structures then serves to alleviate such concern. A structure is deemed inconsistent with the data if there is sufficient evidence of nucleotides that are paired in this structure but are highly reactive. This is because we expect highly reactive sites to be predominantly unpaired in the ensemble. We define all sites whereas highly reactive. Strictly, if a structure has one of these sites paired, it should be removed. However, andmight not always be highly accurate, especially when data are very noisy. Additionally, highly reactive sites are the most susceptible to measurement errors and typically have the highest variance. For these reasons, we only remove structures that were inconsistent with more than a single nucleotide, specifically, more than half the highly reactive sites. Relative abundances were estimated using standard NNLS fitting. Structures whose estimated abundances exceed a user-specified threshold are selected, and then abundances are normalized to sum to 1. In this work, we thresholded at 1%.

Post-processing SLEQ's outputs

Structures selected by SLEQ and their abundance estimates were used to estimate the probabilities of each possible base pair (P ij for any two sites i and j) and the probability that a site is unpaired (P ii for any site i) in the reconstructed ensemble. For each site i, P ij (j ≠ i) and P ii form a probability distribution, whose Shannon entropy is,

To account for variable lengths of intermediate transcripts, site-specific entropies were averaged over all sites. Base-pairing entropy was used as an approximation of structural entropy, because selected structures change as sequence length increases, which renders the underlying distributions and their entropies incomparable. Furthermore, structures sometimes share certain motifs but not others, which is well-captured by base-pairing entropy but overlooked by structural entropy. All curves in Fig. 4 and Supplementary Figs. 4 and 7 are smoothed with a mean filter of window size 3 to improve signal-to-noise ratio.

---

### Elranatamab-bcmm (Elrexfio) [^ad17527e]. FDA (2025). Medium credibility.

2.1 Important Dosing Information

Administer ELREXFIO subcutaneously according to the step-up dosing schedule to reduce the incidence and severity of cytokine release syndrome (CRS).

Administer pre-treatment medications prior to each dose in the ELREXFIO step-up dosing schedule, which includes step-up dose 1, step-up dose 2, and the first treatment dose as recommended [see Dosage and Administration (2.2, 2.3)].

ELREXFIO should only be administered by a qualified healthcare professional with appropriate medical support to manage severe reactions such as CRS and neurologic toxicity, including ICANS [see Warnings and Precautions (5.1, 5.2)].

Due to the risk of CRS, patients should be hospitalized for 48 hours after administration of the first step-up dose, and for 24 hours after administration of the second step-up dose.

2.2 Recommended Dosage

For subcutaneous injection only.

The recommended dosing schedule for ELREXFIO is provided in Table 1. The recommended dosages of ELREXFIO subcutaneous injection are: step-up dose 1 of 12 mg on Day 1, step-up dose 2 of 32 mg on Day 4, followed by the first treatment dose of 76 mg on Day 8, and then 76 mg weekly thereafter through week 24.

For patients who have received at least 24 weeks of treatment with ELREXFIO and have achieved a response [partial response (PR) or better] and maintained this response for at least 2 months, the dose interval should transition to an every two-week schedule. For patients who have received at least 24 weeks of treatment with ELREXFIO at the every two-week dosing schedule and have maintained the response, the dose interval should transition to an every four-week schedule.

Continue treatment with ELREXFIO until disease progression or unacceptable toxicity.

Administer pre-treatment medications prior to each dose in the ELREXFIO step-up dosing schedule, which includes step-up dose 1, step-up dose 2, and the first treatment dose as recommended [see Dosage and Administration (2.3)].

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^dee6cacc]. CDC (2011). Medium credibility.

Community viral load sample size calculations — sample size to detect differences in geometric mean (GM) viral load depends on power and standard deviation (S). The table specifies "α = 0.05 and W = 0.8", with S columns 1 to 1.5; for k = 3 the minimum sample sizes by S are 54, 66, 78, 92, 106, and 122, and "1.2 is the standard deviation of national VL data", so at S = 1.2 the needed sample size is 78.

---

### Practice bulletin No 182: hereditary breast and ovarian cancer syndrome [^7c641a02]. Obstetrics and Gynecology (2017). Medium credibility.

U.S. Preventive Services Task Force evidence levels — studies were reviewed and evaluated for quality according to this method: I Evidence obtained from at least one properly designed randomized controlled trial; II-1 Evidence obtained from well-designed controlled trials without randomization; II-2 Evidence obtained from well-designed cohort or case–control analytic studies, preferably from more than one center or research group; II-3 Evidence obtained from multiple time series with or without the intervention, and dramatic results in uncontrolled experiments also could be regarded as this type of evidence; and III Opinions of respected authorities, based on clinical experience, descriptive studies, or reports of expert committees. Based on the highest level of evidence found in the data, recommendations are graded as Level A — Recommendations are based on good and consistent scientific evidence, Level B — Recommendations are based on limited or inconsistent scientific evidence, and Level C — Recommendations are based primarily on consensus and expert opinion.

---

### Survival probability of stochastic processes beyond persistence exponents [^12e77d9f]. Nature Communications (2019). High credibility.

For many stochastic processes, the probability [Formula: see text] of not-having reached a target in unbounded space up to time [Formula: see text] follows a slow algebraic decay at long times, [Formula: see text]. This is typically the case of symmetric compact (i.e. recurrent) random walks. While the persistence exponent [Formula: see text] has been studied at length, the prefactor [Formula: see text], which is quantitatively essential, remains poorly characterized, especially for non-Markovian processes. Here we derive explicit expressions for [Formula: see text] for a compact random walk in unbounded space by establishing an analytic relation with the mean first-passage time of the same random walk in a large confining volume. Our analytical results for [Formula: see text] are in good agreement with numerical simulations, even for strongly correlated processes such as Fractional Brownian Motion, and thus provide a refined understanding of the statistics of longest first-passage events in unbounded space.

---

### First-passage times in complex scale-invariant media [^895e8171]. Nature (2007). Excellent credibility.

How long does it take a random walker to reach a given target point? This quantity, known as a first-passage time (FPT), has led to a growing number of theoretical investigations over the past decade. The importance of FPTs originates from the crucial role played by first encounter properties in various real situations, including transport in disordered media, neuron firing dynamics, spreading of diseases or target search processes. Most methods of determining FPT properties in confining domains have been limited to effectively one-dimensional geometries, or to higher spatial dimensions only in homogeneous media. Here we develop a general theory that allows accurate evaluation of the mean FPT in complex media. Our analytical approach provides a universal scaling dependence of the mean FPT on both the volume of the confining domain and the source-target distance. The analysis is applicable to a broad range of stochastic processes characterized by length-scale-invariant properties. Our theoretical predictions are confirmed by numerical simulations for several representative models of disordered media, fractals, anomalous diffusion and scale-free networks.

---

### Inferring time derivatives including cell growth rates using gaussian processes [^d99eaf3f]. Nature Communications (2016). Medium credibility.

Inferring the first and second time derivatives

To determine the time derivative of the data, we use that the derivative of a Gaussian process is another Gaussian process. We can therefore adapt standard techniques for Gaussian process to allow time derivatives to be sampled too.

Building on the work of Boyle, we let g (x) and h (x) be the first and second derivatives with respect to x of the latent function f (x). If f (x) is a Gaussian process then so are both g (x) and h (x). Writing ∂ 1 and ∂ 2 for the partial derivatives with respect to the first and second arguments of a bivariate function, we have

and that

as well as

following ref.

Consequently, the joint probability distribution for y and f *, g * and h * evaluated at points X * is again Gaussian (cf. equation (7)):

where we write K = K (X, X) and K ✱ = K (X *, X *) for clarity.

The covariance function is by definition symmetric: k (x i, x j) = k (x j, x i) from equation (1). Therefore, and so

for all positive integers k and. Consequently, the covariance matrix in equation (13) is also symmetric.

Conditioning on y now gives that the distribution P (f *, g *, h *| X, y, θ, X *) is Gaussian with mean

and covariance matrix

Equation (16) includes equation (9) and shows that

which gives the error in the estimate of the first derivative. Similarly,

is the error in estimating the second derivative.

Using an empirically estimated measurement noise

Although our derivation is given for a Gaussian process where the measurement errors in the data are independent and identically distributed with a Gaussian distribution of mean zero, the derivations are unchanged if the measurement noise has a different s.d. for each time point.

When the magnitude of the measurement noise appears to change with time, we first empirically estimate the relative magnitude of the measurement noise by the variance across all replicates at each time point. We then smooth this estimate over time (with a Gaussian filter with a width of 10% of the total time of the experiment, but the exact choice is not important) and replace the identity matrix, I, in equations (6), (15) and (16) by a diagonal matrix with the relative measurement noise on the diagonal in order to make predictions.

---

### Universal law of hierarchical dynamics in gels arising from confluence of local physically dynamic bonds [^492a33e3]. Nature Communications (2025). High credibility.

For chemically crosslinked gels, g 1 (q, t) is proportional to the time correlation function of the end-to-end distance vector X m (t) of the polymer strands with m monomers between two adjacent crosslinks:

As well-established in the literature over five decades for chemically crosslinked gels –, g 1 (q, t) of gels depends on the modulus and the friction coefficient and also obeys an exponential decay with Г = D gel q 2. Actually, nothing is diffusive in the gel networks, the gel diffusion coefficient D gel indicates the gel elasticity with D gel = M / f, where M is the longitudinal modulus and f is the friction coefficient between the gel strands and the solvent. And the longitudinal modulus M = K + 4/3 G' is related to bulk modulus K and elastic modulus G'.

The electric field correlation function g 1 (q, t) can be fitted with a sum of several relaxation modes. The characteristic relaxation rate Г i was analyzed by using the CONTIN methodand multiple exponential fitting methods at each scattering angle. CONTIN method fits a weighted distribution of relaxation times and was first used to confirm multiple modes as, where P (Γ) is the weight of each relaxation rate Γ, as determined by an inverse Laplace transform.

For the multiple exponential fitting method, the correlation function g 1 (q, t) can be fitted by a sum of several exponential decays and a stretched exponential decayas,

The function g 1 (q, t) was fitted by ORIGIN software, and the error between the raw data and the fitted function was minimized after iterations were performed. Iterations were performed until the best-fitting curve was obtained within the tolerance limit. Residuals of the difference between the fitted curve and the raw data are randomly distributed about the mean of zero, and the residuals do not have systematic fluctuations about their mean value.

---

### Corrections [^e34f1524]. The Lancet: Respiratory Medicine (2016). Medium credibility.

[This corrects the article DOI: 10.1016/S2213-2600(15)00283–0.].

---

### Nonlinear wave propagation governed by a fractional derivative [^db5b4cd7]. Nature Communications (2025). High credibility.

The idea of fractional derivatives has a long history that dates back centuries. Apart from their intriguing mathematical properties, fractional derivatives have been studied widely in physics, for example in quantum mechanics and generally in systems with nonlocal temporal or spatial interactions. However, systematic experiments have been rare because the physical implementation is challenging. Here we report the observation and full characterization of a family of temporal solitons that are governed by a fractional nonlinear wave equation. We demonstrate that these solitons have non-exponential tails, reflecting their nonlocal character, and have a very small time-bandwidth product.

---

### Calculating T2 in images from a phased array receiver [^23db2d0c]. Magnetic Resonance in Medicine (2009). Low credibility.

Images reconstructed from multielement, phased array coils and presented as the square root of the sum of the squares of the signals received by the individual elements have a distribution of signal and noise that distorts the relationship between the image intensity and the underlying signal. The distortion is accentuated for long echo times for which the signal-to-noise ratio (SNR) may be low. When measuring T(2) or T(2)* this signal distortion leads to biased estimates of these parameters. We demonstrate this effect and its dependence on the image SNR and the number of elements in a phased array coil. We evaluated the effects of four techniques for calculating T(2) from data acquired in phased array coils (log transform, least squares, lookup table correction, and maximum likelihood [ML] estimation). The ML estimation gave the most accurate T(2) in the presence of this bias.

---

### Single photon emission computed tomography (SPECT) myocardial perfusion imaging guidelines: instrumentation, acquisition, processing, and interpretation [^81c8bd3b]. Journal of Nuclear Cardiology (2018). Medium credibility.

Flood image acquisition for uniformity — counts and matrices — specifies that flood images that will be inspected visually should be acquired as 256 × 256 matrices for 3 million counts (5 million counts for larger rectangular detectors), and digitized flood fields should be recorded. Flood images used for calculations of uniformity require two to three times more counts, with the recommended number of counts at least 4,500 counts/cm 2 (e.g., 3 million counts for a 400-mm diameter circular detector). For accurate quantification of uniformity, the NEMA recommends acquiring a minimum of 10,000 counts for the center (6.4 mm) pixel of the flood image, or approximately 50 million total counts for 40 cm × 50 cm detectors. The flood images should be examined each day for each detector prior to use to verify the detectors are properly peaked and the floods are uniform, and if the flood images are acquired in a larger matrix size, the pixel matrix should be reduced to 64 × 64 by summing or averaging pixels prior to uniformity calculation.

---

### 2006 consensus guidelines for the management of women with cervical intraepithelial neoplasia or adenocarcinoma in situ [^853c9b14]. Journal of Lower Genital Tract Disease (2007). Medium credibility.

Rating the recommendations (Table 1) — strength, evidence quality, and terminology define how guidance is expressed: Strength of recommendation A is "Good evidence for efficacy and substantial clinical benefit support recommendation for use", B is "Moderate evidence for efficacy or only minor clinical benefit supports recommendation for use", C is "Evidence for efficacy is insufficient to support a recommendation for or against use, but recommendations may be made on other grounds", D is "Moderate evidence for lack of efficacy or for adverse outcome supports a recommendation against use", and E is "Good evidence for lack of efficacy or for adverse outcome supports a recommendation against use". Quality of evidence I is "Evidence from at least 1 randomized controlled trial", II is "Evidence from at least 1 clinical trial without randomization, from cohort or case-control analytic studies (preferably from more than one center), or from multiple time-series studies, or dramatic results from uncontrolled experiments", and III is "Evidence from opinions of respected authorities based on clinical experience, descriptive studies, or reports of expert committees". Terminology used for recommendations defines Recommended as "Good data to support use when only one option is available", Preferred as "Option is the best (or one of the best) when there are multiple options available", Acceptable as "One of multiple options when there are data indicating that another approach is superior or when there are no data to favor any single option", and Unacceptable as "Good data against use".

---

### Trend algorithms… [^b736a419]. seer.cancer.gov (2025). Medium credibility.

\}{Initial \textbf{ } Rate} }\right) \times {\Large10 }\). \ } \) \ } \). Annual Percent Change The Annual Percent Change is calculated by fitting a least squares regression line to the natural logarithm of the rates, using the calendar year as a regressor variable. n = number of years r = rates y = Ln x = calendar year.

y = mx + b \ \) Because the methods used in the calculation of PC and APC are not directly related, it is possible that the signs of the PC and the APC may disagree. See Calculating APC with Weighting for the formulas used in calculating Weighted APCs. Confidence Intervals for APC The endpoints of a × 100% confidence interval are calculated as: \\right)}
- 1 }\right) \times {\Large100 }\). \\right)}
- 1 }\right) \times {\Large100 }\) Where \\) is the inverse of the.

---

### Guidelines for the prevention and treatment of opportunistic infections in children with and exposed to HIV [^f103f027]. HIV.gov (2025). High credibility.

Polio vaccination (IPV) — the final dose in the series should be administered on or after the child's fourth birthday and at least 6 months after the previous dose; for catch-up dosing of adolescents aged 18 years who are suspected to have received either a partial series or no doses of IPV, the remaining doses should be administered to complete a three-dose primary series; for adolescents aged 18 years who completed the primary series and are at increased risk of exposure to poliovirus, one lifetime booster may be administered.

---

### Somatropin (Humatrope) [^a6c96ae9]. FDA (2025). Medium credibility.

 Do not store the Pen with a Needle attached.
Follow the instructions in Section 2, Step 2D (Remove air from new Cartridge) to remove air bubbles from the Cartridge.
It is normal to see small air bubbles. It will not cause any harm or affect the dose.
What should I do if the screw does not move out when there is no Cartridge attached to the Pen?
The screw may not move out when you push the Injection Button unless there is a Cartridge in the Pen. This allows you to easily push the screw into the Pen Body when replacing a Cartridge.
After a Cartridge is attached, the screw will move out when the Injection Button is pushed.
What should I do if I cannot attach the Cartridge to the Pen Body?
Check that the Cartridge is not damaged or broken.
Carefully line up the Cartridge with the Pen Body and screw them together until secure. If the Cartridge and Pen cannot be screwed together, contact your healthcare provider.
Why is it hard to push the Injection Button when I try to inject the dose?
The Needle may be clogged. Try attaching a new Needle.
Pushing the Injection Button down quickly may make the Injection Button harder to push. Pushing the Injection Button more slowly may make it easier.
Using a larger Needle will make it easier to push the Injection Button during injection. Ask your healthcare provider which Needle is best for you.
The Injection Button may become harder to push if the inside of the Pen gets dirty with HUMATROPE, food, drink, or other materials.
What should I do if the dose knob does not go to zero (0) when I inject the dose?
This can happen if the HUMATROPE Cartridge does not have enough HUMATROPE left in it for the full dose.
At the end of the injection, the number in the dose window should be 0.00. If you see any number other than "0.00", this is the amount of HUMATROPE you did not receive. Write this number down and follow the steps below to get the rest of your dose.
To get the rest of your dose:
Remove the Needle and empty Cartridge from the Pen.
Insert a new Cartridge, attach a new Needle, and remove air from the new Cartridge (see Steps 2A through 2D).
Complete your dose by injecting only the amount you did not receive (see Step 3C).
If you still do not think you received your full dose, do not take another dose. Call Lilly at 1-800-545-5979 or your healthcare provider for assistance.
What should I do if I see HUMATROPE leaking from the Needle after I have finished the injection?
It is normal for a single drop to remain on the tip of the Needle after the injection is complete. If you see more than 1 drop:
The full dose may not have been delivered. Do not inject another dose. Talk with your healthcare provider for assistance.
To prevent this, for the next dose, firmly push and hold the Injection Button in and slowly count to 5 (see Section 3, Step 3C).
How can I tell when the injection is complete?
The injection is complete when:
You place your thumb on the injection Button, then slowly and firmly push the Injection Button until it stops moving.
Continue to hold the Injection Button for 5 seconds, then remove the Needle from the skin.
You will see 0.00 in the center of the dose window.

---

### Next generation reservoir computing [^398d0bfe]. Nature Communications (2021). High credibility.

Reservoir computing is a best-in-class machine learning algorithm for processing information generated by dynamical systems using observed time-series data. Importantly, it requires very small training data sets, uses linear optimization, and thus requires minimal computing resources. However, the algorithm uses randomly sampled matrices to define the underlying recurrent neural network and has a multitude of metaparameters that must be optimized. Recent results demonstrate the equivalence of reservoir computing to nonlinear vector autoregression, which requires no random matrices, fewer metaparameters, and provides interpretable results. Here, we demonstrate that nonlinear vector autoregression excels at reservoir computing benchmark tasks and requires even shorter training data sets and training time, heralding the next generation of reservoir computing.

---

### A random-walk benchmark for single-electron circuits [^e1c84bfc]. Nature Communications (2021). High credibility.

Although the individual accuracy of the active components can exceed metrological precision, their simultaneous operation in a mesoscopic circuitprecludes the prediction of transfer fidelity from component-wise characterization due to interactions and crosstalk between the elements in the chain, exemplifying the need for circuit-level benchmarking. Experimental evidence for strong discord between component-wise and circuit-level characterization is given in Supplementary Note 3.

Implementation of a random-walk benchmark

Here we report the measurement results on two devices: device A introduces the methodology to resolve effects beyond statistical noise of independent-error accumulation in a high-fidelity circuit, while device B demonstrates the effects of memory with increased repetition frequency. Both devices share very similar device geometries and parameters.

Figure 2 a shows the counting statistics measured for device A at f = 30 MHz for t up to 10 4 compared to predictions of the baseline model. General trends expected from the random walk are evident: for short sequences, the power-law rise of the probabilitiescorresponds to the exponential decay of error-free transfer fidelity, which remains close to 1. For longer sequences the distribution spreads and the weight of self-intersecting paths (e.g. orange line in Fig. 1) increases, in accordance with Eq. (1).

Fig. 2
Consistency of random-walk models with respect to the experimental statistics of accumulated errors.

a Measuredfor device A; error bars are given by the standard deviation of the binomial distribution, solid lines show a least-squares fit of Eq. (1). b Likelihood-maximizing P ± (white dots) and p > 0.05 consistency regions estimated separately for each sequence length (coded by color). The inset shows the probability density function of the Dirichlet distribution with parameter α = (2.43 × 10 3, 3.50 × 10 7, 7.47 × 10 2). The corresponding global best-fit values for P ± are marked by red lines on the axes of the consistency-region plot. The color scale indicates the level of confidence at different coverage factors k for a symmetric normal distribution; the red circle in both plots and the marker in the color scale indicates the region corresponding to k = 4. c Empirical cumulative distribution of p -values for different models in comparison to the uniform distribution (black line).

---

### 2006 consensus guidelines for the management of women with abnormal cervical screening tests [^c7fdd86e]. Journal of Lower Genital Tract Disease (2007). Medium credibility.

Table 1 — Rating the recommendations — Strength of recommendation is defined as: A, Good evidence for efficacy and substantial clinical benefit support recommendation for use; B, Moderate evidence for efficacy or only limited clinical benefit supports recommendation for use; C, Evidence for efficacy is insufficient to support a recommendation for or against use, but recommendations may be made on other grounds; D, Moderate evidence for lack of efficacy or for adverse outcome supports a recommendation against use; E, Good evidence for lack of efficacy or for adverse outcome supports a recommendation against use. Quality of evidence is defined as: I, Evidence from at least one randomized, controlled trial; II, Evidence from at least one clinical trial without randomization, from cohort or case-controlled analytic studies (preferably from more than one center), or from multiple time-series studies, or dramatic results from uncontrolled experiments; III, Evidence from opinions of respected authorities based on clinical experience, descriptive studies, or reports of expert committees. The text also notes terminology including recommended, preferred, acceptable, and unacceptable.

---

### Modelling sequences and temporal networks with dynamic community structures [^7b18f16a]. Nature Communications (2017). Medium credibility.

Combining this result with eq. (9), we have the complete likelihood of the temporal network, conditioned only on the partitions. As we show in detail in the Methods section Temporal networks, this model is a direct generalisation of the static DCSBM, with a likelihood composed of two separate static and dynamic terms. One recovers the static DCSBM exactly by choosing B N = B M = 1, making the state transitions memoryless.

Finally, to make the model nonparametric, we again include the same prior as before for the node partition c, in addition to token and memory partition b, such that the total nonparametric joint likelihood is maximised, In this way, we again protect against overfitting, and we can infer not only the number of memory groups B N and token groups B M, as before, but also the number of groups in the temporal network itself, C. If, for example, the temporal network is completely random — that is, the edges are placed randomly both in the aggregated network as well as in time — we again infer B N = B M = C = 1 with the largest probability. We refer to the Methods section Temporal networks for a complete derivation of the final likelihood.

We employ this model in a variety of dynamic network data sets from different domains (for details, see Table 2 and Methods section Data sets). In all cases, we infer models with n > 0 that identify many groups for the tokens and memories, meaning that the model succeeds in capturing temporal structures. In most cases, models with n = 1 best describe the data, implying that there is not sufficient evidence for higher-order memory, with exception of the network of chess moves, which is best described by a model with n = 2. This result is different from the results for the comparably long non-network sequences in Table 1, where we identified higher-order Markov chains. Again, this is because the alphabet size is much larger for temporal networks — corresponding to all possible edges that can be encountered. Hence, for the data sets in Table 2 the size of the alphabet is often comparable with the length of the sequence. In view of this, it is remarkable that the method can detect any structure at all. The intermediary layer where the Markov chain generates edge types instead of the edges directly is crucial. If we fit the original model without this modification, we indeed get much larger description lengths and we often fail to detect any Markov structure (not shown).

---

### Stochastic representation of many-body quantum States [^cf5b8aa3]. Nature Communications (2023). High credibility.

The quantum many-body problem is ultimately a curse of dimensionality: the state of a system with many particles is determined by a function with many dimensions, which rapidly becomes difficult to efficiently store, evaluate and manipulate numerically. On the other hand, modern machine learning models like deep neural networks can express highly correlated functions in extremely large-dimensional spaces, including those describing quantum mechanical problems. We show that if one represents wavefunctions as a stochastically generated set of sample points, the problem of finding ground states can be reduced to one where the most technically challenging step is that of performing regression-a standard supervised learning task. In the stochastic representation the (anti)symmetric property of fermionic/bosonic wavefunction can be used for data augmentation and learned rather than explicitly enforced. We further demonstrate that propagation of an ansatz towards the ground state can then be performed in a more robust and computationally scalable fashion than traditional variational approaches allow.

---

### Cutting edge or blunt instrument: how to decide if a stepped wedge design is right for you [^471f52f2]. BMJ Quality & Safety (2021). High credibility.

When might I consider doing a stepped wedge trial?

Research designs are shaped as much by practical constraints as by abstract schemes, and it is always a good idea to start with the constraints and work towards a design, rather than start with a design and try to fit it to constraints. These constraints will be unique to each research context, and box 1 lists some areas to think about. Still, there are some common features of settings where a stepped wedge trial might be considered as a possible design, and we now review these.

Box 1
Practical constraints on the design of a longitudinal cluster randomised trial

Are there limits on the time available to complete the evaluation, on the number of clusters, or on the number of participants (or the rate at which you can recruit participants) at each cluster? These constraints put limits on the overall scale of the evaluation, or force trade-offs between different design characteristics.
How will participants and their data be sampled in your study: as a series of cross-sectional surveys, as a continuous stream of incident cases, as a cohort followed over time, or some other way? Does the timescale divide into cycles, seasons or milestones that influence how you will sample participants and data?
Is there a limit on how many clusters can implement the intervention at the same time in the evaluation? If this is constrained by research resources (eg, if there are only enough trained research staff to implement the intervention one cluster at a time) then implementation must be staggered in some way.
If implementation is to be staggered, is there a minimum 'step length'? If the same team delivers the intervention in different clusters at different steps, then bear in mind it may take some time to get the intervention fully operational at a site, and the team will also need time to relocate from one cluster to the next.

---

### SSFP signal with finite RF pulses [^29a35a7d]. Magnetic Resonance in Medicine (2009). Low credibility.

The theoretical description of steady state free precession (SSFP) sequences is generally well accepted and unquestioned, although it is based on instantaneously acting radiofrequency (RF) pulses. In practice, however, all excitation processes are finite, thereby questioning the overall validity of the common SSFP signal description for use with finite RF pulses. In this paper, finite RF pulse effects on balanced SSFP signal formation are analyzed as a function of the RF time, the pulse repetition time, the flip angle (alpha) and relaxation times (T(1,2)). The observed signal modulations from finite RF pulses (compared to infinitesimal ones) can range from only a few percent (for RF time/pulse repetition time < < 1, alpha < < 90 degrees, T(2)/T(1) approximately 1) to over 10% (for RF time/pulse repetition time < < 1, alpha approximately 90 degrees, T(2)/T(1) < < 1) and may even exceed 100% in the limit of RF time/pulse repetition time → 1 (for alpha approximately 90 degrees, T(2)/T(1) < < 1). As a result, a revision of SSFP signal theory is indicated not only for reasons of completeness but also seems advisable, e.g., for all quantitative SSFP methods. A simple modification for the common balanced SSFP equation is derived that provides an accurate framework for SSFP signal description over a wide variety of practical and physiologic parameters.

---

### Validation of monte carlo estimates of three-class ideal observer operating points for normal data [^26b6e5d9]. Academic Radiology (2013). Low credibility.

Rationale and Objectives

Traditional two-class receiver operating characteristic (ROC) analysis is inadequate for the complete evaluation of observer performance in tasks with more than two classes.

Materials and Methods

Here, a Monte Carlo estimation method for operating point coordinates on a three-class ROC surface is developed and compared with analytically calculated coordinates in two special cases: (1) univariate and (2) restricted bivariate trinormal underlying data.

Results

In both cases, the statistical estimates were found to be good in the sense that the analytical values lay within the 95% confidence interval of the estimated values about 95% of the time.

Conclusions

The statistical estimation method should be key in the development of a pragmatic performance metric for evaluation of observers in classification tasks with three or more classes.

---

### Approaches to working in high-dimensional data spaces: gene expression microarrays [^ab1261e3]. British Journal of Cancer (2008). Low credibility.

PREDICTIVE CLASSIFICATION

Performance of a predictive model depends on the interrelationship between sample size, data dimensionality, and model complexity. The accuracy of learned models tends to deteriorate in high dimensions, a phenomenon called the 'curse of dimensionality'. This phenomenon is illustrated for classification by an example by. Consider two equally probable, normally distributed classes with common variance in each dimension. For the feature indexed by n = 1,2,3…, class 1 has mean 1/ n 1/2 and class 2 has mean −1/ n 1/2. Thus, each additional feature has some class discrimination power, albeit diminishing as n increases. Trunk evaluated error rates for the Bayes decision rule, applied as a function of n, when the variance is assumed known but the class means are estimated based on a finite data set. Trunk found that (1) the best test error was achieved using a finite number of features; (2) using an infinite number of features, test error degrades to the accuracy of random guessing; and (3) the optimal dimensionality increases with increasing sample size. These observations are consistent with the 'bias/variance dilemma'. Simple models may be biased but will have low variance. More complex models have greater representation power (low bias) but overfit to the particular training set (high variance). Thus, the large variance associated with using many features (including those with modest discrimination power) defeats any possible classification benefit derived from these features (Figure 1). With severe limits on available samples in microarray studies, complex models using high-feature dimensions will severely overfit, greatly compromising classification performance. Computational learning theory provides distribution-free bounds on generalisation accuracy in terms of a classifier's capacity, related to model complexity. Relevance of these bounds to the microarray domain is discussed e.g. by.

---

### Guidelines for the prevention and treatment of opportunistic infections in children with and exposed to HIV [^872bfc22]. HIV.gov (2025). High credibility.

COVID-19 vaccine (2vCOV-mRNA, 1vCOV-aPS) — minimum age is 6 months; people 6 months of age or older with immunosuppressive HIV are recommended for a primary series of 2024–2025 COVID-19 vaccine, the primary series consists of three doses, with Moderna intervals of 4 weeks and Pfizer intervals of 3 weeks between dose 1 and dose 2 and 8 weeks between dose 2 and dose 3, and doses of non-2024–2025 COVID-19 vaccine count toward the three-dose primary series; after the primary series, a booster dose of 2024–2025 COVID-19 vaccine is recommended at least 8 weeks after the final dose and additional doses may be given at 2-month intervals; for those aged ≥ 12 years who have a contraindication to or who refuse mRNA vaccination, Novavax (1vCOV-aPS) is authorized for a two-dose primary series and one (and only one) booster dose, and previous doses of mRNA vaccine count toward primary series doses.

---

### Correction [^e988dd6f]. JACC: Advances (2022). Medium credibility.

[This corrects the article DOI: 10.1016/j.jacadv.2022.100037.].

---

### Correction [^92d63087]. JAAD Case Reports (2021). Medium credibility.

[This corrects the article DOI: 10.1016/j.jdcr.2021.05.040.].

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^6517cbfd]. Annals of the American Thoracic Society (2023). High credibility.

Basic stereological measurements — first-order parameters and probes identify that "First-order parameters are volume (three-dimensional [3D]), surface (two-dimensional [2D]), length (one-dimensional [1D]), and number (zero-dimensional [0D])". Measurements "are performed using geometric test probes, such as points (0D), lines (1D), planes (2D), or volumes (3D)". These probes create countable events in the image, and "Raw counts provide ratios… that are multiplied by the reference space volume to obtain absolute measures for the lung or subcompartment".

---

### Guidelines for handling decedents contaminated with radioactive materials [^3c500666]. CDC (2007). Medium credibility.

Rule of Seven for radiation dose rates — fallout dose rate after a detonation can be calculated and estimated as follows: "Dose rate = Initial dose rate x t -1.2", and "the dose rate decreases by a factor of 10 for each sevenfold increase in time following a fission detonation"; this rule "is valid for only the first 2 weeks after a detonation", and "rain accelerates the decrease". Table values indicate percentage of initial dose rate remaining of 100% at 0 hour, 10% at 7 hours, 1% at 49 hours (2 days), and 0.1% at 343 hours (14 days).

---

### Correction [^d3680e7c]. JAAD Case Reports (2022). Medium credibility.

[This corrects the article DOI: 10.1016/j.jdcr.2021.11.022.].

---

### Stochastic reservoir computers [^415de659]. Nature Communications (2025). High credibility.

Fig. 3
Diagram of the design for the stochastic optical network.

In the stochastic optical network, the measurement outcomes x k at time step k determine whether the input beams are turned on or off at this step. These beams are fed into a matrix-vector multiplier and combined with input signals before being measured by an array of single photon detectors (SPDs). The detectors report either the presence or absence of photons, which forms the next set of measurement outcomes x k +1.

Figure 4 a shows a comparison of the NMSE values associated with the Lorenz X task, averaged over 100 different choices of A matrices and B vectors for the ESN, while Fig. 4 b shows a comparison of the percentage of errors recorded for the Sine-Square wave identification task averaged over 1000 choices. We see in the figure that the stochastic ESN using the exact probabilities once again performs much better than the deterministic one using similar hardware. With finite statistics, we see the same initially strong performance as in the previous example followed by a leveling off of improvement, with the deterministic ESN outperforming it on average by 6 detectors in the Lorenz X task and by 5 detectors in the Sine-Square wave task.

Fig. 4
Performance of the stochastic optical network.

a Average NMSE values for the Lorenz X task as a function of the number of detectors. Each data point represents the average over 100 random choices of A and B, and the error bars give the standard deviations of each data point over this sample of ESNs. The orange dots correspond to the stochastic ESN averaged over 100000 runs. b Average error percentages for the Sine-Square wave identification task as a function of the number of detectors. Each data point represents the average over 1000 random choices of A and B, and the error bars give the standard deviations of each data point over this sample of ESNs. The orange dots correspond to the stochastic ESN averaged 10000 runs. In both plots, the blue dots correspond to the deterministic ESN, and the green dots correspond to the stochastic ESN results in the theoretical limit of infinite statistics, using the exact probabilities.

---

### Wafer-scale functional circuits based on two dimensional semiconductors with fabrication optimized by machine learning [^462701fd]. Nature Communications (2021). High credibility.

Fig. 5
Wafer-scale integrated circuits built from MoS 2 FETs.

a Photograph of a 2-inch MoS 2 wafer with 1-bit full-adder arrays as functional circuits in the center, and MoS 2 TG-FET arrays used as monitoring devices locating in the surrounding regions. All MoS 2 TG-FETs and 1-bit full-adders are fabricated within the red dashed square. The zoom-ins of blue dashed boxes are optical microscope images of the corresponding MoS 2 TG-FET and 1-bit full-adder arrays. b Wafer maps of mobility (left) and V T statistics (center) extracted from devices in the surrounding regions. The yellow scale bars show mobility and V T values. Each block's color scale represents a value averaged from 16 FETs, and the entire wafer has 81 blocks. The right graph illustrates the yield of 1-bit full-adder circuit arrays. The red and gray squares represent the proportion of working and nonworking circuits, respectively. c Optical microscope image of a 4-bit full adder under which is the truth table for logical combinations. d Functional measurements of the 4-bit full adder with V DD = 3 V. The 4-bit full-adder was tested using a series of input combinations (A, B) in the following order: (0000 + 0000, 0000 + 0111, 1111 + 1000, 1111 + 1111) with C i = 0 and C i = 1. The y axes are the voltage of carry input (C i) and carry output (C o), and the output voltage of sum 0 (S 0), sum 1 (S 1), sum 2 (S 2), sum 3 (S 3).

---

### Correction [^b9d1ef9e]. JACC: CardioOncology (2024). Medium credibility.

[This corrects the article DOI: 10.1016/j.jaccao.2022.09.005.].

---

### Guidelines for handling decedents contaminated with radioactive materials [^5f9e64b9]. CDC (2007). Medium credibility.

Radiation dose concepts — dose, dose rate, and health effects state that dose is the amount of energy from radiation absorbed in tissue, and dose rate at a given location is the amount of energy per unit time a person standing at that location would absorb. Radiation has two effects on health, deterministic and stochastic; deterministic effects are clinically observable symptoms and their severity is directly proportional to dose with time from exposure to symptom manifestation inversely proportional to dose, whereas stochastic effects are long-term effects like cancer in which the risk is proportional to dose but severity is independent of dose.

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^89cdccc2]. Annals of the American Thoracic Society (2023). High credibility.

V_A/Q distribution modeling — derived from cumulative plots of ventilation or perfusion versus V_A/Q — is binned into 50 equally spaced log-scale compartments, with shunt and dead space handled separately. Specifically, V_A/Q ratios < 0.005 (shunt) and > 100 (dead space) are calculated as separate components, and relative dispersion and gravitational gradients may also be assessed.

---

### The snm procedure guideline for general imaging 6.0 [^0e7310d4]. SNMMI (2010). Medium credibility.

SNM Procedure Guideline for General Imaging 6.0 — field uniformity testing for scintillation cameras states that field uniformity should be tested each day a scintillation camera is used, using a solid- or liquid-filled uniform flood source to evaluate system (collimator on) uniformity; an acceptable alternative is to remove the collimator and produce an image using a distant (5 times the maximum detector dimension) point source, and the point source must be on the central axis. For planar imaging, images for small-field cameras must contain at least 1.25M counts, large-field circular, 2.5M counts and large rectangular field cameras, 4M counts, and all imaging parameters and the technologist's initials should be recorded.

---

### Hydrocodone bitartrate (hysingla ER) [^0c03a0bc]. FDA (2023). Medium credibility.

Warnings and precautions regarding the use of hydrocodone bitartrate ER PO (also known as Hysingla ER):
- **Adrenal insufficiency**: use caution in patients taking the drug for a prolonged period (> 1 month).
- **Central sleep apnea**: use caution in patients taking higher doses.
- **Decreased serum hydrocodone level**: use caution in patients taking CYP3A4 inducers (such as rifampin, carbamazepine, or phenytoin) or discontinuing CYP3A4 inhibitors (such as macrolide antibiotics, azole antifungals, or protease inhibitors).
- **Erectile dysfunction, infertility**: use caution in patients taking the drug for a prolonged period.
- **Exacerbation of increased ICP**: use caution in patients with increased ICP, brain tumor, or head injury.
- **Growth suppression**: use caution in patients with chronic corticosteroid therapy. Monitor growth regularly in pediatric patients receiving chronic corticosteroid therapy. Reassess the need for hydrocodone regularly and adjust the corticosteroid therapy as appropriate.
- **Hypotension, syncope**: use caution in patients with reduced blood volume or taking other CNS depressants. Monitor BP after initiation and dose titration. Avoid use in patients with circulatory shock.
- **Mask symptoms of head injury**: use caution in patients with head injury. Avoid use in patients with impaired consciousness or coma.
- **Opioid overdose**: use caution in patients taking CNS depressants or with a history of opioid use disorder or prior opioid overdose. Consider prescribing naloxone based on the patient's risk factors for overdose.
- **Opioid withdrawal syndrome**: do not discontinue abruptly in patients physically dependent on opioids.
- **Opioid withdrawal syndrome**: use caution in patients taking CYP3A4 inducers (such as rifampin, carbamazepine, or phenytoin) or discontinuing CYP3A4 inhibitors (such as macrolide antibiotics, azole antifungals, or protease inhibitors).
- **Opioid withdrawal syndrome**: use extreme caution in patients taking mixed agonist/antagonist analgesics (such as pentazocine, nalbuphine, or butorphanol) or partial agonist analgesics (such as buprenorphine).
- **Prolonged QT interval**: use caution in patients with congestive HF, bradyarrhythmia, electrolyte abnormalities, or taking drugs prolonging QT interval. Avoid use in patients with congenital long QT syndrome. Do not exceed 90 mg BID in patients developing QT prolongation.
- **Seizure**: use caution in patients with seizure disorder.
- **Serotonin syndrome**: use caution in patients taking serotonergic drugs.
- **Serotonin syndrome**: use extreme caution in patients taking MAOIs or within 14 days of stopping treatment.
- **Somnolence**: use extreme caution in patients performing activities requiring mental alertness, such as driving or operating machinery.
- **Sphincter of Oddi dysfunction**: use caution in patients with biliary tract disease and acute pancreatitis.

---

### Familial hyperaldosteronism: an European Reference Network on Rare Endocrine Conditions clinical practice guideline [^c05ecd9d]. European Journal of Endocrinology (2024). High credibility.

Regarding diagnostic investigations for primary aldosteronism, more specifically with respect to plasma aldosterone-to-renin ratio, initial testing, Endo-ERN 2024 guidelines recommend to obtain plasma aldosterone-to-renin ratio as the initial screening test for familial hyperaldosteronism.

---

### Modelling sequences and temporal networks with dynamic community structures [^472a53f8]. Nature Communications (2017). Medium credibility.

In evolving complex systems such as air traffic and social organisations, collective effects emerge from their many components' dynamic interactions. While the dynamic interactions can be represented by temporal networks with nodes and links that change over time, they remain highly complex. It is therefore often necessary to use methods that extract the temporal networks' large-scale dynamic community structure. However, such methods are subject to overfitting or suffer from effects of arbitrary, a priori-imposed timescales, which should instead be extracted from data. Here we simultaneously address both problems and develop a principled data-driven method that determines relevant timescales and identifies patterns of dynamics that take place on networks, as well as shape the networks themselves. We base our method on an arbitrary-order Markov chain model with community structure, and develop a nonparametric Bayesian inference framework that identifies the simplest such model that can explain temporal interaction data. The description of temporal networks is usually simplified in terms of their dynamic community structures, whose identification however relies on a priori assumptions. Here the authors present a data-driven method that determines relevant timescales for the dynamics and uses it to identify communities.

---

### Calculating with light using a chip-scale all-optical abacus [^d88da093]. Nature Communications (2017). Medium credibility.

Machines that simultaneously process and store multistate data at one and the same location can provide a new class of fast, powerful and efficient general-purpose computers. We demonstrate the central element of an all-optical calculator, a photonic abacus, which provides multistate compute-and-store operation by integrating functional phase-change materials with nanophotonic chips. With picosecond optical pulses we perform the fundamental arithmetic operations of addition, subtraction, multiplication, and division, including a carryover into multiple cells. This basic processing unit is embedded into a scalable phase-change photonic network and addressed optically through a two-pulse random access scheme. Our framework provides first steps towards light-based non-von Neumann arithmetic.