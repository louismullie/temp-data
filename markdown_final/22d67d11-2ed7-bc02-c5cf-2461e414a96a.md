# Medical Question & Answer

**Sample ID**: 22d67d11-2ed7-bc02-c5cf-2461e414a96a
**Dataset Index**: 212118

---

## Question

11. If numbers a₁, a₂, a₃ are taken in increasing order from the set 1,2, …, 14, such that both a₂ − a₁ ≥ 3 and a₃ − a₂ ≥ 3 are satisfied, then the total number of different ways to choose such numbers is?

---

## Answer

> Let's see… What do we have here? The user is asking how many ways we can choose three numbers a1 < a2 < a3 from the set {1, 2, …, 14} such that a2 − a1 ≥ 3 and a3 − a2 ≥ 3. Let's break this down step-by-step. First, I need to think about how to transform the spacing constraints into a simpler counting problem. Then, I should verify the mapping to a stars-and-bars framework and ensure the bounds are correct. Next, I will compute the binomial coefficient and double-check the arithmetic. Finally, I will consider whether an alternative approach yields the same answer to confirm consistency.

> Let me first restate the constraints clearly: we need 1 ≤ a1 < a2 < a3 ≤ 14 with a2 − a1 ≥ 3 and a3 − a2 ≥ 3. I should confirm that these inequalities imply a3 − a1 ≥ 6, which they do by transitivity, but I won't need that directly for the transformation I'm planning to use.

> Now, I will transform the problem to remove the minimum gaps. Let me define new variables that absorb the required spacings: set b1 = a1, b2 = a2 − 2, and b3 = a3 − 4. Wait, let me verify the effect of these shifts: a2 − a1 = (b2 + 2) − b1 = b2 − b1 + 2, so to ensure a2 − a1 ≥ 3 I need b2 − b1 ≥ 1, which is simply b2 > b1. Similarly, a3 − a2 = (b3 + 4) − (b2 + 2) = b3 − b2 + 2, so to ensure a3 − a2 ≥ 3 I need b3 − b2 ≥ 1, which is b3 > b2. Thus, the transformed variables satisfy b1 < b2 < b3, and I need to check the upper bound for b3. Since a3 ≤ 14, we have b3 = a3 − 4 ≤ 10, so the transformed problem is to choose three distinct integers b1, b2, b3 from 1 to 10 with b1 < b2 < b3, which is a standard combination problem.

> Hold on, I should verify the lower bound as well. Because a1 ≥ 1, b1 = a1 ≥ 1, and since b1 < b2 < b3, the smallest possible values are 1, 2, 3, which correspond to a1 = 1, a2 = 4, a3 = 7, satisfying the original gap constraints. So the mapping is valid and bijective.

> Next, I should review the counting. The number of ways to choose 3 distinct numbers from 10 is C(10, 3). Let me compute that: C(10, 3) = 10 × 9 × 8 / (3 × 2 × 1) = 120. I should double-check the arithmetic: 10 × 9 = 90, 90 × 8 = 720, 720 ÷ 6 = 120. Yes, that's correct.

> But wait, what if I tried a direct stars-and-bars approach on the original variables? Let me consider that alternative to sanity-check. If I let x1 = a1, x2 = a2 − a1, x3 = a3 − a2, and x4 = 14 − a3, then x1 ≥ 1, x2 ≥ 3, x3 ≥ 3, x4 ≥ 0, and x1 + x2 + x3 + x4 = 14. Substituting y1 = x1 − 1, y2 = x2 − 3, y3 = x3 − 3, y4 = x4, I get y1 + y2 + y3 + y4 = 7 with y1, y2, y3, y4 ≥ 0, which has C(7 + 4 − 1, 4 − 1) = C(10, 3) = 120 solutions. This matches the previous result, which reassures me that the answer is consistent across methods.

> Final answer: 120.

---

The number of ways to choose three increasing numbers (a₁, a₂, a₃) from ({1, 2, …, 14}) such that (a₂ − a₁ ≥ 3) and (a₃ − a₂ ≥ 3) is **120**. This is found by transforming the problem into selecting three distinct integers from a reduced range, which yields (C(10, 3) = 120) valid combinations.

---

## Step-by-step solution

### Step 1: Transform the problem

Let (a₁, a₂, a₃) be the chosen numbers with (1 ≤ a₁ < a₂ < a₃ ≤ 14) and the given spacing constraints. Define new variables to absorb the minimum gaps:

- (b₁ = a₁)
- (b₂ = a₂–2) (so (a₂ = b₂ + 2))
- (b₃ = a₃–4) (so (a₃ = b₃ + 4))

Then the constraints become:

- (b₁ < b₂ < b₃) (since (a₂ − a₁ ≥ 3) and (a₃ − a₂ ≥ 3))
- (1 ≤ b₁ < b₂ < b₃ ≤ 10) (because (a₃ ≤ 14) implies (b₃ ≤ 10))

Thus, the problem reduces to choosing three distinct integers from 1 to 10.

---

### Step 2: Count the combinations

The number of ways to choose three distinct integers from 1 to 10 is given by the binomial coefficient:

C(10, 3) = [10 × 9 × 8/(3 × 2 × 1)] = 120

---

### Step 3: Verify the transformation

Each valid triple ((b₁, b₂, b₃)) maps uniquely back to a valid triple ((a₁, a₂, a₃)) satisfying the original constraints, so the count is exact.

---

The total number of different ways to choose such numbers is **120**.

---

## References

### A neural theory for counting memories [^0c7aca6d]. Nature Communications (2022). High credibility.

The primary setting we consider is one in which there are N distinct items (e.g. odors) that are well-separated from each other, in the sense that the distance between them is roughly what would be expected if they were chosen independently at random; this is formalized in Assumption 1. The sketching scheme is shown a sequence of n observations drawn from these N items, where the items are interleaved arbitrarily and might appear multiple times. Information about the observations gets coded in the weights w j, and when a subsequent query x (also one of the N items) is made, the sketch produces a frequency estimate for it. We study how close this frequency estimate is to the actual number of times x appeared in the sequence. All bounds hold with probability 1 − δ, where the confidence parameter 0 < δ < 1 impacts the manner in which k and m must be set.

For the neural count sketch, we prove (Theorem 2) that frequencies upto a value f are estimated within ± 1 if the number of encoding neurons, m = O (k n), and if the sparsity. For the 1-2-3-many sketch, we prove (Theorem 5) that it is sufficient to have m = O (k N) and, which improves upon the neural count sketch in two important ways. First, the bound depends on the number of distinct items (N), rather than the total number of observations including repetitions (n), which could be far larger. Second, a significantly smaller setting of k (and thus m) is sufficient. In other words, the 1-2-3-many sketch only needs a few synapses to be allocated per unique item to generate good estimates.

---

### High-order species interactions shape ecosystem diversity [^083fa385]. Nature Communications (2016). Medium credibility.

We defined a community as feasible if all the species existed at the end of the simulation, with an abundance above a defined threshold (set as 10 −5 / N). The probability that a community would be feasible was then defined as the fraction of stable communities out of the R random communities simulated. To find the critical strength of interactions for a given number of species, we used a fixed collection of R sets of pairwise, three- and four-way interaction matricesand increased the interaction parameters until 5% of the communities exhibited extinctions. In Fig. 2 (as well as Supplementary Figs 2–5) this was done by increasing the strength of interactions of a given order, while the rest are kept at 0. In Fig. 3a this was done along radial lines in log (α), log (γ) space: α = 0.001·1.2 r ·cos θ, γ = 0.05·1.3 r ·sin θ, such that for each value of θ, we increased r until stability was lost.

Deriving stability criterion by considering effective pairwise interactions

The Jacobian of the system at a given point is:

Where, is the Kronecker delta. At a fixed point, the first term vanishes. We may also neglect the terms in the last line which are derivatives of the normalization, as we will justify below. Assuming stability of all species coexisting, the value of species abundances at this fixed point must scale as x i ≈1/ N (because the total abundance is fixed to 1). Equation 1 can therefore be approximated as pairwise dynamics:

with

the fixed point will be stable if the eigenvalues of the Jacobian all have negative real parts. Within our assumption x i ≈1/ N, we may focus on the eigenvalues of A eff, which differs from the Jacobian by a constant factor N that does not affect the signs of the eigenvalues (as previously reported, the actual Jacobian in a steady-state deviates somewhat from this simple analysis due to deviation of the fixed-point x i from 1/ N; Supplementary Fig. 9).

---

### [^1d4736be]. Developmental Medicine and Child Neurology (2019). Medium credibility.

5.2.3 书写

对于DCD患者，尤其是DCD儿童，书写是一个特殊的问题；实际上许多患儿都专门针对书写问题接受了治疗。

指南第一版发布后，尚没有DCD儿童书写干预的论文发表，因此第一版这方面的内容暂时不变。仅一项病例研究 264 报道了书写技能训练的相关内容，但只涉及3名儿童。经过强化训练后（每周2次，每次 45分钟；以及持续13周的家庭作业），其中2名儿童有所改善。

虽然智能手机、平板电脑和笔记本电脑等现代技术使用有所增加，并且在某种程度上（取决于文化背景和社会环境）儿童对书写的依赖程度有所下降，但由书写障碍引发的参与问题仍是DCD儿童面临的一个严峻考验。操作键盘和鼠标也是精细运动能力，这可能会给DCD儿童造成问题，他们可能需要接受训练。为防止DCD儿童书写或操作电脑问题带来的对学业成绩的影响（被贴上差生或学习成绩差的标签），可建议进行键盘和平板电脑操作的同步训练。

---

### Accelerated search for materials with targeted properties by adaptive design [^ef26033d]. Nature Communications (2016). Medium credibility.

Evaluation of regressor and selector combinations

As there are many ways to choose the regressor:selector pairs, we first need to choose the best regressor:selector combination. The approach we took to selecting the regressor:selector pair was to use cross-validation on the full set of training compounds we counted the average number of samples needed to find the best Δ T when trained on the randomly chosen subsets of the training data. For example, (a) we randomly selected (without replacement) s = 2 samples of the training data, (b) trained using those two samples and the known Δ T for those samples, (c) chose the next samples from the full training set using a given regressor:selector pair and (d) counted the total number of tries needed to find the best sample in the training set. We repeated this process 2,000 times using different pairs of randomly picked samples. When counting the number of tries to find the best sample in the training set, we included the random picks in the count, for example, if we picked the best compound on the first random sample, we counted one. If we did not find the best sample in the first two random picks and had to run the regressor:selector pair three times to find the best compound, then we counted five. Therefore, every random selection of two training samples gave us a count between 1 and 22, inclusive. The average of these counts was calculated over all 2,000 cross-validation runs to obtain the average number of tries for that regressor:selector combination.

---

### MINE: maximally informative next experiment-toward a new GWAS experimental design and methodology [^63ddd2be]. G3 (2025). Medium credibility.

Fig. 2.
A visual explanation of the relation between the parameter space and phenotype space (Y). a) If we maximize the volume (green square) of our phenotypic observations on the quantitative trait, then the choice of parameters (brown square) will be shrunk. If we set up various experiments, adding 1 more experiment each time, then the next parameter choice will be better and volume, tighter. Panel (a) is derived from. b) If we randomly choose 2 models (A 1 and A 2) with different prediction vectors, they can be correlated under one experimental condition (LHS) or uncorrelated (RHS) or another experimental condition. Replace the vectors by their expectations. The completed parallelepiped or square has volume det(D) or det(E) depending on the MINE criterion chosen in (18) or (19).

The covariance approach defines a volume (in the trait observation space) based on the uncertainty ellipsoid from the covariances of K observable predictions to reduce the volume in the parameter space (Fig. 2a). On the other hand, the correlation approach uses the Pearson correlation of K observable predictions for defining the ellipsoid. The MINE approach then uses the estimated parameters for the model by ensemble methods to calculate expected values of a predicted sample statistic G k in the next year.

where k = 1,2,3… K; j = 1,2,3… K, and i = 1,2,3, …, N. In general, the dimension of the prediction vector need not be the same dimension as the number of genotypes to be selected in the MINE problem, but here it is formulated as such. The constant K also represents the number of genotypes to be chosen in a given year to obtain the maximum amount of information about the model parameters. As examples, the covariance and correlation criteria between predictions in the next experiment are based on a matrix of covariances or correlations, respectively, computed from (14) and (15). Each element of the covariance matrix is defined as follows:

---

### [^4288f92c]. Developmental Medicine and Child Neurology (2019). Medium credibility.

4.2.4 临床检查：包含标准③和②

除病史采集，临床检查是排除其他导致运动障碍的医学状况的必要手段。神经行为检查的目的是排除其他运动障碍性疾病以符合标准③。同时应该进行全面的临床检查，以确认运动障碍不是由心理社会状况（例如剥夺、儿童虐待）和/或一般医学状况（例如脑瘫、偏瘫或肌营养不良）引起的。

排除由皮质脊髓束、小脑、锥体外系或神经肌肉等引起的神经系统疾病；神经代谢异常或获得性神经系统疾病（产前、围产期、产后）；周围神经系统疾病的体征。自上一版指南发布以来，尚未发现进行轻度神经功能障碍或神经软体征对DCD患者诊断存在价值。

由于注意力缺陷、学习障碍和孤独谱系障碍是DCD常见的共患病，建议对所有患有DCD的儿童进行行为和认知评估。如果存在适应性行为不良或情感问题，则必须根据各类疾病的指南进行进一步检查。

如果有正常的学习表现和学业成绩，则无需通过客观测量（例如智商测试）来评估认知功能。但若存怀疑，建议进行智力测验。

---

### Inconsistency between overall and subgroup analyses [^eed221ea]. General Psychiatry (2022). Medium credibility.

In table 3, we use numerical examples to show that if p₁ ≠ p₂ and d₁d₂ ≠ 0, all combinations of 1–9 and 13 in table 2 may occur.

Table 3
Numerical examples of Δ₀, Δ₁ and Δ when p₁ ≠ p₂ and d₁d₂ ≠ 0

The following theorem gives a more general sufficient condition of consistency than the first three cases discussed above.

Theorem: given Δ₀ and Δ₁, for any p₁ and p₂ between 0 and 1, there always exists a p between 0 and 1 such that Δ = Δ₀p + Δ₁(1 − p) if and only if p₁ = p₂ or d₁d₂ ≤ 0.

The proof of this theorem is available on request. Note that d₁d₂ = 0 implies d₁d₂ ≤ 0.

Unfortunately, if we are only given the information that p₁ ≠ p₂ and d₁d₂ > 0, we cannot determine whether the inconsistency will happen. For example, combinations 1 and 3 satisfy the condition of p 1 ≠ p 2 and d 1 d 2 > 0. In combination 1, the overall difference is consistent with the subgroup differences, while it is not in combination 3.

---

### Aspiration dynamics generate robust predictions in heterogeneous populations [^24589d16]. Nature Communications (2021). High credibility.

Fig. 2
Robust predictions generated by aspiration dynamics on weighted networks.

For the game, we set payoff value b = 0, c = 5, d = 1 and leave a as a tunable parameter. We plot the average frequency of strategy A, 〈 x A 〉, as a function of a. Symbols represent simulation results while solid lines are analytical ones. We construct weighted graphs by first generating an undirected graph with average degreeand then assigning weights to edges. The undirected graphs considered are random graph (left), regular graph (middle), and scale-free network (right). For each type of network, we test three edge weight distributions: homogeneous — every edge has weight one (i.e. unweighted network); uniform — edge weights are uniformly selected from the integer set {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}; power-law — edge weights are randomly selected from the discrete power-law distribution (Zipf distribution with the value of the exponent equal to 3). Each data point is obtained by averaging 〈 x A 〉 in 200 independent runs. For each run, we calculate 〈 x A 〉 by averaging the frequency of strategy A in the last 1 × 10 7 time steps after a transient time of 1 × 10 7 time steps. Other parameters: N = 1000, α l = 2.0 (l = 1, 2, ⋯, N), and β = 0.01.

---

### Model selection may not be a mandatory step for phylogeny reconstruction [^fd96e9fd]. Nature Communications (2019). High credibility.

This study is not the first to investigate the importance of model selection and the relative effectiveness of the various criteria. Yet, previous studies have arrived at conflicting conclusions regarding the preferred criterion. A possible cause for this ambiguity could be the relatively restricted conditions that were used to simulate the data. Namely, several delimited values of sample size and sequence divergence were defined, and simulated data were generated according to combinations of them. Although this approach indeed varies the examined test cases and allows fair comparisons over different scenarios, it still does not reflect the complex patterns concealed within empirical datasets, particularly with the ongoing accumulation of data. To overcome this limitation, here we used realistic features that were extracted from thousands of empirical datasets, spanning three databases that differ in their biological context. The datasets in PlantDBare typically of low divergence, as each contains homologous sequences within a single plant genus. The datasets in Selectomecontain homologous coding sequences across greater taxonomic groups, and were assembled for the inference of positive selection. These datasets thus exhibit rather high divergence but low number of taxa. PANDIT, on the other hand, covers common protein domains and thus the alignments are short but include many homologous protein-coding genes. Altogether, these form a diverse set of biological datasets that extends over a wide range of intricate realistic properties. Still, generating simulated datasets according to the properties of empirical datasets while relying on homogeneous substitution models does not reach the complexity of empirical datasets. To increase the complexity of these datasets, we generated additional simulation sets, c 1 and c 2, that integrate heterogeneity of the substitution models and evolutionary rates across sequence sites. An additional simulation set, c 3, was generated according to a codon model that illustrates processes which are dissimilar to those portrayed by the models available for inference. Our analyses over these assortments did not yield prominent differences between the criteria. Notably, even though these settings led to higher incongruencies, they still did not reach the intricacy of empirical datasets (Fig. 1), indicating that real data consist of patterns that are substantially more complex than the simple models commonly used for phylogeny reconstruction.

---

### Modelling sequences and temporal networks with dynamic community structures [^b9306130]. Nature Communications (2017). Medium credibility.

Fig. 1
Unified modelling of dynamics on and of networks. Our modelling framework simultaneously describes: a Arbitrary dynamics taking place on networks, represented as a sequence of arbitrary tokens that are associated with nodes, in this example { x t } = 'It-was the best of times'. b Dynamics of networks themselves, where the tokens are node-pair edges that appear in sequence, in this example { x t } = {(1, 2), (4, 3), (5, 2), (10, 8), (7, 2), (9, 3), (3, 4)}

In both cases, we employ a nonparametric Bayesian inference framework that allows us to select, according to the statistical evidence available, the most parsimonious model among all its variations. Hence we can, for example, identify the most appropriate Markov order and the number of communities without overfitting. In particular, if the dynamics on or of a network are random, our method will not identify any spurious patterns from noise but conclude that the data lack structure. As we also show, the model can be used to predict future network dynamics and evolution from past observations. Moreover, we provide publicly available and scalable code with log-linear complexity in the number of nodes independent of the number of groups.

---

### Model selection: finding the right fit [^6940b900]. The Journal of Foot and Ankle Surgery (2019). Medium credibility.

There are numerous possible goals for building statistical models. Those statistical goals, the associated model types, and each statistical tool involved in model building come with its own assumptions and requirements. In turn, these requirements must be met if we are to ensure that our models produce meaningful, interpretable results. However, beyond these technical details is the intuition, and the additional set of tools and algorithms, used by the statistician, to build the contextually appropriate model: not only must we build an interpretable model, we must build a model that answers the particular question at hand and addresses the particular goal we have in mind. In this column we discuss the methods by which statisticians build models for description, risk factor identification, and prediction.

---

### Identifying domains of applicability of machine learning models for materials science [^72c4ba76]. Nature Communications (2020). High credibility.

An illustrative example

Before describing the details of DA identification and its integration into the ML process, let us illustrate the concept and its utility via a synthetic example (see Fig. 1). We consider a simple two-dimensional representation consisting of independent features x 1 and x 2 that are each distributed according to a normal distribution with mean 0 and variance 2 (N (0, 2)) and a target property y that is a third-degree polynomial in x 1 with an additive noise component that scales exponentially in x 2 :That is, the y values are almost determined by the third-degree polynomial for low x 2 values but are almost completely random for high x 2 values. Discovering applicable domains reveals how different models cope differently with this setting even if they have a comparable average error. To show this, let us examine the error distributions obtained from three different kernelized regression models of the formwith parameter vector ν that are fitted around a training, or fitting (F), setwith three different choices for the kernel function k. We observe:
When using the linear (lin) kernel, the resulting linear model is globally incapable to trace the variation of the third-order polynomial except for a small stripe on the x 1 -axis where it can be approximated well by a linear function. Consequently, there is a very high error globally that is substantially reduced in the DA described by σ lin (x 1, x 2) ≡ −0.3 ≤ x 1 ≤ 0.3.
When using the Gaussian kernel), the resulting radial basis function (rbf) model is able to represent the target property well locally unless (a) the noise component is too large and (b) the variation of the target property is too high relative to the number of training points. The second restriction is because the rbfs have non-negligible values only within a small region around the training examples. Consequently, the discovered DA is not only restricted in x 2 -direction but also excludes high absolute x 1 -values: σ rbf ≡ −3.3 ≤ x 11 ≤ 3.1 ∧ x 2 ≤ 0.1.
In contrast, when using the non-local third-degree polynomial (poly) kernel, data sparsity does not prevent an accurate modeling of the target property along the x 1 -axis. However, this non-locality is counterproductive along the x 2 -axis where overfitting of the noise component has a global influence that results in higher prediction errors for the almost deterministic data points with low x 2 -values. This is reflected in the identified DA σ poly (x 1, x 2) ≡ −3.5 ≤ x 2 ≤ 0.1, which contains no restriction in x 1 -direction, but excludes both high and low x 2 -values. This highlights an important structural difference between the rbf and the polynomial model that is not reflected in their similar average errors.

---

### Understanding quantum machine learning also requires rethinking generalization [^b9a50be8]. Nature Communications (2024). High credibility.

Random labels

We start our randomization tests by drawing data from, wherein the true labels have been replaced by random labels sampled uniformly from {(0, 0), (0, 1), (1, 0), (1, 1)}. In order to sample from, a labeled pair can be obtained from the original data distribution, after which the label y can be randomly replaced. In this experiment, we have employed QCNNs with varying numbers of qubits n ∈ {8, 16, 32}. For each qubit number, we have generated training sets with different sizes N ∈ {5, 8, 10, 14, 20} for both random and real labels. The models were trained individually for each (n, N) combination.

In Fig. 3 a, we illustrate the results obtained when fitting random and real labels, as well as random states (discussed later). Each data point in the figure represents the average generalization gap achieved for a fixed training set size N for the different qubit numbers n. We observe a large gap for the random labels, close to 0.75, which should be seen as effectively maximal: perfect training accuracy and the same test accuracy as random guessing would yield. This finding suggests that the QCNN can be adjusted to fit the random labels in the training set, despite the labels bearing no correlation to the input states. As the training set sizes increase, since the capacity of the QCNN is fixed, achieving a perfect classification accuracy for the entire training set becomes increasingly challenging. Consequently, the generalization gap diminishes. It is worth noting that a decrease in training accuracy is also observed for the true labeling of data.

---

### One-way dependent clusters and stability of cluster synchronization in directed networks [^372bae27]. Nature Communications (2021). High credibility.

Figure 7 g shows the MLEs Λ q corresponding to cluster C q (q = 1, …, Q) vs. σ 2 (again, all the curves overlap because the clusters are intertwined and the same MLE determines the stability of all clusters for each value of σ 2): it is evident that all these clusters are stable for σ 2 exceeding a threshold.

Figure 7 h, i (obtained for σ 2 = 2.3 but with different initial conditions) shows time plots of the first component of the q th cluster state s q (t), for layer I (upper traces) and layer II (lower traces). Traces are colored as the corresponding clusters in Fig. 7b, e, respectively. In Fig. 7 i we see that almost each cluster corresponds to a different time trace, with the exception of the pairs { C 1, C 3 } and { C 7, C 9 }, which are synchronized: this means that, in the original network, nodes 1, 3, and 9 are in a cluster and 11, 13, 19 in a different one. By contrast, in Fig. 7 h, the traces correspond to the minimal balanced coloring. This indicates the presence of multistability for σ 2 = 2.3. We remark that also other stable patterns may emerge: for instance, by setting initial conditions such that the states of C4 and C6 are close, also C4 and C6 synchronize.

Now we consider the same network in Fig. 7 a, but we remove all interlayer connections from the upper layer to the lower layer. In this case, the matrices T and B do not depend on σ 2. In both cases, we focus on the minimal balanced coloring only. By removing the connections from the upper to the lower layer, the 2 × 2 submatrix B ⊥ is block-upper triangular and the structure of the corresponding matrix T evidences that cluster C 1 is one-way-dependent on C 2 (see the Supplementary Note 3.4, for details). Conversely, if we remove the connections from the lower to the upper layer, i.e. the thick red connections from layer II to layer I, B ⊥ is again a unique 2 × 2 block-upper triangular submatrix and the structure of the corresponding matrix T evidences that cluster C 2 is one-way-dependent on C 1.

Further details about the matrices obtained by applying our method are provided in Supplementary Data 2.

---

### Newborns' asymmetrical processing of order from sequentially presented magnitudes [^f4a344c9]. Child Development (2025). Medium credibility.

2.1.2 Stimuli

Stimuli were sequences of three visual displays generated using E‐Prime 1.0 software. Each display contained a fixed number of white, rectangular items arranged randomly within a virtual black area. The black area appeared on a black background, creating a seamless presentation where the outline of the item‐containing area was indistinguishable (Figure 1). Three sets of stimuli were used for the habituation phase and one for the test phase. The number of items contained in each sequence differed by a 1:3 ratio. The three habituation sets contained 4‐12‐36, 6‐18‐54, and 7‐21‐63 items, while the test set contained 5‐15‐45 items. In the habituation sets, non‐numerical continuous variables were controlled by keeping cumulative surface area and contour length constant within each set. To achieve this, item size was inversely correlated to number according to a 1:3 ratio, resulting in approximate sizes of 132, 44, and 14.7 cm 2 for the smaller, medium, and larger items respectively. The virtual area of each display was kept constant at 800 × 800 pixels, so that number covaried with density. For test displays, item size was kept constant at the size of the smallest items from habituation (14.7 cm 2), so that the cumulative surface area was positively correlated with number. Virtual display area increased proportionally (i.e. tripled) to the number of items, so that density was held constant across numerosities. This way, the continuous, non‐numerical variables that varied during habituation were held constant during the test, and vice versa.

FIGURE 1
The four sets of stimuli presented in Experiment 1. In the habituation stimuli (left), item size was inversely correlated to number according to a 1:3 ratio, and virtual display area remained constant. This maintained constant cumulative surface area and contour length while allowing density to covary with numerosity. The test stimuli (right) kept item size constant but increased the virtual display area threefold as the number of items increased. This resulted in a threefold increase in cumulative surface area and contour length, while density remained constant across numerosities.

---

### Concerns regarding… [^70dc23c9]. EClinicalMedicine (2024). Medium credibility.

Controversy about hyperparameter optimization of various machine learning models

In this study, the process of hyperparameter optimization is not described, even though different combinations of hyperparameters are crucial for model building and validation.

In the field of machine learning, using default hyperparameters may not be suitable for specific datasets. Lack of hyperparameter optimization can prevent the model from fully exploiting feature information, leading to less stable performance, high heterogeneity, and insufficient robustness, which negatively affects model building and generalization. Different combinations of hyperparameters significantly impact the predictive efficacy of the model. Failure to adequately perform hyperparameter optimization may result in models that do not achieve optimal predictive efficacy. This causes selection bias in the model and ultimately leads to incorrect decisions.

Machine learning is prone to model overfitting and poor generalization if the built-in hyperparameters are not properly set. Therefore, hyperparameter optimization in the field of machine learning is a scientific, rigorous, and reliable practice.

---

### Learning meaningful representations of protein sequences [^6646110e]. Nature Communications (2022). High credibility.

How we choose to represent our data has a fundamental impact on our ability to subsequently extract information from them. Machine learning promises to automatically determine efficient representations from large unstructured datasets, such as those arising in biology. However, empirical evidence suggests that seemingly minor changes to these machine learning models yield drastically different data representations that result in different biological interpretations of data. This begs the question of what even constitutes the most meaningful representation. Here, we approach this question for representations of protein sequences, which have received considerable attention in the recent literature. We explore two key contexts in which representations naturally arise: transfer learning and interpretable learning. In the first context, we demonstrate that several contemporary practices yield suboptimal performance, and in the latter we demonstrate that taking representation geometry into account significantly improves interpretability and lets the models reveal biological information that is otherwise obscured.

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^aeeaae45]. Nature Communications (2025). High credibility.

Fitting models to data is an important part of the practice of science. Advances in machine learning have made it possible to fit more-and more complex-models, but have also exacerbated a problem: when multiple models fit the data equally well, which one(s) should we pick? The answer depends entirely on the modelling goal. In the scientific context, the essential goal is replicability: if a model works well to describe one experiment, it should continue to do so when that experiment is replicated tomorrow, or in another laboratory. The selection criterion must therefore be robust to the variations inherent to the replication process. In this work we develop a nonparametric method for estimating uncertainty on a model's empirical risk when replications are non-stationary, thus ensuring that a model is only rejected when another is reproducibly better. We illustrate the method with two examples: one a more classical setting, where the models are structurally distinct, and a machine learning-inspired setting, where they differ only in the value of their parameters. We show how, in this context of replicability or "epistemic uncertainty", it compares favourably to existing model selection criteria, and has more satisfactory behaviour with large experimental datasets.

---

### Analysis and prediction of older adult sports participation in south Korea using artificial neural networks and logistic regression models [^a8ad33c6]. BMC Geriatrics (2023). Medium credibility.

Artificial neural network model

The application of the artificial neural network model proceeded as follows. First, the algorithm applied an equation for prediction. Second, parameter estimation was organized as a ratio of 70% training set and 30% test set. Third, the training method used sigmoid functions (activation functions characterized by collecting signal strengths from multiple neurons and converting them into numbers close to 1 as the signal strength becomes greater than 0, and vice versa) that are commonly used in non-linear functions and artificial neural networks; the weights were designated as 0.9 to limit the demand for infinitely large weight values. Fourth, the learning rate eta played a role in adjusting the weight modified in the process of finding the target variable by finding the direction to adapt to, and the artificial neural network model repeatedly, and this study was conducted by fixing it to the most commonly used eta value of 0.3. Fifth, the number of neurons in the hidden layer determined from the results were compared by applying the number of nodes in the hidden layer in various ways, such as 1, 2, 3, 4, 8, 16, and 32. In general, the rules for determining the number of neurons are as follows. First, "the number of hidden layer neurons is 2/3 of the size of the input layer". Second, "the number of neurons in the hidden layer must be less than twice the number of neurons in the input layer". Third, "the size of the hidden layer neuron is between the input layer size and the output layer size". Given that the number of input layers was 14 and the number of output layers was two, the most suitable number of hidden layers was identified as three. The study was conducted by designating all clusters as the final three hidden layers. These steps were applied to analyze the artificial neural network model for each cluster. Clusters 1 (60.45%), 2 (79.1%), 3 (66.8%), 4 (68.3%), and 5 (61.3%) had the highest possibilities of medical cost reduction (Table 3).

Table 3
Predictive probability (classification accuracy) analysis of medical cost reduction by cluster through an artificial neural network model

---

### A space-time tradeoff for implementing a function with master equation dynamics [^57fa10b0]. Nature Communications (2019). High credibility.

Master equations are commonly used to model the dynamics of physical systems, including systems that implement single-valued functions like a computer's update step. However, many such functions cannot be implemented by any master equation, even approximately, which raises the question of how they can occur in the real world. Here we show how any function over some "visible" states can be implemented with master equation dynamics-if the dynamics exploits additional, "hidden" states at intermediate times. We also show that any master equation implementing a function can be decomposed into a sequence of "hidden" timesteps, demarcated by changes in what state-to-state transitions have nonzero probability. In many real-world situations there is a cost both for more hidden states and for more hidden timesteps. Accordingly, we derive a "space-time" tradeoff between the number of hidden states and the number of hidden timesteps needed to implement any given function.

---

### Thinking outside the curve, part I: modeling birthweight distribution [^b60d1510]. BMC Pregnancy and Childbirth (2010). Low credibility.

b. Simulation study on calibrating confidence intervals

For our second simulation study we generated 25 overlapping data sets of size 50,000 from design C in Table 3, the degree of overlap consistent with a population of 200,000. For each of various C between 2.0 and 5.0, we used Equation (7) to form confidence intervals for the mixture parameters p 1, p 2, p 3, p 4, μ 1, μ 2, μ 3, μ 4, σ 1, σ 2, σ 3, σ 4. We recorded how many of the mixture parameters were contained in their respective confidence intervals. This was repeated nine more times, and we tabulated how many of the 120 = 12 × 10 confidence intervals contained their targets. Confidence intervals were also formed using Equation (6) for comparative purposes. The above steps were repeated with overlapping data sets consistent with a population of 1,000,000 and with nonoverlapping data sets consistent with an effectively infinite population.

The results are summarized in Table 5. With an effectively infinite population, only 81.7% of the confidence intervals formed using Equation (6) contained their targets at C = 5.0. The confidence intervals formed using Equation (7) contained their targets 95.0% of the time at C = 2.5. The adjustment suggested by Equation (8) appears reasonable: φ = .05 = 50,000/1,000,000 and N rep = 25 yield C φ = 1.315 C 0, which accords with the 95.8% capture of mixture parameters at C = 3.5 ≈ 1.315 × 2.5 with a population of 1,000,000.

Table 5
Confidence Interval Coverage Probabilities in Simulation Studies

The row with " C " = 2 and "Population size" = 200,000 identifies the numbers and percentages of confidence intervals containing their targets of mixture parameters, based on 10 repetitions in each of which 25 samples of size 50000 were simulated from a 4-component normal mixture with 12 parameters; results under the heading of "Bias adjustment included" are based on Equation (7) with C = 2, results under the heading of "Bias adjustment omitted" are based on Equation (6) with C = 2, and the 25 samples of size 50000 had overlap consistent with a population size of 200,000. Other rows correspond to different choices of C and/or population sizes.

---

### Observing network dynamics through sentinel nodes [^fc98f793]. Nature Communications (2025). High credibility.

Next, we seek to approximate thevs. D curve by tracking a small number n of sentinel nodes determined by our optimization algorithm. In Fig. 1 c, we show the outcome for n = 1 (orange line). To be specific, we selected the best single sentinel out of the N nodes, which minimizes ε in Eq. (3). We observe a gap between the sentinel approximation (purple) and the exact(black), and hence we continue to search for larger sentinel node sets S. Indeed, as we increase n in Fig. 1 d–f, we find that the sentinel approximation follows the black solid line more closely. At n = 4, we find that S successfully recovers the observed multistage transition, yielding ε = 0.004, a negligible discrepancy from the full system average. Notably, the smaller sets are not included as part of the larger ones, namely, the single node (n = 1) in Fig. 1 c is not part of the sentinel node sets with n = 2 (Fig. 1 d), 3 (Fig. 1 e), or 4 (Fig. 1 f). The sentinel property is a characteristic of the set S, not of its individual elements.

---

### Triazolam [^59f91090]. FDA (2025). Medium credibility.

WARNING: RISKS FROM CONCOMITANT USE WITH OPIOIDS; ABUSE, MISUSE, AND ADDICTION; and DEPENDENCE AND WITHDRAWAL REACTIONS

Concomitant use of benzodiazepines and opioids may result in profound sedation, respiratory depression, coma, and death. Reserve concomitant prescribing of these drugs in patients for whom alternative treatment options are inadequate. Limit dosages and durations to the minimum required. Follow patients for signs and symptoms of respiratory depression and sedation [see Warnings and Precautions (5.1), Drug Interactions (7.1)].
The use of benzodiazepines, including triazolam, exposes users to risks of abuse, misuse, and addiction, which can lead to overdose or death. Abuse and misuse of benzodiazepines commonly involve concomitant use of other medications, alcohol, and/or illicit substances, which is associated with an increased frequency of serious adverse outcomes. Before prescribing triazolam and throughout treatment, assess each patient's risk for abuse, misuse, and addiction [see Warnings and Precautions (5.2)].
The continued use of benzodiazepines, including triazolam, may lead to clinically significant physical dependence. The risks of dependence and withdrawal increase with longer treatment duration and higher daily dose. Abrupt discontinuation or rapid dosage reduction of triazolam after continued use may precipitate acute withdrawal reactions, which can be life-threatening. To reduce the risk of withdrawal reactions, use a gradual taper to discontinue triazolam or reduce the dosage [see Dosage and Administration (2.3), Warnings and Precautions (5.3)].

WARNING: RISKS FROM CONCOMITANT USE WITH OPIOIDS; ABUSE, MISUSE, AND ADDICTION; and DEPENDENCE AND WITHDRAWAL REACTIONS

See full prescribing information for complete boxed warning.

Concomitant use of benzodiazepines and opioids may result in profound sedation, respiratory depression, coma, and death. Reserve concomitant prescribing of these drugs in patients for whom alternative treatment options are inadequate. Limit dosages and durations to the minimum required. Follow patients for signs and symptoms of respiratory depression and sedation (5.1, 7.1).
The use of benzodiazepines, including triazolam, exposes users to risks of abuse, misuse, and addiction, which can lead to overdose or death. Before prescribing triazolam and throughout treatment, assess each patient's risk for abuse, misuse, and addiction (5.2).
Abrupt discontinuation or rapid dosage reduction of triazolam after continued use may precipitate acute withdrawal reactions, which can be life-threatening. To reduce the risk of withdrawal reactions, use a gradual taper to discontinue triazolam or reduce the dosage (2.3, 5.3).

---

### Machine learning in spectral domain [^b6a6f35e]. Nature Communications (2021). High credibility.

In Fig. 6, we report the results of the tests performed when operating under the deep linear configuration. Symbols are analogous to those employed in Fig. 5. In all inspected cases, the entry layer is made of N 1 = 784 elements and the output one has N ℓ = 10 nodes. The first five points, from left to right, refer to a three layers (linear) neural network. Hence, ℓ = 3 and the size of the intermediate layer is progressively increased, N 2 = 20, 80, 100, 500, 800. The total number of trained eigenvalues is N 2 + N 3, and gets therefore larger as the size of the intermediate layer grows. The successive four points of the collections are obtained by setting ℓ = 4. Here, N 2 = 800 while N 3 is varied (= 100, 200, 400, 600). The training impacts on N 2 + N 3 + N 4 parameters. Finally, the last point in each displayed curve is obtained by working with a five layers deep neural network, ℓ = 5. In particular, N 2 = 800, N 3 = 600 and N 4 = 500, for a total of N 2 + N 3 + N 4 + N 5 tunable parameters. Also in this case, the spectral algorithm performs better than conventional learning schemes constrained to operate with an identical number of free parameters. Similarly, the distribution of the weights of an equivalent perceptron trained in reciprocal space matches that obtained when operating in the space of the nodes and resting on a considerably larger number of training parameters. To sum up, eigenvalues are parameters of key importance for neural networks training, way more strategic than any other set of equivalent cardinality in the space of the nodes. As such, they allow for a global approach to the learning, with significant reflexes of fundamental and applied interest. In all cases here considered, the learning can extend to the eigenvectors: an optimised indentation of the eigen-directions contribute to enhance the overall performance of the trained device.

---

### Dynamics of ranking [^58e0eeab]. Nature Communications (2022). High credibility.

Virtually anything can be and is ranked; people, institutions, countries, words, genes. Rankings reduce complex systems to ordered lists, reflecting the ability of their elements to perform relevant functions, and are being used from socioeconomic policy to knowledge extraction. A century of research has found regularities when temporal rank data is aggregated. Far less is known, however, about how rankings change in time. Here we explore the dynamics of 30 rankings in natural, social, economic, and infrastructural systems, comprising millions of elements and timescales from minutes to centuries. We find that the flux of new elements determines the stability of a ranking: for high flux only the top of the list is stable, otherwise top and bottom are equally stable. We show that two basic mechanisms - displacement and replacement of elements - capture empirical ranking dynamics. The model uncovers two regimes of behavior; fast and large rank changes, or slow diffusion. Our results indicate that the balance between robustness and adaptability in ranked systems might be governed by simple random processes irrespective of system details.

---

### Changes in semantic memory structure support successful problem-solving and analogical transfer [^5231072f]. Communications Psychology (2024). Medium credibility.

Introduction

In our daily life, we constantly deal with problems, ranging from the most mundane (e.g. what to cook for dinner given the ingredients at our disposal), to professional activities (e.g. how to reorganize our current plans to meet a new deadline), up to major societal challenges (e.g. how to find innovative solutions against global warming). How do we find new solutions to problems? While the ability to solve problems is a critical skill for adapting to new situations and innovating, the mechanisms underlying the problem-solving process remain largely unknown.

Among the new problems we face each day, some are well-defined (e.g. playing a jigsaw puzzle). The initial state (i.e. the number of independent pieces) and goal state (i.e. assembling the pieces so it looks like the picture model) are clear, and the solver can apply a set of operations (i.e. interlocking the pieces as a function of their shape) to reach the goal. However, for many of our problems (e.g. organizing work activities during the COVID-19 pandemic), the problem space is ambiguous. No heuristics or existing rules could be applied to transform the initial state into the goal state. Such "ill-defined" problemsthus require additional mental processes, which have been tightly linked to creative thinking –. Ill-defined problem-solving (or creative problem-solving) is often referred to as insight solving, where the solution comes to mind suddenly and effortlessly, with a "Eureka" phenomenon –. According to the Representational Change Theory, solving such problems involves restructuring the initial problem mental representational space, which presumably entails combining elements related to the problem in a new way. In theory, restructuring allows one to change perspective, reframe the problem, or escape its implicitly imposed constraints, leading to creative associations. For instance, consider the following problem: "A man walks into a bar and asks for a glass of water. The bartender points a shotgun at the man. The man says, 'Thank you', and walks out". The problem is ill-defined because the path to finding the solution is to be discovered, and the goal state is vague. Solving this problem first requires asking the right question: in which context would a shotgun and a glass of water help somebody? Rather than relying on obvious associations (e.g. a glass of water is related to thirst), solvers must fill the missing link between the relevant elements of the problem (a shotgun induces fear, and fear can be a remedy for hiccups, as can drinking a glass of water). Hence, restructuring the initial representation of a given problem would allow one to see this link and find its solution.

---

### Anticipating regime shifts by mixing early warning signals from different nodes [^95e0966d]. Nature Communications (2024). High credibility.

Fig. 3
Early warning signals with different node sets in a network with N = 2 nodes connected by a directed edge.

The solid lines represent the mean. The shaded regions represent the standard deviation. We set w = 0.5, σ 1 = 0.1, and L = 100. a (σ 2, Δ r) = (0.1, 1). b (σ 2, Δ r) = (0.1, 0.5). c (σ 2, Δ r) = (0.2, 1).

To rank these different candidates of early warning signals, we define the following index. We measure each early warning signal's mean and standard deviation at r = − 0.3 and r = − 0.1. We do so based on the premise that it is necessary to measure signals at least at two r values to estimate how responsive the signal is to the change in the environment, i.e. r. As we explained in Supplementary Note 1, andobey a normal distribution at equilibrium. Therefore, we measure a distance, denoted by d, between the normal distribution at r = − 0.3 and that at r = − 0.1 for each early warning signal. If d is large, it is relatively easy to tell the increase in the signal with relatively small uncertainty as r increases from − 0.3 to − 0.1, approaching a tipping point. Therefore, we propose that an early warning signal attaining a larger d value is better. Although there are various measures of the separability of two normal distributions, inspired by the t -statistic, we definewhere μ 1 and var 1 are the mean and variance of the normal distribution at r = − 0.3, and μ 2 and var 2 are those at r = − 0.1. We show in Supplementary Note 2 that the results shown in the remainder of this section are robust with respect to the choice of the two r values for calculating d. Note that d can be small even if the mean of the early warning signal grows by a large amount between the two r values. This is the case when var 1 or var 2 is large, i.e. when the uncertainty of the signal is large. In this situation, tracking the early warning signal is relatively uninformative. In contrast, even if the absolute magnitude of the increase in the signal is small (i.e. small), the signal provides a good early warning if the uncertainty of the signal is small.

---

### Summary benchmarks-full set – 2024 [^a9241842]. AAO (2024). High credibility.

Dry eye syndrome — care management advises to treat any causative factors that are amenable to treatment as patients with dry eye symptoms often have many contributory factors, and that specific therapies may be chosen from any category regardless of the level of disease severity, depending on physician experience and patient preference.

---

### A coreduction to syngas.O-CDots-CNthree component electrocatalyst design concept for efficient and tunable CO [^7666a13e]. Nature Communications (2017). Medium credibility.

Investigation of the catalytic mechanisms

A series of controlled experiments were carried out to validate and further understand the catalytic mechanism operating with the Co 3 O 4 -CDots-C 3 N 4 proposed in a previous section (Fig. 2). Issues investigated included: the generation sites of H•, CO, and H 2; the role of H +; the roles of CDots, C 3 N 4, and Co 3 O 4; the role of the proximity between the CDots, C 3 N 4, and Co 3 O 4; ways in which the relative significance of the different reaction channels and the resulting CO/H 2 volume ratio can be manipulated; other HER-CDots-C 3 N 4 systems with different HER materials. We first studied the linear sweep voltammetry (LSV) curves of different combinations of the three components comprising the ternary Co 3 O 4 -CDots-C 3 N 4. The comparison of the different LSV curves (Fig. 4a) and the complementary study of the CO and H 2 composition generated during the electrocatalytic processes catalyzed by the different components (Fig. 4b) enabled a clear determination of both the generation sites of the CO and H 2 and the role of the three basic components used. We first discuss the LSVs showing the apparent current density (current density per geometrical area), which is also equivalent to the mass activity of the catalysts (current per gram catalyst) as shown in Fig. 4a. Further on, we also give the corresponding turn on frequencies (TOFs) and the real current densities derived from the real surface areas of the components of the different combinations of catalysts in Fig. 4a. LSVs of C 3 N 4 (Fig. 4a, curve 1, black trace, morphology of C 3 N 4 is shown in Supplementary Fig. 8a), CDots (Fig. 4a, curve 2, cyan trace, morphology of CDots is shown in Supplementary Fig. 1), Co 3 O 4 (curve 3, gray trace, morphology of Co 3 O 4 is shown in Supplementary Fig. 8b), CDots-C 3 N 4 (curve 4, blue trace, morphology of CDots-C 3 N 4 is shown in Supplementary Fig. 8c), Co 3 O 4 -C 3 N 4 (curve 5, green trace, morphology of Co 3 O 4 -C 3 N 4 is shown in Supplementary Fig. 8d), Co 3 O 4 -CDots (curve 6, purple trace, morphology of Co 3 O 4 -CDots is shown in Supplementary Fig. 8e), and Co 3 O 4 -CDots-C 3 N 4 (curve 7, red trace) were performed using a CO 2 -saturated 0.5 M KHCO 3 (pH = 7.2) solution (Fig. 4a). The LSVs of C 3 N 4 (curve 1, black trace), CDots (curve 2, cyan trace), Co 3 O 4 (curve 3, gray trace), CDots-C 3 N 4 (curve 4, blue trace), and Co 3 O 4 -C 3 N 4 (curve 5, green trace) show poor electrocatalytic performances evident by their low current densities and high onset potentials (Fig. 4a). Only the Co 3 O 4 -CDots (curve 6) and the Co 3 O 4 -CDots-C 3 N 4 (curve 7) exhibit high current densities and low onset potentials (Fig. 4a). CDots generate H 2 only, but not CO (Fig. 4b). We explain it by their trapping and stabilizing H•, which may generate a small amount of H 2 even in the absence of a HER catalyst. The LSV of Co 3 O 4 (curve 3, gray trace) shows a relatively small activity producing only H 2 (Fig. 4b). In comparison, Co 3 O 4 -CDots (curve 6, purple trace) produces a much larger amount of H 2 compared to pure CDots or pure Co 3 O 4. We attribute it to the effect of the CDots, which greatly enhance the electrocatalytic performance of Co 3 O 4 by providing H•, which is necessary for generation of H 2. We thus conclude that Co 3 O 4 is the H 2 generation site while CDots are the generation site of H• and both are needed for a large generation rate of H 2. Now, we prove that the generation site of CO is C 3 N 4. Figures 3 a, b and 4a, b indicate that significant amounts of CO are generated by either the CDots-C 3 N 4 or the Co 3 O 4 -CDots-C 3 N 4. Co 3 O 4 was shown to generate H 2 only, which leaves CDots-C 3 N 4 as the producer of CO. CDots per se produce only H 2 as previously discussed. They however are essential for CO generation by CDots-C 3 N 4 since they significantly enhance the CO 2 adsorption, adsorb H +, and stabilize H•. C 3 N 4 per se catalyzes only a very small (negligible) current (Fig. 4a, curve 1) so that the very little amount of gas produced by pure C 3 N 4 (Fig. 4b) contains more H 2 than CO. The supply of H• by the CDots is necessary to promote the generation of CO in C 3 N 4. Figure 4a shows that each catalyst component adds to the activity by enhancing one of the three reactions (H• generation, H 2 generation, and CO generation) but the complete three components composite Co 3 O 4 -CDots-C 3 N 4 is necessary for intense generation of syngas. We further studied the effect of the type of mixing of the different catalyst components on the catalytic activity. The LSVs of physical mixtures of catalysts (Co 3 O 4 + C 3 N 4, CDots + C 3 N 4, and Co 3 O 4 + CDots + C 3 N 4) were compared to those of composites of the same components chemically blended (Co 3 O 4 -C 3 N 4, CDots-C 3 N 4, and Co 3 O 4 -CDots-C 3 N 4). The chemically prepared composites had much larger activities (current densities) than their corresponding physical mixtures (Supplementary Fig. 9). We attribute the large activity of the chemically prepared composites to the close proximity between the different catalysts (active sites) in the composite materials. In contrast, physical mixing does not provide such a proximity so that the large distance between the active sites hinders the activity of the physically mixed catalysts.

---

### Clustering knowledge and dispersing abilities enhances collective problem solving in a network [^a4ac9110]. Nature Communications (2019). High credibility.

NK spaces are analytically useful for agent-based models but are conceptually abstract and difficult to ground to real-world examples. Here, we can instead understand an NK problem's complexity in terms of how incremental contributions made by any subset of activities are also contingent on other activities. Thus, a solution that an agent comes up within the NK space — captured by a unique sequence of 1s and 0s — reflects a basket of activities people undertake to solve a real-world problem, where a 1 represents the presence of an activity and a 0 its absence.

There are many examples of social collectives solving a common problem that are analogous to the NK space setup. In these instances, the solutions and performances are visible at least some subset of other actors, and there are no positive or negative externalities from one actor to another (beyond the informational spillover). For instance, consider national ministries of health of various countries. They all have different approaches to healthcare (e.g. insurance markets like in Germany or Austria or a single-public option like in Canada or the U.K.). And the improvement or decline of one country's healthcare does not adversely affect or hamper the healthcare of another. The interest of some countries in the policies employed by other countries may be limited by physical adjacency or similar political systems or cultures, so their information about the "problem space" is more localized.

---

### A machine learning automated recommendation tool for synthetic biology [^5caa5c39]. Nature Communications (2020). High credibility.

Optimization-suggesting next steps

The optimization phase leverages the predictive model described in the previous section to find inputs that are predicted to bring us closer to our objective (i.e. maximize or minimize response, or achieve a desired response level). In mathematical terms, we are looking for a set of N r suggested inputs, that optimize the response with respect to the desired objective. Specifically, we want a process that:
i. optimizes the predicted levels of the response variable;
ii. can explore the regions of input phase space (in Eq. (1)) associated with high uncertainty in predicting response, if desired;
iii. provides a set of different recommendations, rather than only one.

We are interested in exploring regions of input phase space associated with high uncertainty, so as to obtain more data from that region and improve the model's predictive accuracy. Several recommendations are desirable because several attempts increase the chances of success, and most experiments are done in parallel for several conditions/strains.

In order to meet these three requirements, we define the optimization problem formally aswhere the surrogate function G (x) is defined as:depending on which mode ART is operating in (see the "Key capabilities" section). Here, y * is the target value for the response variable, y = y (x), and Var(y) denote the expected value and variance, respectively (see "Expected value and variance for ensemble model" in Supplementary Information), denotes Euclidean distance, and the parameter α ∈ [0, 1] represents the exploitation-exploration trade-off (see below). The constraintcharacterizes the lower and upper bounds for each input feature (e.g. protein levels cannot increase beyond a given, physical, limit). These bounds can be provided by the user (see details in the "Implementation" section in Supplementary Information); otherwise, default values are computed from the input data as described in the "Input space set" section in Supplementary Information.

---

### Cutting edge or blunt instrument: how to decide if a stepped wedge design is right for you [^471f52f2]. BMJ Quality & Safety (2021). High credibility.

When might I consider doing a stepped wedge trial?

Research designs are shaped as much by practical constraints as by abstract schemes, and it is always a good idea to start with the constraints and work towards a design, rather than start with a design and try to fit it to constraints. These constraints will be unique to each research context, and box 1 lists some areas to think about. Still, there are some common features of settings where a stepped wedge trial might be considered as a possible design, and we now review these.

Box 1
Practical constraints on the design of a longitudinal cluster randomised trial

Are there limits on the time available to complete the evaluation, on the number of clusters, or on the number of participants (or the rate at which you can recruit participants) at each cluster? These constraints put limits on the overall scale of the evaluation, or force trade-offs between different design characteristics.
How will participants and their data be sampled in your study: as a series of cross-sectional surveys, as a continuous stream of incident cases, as a cohort followed over time, or some other way? Does the timescale divide into cycles, seasons or milestones that influence how you will sample participants and data?
Is there a limit on how many clusters can implement the intervention at the same time in the evaluation? If this is constrained by research resources (eg, if there are only enough trained research staff to implement the intervention one cluster at a time) then implementation must be staggered in some way.
If implementation is to be staggered, is there a minimum 'step length'? If the same team delivers the intervention in different clusters at different steps, then bear in mind it may take some time to get the intervention fully operational at a site, and the team will also need time to relocate from one cluster to the next.

---

### How to read articles that use machine learning: users' guides to the medical literature [^f5d20a58]. JAMA (2019). Excellent credibility.

In recent years, many new clinical diagnostic tools have been developed using complicated machine learning methods. Irrespective of how a diagnostic tool is derived, it must be evaluated using a 3-step process of deriving, validating, and establishing the clinical effectiveness of the tool. Machine learning-based tools should also be assessed for the type of machine learning model used and its appropriateness for the input data type and data set size. Machine learning models also generally have additional prespecified settings called hyperparameters, which must be tuned on a data set independent of the validation set. On the validation set, the outcome against which the model is evaluated is termed the reference standard. The rigor of the reference standard must be assessed, such as against a universally accepted gold standard or expert grading.

---

### Abstract representations emerge naturally in neural networks trained to perform multiple tasks [^5c3b6bfe]. Nature Communications (2023). High credibility.

For the input model, we use random Gaussian processes with radial basis function kernels of different length scales. To illustrate this approach, we begin with a D = 1 normally distributed latent variable (Fig. 5 a, left), then generate random Gaussian process functions that map this variable to a scalar output (three functions in Fig. 5 a, center). The scalar outputs of many random Gaussian process functions are then used as input to the multi-tasking model (Fig. 5 a, right; and see "Random Gaussian process inputs" in Methods for more details). Where we show results from these random Gaussian process inputs, we use D = 5 rather than the D = 1 used in the example.

The length scale of the random Gaussian process kernel controls how far two points need to be from each other in latent variable space before they become uncorrelated in representation space. As a result, the length also controls how nonlinear and non-abstract the resulting input representation of the latent variables is (Fig. 5 b). In particular, a low length scale (e.g. < 1) means that the input representation is both relatively high-dimensional (Fig. 5 c, left) and non-abstract (Fig. 5 f, g, gaussian process input). Alternatively, a high length scale (e.g. > 4) produces low-dimensional (Fig. 5 c, right) and abstract representations (Fig. 5 f, g, gaussian process input). We show that the multi-tasking model achieves high classifier generalization performance for all random Gaussian process input length scales that we investigated (Fig. 5 f, multi-tasking model, top row). We also show that the multi-tasking model achieves moderate regression generalization performance for many different length scales as well, though regression generalization performance remains at chance for the shortest length scales that we investigated (Fig. 5 g, multi-tasking model, top row).

---

### Identifying domains of applicability of machine learning models for materials science [^3de76373]. Nature Communications (2020). High credibility.

Table 1
Features used for discovery of domain of applicability (DA) selectors.

The DA optimization and validation can be performed as a by-product from the labels and ML predictions of the test set. However, just as for the ML-model fitting itself, we can only estimate these quantities based on empirical data. For that purpose, it is sensible to also split the test data into two parts: a DA identification set for optimizing the empirical impact and a DA validation set for obtaining an unbiased performance estimate of the identified DA (see Fig. 2 for an illustration of the overall workflow). Technically, the data points withheld in the DA validation set mimic novel independent sample points that can be used to evaluate both the coverage of the DA, as well as, the reduction in model error. As an extension of this, one can also repeat the DA optimization/validation on several splits (cross-validation) to reduce the variance of the coverage and model error estimates and, moreover, to assess the stability of the DA selector elements.

Fig. 2
Workflow for domain of applicability (DA) identification and validation for an ML model.

The DA is described by a selector (σ f) that is comprised of logical conjunctions of a representation space (here symbolized by a single dimension x for simplicity but may be multidimensional). The selector is identified by applying subgroup discovery (SGD) to the individual ML-model errors for subset of test set (DA identification set). An unbiased estimate of the model performance within the DA is obtained on the remaining samples of the test set that were left out of the DA identification (DA validation set).

For ease of notation we assume the DA identification set consists of the first k points of the test set. We end up with the following objective function for the SGD algorithm:where s denotes the number of points in the DA identification set selected by σ and I (σ) = { i: 1 ≤ i ≤ k, σ (x i) = true} denotes the set of selected indices itself. Here, we focus on DA identification based on the relative error, as it is less correlated with the target values than the absolute error. Thus, this choice promotes DAs that contain a representative distribution of target values and, by extension, more distinct and thus more characteristic DAs for the different models (see Supplementary Note 2 for a discussion of the DAs resulting from using the absolute error).

---

### A framework for evaluating clinical artificial intelligence systems without ground-truth annotations [^66a3bb92]. Nature Communications (2024). High credibility.

Cycle through the pseudo-labels

We repeated the above approach (Fig. 1 b, Steps 3, 4, and 5) however with two distinct changes. First, we pseudo-labelled the sampled data points with class 1 (instead of class 0). In doing so, we are hypothesizing that these data points belong to class 1. Second, we trained a classifier to distinguish between such newly-labelled data points and data points with ground-truth labels from class 0.

When experimenting with the distinct pseudo-labels, we always sample the same set of data points, as enforced by a random seed. Doing so ensures that any difference in the predictive performance of the learned classifiers, g ϕ, is less attributable to differences in the sampled data points and, instead, more attributable to the pseudo-labels that we have assigned. Moreover, to ensure that our approach is not constrained by a particular subset of sampled data points, we repeat this entire process multiple (k = 5) times, each time sampling a different subset of data points from each probability interval, as enforced by a random seed (e.g. 0 to 4 inclusive).

Derive the pseudo-label discrepancy

The discrepancy between, and ranking of, the classifier performances above is indicative of data points that are more likely to belong to one class than another. Concretely, if the classifier, g ϕ, achieves higher performance when presented with sampled data points that are pseudo-labelled as class 0 than as class 1, then the set of pseudo-labelled data points are more likely to belong to class 0. We refer to this discrepancy in performance under different scenarios of pseudo-labels as the pseudo-label discrepancy, or SUDO.

Implementation details of SUDO experiments

SUDO involves selecting several hyperparameters. These can include the granularity and number of probability intervals, the number of data points to sample from each probability interval, the number of times to repeat the experiment, and the type of classifier to use. We offer guidelines on how to select these hyperparameters in the Results.

---

### Dynamics of extinction debt across five taxonomic groups [^8cc1c1ae]. Nature Communications (2016). Medium credibility.

To make predictions using equation (4), we need to know the species number and the average number of individuals per species before habitat loss. In the absence of direct knowledge of S 0, the continental Arrhenius SAR can be used to describe the community before habitat loss. In Fig. 3, we use this approach to examine how the time constants, t F, t 50 and t L, change with area for two of the most commonly studied groups: small mammals and tropical birds. All three constants increase with area but with different exponents. For example, a habitat remnant of 10 km 2 for birds has a half-life of 351 years, but the first extinction happens much sooner, in < 10 years. In Supplementary Note 3, we show that for a SAR with exponent z, the half-life and time until first determined extinction is related to area as t 50 ∼ A α-zα and t F ∼ A α-z (1+ α), respectively, so that if α = 0.5 and z = 0.15, then t 50 ∼ A 0.425 and t F ∼ A 0.275. Thus, the time to first determined extinction, an important threshold for conservation action, increases only slowly with area. Even for a 1,000 km 2 area, the first extinction is expected to happen in just 32 years. This means that the delay in extinction is not large enough to constitute a policy excuse to 'kick the can down the road' to the next (human) generation.

---

### Designing accurate emulators for scientific processes using calibration-driven deep models [^5e03881e]. Nature Communications (2020). High credibility.

Predictive models that accurately emulate complex scientific processes can achieve speed-ups over numerical simulators or experiments and at the same time provide surrogates for improving the subsequent analysis. Consequently, there is a recent surge in utilizing modern machine learning methods to build data-driven emulators. In this work, we study an often overlooked, yet important, problem of choosing loss functions while designing such emulators. Popular choices such as the mean squared error or the mean absolute error are based on a symmetric noise assumption and can be unsuitable for heterogeneous data or asymmetric noise distributions. We propose Learn-by-Calibrating, a novel deep learning approach based on interval calibration for designing emulators that can effectively recover the inherent noise structure without any explicit priors. Using a large suite of use-cases, we demonstrate the efficacy of our approach in providing high-quality emulators, when compared to widely-adopted loss function choices, even in small-data regimes.

---

### How I read an article that uses machine learning methods [^d784cc52]. Blood Advances (2023). Medium credibility.

Step 3: Familiarize yourself with the ML methods used. The next step is to understand the ML methods that the authors have used to solve the problem. Many papers will provide a brief overview of the methods used (in clinical or applied journals), but it is important to have a good understanding of the underlying concepts. It is critical to familiarize yourself with some of these terminologies presented in Table 1. There are many papers that explain these terminologies in a very simple manner. It is also important to understand the key issues in building ML (Figure 1) models and what the authors did to address these at each step.

Table 1.
ML terminologies

Figure 1.
Steps to build a machine learning model. Problem formulation: The first step is to clearly define the problem that you want to solve. This involves defining the inputs and outputs of your model, as well as the type of problem you are trying to solve (classification, regression, clustering, etc). It is important to have a clear understanding of the problem you are trying to solve before you start building a model. Data collection: Once you have formulated the problem, the next step is to collect the relevant data. This may involve scraping data from websites, downloading data sets from public repositories, or collecting data through surveys or experiments. It is important to collect enough data to train your model and validate its performance. Data preparation: After collecting the data, you will need to clean and preprocess it. This involves removing any irrelevant data, dealing with missing values, and transforming the data into a suitable format for ML algorithms. It also includes dividing the data set into training, validation, and test cohorts. This step can take a lot of time and effort, but it is essential for building an accurate and effective model. Feature engineering: Feature engineering is the process of selecting and transforming the input variables (features) in a way that will improve the performance of the model. This may involve selecting the most relevant features, transforming them into a different representation (eg, using one-hot encoding), or creating new features based on existing ones. Feature engineering can have a significant impact on the performance of the model. Model selection: Once you have prepared the data and engineered the features, the next step is to select a suitable ML algorithm. This involves choosing the type of algorithm (eg, decision trees, neural networks, support vector machines) and the specific parameters of the algorithm. This step requires some knowledge of ML and experience with different algorithms. Model training: After selecting the algorithm, the next step is to train the model on the prepared data. This involves feeding the input data into the algorithm and adjusting the model parameters to optimize its performance. This step can take a lot of time and computational resources, especially for large data sets and complex models. Model evaluation: Once the model has been trained, the next step is to evaluate its performance on a separate test set of data. This involves measuring metrics, such as accuracy, precision, recall, and F1 score, to assess the performance of the model. It is important to test the model on data that it has not seen before to ensure that it can be generalized to new data. Model optimization: If the model performance is not satisfactory, then the next step is to optimize the model. This involves tweaking the model parameters, changing the algorithm, or modifying the feature engineering process to improve the model's performance. This step may require several iterations until the desired level of performance is achieved. Model deployment: Once you have built a satisfactory model, the final step is to deploy it in a production environment. This may involve integrating the model into a web application, creating an application programming interface for other developers to use, or deploying it as a stand-alone application. It is important to ensure that the model is well documented and tested thoroughly before it is deployed.

---

### Estimating the household secondary attack rate and serial interval of COVID-19 using social media [^d8aa7500]. NPJ Digital Medicine (2024). Medium credibility.

Analysis

For a given month (week), we determine the set of users, A, who tweeted that they and/or a household member had COVID-19 in this month. The set A is the union of A 1,0 (those that only tweet about themselves), A 2,0 (only about household members), A 1,2 (those that tweet about themselves and then household members), and A 2,1 (household members then themselves).

Consider the two cases A 1 = A 1,0 ∪ A 1,2, i.e. the subset of users who only tweet about themselves having COVID-19 or who tweet that they have COVID-19 before tweeting about a household member having COVID-19, and A 2 = A 2,0 ∪ A 2,1, i.e. the subset of users who only tweet about a household member having COVID-19 or who tweet that a household member has COVID-19 before tweeting about themselves having COVID-19. Let a 1 and a 2 denote the actions of tweeting about the user or a household member having COVID-19, respectively. Then the probability that individual, i, will tweet about a household member having COVID-19 after tweeting that they have COVID-19 iswhere E 2 represents the event that a household member has COVID-19, andrepresents the probability that a household member will get infected given α 1 is the secondary attack rate and n is the size of the household. The average household size for UK is n = 2.3, so (n − 1) = 1.3 ≈ 1, which gives usAs in, we assume that P i (a 2 ∣ E 2, a 1) is equal to P i (a 3), where, a 3 denotes the action of user i tweeting about a household member, irrespective of the subject.

---

### Hydrocodone bitartrate (hysingla ER) [^0c03a0bc]. FDA (2023). Medium credibility.

Warnings and precautions regarding the use of hydrocodone bitartrate ER PO (also known as Hysingla ER):
- **Adrenal insufficiency**: use caution in patients taking the drug for a prolonged period (> 1 month).
- **Central sleep apnea**: use caution in patients taking higher doses.
- **Decreased serum hydrocodone level**: use caution in patients taking CYP3A4 inducers (such as rifampin, carbamazepine, or phenytoin) or discontinuing CYP3A4 inhibitors (such as macrolide antibiotics, azole antifungals, or protease inhibitors).
- **Erectile dysfunction, infertility**: use caution in patients taking the drug for a prolonged period.
- **Exacerbation of increased ICP**: use caution in patients with increased ICP, brain tumor, or head injury.
- **Growth suppression**: use caution in patients with chronic corticosteroid therapy. Monitor growth regularly in pediatric patients receiving chronic corticosteroid therapy. Reassess the need for hydrocodone regularly and adjust the corticosteroid therapy as appropriate.
- **Hypotension, syncope**: use caution in patients with reduced blood volume or taking other CNS depressants. Monitor BP after initiation and dose titration. Avoid use in patients with circulatory shock.
- **Mask symptoms of head injury**: use caution in patients with head injury. Avoid use in patients with impaired consciousness or coma.
- **Opioid overdose**: use caution in patients taking CNS depressants or with a history of opioid use disorder or prior opioid overdose. Consider prescribing naloxone based on the patient's risk factors for overdose.
- **Opioid withdrawal syndrome**: do not discontinue abruptly in patients physically dependent on opioids.
- **Opioid withdrawal syndrome**: use caution in patients taking CYP3A4 inducers (such as rifampin, carbamazepine, or phenytoin) or discontinuing CYP3A4 inhibitors (such as macrolide antibiotics, azole antifungals, or protease inhibitors).
- **Opioid withdrawal syndrome**: use extreme caution in patients taking mixed agonist/antagonist analgesics (such as pentazocine, nalbuphine, or butorphanol) or partial agonist analgesics (such as buprenorphine).
- **Prolonged QT interval**: use caution in patients with congestive HF, bradyarrhythmia, electrolyte abnormalities, or taking drugs prolonging QT interval. Avoid use in patients with congenital long QT syndrome. Do not exceed 90 mg BID in patients developing QT prolongation.
- **Seizure**: use caution in patients with seizure disorder.
- **Serotonin syndrome**: use caution in patients taking serotonergic drugs.
- **Serotonin syndrome**: use extreme caution in patients taking MAOIs or within 14 days of stopping treatment.
- **Somnolence**: use extreme caution in patients performing activities requiring mental alertness, such as driving or operating machinery.
- **Sphincter of Oddi dysfunction**: use caution in patients with biliary tract disease and acute pancreatitis.

---

### Structure and inference in annotated networks [^8fbc66a4]. Nature Communications (2016). Medium credibility.

For many networks of scientific interest we know both the connections of the network and information about the network nodes, such as the age or gender of individuals in a social network. Here we demonstrate how this 'metadata' can be used to improve our understanding of network structure. We focus in particular on the problem of community detection in networks and develop a mathematically principled approach that combines a network and its metadata to detect communities more accurately than can be done with either alone. Crucially, the method does not assume that the metadata are correlated with the communities we are trying to find. Instead, the method learns whether a correlation exists and correctly uses or ignores the metadata depending on whether they contain useful information. We demonstrate our method on synthetic networks with known structure and on real-world networks, large and small, drawn from social, biological and technological domains.

---

### Disproportionate declines of formerly abundant species underlie insect loss [^715ce79f]. Nature (2024). Excellent credibility.

The community data available to us can be divided into three types, with different processing requirements:
The study provided one species-by site-by-year matrix. We used this directly to calculate all biodiversity metrics for analysis.
Species-by-site by-year data were provided together with an account of the sampling effort (number of samples underlying each row is reported, but no raw data were provided per individual sample). Here, we subsampled the years with higher trapping effort to provide the expected community composition if the number of samples had been the same as in the year with the lowest sample size. We repeated this subsampling procedure 100 times, and calculated the means of each biodiversity metric, to be used as model input. In some cases, it was clear that subsampling to the lowest sample size would lead to a large loss of data, and in such cases, it was more economical to discard a year than lose a large amount of data. A practical example: if year 1 has 300 individuals in 6 traps and year 10 has 500 individuals in 4 traps, we subsampled 200 individuals (4/6 of 300) from year 1, so that each year has the number of individuals equivalent to 4 traps. We repeated this resampling 100 times to derive 100 hypothetical community compositions, and calculated the biodiversity metrics and population abundances for each iteration.
Exact data were provided per sampling event (number of individuals per species per trap/site per date), but the number of traps, dates or periods sampled is variable among years (owing to trap malfunctions or changes in sampling design or sampling frequency). Here, we first divided the years into monthly or bimonthly periods, and then decided whether we could maximize data use by including more months or more years (that is, whether to exclude periods that were not sampled each year, or exclude years that did not have samples in each period). The aim here was to maximize data use, so that we could attain the highest quality of data for a maximum number of years. For each period, we then subsampled the minimum number of samples observed in any year from the available samples, and added these together to form one yearly sample. This was repeated 100 times, and the biodiversity metrics were calculated each time. The mean values of these 100 iterations were used for model input. A practical example: ten pitfall traps per site were usually emptied weekly, but sometimes fortnightly, the trapping period changed from year-round at the start to May–September in later years, and sometimes samples were missing owing to flooding or trampling by livestock. Here, we first discarded all months other than May–September. Then we allocated five monthly periods and calculated the number of sampling days (sum of sampling period × number of traps) for each month. We then drew random samples (without replacement) from the available samples per month, until the total number of sampling days added up to the minimum number sampling days observed. We added the months up to form one yearly sample and calculated our metrics. This was repeated 100 times.

---

### Detecting the ultra low dimensionality of real networks [^d3b6a33c]. Nature Communications (2022). High credibility.

At the light of these results, we have selected the K-NN method due to its simplicity and geometric interpretation. The implementation of machine learning methods (including K-NN) used in our experiments are those found in. Finally, to maximize the accuracy of our method, we select the value of the parameter K as the one maximizing the proportion of synthetic networks correctly classified with respect to their dimension. This value is then used to infer the dimension of the original network using the K-NN method.

Calculation of confusion matrices

Tables 1 and 2.

Table 1
Intervals of C t for D = 1 used to evaluate confusion matrices for different regions of clustering and levels of heterogeneity

Table 2
Number of surrogates used to evaluate confusion matrices for different regions of clustering and levels of heterogeneity

Reporting summary

Further information on research design is available in the Nature Research Reporting Summary linked to this article.

---

### Statistical methods in the global enteric multicenter study (GEMS) [^9ad11cb4]. Clinical Infectious Diseases (2012). Low credibility.

Suppose the CLR model for 4 pathogens, the first 2 in the set for which AF is to be estimated and the last two in the conditioning set, includes terms for all main effects and 2-way interactions (but not higher-order interactions), with coefficients b ijkl for combination (ijkl). Then, for example, the combination of pathogens 1, 2, and 4 corresponds to T 1101 = exp(b 1000 + b 0100 + b 1100 + b 1001 + b 0101). Note that a coefficient with > 1 of the subscripts equal to 1 is the coefficient of an interaction term; b 1100 is the coefficient of the product y 1 y 2, etc. For this example Table 1 gives all possible values of the natural logarithm of T, log e (T ijkl), in terms of the estimates b ijkl from the CLR model. Note that any combination (ijkl) for which no pathogen from set I is present will have log e (T ijkl) = 0 (ie, T ijkl = 1) in formula (3).

Table 1.
Natural Logarithm of Factors (Ratios of Odds Ratios) Corresponding to Combinations of Pathogens in Example of Adjusted Attributable Fraction (AF) Calculation: AF Is Calculated for Pathogens A1 and A2, Adjusted for Pathogens A3 and A4

In the GEMS data we have occasionally seen evidence of 2-way interactions, but we have seen no evidence of 3-way or higher interactions; thus, only interactions involving 2 pathogens need be considered in the GEMS analysis.

Cases were sampled for GEMS in approximately equal numbers during each 2-week period, regardless of the number of MSD cases appearing at the SHCs. We estimate AF both unweighted and using weights defined as (number of eligible cases/number of enrolled cases), ie, as the inverse of the sampling fraction for MSD cases. Data for adjacent 2-week periods are combined when there are no enrolled cases in a period. AF estimation is done separately for the 3 age strata (0–11, 12–23, and 24–59 months) within which MSD cases were sampled.

---

### Exploiting redundancy in large materials datasets for efficient machine learning with less data [^ac9b4284]. Nature Communications (2023). High credibility.

The examination of data redundancy is also important in other scenarios in materials science. Methods developed for selecting the most informative data can be used as the strong baselines for active learning algorithms, which are increasingly common in ML-driven materials discovery workflows –. Analysis of information richness can also improve our understanding of the material representation and guide the design of active learning algorithms. In the multi-fidelity data acquisition setting, one can perform high-fidelity measurement only on the informative materials down-selected from the larger but low-fidelity datasets.

In this work, we present a systematic investigation of data redundancy across multiple large material datasets by examining the performance degradation as a function of training set size for traditional descriptor-based models and state-of-the-art neural networks. To identify informative training data, we propose a pruning algorithm and demonstrate that smaller training sets can be used without substantially compromising the ML model performance, highlighting the issue of data redundancy. We also find that selected sets of informative materials transfer well between different ML architectures, but may transfer poorly between substantially different material properties. Finally, we compare uncertainty-based active learning strategies with our pruning algorithm, and discuss the effectiveness of active learning for more efficient high throughput materials discovery and design.

---

### Designing clinical databases [^3b5f01d2]. Otolaryngologic Clinics of North America (2002). Low credibility.

Evidence-based medicine has always relied on databases. Anyone who needs to know anything about the outcome of a treatment needs a database, but individual practitioners may feel that database design is beyond them for various reasons. Effective database design does not require complexity. This article seeks to be a practical guide for those who are contemplating development of their own database.

---

### Aggregation-fragmentation and individual dynamics of active clusters [^7c5c8762]. Nature Communications (2018). Medium credibility.

It remains to determine the rates, the essential inputs of an aggregation–fragmentation kinetic model. Although investigated in two recent simulation studies, they were never measured in experiments so far. As regards their dependence in cluster size, simple expectations exist (see refinements in refs.). Assuming monomers move ballistically and aggregate whenever they collide, the aggregation rate is proportional to the cross-section. For compact clusters and large size, one thus expects A N ~ N 1/2. In the simplest picture of fragmentation events, perimeter colloids are treated as independent and leave the cluster when pointing outward, leading again to F N ~ N 1/2. The aggregation and fragmentation rates as deduced from experimental data are plotted in Fig. 2d, together with the expressionsNote that A N vanishes for N = 2, consistent with the fact that dimers do not exist. Combining Eqs. (4) and (5), we obtain the cluster size distributionwhere κ = κ A / κ F. A two-parameter fit of the three data sets (C 1 A N, F N and C N) yields κ A = 0.50 ± 0.005 s −1 and κ F = 0.52 ± 0.005 s −1. The overall satisfactory agreement visible in Fig. 2b, d calls for two comments. First, thedependence of rates is consistent with data, providing support for the basic picture of monomer arrivals and departures, although data range is arguably too limited for a critical test of power law exponent. Second, Eqs. (5) and (6) do capture the small size behavior, which is not a genuine power law, and the tail, which is asymptotically exponential. Note that for completeness, Fig. 2b also displays the envelope associated with relative uncertainties on κ. Due to the exponential dependence, significant statistical spread remains despite very small 2% uncertainties, but it has to be appreciated in view of other predictions obtained from the rates alone as we show now.

---

### Local measures enable COVID-19 containment with fewer restrictions due to cooperative effects [^9bdb0f2b]. EClinicalMedicine (2021). Medium credibility.

Background

Many countries worldwide are faced with the choice between the (re)surgence of COVID-19 and endangering the economic and mental well-being of their citizens. While infection numbers are monitored and measures adjusted, a systematic strategy for balancing contact restrictions and socioeconomic life in the absence of a vaccine is currently lacking.

Methods

In a mathematical model, we determine the efficacy of regional containment strategies, where contact restrictions are triggered locally in individual regions upon crossing critical infection number thresholds. Our stochastic meta-population model distinguishes between contacts within a region and cross-regional contacts. We use current data on the spread of COVID-19 in Germany, Italy, England, New York State and Florida, including the effects of their individual national lockdowns, and county population sizes obtained from census data to define individual regions. As a performance measure, we determine the number of days citizens will experience contact restrictions over the next 5 years ('restriction time') and compare it to an equivalent national lockdown strategy. To extract crucial parameters, we vary the proportion of cross-regional contacts (between 0% and 100%), the thresholds for initiating local measures (between 5 and 20 active infections per 100,000 inhabitants) as well as their duration after infection numbers have returned below the threshold (between 7 and 28 days). We compare performance across the five different countries and test how further subdivision of large counties into independently controlled regions of up to 100,000 or 200,000 inhabitants affects the results.

Findings

Our numerical simulations show a substantially reduced restriction time for regional containment, if the effective reproduction number of SARS-CoV-2 without restrictions, R 0, is only slightly larger than 1 and the proportion of cross-regional contacts (the so-called leakiness) is low. In Germany, specifically, for R 0 = 1.14, a leakiness of 1% is sufficiently low to reduce the mean restriction time from 468 days (s.d. 3 days) for the national containment strategy to 43 days (s.d. 3 days across simulations) for the regional strategy, when local measures are initiated at 10 infections per 100,000 inhabitants in the past 7 days. For R 0 = 1.28, the allowed leakiness for minimal restriction time reduces to approximately 0.3%. The dependence of the restriction time on the leakiness is threshold-like only for regional containment, due to cooperative effects. It rises to levels similar to the national containment strategy for a leakiness > 10% (517 days national vs. 486 days regional for leakiness 32% and R 0 = 1.14). We find a strong correlation between the population size of each region and the experienced restriction time. For countries with large counties, this can result in only a mild reduction in restriction time for regional containment, which can only be partly compensated by lower thresholds for initiating local measures and increasing their duration. In contrast, further subdividing large counties into smaller units can ensure a strong reduction of the restriction time for the regional strategy.

Interpretation

The leakiness, i.e. the proportion of cross-regional contacts, and the regional structure itself were crucial parameters for the performance of the regional strategy. Therefore, regional strategies could offer an adaptive way to contain the epidemic with fewer overall restrictions, if cross-regional infections can be kept below the critical level, which could be achieved without affecting local socioeconomic freedom. Maintaining general hygiene and contact tracing, testing should be intensified to ensure regional measures can be initiated at low infection thresholds, preventing the spread of the disease to other regions before local elimination. While such tight control could lead to more restrictions in the short run, restrictions necessary for long-term containment could be reduced by up to a factor of 10. Our open-source simulation code is freely available and can be readily adapted to other countries.

Funding

This work was supported by the Max Planck Society.

---

### Data-driven control of complex networks [^37bb115f]. Nature Communications (2021). High credibility.

Data-driven minimum-energy control

By letting Q = 0 and R = I in Eq. (4), we recover a data-driven expression for the T -step minimum-energy control to reach y f. We remark that the family of minimum-energy controls has been extensively employed to characterize the fundamental capabilities and limitations of controlling networks, e.g. see. After some algebraic manipulations, the data-driven minimum-energy control input can be compactly rewritten as (Supplementary Note 5)The latter expression relies on the final output measurements only (matrix Y T) and, thus, it does not exploit the full output data (matrix Y 1: T −1). An alternative control expression isThis is a simple, suboptimal data-based control sequence that correctly steers the network to y f in T steps, as long as y f belongs to the range space of Y T (a condition that is normally satisfied when p randomly generated data are available). Further, and more importantly, when the input data samples are drawn randomly and independently from a Gaussian distribution with zero mean and finite variance, Eq. (6) converges to the minimum-energy control in the limit of infinite data (Supplementary Note 6).

Figure 3 a compares the performance (in terms of control effort and error in the final state) of the two data-driven expressions in Eqs. (5) and (6), and the model-based control as a function of the data size N. While the data-driven control in Eq. (5) becomes optimal for a finite number of data (precisely, for N = m T independent data), the approximate expression in Eq. (6) tends to the optimal control only asymptotically in the number of data (Fig. 3 a, left). In both cases, the error in the final state goes to zero after collecting N = p data (Fig. 3 a, right). For the approximate control in Eq. (6), we also establish upper bounds on the size of the dataset to get a prescribed deviation from the optimal control, in the case of Gaussian input data. Our nonasymptotic analysis indicates that this deviation is proportional to the worst-case control energy required to reach a unit-norm target. This, in turn, implies that networks that are easy to control require fewer trials to attain a prescribed approximation error (Supplementary Note 6).

---

### Summary benchmarks-full set – 2024 [^76ffec10]. AAO (2024). High credibility.

Preferred Practice Pattern (PPP) guidelines — scope and principles emphasize that Preferred Practice Patterns provide guidance for the pattern of practice, not for the care of a particular individual, and that the Preferred Practice Pattern guidelines are not medical standards to be adhered to in all individual situations. The series is based on three principles: each Preferred Practice Pattern should be clinically relevant and specific enough to provide useful information to practitioners; each recommendation that is made should be given an explicit rating that shows its importance to the care process; and each recommendation should also be given an explicit rating that shows the strength of evidence that supports the recommendation and reflects the best evidence available. Adherence to these Preferred Practice Patterns will not ensure a successful outcome in every situation, and these practice patterns should not be deemed inclusive of all proper methods of care or exclusive of other methods of care reasonably directed at obtaining the best results.

---

### Substantial underestimation of SARS-CoV-2 infection in the United States [^98cd20fc]. Nature Communications (2020). High credibility.

Constraining of prior distributions

The quantity P(S 0 | test+) is a function of other parameters P(S 1 | untested), α, β, meaning its distribution is entirely determined by those three parameters. However, we considered estimates of P(S 0 | test +) to be more reliable than those of P(test + | S 1) and P(test + | S 0) because P(S 0 | test +) has been estimated in numerous published studies, including some with more representative sampling. Thus, we used Bayesian melding to incorporate both sources of information into a joint (melded) distribution on P(S 0 | test +) and P(S 1 | test +) = 1 – P(S 0 | test +). Because the distribution of P(S 0 | test +) is constrained to take on plausible values in accordance with other parameters, it is unlikely that the priors disagree strongly with observed data, and probabilistic bias analysis should resemble a fully Bayesian approach.

The relationship between φ = P(S 0 | test +) and the parameters θ = {P(S 1 | untested), α, β } is given by a function (M: θ → φ):

Mathematically, once a distribution is assigned to θ and M is defined, the distribution of φ is fully specified. Bayesian melding, allows one to combine prior distributions on function inputs θ and output φ to generate a joint melded prior distribution on { θ, φ }. We assumed that P(S 0 |test +) followed a truncated beta distribution (defined below) which was combined with θ = { α, β, P(S 1 | untested)} and the function M given in Eq. (1) to produce the final distribution of P(S 0 |test +), shown in Supplementary Fig. 3. The advantage of this method is that it incorporates all available prior uncertainty as well as constraints on plausible output given by M. To sample values of { θ, φ } to be used as input to the probabilistic bias correction, we sampled 10 5 variates from the melded prior distribution using a Sampling-Importance-Resampling algorithm. We also sampled P(S 1 |test +) = 1 − P(S 0 |test +) using the same procedure.

---

### Rigorous location of phase transitions in hard optimization problems [^d98ebea5]. Nature (2005). Excellent credibility.

It is widely believed that for many optimization problems, no algorithm is substantially more efficient than exhaustive search. This means that finding optimal solutions for many practical problems is completely beyond any current or projected computational capacity. To understand the origin of this extreme 'hardness', computer scientists, mathematicians and physicists have been investigating for two decades a connection between computational complexity and phase transitions in random instances of constraint satisfaction problems. Here we present a mathematically rigorous method for locating such phase transitions. Our method works by analysing the distribution of distances between pairs of solutions as constraints are added. By identifying critical behaviour in the evolution of this distribution, we can pinpoint the threshold location for a number of problems, including the two most-studied ones: random k-SAT and random graph colouring. Our results prove that the heuristic predictions of statistical physics in this context are essentially correct. Moreover, we establish that random instances of constraint satisfaction problems have solutions well beyond the reach of any analysed algorithm.

---

### Elacestrant (Orserdu) [^25e5bc7d]. FDA (2024). Medium credibility.

The dosage of elacestrant PO for treatment of breast cancer in postmenopausal female adults with ESR1 mutation (advanced or metastatic, ER-positive, HER2-negative, progression after ≥ 1 line of endocrine therapy) is 345 mg PO daily until disease progression or unacceptable toxicity

---

### Clinical practice guideline for management of tinnitus: recommendations from the US VA / DOD clinical practice guideline work group [^8aa06c0d]. JAMA Otolaryngology — Head & Neck Surgery (2025). High credibility.

VA/DoD tinnitus guideline — EMBASE search strategy outlines predefined sets and Boolean combinations for study identification. For KQ 1, the sets specify #12 Combine inclusions (AND) using #9 AND #10 AND #11; #13 Systematic reviews and meta-analyses; #14 Randomized controlled trials; #15 Observational comparative studies; #16 Combine study types (OR) as #13 OR #14 OR #15; and #17 Apply all filters as (#4 NOT #8) AND #12 AND #16. For KQ 2, the sets define #1 Tinnitus ('tinnitus'/de OR tinnitus:ti, ab, kw), #2 Reassurance / education, and #3 Combine sets (#1 AND #2).

---

### An introduction to machine learning [^6efbe6ea]. Clinical Pharmacology and Therapeutics (2020). Medium credibility.

Main takeaways

Clustering can be used to understand structure in data by grouping similar observations together.
k ‐means clustering is a simple yet powerful tool, however, the number of clusters must be specified in advance.
Density‐based methods do not require a prespecified number of clusters and allows identification of complex patterns in the data.
Hierarchical clustering provides an overview of the relationship on multiple levels.
Dimensionality reduction is used not only for data visualization but also to drop uninformative features.

---

### Competition and evolutionary selection among core regulatory motifs in gene expression control [^0179e0c9]. Nature Communications (2023). High credibility.

Methods

Parameters

Wild-type E. coli grown under optimal conditions typically has mutation rates on the order of 10 −3 mutations per genome per generation. Considering the typical genome size of bacteria, this corresponds to approximately 10 −9–10 −8 mutations per base per generation –, though this rate may depend on population sizeand expression levels. Furthermore, hypermutators with up to 10 4 -fold greater mutation rates can occur under laboratory conditions, and more frequently in natural bacterial populations. Assuming roughly 100 sensitive nucleotide positions, we thus estimate the rate of loss-of-function mutations to span the range ν − ≈ 10 −7–10 −3 per generation. Since gain-of-function mutations are assumed to be less probable, we consider ν + = ν − /10 throughout the paper matching experimental estimates. Selection intensity is notoriously hard to measure –, however, based on estimates for codon bias, we consider s p / ν − = 10 and s p / ν − = 100 in case of weak and strong selection, respectively. Finally, we assume that s r < s p for a typical target gene, otherwise there would be no evolutionary selection pressure to regulate the expression of the product. Throughout the paper, the period T is measured in number of generations, whereas the mutation rates ν − and ν + as well as the selection intensities s p and s r are all given in 1/generation.

Stochastic simulation of the evolutionary dynamics

The standard Wright-Fisher model assumes the following: discrete and non-overlapping generations with a constant population size N, with each member replaced in every generation. Introduce s = s n b − s b where s n b and s b are the fitness cost incurred by the non-binders and binders, respectively, and let n t denote number of non-binders in the population in generation t (i.e. n t = 0, 1, 2, …, N). We first generate the number of gain-of-function and loss-of-function mutations m + and m − from Poisson distributions with means n t ν + and (N − n t) ν −, respectively, so that the fraction of non-binders in the population becomes x = (n t + m − − m +)/ N. The number of non-binders n t +1 in the next generation is drawn from a Binomial distribution with success probabilityto account for the fitness difference s (thus the number of offsprings produced) between non-binders and binders.

---

### Summary benchmarks-full set – 2024 [^e2c40ddc]. AAO (2024). High credibility.

Preferred Practice Pattern (PPP) guidelines — GRADE recommendation categories are specified as: "Strong recommendation (SR): Used when the desirable effects of an intervention clearly outweigh the undesirable effects or clearly do not" and "Discretionary recommendation (DR): Used when the trade-offs are less certain — either because of low-quality evidence or because evidence suggests that desirable and undesirable effects are closely balanced".

---

### Machine learning in nuclear medicine: part 1-introduction [^19d93384]. Journal of Nuclear Medicine (2019). Medium credibility.

This article, the first in a 2-part series, provides an introduction to machine learning (ML) in a nuclear medicine context. This part addresses the history of ML and describes common algorithms, with illustrations of when they can be helpful in nuclear medicine. Part 2 focuses on current contributions of ML to our field, addresses future expectations and limitations, and provides a critical appraisal of what ML can and cannot do.

---

### New designs for phase 2 clinical trials [^a90544dc]. Blood (2003). Low credibility.

Conventional phase 2 clinical trials are typically single-arm experiments, with outcome characterized by one binary "response" variable. Clinical investigators are poorly served by such conventional methodology. We contend that phase 2 trials are inherently comparative, with the results of the comparison determining whether to conduct a subsequent phase 3 trial. When different treatments are studied in separate single-arm trials, actual differences between response rates associated with the treatments, "treatment effects", are confounded with differences between the trials, "trial effects". Thus, it is impossible to estimate either effect separately. Consequently, when the results of separate single-arm trials of different treatments are compared, an apparent treatment difference may be due to a trial effect. Conversely, the apparent absence of a treatment effect may be due to an actual treatment effect being cancelled out by a trial effect. Because selection involves comparison, single-arm phase 2 trials thus fail to provide a reliable means for selecting which therapies to investigate in phase 3. Moreover, reducing complex clinical phenomena, including both adverse and desirable events, to a single outcome wastes important information. Consequently, conventional phase 2 designs are inefficient and unreliable. Given the limited number of patients available for phase 2 trials and the increasing number of new therapies that must be evaluated, it is critically important to conduct these trials efficiently. These concerns motivated the development of a general paradigm for randomized selection trials evaluating several therapies based on multiple outcomes. Three illustrative applications of trials using this approach are presented.

---

### Learning noise-induced transitions by multi-scaling reservoir computing [^9b089041]. Nature Communications (2024). High credibility.

In the training phase, we generated a series consisting of 40000 δ t. The training set is t ∈ [0, 40], and the predicting set is t ∈ [40, 80]. Figure 4 b displays the training set. Following the method in our framework, the deterministic part is reconstructed as shown in Fig. 4 c. A proper set of hyperparameters is listed in Table 1 (Example 3). We observe that the generated time series starting from the ten initial points converge to two potential wells, where the time series has rotational dynamics. In the predicting phase, we perform rolling prediction within t ∈ [40, 80] (Fig. 4 d).

In the evaluation, we predict 100 replicates of the time series, and compare them with 100 replicates simulated from Eqs. (14) and (15). Figure 4 e presents histograms of the number of transitions for the 100 predicted and test time series, in t ∈ [40, 80]. Figure 4 f presents histograms of transition time for the 100 predicted and test time series, in t ∈ [40, 80]. The results demonstrate that, for the 2D bistable non-gradient system, the present method accurately learns the dynamics and yields precise estimations on the number of transitions and transition time.

---

### The case for open computer programs [^0f6e8786]. Nature (2012). Excellent credibility.

Scientific communication relies on evidence that cannot be entirely included in publications, but the rise of computational science has added a new layer of inaccessibility. Although it is now accepted that data should be made available on request, the current regulations regarding the availability of software are inconsistent. We argue that, with some exceptions, anything less than the release of source programs is intolerable for results that depend on computation. The vagaries of hardware, software and natural language will always ensure that exact reproducibility remains uncertain, but withholding code increases the chances that efforts to reproduce results will fail.

---

### Clustering knowledge and dispersing abilities enhances collective problem solving in a network [^9f543c96]. Nature Communications (2019). High credibility.

The severity of this problem's complexity is not additive or linear in nature, but interdependent on all of the sub-problems that need to be solved for this to work. Furthermore, adopting a solution might have unintended, if not non-linear, consequences to the system's overall outcome that were not obvious or foreseeable. For instance, allowing students to use their mobile phones in schools might seem like a good idea to improve class engagement through other media, but an unintended consequence might be distraction or discouraging deeper critical thinking. The objective of the NK space is to capture the complex interaction among activities that yield performance. The NK problem space's popularity in modeling human decision-making stems from its verisimilitude with the complex and multidimensional problems that face problem-solving tasks, and because researchers can easily generate a large number of statistically similar problem spaces for robustness checks. (We note, however, in order to make sure our results are not an artifact of the idiosyncrasies of the NK problem space, we replicated all of our results in another rugged problem space, the Traveling Salesperson Problem — findings available in Supplemental Methods in Supplementary Fig. 2 through 9 and 11).

---

### Identifying domains of applicability of machine learning models for materials science [^9c5f816f]. Nature Communications (2020). High credibility.

A predictive ML model is then a functionaiming to minimize the expected error (also called prediction risk)measured by some non-negative loss function l that quantifies the cost incurred by predicting the actual property value y with f (x). Examples for loss functions are the squared error, the absolute error, and, for non-zero properties, the relative error. Here P denotes some fixed probability distribution that captures how candidate materials are assumed to be sampled from the materials class (this concept, while commonly assumed in ML, is an unnecessary restriction for high-throughput screening as we discuss in more detail below). Since the true prediction risk is impossible to compute directly without perfect knowledge of the investigated materials class, models are evaluated by the test error (or empirical risk)defined as the average of the individual errors (losses) e i (f) = l (f (x i), y i) on some test set of m reference data points. The samples in this test set are drawn independently and identically distributed according to P and are also independent of the model — which means in practice that it is a random subset of all available reference data that has been withheld from the ML algorithm. In order to reduce the variance of this estimate, a common strategy is cross-validation, where this process is repeated multiple times based on partitioning the data into a number of non-overlapping "folds" and then to use each of these folds as test sets and the remaining data as a training set to fit the model.

This test error properly estimates the model performance globally over the whole representation space X (weighted by the distribution P used to generate the test points). This is an appropriate evaluation metric for selecting a model that is required to work well on average for arbitrary new input materials that are sampled according to the same distribution P. This is, however, not the condition of high-throughput screening. Here, rather than being presented with random inputs, we can decide which candidate materials to screen next. This observation leads to the central idea enabled by the DA analysis proposed in this work: if the employed model is particularly applicable in a specific subdomain of the materials class, and if that subdomain has a simple and interpretable shape that permits to generate new materials from it, then we can directly focus the screening there.

---

### Quantitative constraints on the scope of negative selection [^09aa71b5]. Trends in Immunology (2003). Low credibility.

Maturing T cells with a high affinity for self-antigens presented in the thymus are deleted in the process of negative selection. Although the expression of various "tissue-specific" antigens has been described in the thymus, it is still controversial what fraction of all self-antigens induces tolerance by this mechanism. We demonstrate that the limited duration of the negative selection phase imposes a constraint on the number of self-peptides that can be reliably selected against. The analysis supports the theory that negative selection is confined to the subset of peptides produced by dendritic cells.

---

### A very simple model to account for the rapid rise of the alpha variant of SARS-CoV-2 in several countries and the world [^fbef93b4]. Virus Research (2021). Medium credibility.

Since its first detection in the UK in September 2020, a highly contagious version of the coronavirus, the alpha or British variant a.k.a. B.1.1.7 SARS-CoV-2 virus lineage, rapidly spread across several countries and became the dominant strain in the outbreak. Here it is shown that a very simple evolutionary model can fit the observed change in frequency of B.1.1.7 for several countries, regions of countries and the whole world with a single parameter, its relative fitness f, which is almost universal f ≈ 1.5. This is consistent with a 50% higher transmissibility than the local wild type and with the fact that the period in which this variant takes over has been in all the studied cases around 22 weeks.

---

### Modularity and stability in ecological communities [^9bf20bd1]. Nature Communications (2016). Medium credibility.

In Fig. 4, we show the effect of modularity on stability in a community composed of 1,000 species, when we set C = 0.2. Take the case of two equally sized subsystems (α = 1/2), for which we derive new results allowing us to express the ratio analytically (Methods; Supplementary Information): when μ ≥ 0, we have no effect of modularity on stability; when μ < 0, on the other hand, a bipartite structure is highly destabilizing, while a modular structure is moderately stabilizing. Both effects are stronger in the case of a negative correlation.

When the two subsystems have different sizes (α < 1/2), the stabilizing effect of modularity found for the case of μ < 0 is greatly diminished, and eventually also modularity can become destabilizing (especially for positive ρ and). For μ > 0, any Q ≠0 is destabilizing, while for μ ≈0, a modular structure is always destabilizing, and a anti-modular structure can be stabilizing, provided that ρ is sufficiently negative. These effects hold qualitatively for different levels of C (Methods; Supplementary Figs 2–3), with higher connectances leading to more marked effects. Though we cannot predict the ratio in full generality for the case of α < 1/2, we can treat the extreme cases of a perfectly modular and perfectly bipartite structure, and this is sufficient to understand the qualitative behaviour of all cases (Methods).

---

### Improved measures for evolutionary conservation that exploit taxonomy distances [^502c721b]. Nature Communications (2019). High credibility.

Data sets

Our main data sets are based on exome variants that originate from 60,706 individuals, which were identified through high-throughput methods and collected by the Exome Aggregation Consortium(ExAC). To avoid multi-isoform redundancy, we only used variants that mapped to the SwissProt human protein sequences (downloaded on August 9, 2017). These ExAC variants were mapped to the human reference genome GRCh37, which is superseded by GRCh38. To avoid cases where different predictors make predictions for identical variants based on different sets of underlining sequences, we only used those variants that map to identical regions in both GCRh37 and GCRh38. Variants of the first amino acid M and those involving nonstandard amino acids were excluded for all analyses. We divided the ExAC data and created three sets (optimization sets 1 and 2, ExAC/ClinVar test set) for optimization and initial testing (Supplementary Table 2). To avoid protein level biasduring optimization and testing, we first divided the SwissProt human protein sequences randomly into two equal sets A and B such that sequences in either set have < 50% identity with those in the other set. Variants that map to proteins in set A were used for optimization only (optimization sets 1 and 2), and those that map to proteins in set B were used for testing only (ExAC/ClinVar; Supplementary Table 2). For optimization set 2 and the ExAC/ClinVar test set, variants in ExAC that are marked by ClinVar as pathogenic were placed in the deleterious class and, from the remaining variants, those that are observed in the human population with an allele frequency (ExAC: Adjusted Alt allele frequency in total ExAC samples) ≥ 1% were considered to be benign and placed in the benign class. The optimization set 2, was used to optimize the hierarchical structure of LIST. The number of variants in optimization set 2 is small (deleterious/benign: 2,146/18,109), especially the number of deleterious variants. Thus, for training of LIST's individual modules, we generated optimization set 1, where the deleterious class is defined as rare variants with allele frequency in the range of 0.015% to 0.03%, and the benign class as frequent variants with allele frequency ≥ 0.5%. Optimization set 1 contains 24,096 benign and 48,142 deleterious variants. When generating the histograms of VST values and raw frequencies for ExAC variants with deleterious and benign effects (Fig. 2 and Supplementary Fig. 1), we used the Optimization set 2 and excluded variants with alignment depth < 50. For Fig. 2a and Supplementary Fig. 1, we also excluded those variants that do not match any amino acid in the raw MSA (43.4% of the deleterious and 15.1% of the benign variants with alignment depth ≥ 50 did not match any amino acid in MSA). Importantly, the trends shown in Fig. 2a and Supplementary Fig. 1 are reproduced when using the entire ExAC/ClinVar data set. The ExAC/ClinVar test set was used to analyze the performance of LIST and compare it to other methods. LIST scores all variants in this test set (see Supplementary Table 2). However, most methods that we compared LIST's performance with do not score all variants. Therefore, for each type of analysis presented, we used the maximal number of variants from the ExAC/ClinVar test set that were scored by all methods used in the comparison.

---

### Society of Critical Care Medicine guidelines on recognizing and responding to clinical deterioration outside the ICU: 2023 [^598e71d3]. Critical Care Medicine (2024). High credibility.

Society of Critical Care Medicine guidelines — classification of recommendation strength distinguishes Strong Recommendation "We recommend… " when "Desirable effects of intervention clearly outweigh undesirable effects, or clearly do not", with implications for patients that "Most individuals in this situation would want the recommended course of action and only a small proportion would not", for clinicians that "Most individuals should receive the recommended course of action… ", and for policymakers that "The recommendation can be adapted as policy in most situations including for use as performance indicators". Conditional Recommendation "We suggest… " applies when "Trade-offs are less certain, either because of low-quality evidence or because evidence suggests desirable and undesirable effects are closely balanced", with implications for patients that "The majority of individuals in this situation would want the suggested course of action, but many would not", for clinicians that "Different choices are likely to be appropriate for different patients, and therapy should be tailored to the individual patient's circumstances", and for policymakers that "Policymaking will require substantial debates and involvement of many stakeholders".

---

### Acacia [^2d1e2884]. FDA (2009). Low credibility.

Volume desired x Concentration desired = Volume needed x Concentration available.

Example 1: If a 1:10 w/v extract is available and it is desired to use a 1:1,000 w/v extract substitute as follows:

Vd x Cd = Vn x Ca

10ml x 0.001 = Vn x 0.1

0.1 mL = Vn

Using a sterile technique, remove 0.10 mL of extract from the 1:10 vial and place it into a vial containing 9.90 mL of sterile diluent. The resulting ratio will be a 10 mL vial of 1:1,000 w/v.

Example 2: If a 10,000 pnu/ml extract is available and it is desired to use a 100 pnu/ml extract substitute as follows:

10ml x 100 = Vn x 10,000

0.1 mL = Vn

Using a sterile technique, remove 0.10 mL of extract from the 10,000 pnu/ml vial and place it into a vial containing 9.90 mL of sterile diluent. The resulting concentration will be a 10 mL vial of 100 pnu/ml.

Example 3: If a 10,000 AU/ml or BAU/ml extract is available and it is desired to use a 100 AU/ml or BAU/ml extract substitute as follows: Vd x Cd = Vn x Ca

---

### Understanding statistical terms: 2 [^351bd399]. Drug and Therapeutics Bulletin (2009). Low credibility.

An increasing number of statistical terms appear in journal articles and other medical information. A working knowledge of these is essential in assessing clinical evidence. With this in mind, we are producing a series of explanatory articles covering various statistical terms and their uses. This, the second article in the series, will focus on some of the most common terms used in reporting the results of randomised controlled trials.1.

---

### Competition and evolutionary selection among core regulatory motifs in gene expression control [^64886f7c]. Nature Communications (2023). High credibility.

Fig. 1
Functionally equivalent core regulatory motifs in gene expression control.

The regulator R and the product P are expressed from their genes r and p, respectively. Gray squares denote the inducer I. In the absence of the inducer, the product (and the regulator for autoregulated motifs) may be synthesized at a low basal level which is negligible compared to the concentration in the induced state (Supplementary Section 1). The motifs may be embedded in complex regulatory pathways (Supplementary Fig. 3). a Expression of R is non-autoregulated. b Expression of R is autoregulated.

While precise temporal control of a beneficial gene product may result in an advantage, the expression of the required regulator carries its own bioenergetic cost. Crucially, for both positive and negative control, the regulator is only required when the inducer is present, otherwise its expression is gratuitous. Therefore, it may be advantageous to have the regulator under autoregulatory control to ensure it is expressed only when needed (Fig. 1 b). Understanding the competition and evolutionary selection among the core regulatory motifs in Fig. 1 could shed light on organizing principles of living organisms as similar just-in-time regulation is a wide-spread feature in natural systems –, as well as guide the design of synthetic gene circuits when selecting among alternative modes of regulation –.

Motivated by the central role of autoregulation in systems and synthetic biology, we characterize how demand for a beneficial gene product, mutation rate of its regulator, population size, selection pressure, regulatory delay, and the timescale of environmental shifts together determine the optimal choice among the motifs in Fig. 1. We show that (i) autoregulation generally dominates, (ii) the dominant strategy typically agrees with the wear-and-tear principle, and (iii) while self-repression of the regulator curbs the spread of loss-of-function mutations, self-activation instead facilitates their propagation. We further demonstrate that the reduced bioenergetic cost of autoregulation may contribute to its ubiquitous nature in gene regulatory networks, and how our work could aid the design of evolutionarily robust synthetic gene circuits.

---

### Competition and evolutionary selection among core regulatory motifs in gene expression control [^6e6d57b8]. Nature Communications (2023). High credibility.

Feedback cost can cause non-autoregulated motifs to outperform autoregulation

In addition to introducing delay, autoregulation can also result in additional burden due to the increased expression of the regulator R that may be required to control its own expression. To capture this, we next assume that while the R-cost for non-autoregulated control remains s r, for autoregulation it instead increases to a s r with a ≥ 1 (Fig. 7 a).

Fig. 7
Feedback cost has a differential impact on positive and negative control.

a Non-autoregulated variants outperform their autoregulated counterparts when a > 1/ D in the absence of mutations. b While feedback cost hinders the spread of loss-of-function mutations in case of self-repression, it instead facilitates their proliferation when relying on self-activation. Data obtained using the stochastic sampling algorithm described in the "Methods". In all plots ν − = 10 −3, ν + = ν − /10, s p = 100 ν −, s r = s p /4, T = 100, N = 1000, D = 0.05 in case of activation (red) and D = 0.95 in case of repression (blue). Darker shades indicate greater values of feedback cost (ranging from a = 1 to a = 2). Gray denotes the distribution when considering the corresponding non-autoregulated control, serving as a baseline for fitness cost normalization. Distributions are visualized with identical maximal heights.

To understand how this change impacts the dominant strategy, we first focus on the case when mutations have minimal impact, so that the population consists entirely of binders. As a result, the P-cost is zero for all four schemes. Furthermore, positive and negative control have identical R-cost, both when considering non-autoregulated and autoregulated motifs (Fig. 2 c). While autoregulation reduces the time when the R-cost is suffered as demand decreases, non-autoregulated strategies outperform their autoregulated counterparts once feedback becomes too expensive (Fig. 7 a). These results hold even when mutations are considered. In particular, data in Supplementary Figs. 32–34 confirm that (i) the pattern of how the use-it-or-lose-it and the wear-and-tear principles emerge behind the dominant strategies remains largely unaffected, (ii) non-autoregulated strategies can outperform their autoregulated counterparts as a increases, and (iii) this transition happens at lower values of a as the demand increases.

---

### Understanding statistical terms: 1 [^50f69b79]. Drug and Therapeutics Bulletin (2009). Low credibility.

An increasing number of statistical terms appear in journal articles and other medical information. A working knowledge of these is essential in assessing clinical evidence. With this in mind, we are producing a series of explanatory articles covering various statistical terms and their uses. This, the first article in the series, will focus on some of the most common terms used in randomised controlled trials.

---

### Complementary task representations in hippocampus and prefrontal cortex for generalizing the structure of problems [^43bd6893]. Nature Neuroscience (2022). High credibility.

The cellular modes are the columns of U, and the temporal modes are the rows of V T. Both modes are unit vectors, so the contribution of each pair to the total data variance is determined by the corresponding element of the diagonal matrix Σ. The modes are sorted in order of explained variance, such that the first cellular and temporal mode pair explains the most variance. The first cellular and temporal mode of PFC activity in three different problems is shown in Fig. 5b, c. It is high throughout the inter-trial interval (ITI) and trial, with a peak at choice time but strongly suppressed after reward (similar to cell 5 in Fig. 2d).

We reasoned that (1) if the same events were represented across problems (for example, initiation, A/B choice and outcome), then the temporal modes would be exchangeable between problems, no matter whether these representations were found in the same cells; (2) if the same cell assemblies were used across problems, then the cellular modes would be exchangeable across problems, no matter whether the cell assemblies played the same role in each problem; and (3) if the same cell assemblies performed the same roles in each problem, then pairs of cellular and temporal modes would be exchangeable across problems.

To see whether the same representations existed in each problem, we first asked how well the temporal modes from one problem could be used to explain activity from other problems. Because the set of temporal modes V is an orthonormal basis, any data of the same rank or less can be perfectly explained when using all the temporal modes. However, population activity in each problem is low dimensional, so a small number of modes explain a great majority of the variance. Modes that explain a lot of variance in one problem will explain a lot of variance in the other problem only if the structure captured by the mode is prominent in both problems. The question is, therefore, how quickly variance is explained in problem 2ʼs data, when using the modes from problem 1 ordered according to their variance explained in problem 1. To assess this, we projected the data matrix D 2 from problem 2 onto the temporal modes V 1 from problem 1, giving a matrix M V whose elements indicate how strongly each temporal mode contributes to the problem 2 activity of each neuron:

---

### Choice selective inhibition drives stability and competition in decision circuits [^71b5a8e0]. Nature Communications (2023). High credibility.

We parametrized connection specificity between choice-selective populations by γ J K between presynaptic population J and postsynaptic population K. The index J, K ∈ {E, I} defines neuron type as excitatory or inhibitory. We translate γ J K to a synaptic weight under a constraint that the total input to each population remains constant for all values of γ J K. To this end, we defined an intermediate weight, where N s = 2 is the number of competing choice-selective populations and w E = w I = 1. We then set connection weights between populations with the same choice selectivity toand between populations with opposite selectivity to. We can rewrite γ in terms of w + and w − as:

Connections to and from nonselective neurons were held at w J = 1. This definition enforces that all neurons receive the same total input weight for any value of γ J K. We set the specificity parameter γ EE = 0.32 as in refs. except in Figs. 1 and 2. We set γ EI = 0.25 except in Figs. 1 and 2.

The effective interaction strengths α 1 describes the recurrent feedback from an excitatory population's activity onto itself fed through other populations in the circuit. This term consists of four components α 1 = λ 1 (α 1 a + α 1 b + α 1 c + α 1 d):

These components of α 1 account for the effect of an excitatory population's activity on its own activity filtered via (a) direct self-coupling, (b) the activity of the inhibitory population with the same choice selectivity, (c) the activity of the inhibitory population with the opposite choice selectivity, and (d) the activity of nonselective inhibitory neurons. Similarly, α 2 describes the influence of one excitatory population's activity onto the other fed through all other populations in the circuit and also consists of four components α 2 = λ 2 (α 2 a + α 2 b + α 2 c + α 2 d):

---

### Clustering knowledge and dispersing abilities enhances collective problem solving in a network [^77613a31]. Nature Communications (2019). High credibility.

Here, however, we find quite different results: intermixing hampers systemic performance. Furthermore, there is a tradeoff in systemic performance over time, as seen in Fig. 6b. Networks with some form of intermixing perform better in the short-run. However, the network with the minimal possible intermixing performs worse in the short-run but best in the long-run. In other words, this suggests an "all or nothing" trade-off over time: the configuration with the least amount of intermixing possible (minimal) performs worse in the short-run, but better in the long-run, while any gradation of increased intermixing yields the same rank-ordering in performance as their counterpart networks in the diversity of ability simulations.

Agents in the intermixed network setups explore for solutions early on in the simulation, which is a double-edged sword. In the short-run, with more intermixing, agents quickly turn to exploitation, as they merely take solutions that are marginally better relative to other setups but are still mediocre in absolute terms. Said differently, agents in setups with at least some intermixing quickly coalesce to whatever the few agents that did explore the problem space found, which are often not the best solutions possible. So, exploring the problem space using worse solutions often leads to modest gains. However, the agents in setups with minimal intermixing are more commonly exploring, rather than exploiting, because their neighbors' initial solutions are less optimal than their counterparts in intermixed networks. This is because there is minimal exposure to diversity in these minimal setups, which inadvertently produces poor solutions in the short-run but allows for more exploration to find better solutions in the long-run. In other words, by finding better solutions through exploration, agents more often uncover pathways to better solutions earlier on. The end result is that the long-run performance of setups with minimal intermixing is best.

---

### A framework for evaluating clinical artificial intelligence systems without ground-truth annotations [^f3b9d969]. Nature Communications (2024). High credibility.

Evaluate classifier

After training g ϕ, we evaluated it on a held-out set of data comprising data points with ground-truth labels (from both class 0 and class 1) (Fig. 1 b, Step 5). The intuition here is that a classifier which can successfully distinguish between these two classes, by performing well on the held-out set of data, is indicative that the training data and the corresponding ground-truth labels are relatively reliable. Since the data points from class 1 are known to be correct (due to our use of ground-truth labels), then a highly-performing classifier would suggest that the class 0 pseudo-labels of the remaining data points are likely to be correct. In short, this step quantifies how plausible it is that the sampled unlabelled data points belong to class 0.

As presented, this approach determines how plausible it is that the sampled set of data points in some probability interval, s ∈ (s 1, s 2], belongs to class 0. It is entirely possible, however, that a fraction of these sampled data points belong to the opposite class (e.g. class 1). We refer to this mixture of data points from each class as class contamination. We hypothesised (and indeed showed) that the degree of this class contamination increases as the probability output, s, by an AI system steers away from the extremes (s ≈ 0 and s ≈ 1). To quantify the extent of this contamination, however, we also had to determine how plausible it was that the sampled set of data points belong to class 1, as we outline next.

---

### Extreme purifying selection against point mutations in the human genome [^940edc7e]. Nature Communications (2022). High credibility.

In the absence of natural selection, we assume a Bernoulli sampling model for the presence (probability P i) or absence (probably 1 − P i) of a rare variant at each site i, where P i reflects the local sequence context and overall rate of mutation. We ignore sites at which common variants occur (similar to refs.). We then assume that natural selection has the effect of imposing a fractional reduction on the rate at which rare variants occur. To a first approximation, we maximize the following likelihood function, where Y i is an indicator variable for the presence of a rare variant at position i in the sample, λ s is a scale factor capturing a depletion of rare genetic variation, and the product excludes sites having common variants. By maximizing this function we can obtain a maximum-likelihood estimate (MLE) of λ s conditional on pre-estimated values P i. (In practice, we use a slighly more complicated likelihood function that distinguishes among the possible alternative alleles at each site; see "Methods" for complete details.) Assuming the P i values are pre-estimated, an approximate, unbiased maximum-likelihood estimator (MLE) for λ s and an estimator for its variance can be obtained in closed form (see "Methods"). Importantly, this variance has almost no sensitivity to variance in the pre-estimated P i values in the regime of interest (see Supplementary Fig. 4), making the model highly robust to uncertainty in mutation rate estimates provided they are unbiased.

When λ s falls between 0 and 1 it can be interpreted as a measure of the prevalence of ultraselection. In this case, λ s can be thought of as the fraction of sites intolerant to heterozygous mutations, although in practice, some sites may be more, and some sites less, intolerant. Notice, however, that λ s can also take values < 0 if rare variants occur at a higher-than-expected rate in the target set of sites. As we discuss below, we do observe a systematic tendency for λ s to take negative values in particular classes of sites, likely reflecting the difficulty of precisely specifying the mutational model at these sites. Across most of the genome, however, estimates of λ s fall between 0 and 1 and show general qualitative agreement with other measures of purifying selection.

---

### Efficient interatomic descriptors for accurate machine learning force fields of extended molecules [^44a6aef0]. Nature Communications (2023). High credibility.

Further analysis reveals that contribution of particular features in the global model can range from linear to stochastic with respect to the interatomic distance (Supplementary Fig. 6a). The proportion of stochastic features increases with the size of the systems and the size of training set (Supplementary Fig. 6b). In the reduced models after retraining most of the selected features have a high coefficient of determination, R 2 (Supplementary Fig. 6c). Contribution of "linear" features to the force prediction decrease quadratically with distance (slope ≈ −2), suggesting the prominence of Coulombic contributions to the interatomic forces.

These findings are general and valid for all descriptor reduction degrees. Although we do not rely on any characteristic lengthscale, we show the effect of descriptor reduction approach on different types of interactions as conventionally defined when imposing length scales. Figure 5 b shows a decomposition of the reduced descriptor in short- and long-range features for different descriptor sizes. We consider the feature as short-ranged if the distance between two atoms across all configurations in the dataset is below 5 Å, and long-ranged otherwise. In all the cases, feature selection removes prevalently long-range features and 10–20% of the local features that always lie under 5 Å (compare dashed and solid green lines in Fig. 5 b). Nevertheless, we emphasize that removing local features might worsen the stability in flexible systems and the best-practice solution is to keep them all in the reduced descriptor.

---

### A summary of the methods that the national clinical guideline centre uses to produce clinical guidelines for the national institute for health and clinical excellence [^e6498189]. Annals of Internal Medicine (2011). Medium credibility.

National Institute for Health and Clinical Excellence (NICE) guideline formats, recommendation wording, and implementation — Each guideline is produced in 4 formats, including a full version detailing methods, evidence, and the link between evidence and recommendations; a NICE version that lists the recommendations; a quick reference guide summarizing recommendations for health care professionals; and an "understanding NICE guidance" version for patients, carers, and the general public. The wording of each recommendation is considered carefully to ensure that it is clear and concise, and recommendations are action-based and attempt to be specific about the intervention and the population to which the recommendation applies; justifications are not normally included because they are available in the full version's section describing the link between evidence and recommendations. The guideline development group selects 5 to 10 recommendations as key priorities for implementation; these are not necessarily all "strong" and are chosen as being likely to do at least 1 of the following: result in significant benefits for patients, reduce variation in care and outcomes, lead to more efficient use of National Health Service resources, promote patient choice, or promote more equitable access to health care. The guideline development group also identifies recommendations likely to benefit from implementation support, with criteria including not being part of routine care, requiring changes in service delivery or retraining, needing implementation across several agencies or settings, being potentially contentious, or being difficult to implement. While assessing the evidence and making recommendations for best practice, the group also considers how the recommendations will be implemented, and NICE has set up a team to assist with implementation that helps disseminate guidelines and provides tools to put them into practice.

---

### Perspectives on validation of clinical predictive algorithms [^eca4a2eb]. NPJ Digital Medicine (2023). Medium credibility.

Machine learning has led to a surge in the development of clinical predictive algorithms. The generalizability of these algorithms often goes untested, leaving the community in the dark on their accuracy and safety when applied to a specific medical setting. We need clear objectives with respect to generalizability that align with the intended use. Journals, funding organizations, and regulatory bodies provide some guidance on generalizability requirements for clinical predictive algorithms, but a clear definition is often lacking. For example, it is considered best practice to 'Describe the generalizability of the model including the performance of the model on validation and testing datasets'. We consider this recommendation too vague. It is not clear what type of generalizability is referred to and whether it is sufficient for the intended use of the algorithm (see Supplementary Table 1 for more examples and suggestions for improvement). This commentary aims to provide clarity on different objectives related to generalizability via an overview of three main types of generalizability summarized from the literature with their associated goals, methodology, and stakeholders.

---

### Machine learning for medical imaging: methodological failures and recommendations for the future [^eff32de1]. NPJ Digital Medicine (2022). Medium credibility.

Research in computer analysis of medical images bears many promises to improve patients' health. However, a number of systematic challenges are slowing down the progress of the field, from limitations of the data, such as biases, to research incentives, such as optimizing for publication. In this paper we review roadblocks to developing and assessing methods. Building our analysis on evidence from the literature and data challenges, we show that at every step, potential biases can creep in. On a positive note, we also discuss on-going efforts to counteract these problems. Finally we provide recommendations on how to further address these problems in the future.

---

### Alternating quarantine for sustainable epidemic mitigation [^f90e49e6]. Nature Communications (2021). High credibility.

Similarly to β, the parameter α is also dictated by the network parameters. A large T 2 and p 2 will favor in-house transmissions, contributing to α, whereas increasing T 1, p 1 will strengthen the role of external transmissions. The network structure also factors in through the average number of external links 〈 k 〉, decreasing α, vs. the typical household size 〈 m 〉, which increases it. This parameter is especially meaningful in the context of quarantine-based strategies, which, by design, suppress only external interactions, and therefore become less effective when θ In is large. Indeed, no matter how strict the quarantine is, it cannot prevent the secondary transmission between household members encapsulated within θ In. In fact, it often increases these in-house infections, as it forces cohabitants to remain close for extended periods. Consequently, a large α can potentially hinder the effectiveness of quarantine in general, and of AQ in particular.

To summarize, in our modeling we vary the implicit model parameters T 1, p 1, T 2, p 2, P (k), to scan an array of potentially relevant scenarios. Once setting these parameters, we use them to extract two observable parameters α and β, that directly characterize the nature of the contagion. The severity of the spread is quantified by β, and the role of the in-house transmission is captured by α. The mapping between the model parameters and the observable α, β is explained in Supplementary Note 1.4.

Projecting the spread of SARS-CoV-2

To evaluate β for the unmitigated COVID-19 spread we collected data on the evolution of the epidemic in 12 different countries, and examined I (t) at the early stages of the spread, prior to the implementation of social distancing policies (Fig. 3 a–l). Fitting to an exponential growth of the form (1) we evaluate β in each country, obtaining a mean growth rate of β = 0.26 days −1, an estimate congruent with other independent evaluations. To further ground this estimate, we use β to evaluate the epidemic reproduction number, providing R 0 ≈ 2.4, again, in line with current valuations (Supplementary Note 1.5). Such fitting, based on the early spreading dynamics, may, generally risk overestimating β and R 0 –. However, in our analysis below we consider a spectrum of different scenarios, with different β values, and hence our results are insensitive to this risk factor.

---

### A primer on performing systematic reviews and meta-analyses [^33cb968a]. Clinical Infectious Diseases (2013). Low credibility.

The number of systematic reviews published in the peer-reviewed literature has increased dramatically in the last decade, and for good reason. They have become an essential resource for clinicians who want unbiased and current answers for their clinical questions; researchers and funders who want to identify the most critical evidence gaps for study; payers and administrators who want to make coverage, formulary, and purchasing decisions; and policymakers who want to develop quality measures and clinical guidelines. Targeted to beginners interested in conducting their own systematic reviews and users of systematic reviews looking for a brief introduction, this primer (1) highlights the differences between review types; (2) outlines the major steps in performing a systematic review; and (3) offers a set of resources to help authors perform and report valid and actionable systematic reviews.

---

### Pediatric HIV infection and decreased prevalence of OPV point mutations linked to vaccine-associated paralytic poliomyelitis [^eb44be0f]. Clinical Infectious Diseases (2018). Low credibility.

OPV-2: 41 revertant isolates ≤ 14 days from vaccination; 25 revertant isolated > 21 days from vaccination.

For more detail regarding all overlapping points, please see Supplementary Table S1.

Statistical analysis was conducted using SAS 9.3. Categorical data were compared using 2-tailed Fisher exact test. To avoid bias resulting from multiple samples from 1 infant, 1 sample per person was chosen for statistical testing. This was done in 2 ways. In the first process, one sample per person was chosen, either from the samples collected after 3 or more OPV doses or, if there were none, the samples collected after the largest number of OPV doses available. The earliest of multiple samples picked by these criteria was used. In the second process, 1 sample per infant was randomly picked. The process was repeated 500 times, and the means of the results of the statistical tests were calculated. A P value of ≤ .05 was considered statistically significant.

---

### Dynamical trade-offs arise from antagonistic coevolution and decrease intraspecific diversity [^3b6942c8]. Nature Communications (2017). Medium credibility.

Population dynamics and dynamical trade-offs

In the simplest scenario of a single prey and predator type, we observe a stochastic version of the classical predator-prey cycles with a one-quarter phase shift at the beginning of our simulations. Over time, prey and predator evolve (Fig. 4), leading to changes in the population dynamics depending on the shape of the trade-off. When the initial prey defense is relatively costly (e.g. m = 3), we see a shift towards out-of-phase cycles when distinct prey types coexist and a return back to classical cycles when a single prey type dominates (See Supplementary Fig. 4a and a detailed discussion in Supplementary Note 4). When the initial prey defense is effective and cheap (e.g. m = 10), we see more often multiple prey types and out-of-phase cycles are prevalent (Supplementary Fig. 4b). Previous models with only prey evolution predict a similar shift in population cycles if distinct prey types can coexist.

Fig. 4
Stochastic coevolutionary dynamics of the prey and predator species. Different prey (a) and predator (b) types coexist with each other. The ancestor prey (g 1 = g max = 1.0) and the ancestor predator (k 1 = k max = 0.3) follow classical predator-prey cycles. New prey and predator types arise by de novo mutations. Some prey and predators invade and are maintained until they ultimately are lost again. Their g and k values are marked beside the corresponding types (parameters b x = 1.0, d x = 0.1, μ x = 0.0001, r c = 0.00005, X 1 (0) = 1000, Y 1 (0) = 100, d y = 0.5, μ y = 0.001, p = 0.005, m = 3)

---

### Type 2 diabetes therapies: a STEPS approach [^fc25c424]. American Family Physician (2019). Medium credibility.

A concise way to evaluate pharmacotherapy options for type 2 diabetes mellitus is to use the five patient-oriented STEPS criteria: safety, tolerability, efficacy, price, and simplicity. The first-line treatment option, metformin, is safe and fairly well-tolerated, has excellent long-term efficacy for patient-oriented outcomes, is moderately priced, and has a simple dosing regimen. However, most patients with type 2 diabetes require more than one medication. The STEPS approach can help choose subsequent medications if metformin does not provide adequate glycemic control.

---

### Use of genetic correlations to examine selection bias [^01aa69a6]. Genetic Epidemiology (2025). Medium credibility.

1 INTRODUCTION

In statistical inference, it is usually assumed that the analysis sample is representative (i.e. a random subsample) of the target population. This assumption could be violated by participants being nonrandomly selected into the analysis sample. This can occur when measured and unmeasured factors affect initial study enrollment or loss to follow up, or when the analysis is limited to a selected group (e.g. only those with no disease at baseline). The distortion of the parameter estimate between the analysis sample and the true value in the target population is known as "selection bias" (Rothman et al.). Selection bias may also be a form of collider bias (Hernán et al.). Collider bias is induced when conditioning on a common effect (selection,) of two (or more) variablesand, as shown in Figure 1a.

---

### A coreduction to syngas.O-CDots-CNthree component electrocatalyst design concept for efficient and tunable CO [^fa0f860c]. Nature Communications (2017). Medium credibility.

The comparison between the LSVs of the different catalyst components and their composites (Fig. 4a) should be done carefully. The catalyst areal density was kept the same (0.127 mg/cm 2) for all (3 mm in diameter) electrodes. Since the composition of the Co 3 O 4 -CDots-C 3 N 4 was 6 wt% Co 3 O 4, 1 wt% CDots, and 93 wt% C 3 N 4, it follows that the amount of CDots or Co 3 O 4 in the CDots, Co 3 O 4, or Co 3 O 4 -CDots electrodes was much larger than in the C 3 N 4 containing composite electrodes. This means that the reaction activity per catalyst mass of curves 2, 3, and 6 is very low compared to that of the Co 3 O 4 -CDots-C 3 N 4 electrode (Fig. 4a, curve 7). Since we deal with three different catalysts with different functions, we should calculate the mass activity (current per gram catalyst) for each component separately. This was done in Supplementary Fig. 10. The reaction activity per mass of a single catalyst component (Co 3 O 4 or CDots) of Co 3 O 4 -CDots (Fig. 4a, curve 6), e.g. is actually lower by more than an order of magnitude than the activity per mass of a single catalyst component of that of the Co 3 O 4 -CDots-C 3 N 4 catalyst (Fig. 4a, curve 7) though curve 6 appears to indicate (Fig. 4a) a larger activity than curve 7 (compare Fig. 4a to Supplementary Fig. 10). Supplementary Fig. 10 clearly shows that starting from a single catalyst component, the addition of each of the two other components increases the activity of syngas production, i.e. all components are necessary for an optimized syngas generation. Similarly, we calculate the turn on frequencies (TOFs) of the catalysts compositions of Fig. 4a. TOF = (number of reacted electrons per time/number of catalyst active sites). We approximate the number of active sites by the number of catalyst atoms. Since we have three different catalyst components with three different functions, we calculate the TOFs per each catalyst component (CDots, Co 3 O 4, and C 3 N 4). Supplementary Fig. 11 shows (similar to Supplementary Fig. 10) that the addition of each single component increases the syngas production activity and all components are necessary for the optimal performance. We further measure the BET and the electrochemical real surface areas of the catalyst components of each combination shown in curves 1–7 (Supplementary Tables 3–4). We use the electrochemical surface areas (ECSAs) to calculate the real current densities related to the specific catalyst components that constitute the seven combinations shown in curves 1–7. Supplementary Fig. 12 shows that similar to the mass activities and the TOFs curves, the real current densities of one component increase with the addition of a second component and are the largest when all three catalyst components combine to a three-component catalyst composite. Figure 3 shows that the ternary concept design indeed yields an optimal performance of its different components balancing between CO 2 reduction and HER. This explains the high value of 10 A/g catalyst obtained at −0.6 V. The CO 2 and H + adsorption measurements (Supplementary Figs. 5 and 6) of the individual different catalyst components (CDots, Co 3 O 4, and C 3 N 4) and their composites (CDots-C 3 N 4, Co 3 O 4 -C 3 N 4, CDots-Co 3 O 4, and CO 3 O 4 -CDots-C 3 N 4) reveal another effect which improves the ternary composite catalyst performance. The synergism of the three components acting simultaneously enhances the adsorption of the ternary composite by a factor of 2–3 with respect to the adsorption of the individual components. Real surface area measurements (BET and electrochemical) of the catalyst compositions (Supplementary Table 3) indicate that the inclusion of the C 3 N 4 component results in a high surface area ((S real /S geometrical) = 200 for BET and 30 for electrochemical) while it is lower by an order of magnitude for the nanoparticle catalysts (CDots, Co 3 O 4, or CDots-Co 3 O 4). The dispersion of the catalysts nanoparticles on the C 3 N 4 surface seems to explain the synergistic adsorption behavior.

---

### Selecting appropriate toys for young children in the digital era [^dd9f979e]. Pediatrics (2019). High credibility.

Toys and child development — general considerations states that "Toys are important in early child development in relation to their facilitation of cognitive development, language interactions, symbolic and pretend play, problem-solving, social interactions, and physical activity, with increasing importance as children move from infancy into toddlerhood".

---

### A versatile computational algorithm for time-series data analysis and machine-learning models [^263e8086]. NPJ Parkinson's Disease (2021). Medium credibility.

Now consider the 3x3 neighborhood around each vector element being constructed from.represents our one-dimensional time-series vector that can be used for time-delay embedding with delay t and embedding dimension m to reconstruct the phase-space trajectory. However, instead of considering the Euclidean norm between data points:we will consider vectors V i and V j in relation to their local 3x3 neighborhood such that:where it can be seen that≠even when = (Fig. 1a). It follows then, that directional and curvature information can be captured by different inequality patterning around the 3x3 neighborhood when computed for allby constructing a new matrix that represents an 8-bit binary code for each point-pair's local neighborhood:where g 0 representsand g n = { g 1, …, g 8 } are its eight connected neighbors. Each neighbor which is larger or equal to g 0 is set to 1, while each neighbor that is smaller than g 0 is set to 0. A binary code is thus created by moving around the central point g 0 (here counterclockwise) where a single integer value is calculated based on the sum of the binary code elements (0 or 1) multiplied by the eight 2 p positional weights (increments of powers of 2: 2 0, 2 1, 2 2, 2 3, 2 4, 2 5, 2 6, 2 7) starting with the preceding points (i.e.). This represents 8-bit binary coding where there are 2 8 (256) different possible integer values, ranging from 0 to 255. This newly created matrix now allows for the identification of graded changes in phase-space trajectories that can be used to identify recurring sets of local topological features embedded within the dynamic signal (LoTRA plot; see Fig. 1b, c for example).

---

### Big data-how to realize the promise [^50a84c47]. Clinical Pharmacology and Therapeutics (2020). Medium credibility.

The increasing volume and complexity of data now being captured across multiple settings and devices offers the opportunity to deliver a better characterization of diseases, treatments, and the performance of medicinal products in individual healthcare systems. Such data sources, commonly labeled as big data, are generally large, accumulating rapidly, and incorporate multiple data types and forms. Determining the acceptability of these data to support regulatory decisions demands an understanding of data provenance and quality in addition to confirming the validity of new approaches and methods for processing and analyzing these data. The Heads of Agencies and the European Medicines Agency Joint Big Data Taskforce was established to consider these issues from the regulatory perspective. This review reflects the thinking from its first phase and describes the big data landscape from a regulatory perspective and the challenges to be addressed in order that regulators can know when and how to have confidence in the evidence generated from big datasets.

---

### Systematic review for the 2018 AHA / ACC / AACVPR / AAPA / ABC / ACPM / ADA / AGS / APhA / ASPC / NLA / PCNA guideline on the management of blood cholesterol: a report of the American college of cardiology / American Heart Association task force on clinical practice guidelines [^4f76e110]. Circulation (2019). High credibility.

ACC/AHA literature search strategy — key combined-set yields: Table 1 reports that combined set #5 (#1 AND #2 AND #3 AND #4) returned 174 items, and set #6 (references cited on the literature search form plus manually selected related references) returned 37 items.

---

### How to find answers to clinical questions [^7cb7fcd9]. American Family Physician (2009). Low credibility.

Many barriers exist to finding answers to physicians' clinical questions. Lack of time, resources, and computer skills, as well as physicians' environment and attitudes about problem solving, all contribute to unanswered questions. Making use of computer-based information resources can give physicians a framework for answering questions and keeping their practice consistent with the best available evidence.