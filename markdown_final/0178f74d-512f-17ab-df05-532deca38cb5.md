# Medical Question & Answer

**Sample ID**: 0178f74d-512f-17ab-df05-532deca38cb5
**Dataset Index**: 238212

---

## Question

2. Let y = f(x) be a quadratic function, the equation f(x) = 0 has two equal real roots, and f′(x) = 2x + 2, then the expression of y = f(x) is.

---

## Answer

> Let's see… What do we have here? The user is asking for the expression of a quadratic function given that it has a double root and its derivative is 2x + 2. Let's break this down step-by-step. First, I need to think about the general form of a quadratic and what a double root implies for its vertex form. Then, I should integrate the derivative to recover the quadratic expression and determine the constant of integration using the double-root condition. Finally, I will verify the solution by checking both the derivative and the discriminant to ensure consistency.

> Let me first confirm the general structure. A quadratic function can be written as f(x) = ax² + bx + c, and if it has two equal real roots, that means the vertex lies on the x-axis and the discriminant b² − 4ac equals zero, which also implies the function can be expressed in vertex form as f(x) = a(x − h)² for some vertex x = h and leading coefficient a ≠ 0.

> Next, I should integrate the derivative to recover f(x). Given f′(x) = 2x + 2, integrating term-by-term gives f(x) = x² + 2x + C, where C is the constant of integration. Wait, let me verify the coefficients: the integral of 2x is x² and the integral of 2 is 2x, so yes, f(x) = x² + 2x + C is correct so far.

> Now, I need to enforce the double-root condition. For f(x) = x² + 2x + C to have exactly one real root (a double root), the discriminant must be zero. The discriminant here is b² − 4ac = 2² − 4(1)(C) = 4 − 4C, so setting this equal to zero gives 4 − 4C = 0, which implies C = 1. Hold on, I should double-check that arithmetic: 4 − 4C = 0 yields C = 1, yes, that's correct.

> Substituting C = 1, the function is f(x) = x² + 2x + 1. I can also write this in vertex form to make the double root explicit: f(x) = (x + 1)², which shows the vertex at x = −1 and confirms the double root at x = −1. Let me verify the vertex expansion: (x + 1)² = x² + 2x + 1, which matches, so the equivalence is correct.

> I should confirm the derivative matches the given information. Taking the derivative of f(x) = x² + 2x + 1 gives f′(x) = 2x + 2, which is exactly what was provided. Good, that checks out.

> Final answer: f(x) = x² + 2x + 1, or equivalently f(x) = (x + 1)².

---

The quadratic function with a double root and derivative (f′(x) = 2x + 2) is (f(x) = x² + 2x + 1). This is found by integrating the derivative to get (f(x) = x² + 2x + C), then setting the discriminant to zero to enforce a double root, which gives (C = 1).

---

## Step 1: Integrate the derivative to find the general form of the quadratic function

Given the derivative (f′(x) = 2x + 2), integrate to find the general form of (f(x)):

f(x) = ∫(2x + 2) dx = x² + 2x + C

where (C) is the constant of integration.

---

## Step 2: Apply the condition of a double root

For a quadratic equation (ax² + bx + c = 0) to have a double root, the discriminant must be zero:

Δ = b² − 4ac = 0

For (f(x) = x² + 2x + C), we have (a = 1), (b = 2), and (c = C). Substitute these into the discriminant condition:

2² − 4(1)(C) = 0 ⇒ 4 − 4C = 0 ⇒ C = 1

---

## Step 3: Write the final expression of the quadratic function

Substitute (C = 1) back into the general form:

f(x) = x² + 2x + 1

---

## Verification

- **Derivative check**: (f′(x) = 2x + 2), which matches the given derivative.
- **Double root check**: The equation (x² + 2x + 1 = 0) has a double root at (x = −1), confirming the condition.

---

The expression of the quadratic function is (f(x) = x² + 2x + 1).

---

## References

### An approach to estimating prognosis using fractional polynomials in metastatic renal carcinoma [^111kooJg]. British Journal of Cancer (2006). Low credibility.

Fractional polynomials

Fractional polynomials were introduced byas an extension of the familiar and well-established polynomial method of modelling with continuous predictors. The aim was to increase the range of functions that could be represented, while maintaining simplicity and mathematical tractability. We will denote by x a continuous prognostic factor. By transforming x, the first-order polynomial (i.e. linear function)is extended to the first-order fractional polynomial or FP1 functionFor technical reasons the power p is restricted to the special set −2, −1, −1/2, 0, 1/2, 1, 2, 3. Here ' x 0 ' denotes the natural log function, log(x). The second order or quadratic polynomialis extended to the second order fractional polynomial or FP2 functionorthe second form being known as a 'repeated powers' model.demonstrated that by varying the powers (p, q) and the coefficients (b, c), a remarkable range of curve shapes could be created from these simple families of mathematical functions. This imparts great flexibility for modelling nonlinear relationships in real data. Note that linear and quadratic polynomials are special cases of the more general FP1 and FP2 fractional polynomials.

Technical details of how a fractional polynomial model for one predictor x is determined (that is, how the powers are estimated) and how significance testing is done will not be given here; interested readers are referred to. An introduction to fractional polynomials in the context of estimating the prognosis of breast cancer patients is given by.

---

### Strong coronal channelling and interplanetary evolution of a solar storm up to earth and mars [^112h4cnp]. Nature Communications (2015). Medium credibility.

Then, r x and r y can be introduced into the definition of an ellipse in cartesian coordinates,

This results in the following expression, which was simplified with the definition of f from equations (2):

This quadratic equation gives two analytic solutions of the front and rear crossings of d with the ellipse as a function of the parameters b, c from equation (7), f from equation (2) and Δ:

The solution with the positive sign in front of the root is the 'front' solution (d 1) and the one with the negative sign the 'rear' solution (d 2). The speed V Δ (t) of the ellipse at the position defined by the angle Δ is derived from the self-similar expansion of the ellipse, which implies a constant half width λ. The assumption of self-similar expansion means that the shape of the ejection must not change in time, so the ratio between speeds and distances for all points along the ellipse must be constant:

Further, the time when d 1 (t) is equal to the heliocentric distance of the planet or the in situ observer gives the arrival time of the ellipse at the in situ location, and the speed V Δ (t) at this time the arrival speed.

---

### The burden of proof studies: assessing the evidence of risk [^115T4CW5]. Nature Medicine (2022). Excellent credibility.

Basis splines, measurement mechanism and shape constraints

We used a Bayesian regularized spline to obtain the general shape of the nonlinear relationship. Basis splines represent nonlinear curves as linear combination of recursively generated basis elements. The basis elements were recursively generated using piecewise smooth polynomials, and were roughly localized to certain regions of the exposure variable in the data. Most of the time, quadratic or cubic polynomials were used, often with linear tails in the presence of sparse data. This approach allowed the common restricted cubic spline and constraints on the shape of the relationship (including nondecreasing and nonincreasing).

Given basis functions f 1, …, f k and coefficient vector β = (β 1. β k), the final curve is obtained as a β -linear combination

Specifically, for any given exposure (x), the prediction using the spline model is given bywhere X is a vector containing (f 1 (x), …, f k (x)). Derivatives and integrals of splines can likewise be expressed as linear combinations of spline coefficient β. For additional details about B -splines see Zheng et al.

Many studies of dose–response relationships report relative risks between categories defined by intervals of consumption. In mathematical notation, these observations are given bywhere y ij is the reported relative risk corresponding to measurement j in study i, [a ij, b ij] delineates the reference group exposure interval, and [c ij, d ij] delineates the alternative group exposure interval.

When f (x) is represented using a spline, each integral is a linear function of β similar to equation (1). The model (equation (2)) is then a ratio of linear functions, with the log relative risk given by

Equation (4) is a nonlinear function of the spline coefficients β.

When studying dose–response relationships, we allow for shape constraints of the inferred mean response. For example, for some harmful risks, such as smoking and air pollution, we assume the relative risk is monotonically increasing with exposure. To regularize the splines, and capture biologically plausible limits, we also allow a maximal derivative constraint, which is similar to penalizing total variation or limiting the spline degree. To introduce each of these constraints, we used the fact that derivatives of splines are linear functions of spline coefficients, similar to equation (1).

---

### Parametric matrix models [^111aPXL9]. Nature Communications (2025). High credibility.

The output functions of neural networks do not correspond to the solutions of any known underlying equations. Rather, they are nested function evaluations of linear combinations of activation functions. Many commonly-used activation functions have different functional forms for x < 0 and x ≥ 0 and therefore are not amenable to analytic continuation. Most of the other commonly-used activation functions involve logarithmic or rational functions of exponentials, resulting in a more complicated analytic structure.

Let us consider a PMM that uses Hermitian primary matrices with matrix elements that are polynomials of the input features up to degree D. Suppose now that we vary one input feature, c l, while keeping fixed all other input features. The output functions of c l can be continued into the complex plane, with only a finite number of exceptional points where the functions are not analytic. These exceptional points are branch points where two or more eigenvectors coincide. A necessary condition for such an exceptional point is that the characteristic polynomial of one of the primary matrices, has a repeated root. This in turn corresponds to the discriminant of the characteristic polynomial equaling zero. If the primary matrix has n × n entries, the discriminant is a polynomial function of c l with degree n (n − 1) D. We therefore have a count of n (n − 1) D branch points as a complex function of c l for each primary matrix. If a branch point comes close to the real axis, then we have a sharp avoided level crossing and the character of the output function changes abruptly near the branch point. Our count of n (n − 1) D branch points for each primary matrix gives a characterization of the Riemann surface complexity for the functions that can be expressed using a PMM.

---

### Application of response surface methods to determine conditions for optimal genomic prediction [^112bBHYM]. G3 (2017). Low credibility.

Since the true response function is unknown, we have to approximate f. Under standard smoothness assumptions, a low-order polynomial function provides a good local approximation to the true f. For example, a first-order main effects model can be written aswhereare coded variables. is the unknown intercept, andare the unknown regression coefficients. Equation 5 is called a main effects linear model because it only contains the linear effects of the p factors on the response with no interaction terms. When the model in Equation 5 includes interactions, we call it the first-order model with interaction, and write it asIn situations such as those illustrated in Figure 1, a second-order model can be also used to model the unknown response function f. A second-order polynomial provides a good local approximation to almost any surface because it can have different functional forms and it is easy to estimate its parameters. In general, the second-order model can be written asLet β denote the vector of unknown regression coefficients with dimension depending on the model. With an interaction term or a second-order model we can introduce curvature into the estimated surface. The model equation can be written in a concise matrix notation aswhere y is anvector of observations, X is andimensional matrix of the levels of the coded explanatory variables, β is avector of the coefficients, and ϵ is anvector of the random error terms. An ordinary least-squared estimator of the model coefficients can be written aswith the variance–covariance matrix ofhaving the formwhereis the variance-covariance matrix ofTo estimate regression coefficients, β in the response function requires data from experiments designed to meet the objective. If the objective is to approximate the response surface, a frequently used treatment design is the factorial. For example, temperature and the degree of drought could be two factors affecting yield, but the number of factors could be more than two and the possible values per factor can be qualitative, quantitative, and numerous. For the example illustrated in Figure 1, a reasonable full factorial model would consist offactor combinations. To observe a response at each factor combination when there are two levels for the p factors, unreplicated treatment combinations are required and the design is called thefactorial design. When p is large and the range of possible values of each factor is also large, finding the combination of the p factors needed to approximate the response surface increases exponentially. For example, three factors with two levels each requirestreatment combinations; whereas if the number of factors is five, the number of factor combinations isAt least some of the treatment combinations would need to be replicated if we want an estimate of the variance of residuals

---

### Parametric and nonparametric statistical methods for genomic selection of traits with additive and epistatic genetic architectures [^117Hzyyt]. G3 (2014). Low credibility.

There are many loss functions used for SVM regression. Some of the popular loss function choices include the squared loss, absolute loss, and the ε -insensitive loss. Here, we present these loss function formulations.
1 The squared loss function has the form L (y − f (x)) = (y − f (x)) 2. It scales the loss quadratically by the size of the error. Using this loss function indicates that outliers are also weighted quadratically, which requires the user to deal with the outliers before the regression analysis.
2 The absolute loss function has the form L (y − f (x)) = | y − f (x)|. The absolute loss function scales the loss linearly by the size of the error eliminating the difficulty of using data sets with outliers.
3 The ε -insensitive loss function has a form:

where ε determines the number of support vectors used in the regression function. By definition, a support vector is a vector x i that satisfies the equation y i (w x i + b) = 1. Increasing ε implies that fewer support vectors are used in the fitting. The ε -insensitive loss function ignores the errors in the regression that have size less than ε. When the error is greater than ε, the loss is | y − f (x)| − ε.

Figure 2 illustrates the absolute loss, squared loss, and ε -insensitive loss functions as a function of the error y − f (x).

Figure 2
Loss functions used for SVM regression. The first panel shows the absolute loss function. The second panel is the square loss function, and the last panel is the ε -insensitive loss function.

---

### Spin dephasing under nonlinear gradients: implications for imaging and field mapping [^1171KZF1]. Magnetic Resonance in Medicine (2012). Low credibility.

This work examines the prototypical MR echo that would be expected for a voxel of spins evolving in a strong nonlinear field, specifically focusing on the quadratic z(2) - ½(x(2) + y(2)) field. Dephasing under nonlinear gradients is increasingly relevant given the growing interest in nonlinear imaging, and here, we report several notable differences from the linear case. Most notably, in addition to signal loss, intravoxel dephasing under gradients creating a wide and asymmetric frequency distribution across the voxel can cause skewed and nonlinear phase evolution. After presenting the qualitative and analytical origins of this difference, we experimentally demonstrate that neglecting these dynamics can lead to significant errors in sequences that assume phase evolution is proportional to voxel frequency, such as those used for field mapping. Finally, simplifying approximations to the signal equations are presented, which not only provide more intuitive forms of the exact expression but also result in simple rules to predict key features of the nonlinear evolution.

---

### What quantile regression does and doesn' T Do: a commentary on petscher and logan (2014) [^111sVEb9]. Child Development (2019). Medium credibility.

Petscher and Logan's (2014) description of quantile regression (QR) might mislead readers to believe it would estimate the relation between an outcome, y, and one or more predictors, x, at different quantiles of the unconditional distribution of y. However, QR models the conditional quantile function of y given x just as linear regression models the conditional mean function. This article's contribution is twofold: First, it discusses potential consequences of methodological misconceptions and formulations of Petscher and Logan's (2014) presentation by contrasting features of QR and linear regression. Second, it reinforces the importance of correct understanding of QR in empirical research by illustrating similarities and differences in various QR estimators and linear regression using simulated data.

---

### Limits on the computational expressivity of non-equilibrium biophysical processes [^112Romk3]. Nature Communications (2025). High credibility.

Fig. 2
The matrix-tree theorem.

A Computing the steady-state occupancy π 1 by summing weights over directed spanning trees. Directed spanning trees are subgraphs containing all graph nodes but no cycles, with edges oriented toward a root node. In each directed spanning tree, the input forces make a positive, negative, or zero contribution to the tree weight. The structural vectorsare shown below each tree; these quantities enter into Equation (3) below. B Schematic illustration of the high-dimensional space of feature vectors ψ (i; θ) and χ (i, F). The depicted arrangement of vectors could solve a binary classification problem.

We define the input multiplicityas the number of edges affected per input variable, which we assume to be the same for each input. To focus on the functional way in which the input driving enters the steady-state probabilities, the driving contributions can be factored out in the algebraic expressions for the numerator and denominator of Equation (1). This has been previously been used to make analytical progress for M = D = 1 in, for example, refs. –. This equivalent formulation of Eq. (1) suggests that steady states of Markov jump processes implement a rational polynomial function of exponentiated input variables. Defining, we rewrite the matrix-tree expression for π i for general D and M We use the multi-index, whereis the set of D input labels and each componentof the multi-index runs over the values, to enumerate themonomials. These monomials y μ (F) in Equation (2) combinatorially depend on the different mixtures μ of input driving, representing a net total μ a of signed contributions from the input force F a, μ b such contributions for F b, and so on for each input. The coefficients, which are functions of the parameters θ, are the sums of weights over all directed spanning trees rooted at node i which have the corresponding mixture μ of signed input contributions. The monomial coefficientsthus represent learnable amplitudes of each polynomial basis function y μ (F). The coefficients in the denominator are defined as. Classification will be successful if, for F ρ drawn from class ρ, the coefficientsand monomials y μ (F ρ) are large for the same μ. In the subsequent sections of the paper and in the Supplementary Information we use the formulation in Equation (2) to show how the classification ability of a non-equilibrium Markov processes may be systematically modulated.

---

### Biophysical ambiguities prevent accurate genetic prediction [^117XsqET]. Nature Communications (2020). High credibility.

Phenotypes to free energy for non-pleiotropic mutations

Mutations in the CI protein can affect protein-folding energy (Δ G F), dimerization energy (Δ G D), binding energy to the operator sites (Δ G OR1–OR3) and tetramerization energy (Δ G T) at the biophysical level. We assume that mutations in CI that alter the free energy of DNA binding do so by the same magnitude for all three operators (ΔΔ G B = ΔΔ G OR1 = ΔΔ G OR2 = ΔΔ G OR3). To calculate only one biophysical change that can lead to the phenotype, we reversed the Forward Function described in the previous section. The Reverse Function for both P PR and P PRM is composed of two sub-functions. The first sub-function is the above-mentioned Forward Function, which calculates phenotypes from biophysical changes. This function is written in the form of y = f(x), where y is the phenotype and x is a set of biophysical parameters including the total expression level of CI. The second sub-function is an Inverse Function that finds all roots for an equation in the form of y – f(x) = 0. A root-finding process is performed using the uniroot.all function in the R package rootSolve. Specifically, for each perturbation of biophysical parameter (∆∆ G), we looked for all the roots within a range of −2–10 kcal per mol, and returned the ∆∆ G values that produce the phenotypes while the other biophysical parameters are not perturbed.

Mutational effects are modelled at a fixed expression level of CIthat corresponds to ~99% repression of the P R promoter and the CI concentration in a lysogen. To calculate changes in the biophysical parameters for single mutants with known effects on expression from P R or P RM, we first generated 136 evenly spaced phenotypes (with an interval of 0.1 in log(2) scale from −13.5 to 0). Then, for a given phenotype, we calculated corresponding changes in any of the four free-energy terms (biophysical parameters), each time allowing only one biophysical parameter to change using the Reverse Function explained in in the previous paragraph.

---

### Nonlinear delay differential equations and their application to modeling biological network motifs [^117Dup3e]. Nature Communications (2021). High credibility.

Methods

Analytics and numerical simulation

Analytics were in general performed by hand, and checked for validity using Mathematica. Numerical simulations were run in Matlab using the dde23 delay differential equation solver for DDEs and ode45 for ODEs. Simulating activators as repressors with n < 0 technically fails when x is identically zero (Eq. (5)), since that would imply division by zero, but the limit as x goes to zero causes the regulation term to be zero, which is the same result as assumed by our notation. An initial value of exactly zero for x can thus lead to a divide-by-zero error in simulations, and so initial conditions of exactly zero were not used, as that case is an uninteresting fixed point for activators in any case. Note also that the consitutive case for Eq. (5) is degenerate, in that n = 0, α ≠ 0 is equivalent to n ≠ 0, α = 0 with α 0 → α 0 + α /2.

Phase plot simulations and analysis

For autoregulation phase plots, simulations were run with 100 constant-history initial conditions spread logarithmically between 10 −4 and 2 η and run from T = 0 to T = 100(γ + 1). Solutions were considered stable if for all 100 simulations the maximum absolute value of the discrete derivative in the last three-quarters of the simulation time was less than 0.1. Stable solutions were sub-categorized as bistable if a histogram of final values over all 100 solutions had more than 1 peak. Solutions were considered oscillatory if the average Fourier transform of the last three-quarters of the simulation time for all 100 solutions had more than zero peaks with amplitude (square root of power) greater than 100. Solutions were considered spiral if this oscillation condition held for the first one-quarter of the simulation time only. For two-component loops, initial conditions were used that ranged between 0 and, for equal X and Y and for apposing X and Y. Bistability was determined as for autoregulation, and a cutoff of 0.05 was used to determine "low" values. All simulation histories were constant except where indicated in Supplementary Fig. 7. Specific parameter values and simulation details are given in the figures and/or made explicit in the MATLAB code in Supplementary Data 1.

---

### Non-linear relationships in clinical research [^115S4XT5]. Nephrology, Dialysis, Transplantation (2025). Medium credibility.

True linear relationships are rare in clinical data. Despite this, linearity is often assumed during analyses, leading to potentially biased estimates and inaccurate conclusions. In this introductory paper, we aim to first describe-in a non-mathematical manner-how to identify non-linear relationships. Various methods are then discussed that can be applied to deal with non-linearity, including transformations, polynomials, splines and generalized additive models, along with their strengths and weaknesses. Finally, we illustrate the use of these methods with a practical example from nephrology, providing guidance on how to report the results from non-linear relationships.

---

### Identification of signal bias in the variable flip angle method by linear display of the algebraic ernst equation [^114SJDP4]. Magnetic Resonance in Medicine (2011). Low credibility.

Noise Propagation with Correlated Errors

The image noise is scaled individually along abscissa (σ S τ) and ordinate (σ S /τ). The errors are equal for τ = 1 or α = 53°. At smaller flip angle (τ → 0), the y -error is enhanced and the x -error is diminished, and vice versa for τ → ∞ (α → π).

In a function F of two variables, x and y, the statistical errors (given by the variances var x and var y) are propagated by the respective partial derivatives ∂ F /∂ x. If the errors in x and y are not independent (i.e. correlated), their covariance has to be taken into account by using the general law of Gaussian error propagation.

Here, F is the residual F (x, y) = y – (A – x /2ρ 1) with x = S × τ and y = S /τ. Thus, the covariance is identical to the image noise variance σ S 2 and independent of τ.

This is in stark contrast to the common linearization of Eq. 2, where the errors and their covariance increase strongly for small flip angles by 1/sin α or 1/tan α. Equation 16 yields

The expression in brackets denotes a τ-dependent scaling factor of the image noise. It can be cancelled by imposing suitable weights w (τ) onto the square residues

in the linear least squares objective function. These weights are maximal at τ E and decrease towards the intercepts with the x - and y - axes.

Short TR

If R 1 TR ≪ 1 then ρ 1 can be approximated and Eq. 9 arranged as

Under this condition, Sτ /2TR can be used as abscissa to incorporate varying values of TR into the regression. Estimates of T 1 are obtained directly as the (negative) slope

This is the generalization of the formula for the dual angle experiment.

---

### Warped linear mixed models for the genetic analysis of transformed phenotypes [^114dHQr6]. Nature Communications (2014). Medium credibility.

Results

Summary of the method

Both, when specifying a phenotype transformation or when inferring it from the data (for example, using WarpedLMM), the implicit assumption is that the quantitative trait under genetic control is unobserved or latent, with the observed phenotype being determined by a nonlinear mapping g that links the latent phenotype to the observed measurements (Supplementary Fig. 1). Thus, to recover the true genetic model, an estimate of the ideal phenotype transformation f (where f = g −1) is needed. If we denote the observed phenotype for individual n as y n, an estimate of the latent phenotype z n can be obtained by applying the function f, optionally parameterized by ψ:

In WarpedLMM, these functions are constrained to be invertible and are termed 'warping functions'. The functional form of f is determined by parameters ψ, which are inferred jointly with the remaining model parameters of the LMM. The most probable transformation can then be inferred by maximizing the sum of the standard log likelihood and a Jacobian term that accounts for the complexity of the fitted warping function. Several functional forms of the warping functions can be chosen (see Methods), differing in number of free parameters and in the complexity of the functions they can represent. In the following, we consider a particular family of functions initially proposed by Snelson et al. which can be expressed as linear combination of a linear scaling term and multiple nonlinear step functions. If the observed phenotype y n does not require a transformation, only the linear term will be used. Otherwise, the function will consist of both the linear term and one or more step functions.

---

### Non-linear relationships in clinical research [^113d418D]. Nephrology, Dialysis, Transplantation (2025). Medium credibility.

Polynomial functions

In Fig. 4, we provide an example of a simple power transformation using the square function, which raises a variable to the power of 2 (x 2), providing a U-shaped curve. Higher powers of x can be also used to determine the shape of the function, such as the cubic function (x 3) which has a distinctive S-shape. These power terms form the core elements of so-called polynomial functions, which along with the coefficients included in the function, offer a flexible way to model various curves. By adjusting the coefficients and power functions, polynomials offer a wide variety of shapes. "Fractional" polynomial functions allow the power terms to be fractions instead of just whole numbers (i.e. x 1/2) [21]. A (fractional) polynomial function often provides sufficient flexibility to follow relatively simple non-linear curves, providing a simpler solution than the more advanced techniques we describe below. However, higher degree polynomials can be sensitive to "noise" in the data, and are not suited for fitting some of the more complex curves (e.g. sudden shifts, discontinuities, or logarithmic curves). They may also suffer from Runge's phenomenon, becoming unstable and oscillating at the edges of the data, and extrapolate poorly beyond the original range of the independent variable.

Regression splines

A powerful approach to dealing with non-linearity is provided by the family of spline functions. Instead of a single function defining the whole curve, such as polynomials or other transformations, splines are constructed by using a series of functions, each defining a different segment of the curve. As splines are constructed segment by segment — or piece by piece — they are often referred to as "piecewise" functions. Segments are connected to each other using so-called "knots", and the spline is restricted to join at these knots so there are no gaps in the curve. The simplest spline function is the linear spline function. This function assumes linearity within each segment, but the overall curve formed by the connected segments can be non-linear. For a small number of segments, linear spline models can be as easy to interpret as linear regression, as each segment can be represented by a single slope. Figure 3 provides an example of a simple linear spline with two knots (in green). These two knots divide the range of the independent variable into three segments, each with its own slope.

---

### A quadratic paradigm describes the relationship between phenotype severity and variation [^115htBgJ]. Nature Communications (2025). High credibility.

In 1942 Waddington observed that phenotype variation among mutant animals is greater than in wild types. Here we update this observation to depict unexpected relationships between phenotype severity and variation. Using a zebrafish mef2ca allelic series representing a range in craniofacial phenotype severity, we tested the straightforward hypothesis that as phenotype severity increases, variation increases. We found that severity and variation were positively correlated, but only to a point. Variation collapsed in the most severe conditions. Mathematically, we found that the best fit for the relationship between severity and variation is a quadratic function. Across both zebrafish craniofacial phenotypes and human genetic disease, wild-type conditions produced low variation, moderate severity was associated with high variation, and conditions of extreme severity resulted in low variation. We propose that the quadratic relationship between severity and variation is a universal principle of biology that until now has not been formally tested.

---

### Bayesian tomography of high-dimensional on-chip biphoton frequency combs with randomized measurements [^115ZWVUa]. Nature Communications (2022). High credibility.

In addition to the parameters forming ρ, the scale factor K must also be suitably parameterized. Following ref. we find it convenient to write K = K 0 (1 + σ z), where K 0 and σ are hyperparameters defined separate of the inference process, and z is taken to follow a standard normal distribution, leading to a normal prior on K of mean K 0 and standard deviation K 0 σ. We take σ = 0.1 and K 0 equal to the sum of the counts in all d 2 bins for the first JSI measurement (r = 1), where the absence of modulation ensures that all initial photon flux remains in measured bins, i.e. This provides an effectively uniform prior, since a fractional deviation of 0.1 is much larger than the maximum amount of fractional uncertaintyexpected from statistical noise at our total count numbers; the use of a normal distribution simplifies the sampling process.

The total parameter set can therefore be expressed as the vector, with the prior distribution

We note that this parameterization entails a total of 4 d 4 + 1 independent real numbers (2 d 4 complex parameters for ρ, one real parameter for K) — noticeably higher than the minimum of d 4 − 1 required to uniquely describe a density matrix. Nevertheless, this ρ (y) parameterization is to our knowledge the only existing constructive method to produce Bures-distributed states, and is straightforward to implement given its reliance on independent normal parameters only.

Following Bayes' rule, the posterior distribution becomeswhereis a constant such that ∫ d x π (x) = 1. We have adopted this notation for Bayes' theorem — rather than the more traditional — to emphasize the functional dependencies on x, which are all that must be accounted for in the sampling algorithm below. From π (x), the Bayesian mean estimator f B of any quantity (scalar, vector, or matrix) expressible as a function of x can be estimated aswhere, in lieu of direct integration, S samples { x (1), …, x (S) } are obtained from the distribution π (x) through Markov chain Monte Carlo (MCMC) techniques, as described below.

---

### Genetic association mapping leveraging gaussian processes [^114u5zkN]. Journal of Human Genetics (2024). Medium credibility.

GP regression and its use in genetic association mapping

Because the GP yielding f (x) has various useful properties inherited from the normal distribution, GP can be used to estimate a nonlinear function f (X) from output dataalong continuous factor X. The extended linear model y = f (X) + ε is referred to as the GP regression and widely used in the machine learning framework. This model can be used to map dynamic genetic associations for normalized gene expression or other common complex quantitative traits (e.g. human height) along the continuous factor x (e.g. cellular states or donor's age). Let us denote the genotype vectorand the kinship matrix R among N individuals, the mapping model, as proposed by us or others can be expressed as follows:whereare all GPs with similar covariance matrices, where ⊙ denotes element wise product between two vectors or matrices with the same dimensions, K = k (X, X) denotes the covariance matrix with a kernel function, and ε denotes the residuals. Intuitively, α models the average baseline change of y in relation to x, while β represents the dynamic genetic effect along x. The effect size is multiplied by the genotype vector g, indicating that the output y i varies between different genotype groups (g i ∈ {0, 1, 2}). In fact, the effect size β (x i) is additive to the baseline α (x i) at each x i, which is the same as the standard association mapping. Here statistical hypothesis testing is performed under the null hypothesis of δ g = 0, as the strength of genetic association is determined by δ g.

---

### Leveraging data-driven self-consistency for high-fidelity gene expression recovery [^115cUYWD]. Nature Communications (2022). High credibility.

ROI selection and histogram equalization

We divide the expression matrix S is divided into N ≥ 4 ROIs, which we denote by S i, i = 1, 2, ⋯, N. Letanddenotes the number of matrix elements and possible expression values, in each ROI S i, respectively. Furthermore, we define the normalized histogram associated with the possible gene expression values in the ROI S i as followswhereis the number of elements in the ROI with the gene count n such that. The corresponding cumulative distribution function (CDF), is given by

Adaptive equalization of histogram

The goal of the histogram adaptive equalization is to find a transformation for the expression values such that the transformed CDF is approximated by the CDF of f (x), where f (x) can be f g (x), f r (x) or f e (x). As example, the CDF of an exponential distribution can be written aswhere λ denotes the decay parameter. SERM uses the transformationfor each ROI to map the expression values to their new values. In particular, consider transforming the expression counts bywhere the floor function ⌊ x ⌋ is the biggest integer not exceeding x. The mapis then applied to each expression value in the ROI S i.

The main intuition behind this transformation comes from the inverse sampling method. In particular, let X denotes continuous random variables whose CDF is strictly increasing on the possible value onwith densities p X and p Y, respectively. Consider the transformationThen, Y has the uniform distribution, i.e. To see this note that

Furthermore, using the inverse transform sampling technique, the random variablehas the exponential distribution, i.e. Z ~ F (t). Combining the transformations T 0 and T 1, we conclude thathas the desired exponential distribution. Now, the transformationdefined in Eq. (11) is an inverse sampling method for the discrete random variable similar to the continuous transformation T 1 ∘ T 0. Similar analytical equations to Eqs. (10)–(15) can be derived for Gaussian and Rayleigh distributions.

Sliding ROI

The ROI is slided throughout the whole expression matrix and gene imputation is performed at each ROI to obtain the resulting imputed gene expression matrix from SERM.

---

### An additive gaussian process regression model for interpretable non-parametric analysis of longitudinal data [^1155sEPY]. Nature Communications (2019). High credibility.

GP models can be made more flexible and interpretable by making them additive, where the kernel (covariance) is a sum of kernels (covariances) and each kernel models the effect of individual covariates or their interactions, i.e. f (x) = f (1) (x) + ⋯ + f (D) (x). Intuitively one can think that each GP component f (j) now implements a nonlinear function that specifies the corresponding effect, and the overall effect of several covariates is then the sum of these nonlinear functions. This is achieved by using specific kernels for different types of covariates, such as squared exponential (se) kernel for continuous covariates, constant (co), binary (bi), and categorical (ca) kernels for discrete covariates, and products of these kernels for interaction terms. Moreover, ns signals can be accounted for by incorporating ns kernels.

Figure 1 shows an example where biomarker data y is simulated from an AGPM that depends on continuous covariates age (age) and time from a disease event (diseaseAge) as well as discrete covariates ID (id) and location (loc) as follows: where id identifies an individual and ε is additive noise. In other words, the underlying regression function f is decomposed into six separate (nonlinear) functions (Fig. 1, top row), and the measurements are corrupted by additive noise ε (Fig. 1, top row, right panel). This example provides an intuitive illustration of nonlinear and ns effects of different kernels mentioned above. For example, continuous covariate age has a nonlinear effect on y and similarly continuous covariate diseaseAge has a nonlinear and ns effect on y, where the largest change in the effect is localized at the time of disease onset. The overall cumulative effect is then defined by the sum of the individual nonlinear effects (Fig. 1, bottom row, second panel from right), and measurements of biomarker y are corrupted by additive noise (Fig. 1, bottom row, right panel). In case a study contains other covariates or interaction terms, the additive GP regression provides a very flexible modelling framework that can be adjusted to a number of different applications.

---

### Effective fitness under fluctuating selection with genetic drift [^113KUe96]. G3 (2023). Medium credibility.

Expected mutant frequency under fluctuating selection does not equal that under constant selection with f G when there is drift

We now evaluate whether the expected mutant frequency after fluctuating selection with drift equals that after constant selection with f G in the presence of drift. Again, for simplicity, we consider 2 generations. We note that f 1 and f 2 can be rewritten as

where. We rewrite the formula in this way so thatandcan be expressed usingand a variable x, where x controls the level of difference betweenand. When; when; and when. With the new notation, Eq. (1) becomes, whereand. We can viewin the above as a function G of x. That is,

We now hope to prove thatfor anyacross the parameter space. Most straightforwardly, if we can prove thatis a strictly monotonic function of x, the problem is solved. However, we found it difficult to analytically prove the monotonicity of. Instead, we attempted to demonstrate the monotonicity ofnumerically. To explore various combinations of parameters, and, we varied each parameter individually while holding the other 2 parameters constant. Specifically, varied from 0.1 to 0.9 with an interval of 0.1, whileandwere fixed at 0.9 and 1,000, respectively (Supplementary Fig. 1a); log 2 varied from −1 to 1 with an interval of 0.2, whileandwere fixed at 0.3 and 1,000, respectively (Supplementary Fig. 1b); varied from 1,000 to 10,000 with an interval of 1,000, whileandwere fixed at 0.3 and 0.9, respectively (Supplementary Fig. 1c). Thecurve was drawn by sampling x from −1 to 1 with an interval of 0.02. The monotonicity ofwas then inspected visually.

---

### Nonlinear delay differential equations and their application to modeling biological network motifs [^115yjrCF]. Nature Communications (2021). High credibility.

After plugging the delayed pseudo-steady state of Y into, one can match the values of the composite Hill-within-Hill function at X = 0, X = 1, and X → ∞ as well its the slope at X = 1 compared to a single Hill function with leakage, and thereby approximate the cascade as a single-step regulation of Z by X (Supplementary Note 1):where the combined regulation parameters are (Supplementary Note 1)The k can be removed by renormalization of X.

Equation (10) has several biological consequences. First, the overall Hill coefficient h is negatively proportional to the product of individual cooperativities n X, n Y. This makes the cascade activating for either two activators or two repressors, and repressive otherwise. It also makes the cascade more cooperative than its components, in line with past analysesand as found experimentally. Third, the leakage is zero for n Y < 0 and negligible (i.e. α 0 ≪ α) for n Y > 0 if η Y ≫ 1. This means Y must be produced in sufficient amount during the delay to repress Z. We generally ignore leakage in the main text; for a more detailed discussion see Supplementary Note 3 and Supplementary Fig. 2.

The total delay in a cascade equals the sum of individual delays

For cascades with delays at each step, the same analysis implies that total delay equals the sum of individual delays (β z / β y + γ y + γ z, with γ y the intrinsic delay between X and Y and γ z the intrinsic delay between Y and Z). Figure 3 shows simulations of ODE, equivalent DDE, and multi-delay DDE cascade models, matching the above analytical results.

---

### Analysis of multi-condition single-cell data with latent embedding multivariate regression [^1132YhkZ]. Nature Genetics (2025). High credibility.

Setting γ (X c :) to the least-sum-of-squares solution of regressing Y on X, the matrix Z andare fit by minimizing the sum of squared residuals (equation (2)), whereis a suitable set of matrix-valued functions that we define in the following. Equation (4) with these additional features can be considered a multi-condition extension of PCA.

Intuitively, this multi-condition PCA finds a function R that generates for each condition (for each distinct row of the design matrix X) a P-dimensional subspace that minimizes the distance to the observed data in that condition by rotating a common base space into the optimal orientation (Fig. 1c, step 1). Z is the orthogonal projection of the data on the corresponding subspace. Stability is ensured by constraining R to come from a setof 'well-behaved', smooth functions of the covariates.

---

### Sign epistasis caused by hierarchy within signalling cascades [^117U9RFN]. Nature Communications (2018). Medium credibility.

Fitness proxy

We considered a variable environment characterised by two input values x 1 and x 2, leading to the respective cascade outputs y 1 and y 2. Each environment has a fitness (or performance) function F 1 and F 2. When these functions follow affine forms of the benefit and cost,… Assuming that an equal time is spent in both environments, the average fitness is. For any (a, b, c), the rangeis a direct proxy for relative fitness changes due to mutations, as the latter only affect y 1 and y 2.

Gaussian fitness function

The centred stretched Gaussian fitness functions rotated of an angle a are of the form, where M the matrix defined by, and.

Biochemical model and parameters

The kinetic model of the cascade consisted of combining repressors that are described by the following input–output Hill function:where M i is the maximum expression level, m i the minimum, K i the dissociation constant and n i the cooperativity. For activators, we used the functional form

Prediction of epistasis

Using the notations above, sign epistasis types were computed using the function R = Out(LacI(TetR(ara min))) − Out(LacI(TetR(ara max))), where ara min and ara max define the bounds of the input range.

Significance of fitness variations

To assess the typical noise in the response of each mutant, we computed the root mean square deviationbetween the averageof triplicates for 16 arabinose values and the smoothed input–output response. To assess the significance of the overall YFP output range R lac, tet of single response curves, we tested the hypothesis ' R lac, tet is strictly positive' using normally distributed statistics forfor significance p < 0.05. Given that variances between mutantsare not equal, significance of fitness variations along trajectories was evaluated with a two-sided Welch t -test for strict positivity of the fitness change over the full paths (p < 0.001, N = 44, given the 48 measurement points and four parameters used to fit). The same test was used to determine significantly decreasing intermediate steps.

---

### Gene regulatory network inference from sparsely sampled noisy data [^11632Qhn]. Nature Communications (2020). High credibility.

The GP framework can handle combinatorial effects, meaning nonlinearities that cannot be decomposed into f (x 1, x 2) = f 1 (x 1) + f 2 (x 2). This is an important property for modelling a chemical system — such as gene expression — where reactions can happen due to combined effects of reactant species. For example, the dynamics corresponding to a chemical reaction x 1 + x 2 → x 3 cannot be modelled by.

In a classical variable selection problem from input–output data η j = f (ξ j) + v j, for j = 1, …, N, where, the task is to find out which components of ξ does the function f depend on. In the GP framework, variable selection can be done by a technique known as "automatic relevance determination", based on estimating the hyperparameters of the covariance function of f. For BINGO, we have chosen the squared exponential covariance function for each component f i :The hyperparameters β i, j ≥ 0 are known as inverse length scales, sincecorresponds to a distance that has to be moved in the direction of the j th coordinate for the value of function f i to change considerably. If β i, j = 0, then f i is constant in the corresponding direction. The mean function for f i is m i (x) = b i − a i x i where a i and b i are regarded as nonnegative hyperparameters corresponding to mRNA degradation and basal transcription, respectively.

---

### Resolving discrepancies between chimeric and multiplicative measures of higher-order epistasis [^114eZs92]. Nature Communications (2025). High credibility.

Multivariate Bernoulli distribution

The three parametrizations we derived for the bivariate Bernoulli distribution extend to the multivariate Bernoulli distribution. Suppose that (X 1, …, X L) ∈ {0, 1} L is distributed according to a multivariate Bernoulli distribution. Then the distribution P (X) of the random variables X is uniquely specified by one of the three following parametrizations.
General parameters: These are 2 L non-negative valuessatisfyingFor example if L = 3, then p 010 = P (X 1 = 0, X 2 = 1, X 3 = 0) and p 110 = P (X 1 = 1, X 2 = 1, X 3 = 0). Note that since, only 2 L − 1 valuesare necessary to define the distribution.
Natural/canonical parameters: These are 2 L real numberssatisfyingSimilar to the general parameters p i, only 2 L − 1 values β S are necessary to uniquely define the distribution. Typically, the parameter, often called a normalizing constant or a partition function of the distribution, is left unspecified. As noted in the bivariate setting, equation (23) shows that the multivariate Bernoulli is an exponential family distribution with 2 L − 1 sufficient statistics of the form ∏ i ∈ S X i for subsets S with ∣ S ∣ > 0. Moreover, by rewriting (23) aswe observe that the natural parameters β correspond to interaction coefficients in a log-linear regression model with response variables p. For example, the natural parameter β 12 is the coefficient of the interaction term x 1 x 2.
Moments/mean parameters: These are 2 L real numberssatisfyingFor example if L = 3, then μ 13 = E [X 1 X 3] while μ 12 = E [X 1 X 2]. The mean parametersare sufficient statistics for the multivariate Bernoulli distribution, as seen in the exponential family form (23) of the multivariate Bernoulli distribution.

---

### Identifying domains of applicability of machine learning models for materials science [^113w9fho]. Nature Communications (2020). High credibility.

An illustrative example

Before describing the details of DA identification and its integration into the ML process, let us illustrate the concept and its utility via a synthetic example (see Fig. 1). We consider a simple two-dimensional representation consisting of independent features x 1 and x 2 that are each distributed according to a normal distribution with mean 0 and variance 2 (N (0, 2)) and a target property y that is a third-degree polynomial in x 1 with an additive noise component that scales exponentially in x 2 :That is, the y values are almost determined by the third-degree polynomial for low x 2 values but are almost completely random for high x 2 values. Discovering applicable domains reveals how different models cope differently with this setting even if they have a comparable average error. To show this, let us examine the error distributions obtained from three different kernelized regression models of the formwith parameter vector ν that are fitted around a training, or fitting (F), setwith three different choices for the kernel function k. We observe:
When using the linear (lin) kernel, the resulting linear model is globally incapable to trace the variation of the third-order polynomial except for a small stripe on the x 1 -axis where it can be approximated well by a linear function. Consequently, there is a very high error globally that is substantially reduced in the DA described by σ lin (x 1, x 2) ≡ −0.3 ≤ x 1 ≤ 0.3.
When using the Gaussian kernel), the resulting radial basis function (rbf) model is able to represent the target property well locally unless (a) the noise component is too large and (b) the variation of the target property is too high relative to the number of training points. The second restriction is because the rbfs have non-negligible values only within a small region around the training examples. Consequently, the discovered DA is not only restricted in x 2 -direction but also excludes high absolute x 1 -values: σ rbf ≡ −3.3 ≤ x 11 ≤ 3.1 ∧ x 2 ≤ 0.1.
In contrast, when using the non-local third-degree polynomial (poly) kernel, data sparsity does not prevent an accurate modeling of the target property along the x 1 -axis. However, this non-locality is counterproductive along the x 2 -axis where overfitting of the noise component has a global influence that results in higher prediction errors for the almost deterministic data points with low x 2 -values. This is reflected in the identified DA σ poly (x 1, x 2) ≡ −3.5 ≤ x 2 ≤ 0.1, which contains no restriction in x 1 -direction, but excludes both high and low x 2 -values. This highlights an important structural difference between the rbf and the polynomial model that is not reflected in their similar average errors.

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^114bxDzu]. Nature Communications (2025). High credibility.

Model discrepancy as a baseline for non-stationary replications

To keep the notation in the following sections more general, we use the generic x and y as independent and dependent variables. To recover expressions for our neuron example, substitute x → t,… Where possible we also use A and B as generic placeholders for a model label.

Our goal is to define a selection criterion which is robust against variations between experimental replications, but which can be computed using knowledge only of the candidate models and the observed empirical data. To do this, we make the following assumption:

EMD assumption (version 1) Candidate models represent that part of the experiment which we understand and control across replications.

More precisely, in the next section we define the empirical model discrepancy functionsuch that if modelexactly reproduces the observations, thenis identically zero. This function therefore measures the discrepancy between model predictions and actual observations. Since we expect misspecified models to have positive discrepancy, in the following we treat the discrepancyas a measure of misspecification.

Under our EMD assumption, misspecification in a model corresponds to experimental conditions we don't fully control, and which could therefore vary across replications of the experiment. Concretely this means that we can reformulate the EMD assumption as

EMD assumption (version 2) The variability of R A across replications is predicted by the model discrepancy.

We also assume that the data-generating processwithin one replication is strictly stationary with finite correlations, i.e. that all observations are identically distributed but may be correlated. For simplicity in fact we treat the samples as i.i.d. since if necessary this could be done by thinning. See referencesorfor discussions on constructing estimators from correlated time series.

Below we further assume a particular linear relationship betweenand the replication uncertainty: the functionis scaled by the aforementioned sensitivity factor c to determine the variance of the stochastic process(which we recall induces the R A -distribution for the risk of model). The parametertherefore represents a conversion from model discrepancy to epistemic uncertainty. A practitioner can use this parameter to adjust the sensitivity of the criterion to misspecification: a larger value of c will emulate more important experimental variations. Since the tail probabilities (equation (14)) we want to compute will depend on c, we will write them

---

### Origin of band gaps in graphene on hexagonal boron nitride [^116bXbAr]. Nature Communications (2015). Medium credibility.

Moire pattern scalar functions and vector fields

The scalar functions used to obtain the moiré superlattice pattern for the height and the displacement vectors from their gradients have used Φ written as a Fourier expansion in G vectors as

where C G is in general a complex number, and we retain up to three nearest G vectors for the scalar field that preserves the symmetry of triangular lattices. The parameters C 0, C 1, C 2 andare real valued constants and we defined auxiliary functions f and g in terms of the triangular lattice structure factors similar as those used in a general tight-binding model of G. The Fourier expansion coefficients within the first shell consisting ofand the first shellare often good representation of the solutions that vary smoothly in real space and respect the symmetry of the triangular superlattice. For brevity in notation, here we use (x, y) to indicate the (d x, d y). The f function

is defined in terms of the structure factors

where G 1 = 4π/3 a, where a is the real-space periodicity of the moiré superlattice and j = 1, 2. These are momentum space analogues of the real space intersublattice hopping structure factors in a honeycomb lattice. The explicit form of the functions defined along the symmetry lines x = 0 or y = 0 can be obtained from sums of

The analytical expression for the g function shell contribution reduces to a simpler form

where G 2 = 4 π, that for the symmetry lines reduces to

The vector fields such as in-plane forces, displacement vectors and stresses can be obtained as gradients of the scalar potentials given by the above forms that can preserve the symmetry of the triangular moiré superlattice. The vector field that can be obtained from the gradient of the scalar field is

---

### Predicting the direction of phenotypic difference [^115xW7gH]. Nature Communications (2025). High credibility.

Mathematical relationship between κ and P

Without loss of generality, let us assume that Δ > 0. Prediction accuracy is the probability that the true phenotypic difference is positive. Reformulating this by plugging in Eq. (1) to replace Δ, we haveNotably, Ω is a sum of identical independent random variables, and therefore, assuming that the effect size distribution Y has a finite variance, we can apply the central limit theorem and show that Ω is approximately normally distributed. Ω has a mean of zero because each of the random variables X i has a zero mean. We can now use the CDF of Ω,(where Φ (⋅) is the standard normal CDF) to explicitly compute the prediction accuracy, Note that because, we also have. Therefore, could be used alternatively to κ as a statistic describing prediction accuracy, although κ is more readily interpretable under the perspective presented in Fig. 1.

---

### Dynamical regimes of diffusion models [^115HSTMu]. Nature Communications (2024). High credibility.

Speciation time

In order to show the existence of regimes I and II, and compute the speciation time, we focus on the following protocol which consists of cloning trajectories. We consider a backward trajectory starting at time t f ≫ 1 from a point x f drawn from a random Gaussian distribution where all components are independent with mean zero and unit variance. This trajectory evolves backward in time, through the backward process until time t < t f. At this time the trajectory has reached the point y (t), at which cloning takes place. One generates for τ < t two clones, starting from the same x 1 (t) = x 2 (t) = y (t), and evolving as independent trajectories x 1 (τ) and x 2 (τ), i.e. with independent thermal noises. We compute the probability ϕ (t) that the two trajectories ending in x 1 (0) and x 2 (0) are in the same class. Defining P (x 1, 0∣ y, t) as the probability that the backward process ends in x 1, given that it was in y at time t, the joint probability of finding the trajectory in y at time t and the two clones in x 1 and x 2 at time 0 is obtained as Q (x 1, x 2, y, t) = P (x 1, 0∣ y, t) P (x 2, 0∣ y, t) P (y, t). Then ϕ (t) is the integral of Q over x 1, x 2, y with the constraint (x 1 ⋅ m)(x 2 ⋅ m) > 0. This simplifies into a one-dimensional integral (see SI Appendix):where G (y, u, v) is a Gaussian probability density function for the real variable y, with mean u and variance v, and, Γ t = Δ t + σ 2 e −2 t. The probability ϕ (t) that the two clones end up in the same cluster is a decreasing function of t, going from ϕ (0) = 1 to ϕ (∞) = 1/2. In the large d limit, the scaling variable controlling the change of ϕ iswhich can be rewritten asby using. This explicitly shows that speciation takes place at the time scale t S on a window of time of order one. As detailed in SI Appendix, this expression for t S coincides with the one obtained from the general criterion (5). We show in Fig. 2 the analytical result from (6) and direct numerical results obtained for increasingly larger dimensions. This comparison shows that our analysis is accurate already for moderately large dimensions. In the limit of infinite d, the analytical curve in Fig. 2 suddenly jumps from one to zero at t / t S = 1, corresponding to a symmetry-breaking phase transition (or a threshold phenomenon) on the time scale t S. In the numerics, following finite size scaling theory, we have defined the speciation time as the crossing point of the curves for different d, which corresponds approximatively to ϕ (t S) = 0.775 and indeed is of the orderfor d → ∞. As it happens in mean-field theories of phase transitions, the large dimensional limit allows to obtain a useful limiting process. In our case, this leads to a full characterization of the asymptotic backward dynamics. At its beginning, i.e. in regime I, the overlap with the centers of the Gaussian model, ± m ⋅ x (t), is of order. Defining, one can obtain a closed stochastic Langevin equation on q in a potential V (q, t) (see SI Appendix), where η (t) is square root of two times a Brownian motion, andAt large d, this potential is quadratic at times, and it develops a double well structure, with a very large barrier, when. The trajectories of q (t) are subjected to a force that drives them toward plus and minus infinity. The barrier between positive and negative values of q becomes so large that trajectories commit to a definite sign of q: this is how the symmetry breaking takes place dynamically at the time scale t S, in agreement with the previous cloning results. Regime II corresponds to the scaling limit q → ∞, where m ⋅ x (t) becomes of order d. In this regime, the rescaled overlap m ⋅ x (t)/ d concentrates, and its sign depends on the set of trajectories one is focusing on. Moreover, the stochastic dynamics of the x i correspond to the backward dynamics for a single Gaussian centered in ± m. This shows that the dynamics generalizes, see SI Appendix (and alsofor similar results).

---

### J-shapedness: an often missed, often miscalculated relation: the example of weight and mortality [^11548TWw]. Journal of Epidemiology and Community Health (2014). Low credibility.

We present three considerations in analysing the association between weight and mortality, as well as other relations that might be non-linear in nature. First, authors must graphically plot their independent and dependent variables in a continuous manner. Second, authors should assess the shape of that relation, and note its shape. If it is non-linear, and specifically, J-shaped or U-shaped, careful consideration should be given to using the 'best' statistical model, of which multivariate fractional polynomial regression is a reasonable choice. Authors should also refrain from truncating their data to avoid dealing with non-linear relations.

---

### Exercise intensity domains determined by heart rate at the ventilatory thresholds in patients with cardiovascular disease: new insights and comparisons to cardiovascular rehabilitation prescription recommendations [^111T8i4j]. BMJ Open Sport & Exercise Medicine (2023). High credibility.

Novel equations for patients with CVD

This study revealed a simple but crucial mathematic limitation inherent to the method for seeking a fixed percentage of peak parameters for prescribing exercise intensity. For example, if we consider 69% of HR peak as the lower limit of exercise prescription (value for HR at VT1 in our data), we will assume that the equation follows a linear equation (Y = A*X +B), in which Y = HR at VT1, A = 0.69, X = HR peak and B = 0 (HR at VT1 = 0.69 * HR peak +0). Thus, the mathematical concept suggests that the relation between HR at VT1 and HR peak (or Y/X) is constant when plotted against HR peak (or X). In other words, when using the %HR peak method, we assume that intercept (B) equals zero without considering any correction for the data dispersion.

However, when we analyse the plotted curve of our data (figure 3A), we observe that the intercept (B) is 30.62, strongly different from zero. Moreover, the coefficient (A) is 0.4707, which is also very different from the value considered for moderate-intensity exercise in the guidelines (Brazilian: 0.70; European: 0.55; American: 0.64). We also noticed a significant association between higher values of %HR peak at VT1 and lower HRpeak values (figure 3B). Regarding VT2, our intercept (B) is much more like previously assumed values (4.397 vs 0), and the same is true for the coefficient (A) (0.8544 vs Brazilian: 0.85; European: 0.7; American: 0.76).

Thus, applying this simple math problem to the real world, we can see that the widely used method of prescribing exercise intensity according to percentages of HR peak has an important limitation. The greater observed error can be related to the method itself and not the percentage values, as we cannot assume that the intercept of the equation is zero, especially concerning VT1, which is considered the lower limit for a moderate-intensity exercise prescription.

---

### Assessing and improving reliability of neighbor embedding methods: a map-continuity perspective [^112qmsDq]. Nature Communications (2025). High credibility.

Pre-screening of points

To further speed up the computation, we use the heuristic that embedding points receiving high perturbation scores are often found at the peripheries of clusters. This heuristic motivates us to calculate the perturbation scores only for the peripheral points in the embedding space, as these points are most likely to be unreliable. We find that applying this pre-screening step tends to find most of the unreliable points (Supplementary Fig. 7) with significantly increased computational speed.

We use the function dbscan in the R package dbscan (version 1.2–0) to identify embeddings on the periphery of clusters.

Singularity score

Given an input data matrixand its embedding matrix, we describe our derivation of singularity scores. If we add an infinitesimal perturbation ϵ e to x i, then by the Taylor expansion of the partial LOO-map f i, the resulting change in the i -th embedding point is expressed aswhere H i denotes the Hessian matrix of the partial LOO loss L i (y; x i) with respect to y at y = y i. Notably, when ϵ = 0 (no perturbation), we have f i (x i) = y i. Denote the total loss asThen, H i can be written asi.e. H i is also equal to the Hessian matrix of the total losswith respect to the i -th variable taking value at y i.

Importantly, H i is independent of the perturbation direction e. The more singular H i is, the more sensitive the embedding point of x i becomes to infinitesimal perturbations. Thus, we define the singularity score of the i -th data point as the inverse of the smallest eigenvalue of the Hessian matrix of, that is. Supplementary Methods 1 provides detailed derivations of Eq. (8), and Supplementary Methods 2 provides expressions of singularity scores for t-SNE, UMAP, and LargeVis.

Scoring metrics and statistical tests

---

### Best (but oft-forgotten) practices: checking assumptions concerning regression residuals [^117D9BKv]. The American Journal of Clinical Nutrition (2015). Low credibility.

The residuals of a least squares regression model are defined as the observations minus the modeled values. For least squares regression to produce valid CIs and P values, the residuals must be independent, be normally distributed, and have a constant variance. If these assumptions are not satisfied, estimates can be biased and power can be reduced. However, there are ways to assess these assumptions and steps one can take if the assumptions are violated. Here, we discuss both assessment and appropriate responses to violation of assumptions.

---

### Using an ordinary differential equation model to separate rest and task signals in fMRI [^116dvZeu]. Nature Communications (2025). High credibility.

Thus the library function Θ(X) can be expressed as the following, and is computed for every observation in time of X (t). Based on the derivative and library terms, we can determine the coefficients Ξ from Eq.(4). Thus, Eq.(4) reduces to a regression. The problem can be reduced to the form of A X = B, where X represents the coefficients Ξ that we are solving for, and A is represented by the library polynomial function Θ(X) and B the observed derivative. The estimated coefficients Ξ represent a Taylor Polynomial between each edge and can be thought of as an approximation of network connectivity using a polynomial basis. We expect that the edges determined by the algorithm are related to the physical wiring between brain regions and, therefore, should be related to other measures of network connectivity, such as through tractography using DTI. Here, we refer to the collection of edge polynomials as Effective Connectivity (E C). Although this term has been used in a slightly different context in the past, here it helps to clarify our methodology. The E C is believed to be sparse in nature as there are few physical connections between the brain regions, as they are restricted by resource constraints, in this case, the amount of volume available inside the skull. This has been shown in simpler systems such as the C. Elegans, but we believe this to be a general property of all nervous systems.

The regression problem can be solved using convex optimization tools that optimize the trade-off between the penalties on the reconstruction and the number of coefficients used, which is controlled by the sparsity parameter. Again in the implementation of SINDy, there are many different optimizer choices, but we utilized the L 2 regression, as L 1 yielded very sparse solutions that did not fit the dynamics well. The residuals represent activity that is not related to the network ODE, and is assumed to be unstructured noise η Z from Eq. (2) and is minimized by the SINDy algorithm. Note that technically, L 2 regression is not sparse, and instead, we employ ridge regression to determine the coefficients. While this is a misnomer when using 'Sparse Identification of Nonlinear Dynamics', the SINDy package offers much more versatile options for system identification than traditionally sparse-only solutions.

---

### Combinatorial code governing cellular responses to complex stimuli [^117CCEA9]. Nature Communications (2015). Medium credibility.

Methods

Identification of the interaction profiles

The number of ways to rank the numbers e 0, e X, e Y and e X+Y was computed using the recurrence relation A n k = A n− 1 k +k A n− 1 k− 1 which, for n = 4 and summing up for k = 1, 2, 3 and 4 gives 75 possible rankings. Each ranking can be seen as a set of outcomes of the following qualitative pairwise comparisons:
e X versus e 0
e Y versus e 0
e X+Y versus e 0
e Y versus e X
e X+Y versus e X
e X+Y versus e Y

In our framework, the outcome of a qualitative comparison can take on the following three values: 0 (equal numbers), 1 (first larger than the second) and −1 (first smaller than the second). Each of 75 rankings was coded uniquely as a vector of six components describing the qualitative outcome of the comparisons 1–6 as listed above. For example, the vector (0,0,1,0,1,1) corresponds to:

e X = e 0, e Y = e, e X+Y > e 0, e Y = e X, e X+Y > e X, e X+Y > e Y

To identify which rankings were compatible with positive or negative interactions, we considered the equations 1–6 together with the inequalities that define positive and negative interactions:
Δ e X+Y > Δ e X + Δ e Y (positive interaction)
Δ e X+Y < Δ e X + Δ e Y (negative interaction)

which can be written as

(7a) e X+Y − e X − e Y + e 0 > 0

(8a) e X+Y − e X − e Y + e 0 < 0

If a ranking is consistent with a positive (or negative) interaction, the inequality constraints encoded in the corresponding vector can be solved simultaneously with 7a (or 8a). To verify this, we developed a constraint satisfaction that attempts to solve the six constraints of each ranking together with the inequality 7a (or 7b). The solution is searched numerically with a MATLAB linear solver. The variables e 0, e X, e Y and e X+Y were constrained to the interval 2–16, taken as an approximate of the range of expression values from Affymetrix chips in log2 scale. The method is implemented in the MATLAB code and is available upon request.

---

### General statistical model shows that macroevolutionary patterns and processes are consistent with darwinian gradualism [^114PYj86]. Nature Communications (2022). High credibility.

Methods

Model

Write the value of some trait Y after an amount of time dt as the outcome of multiplicative diffusion from its starting position at time t and a change term incorporating possible directional and evolvability effects:where β is a directional change (per unit time), random changes ε are time-independent and homogeneous such that, and υ transforms the variance of ε to υσ 2. These parameters correspond to the directional and evolvability changes as described in Fig. 1. By definition, directional changes occur along phylogenetic branches, introducing a mean offset to all 'downstream' species, but with no change to the Brownian variance. Evolvability effects occur at phylogenetic nodes, altering the Brownian variance σ 2 of the descendant clade.

The model of Eq. (1) captures the commonly observed dependency in morphological data between a trait's value, Y, and its variance. Writing the model in logarithmic form yieldsand Eq. (2) defines a linear model with constant and normally distributed errors ε, independent of the value of Y. For traits with variance independent of Y on the natural (un-transformed) scale, the model of Eq. (2) is still applicable, but Y is not logarithmically transformed.

Positive values of β denote increases in the value of the trait, negative values denote reductions. When the trait's change along a branch is compatible with that which is likely under neutral drift, β = 0, its default value. The default value of υ is 1; values of υ > 1 signal an increase in the potential for evolutionary exploration, those < 1 indicate reduced exploration of the trait-space. The existence and magnitude of directional and evolvability parameters that differ from their default values will vary throughout the tree according to the macroevolutionary signals retained in the species.

The model of Eq. (2), when applied to the branches leading from the root of the phylogenetic tree to the tips, yields a description of each species as the sum of the common ancestral state at the root, α, any directional changes that have occurred along the branches leading to that species, and a normally distributed error. Writing the i th species' value as Y i, and treating Y i as having been logarithmically transformed where necessary, thenwhere the β ij are directional changes occurring in the branches of length t j (replacing the dt of Eq. (2)) leading to species i, and the error term has variance.

---

### An additive gaussian process regression model for interpretable non-parametric analysis of longitudinal data [^113CyFDc]. Nature Communications (2019). High credibility.

Results

Additive GP

Linear models and their mixed effect variants have become a standard tool for longitudinal data analysis. However, a number of challenges still persist in longitudinal analysis, e.g. when data contains nonlinear and ns effects.

GP are a flexible class of models that have become popular in machine learning and statistics. Realizations from a GP correspond to random functions and, consequently, GPs naturally provide a prior for an unknown regression function that is to be estimated from data. Thus, GPs differ from standard regression models in that they define priors for entire nonlinear functions, instead of their parameters. While nonlinear effects can be incorporated into standard linear models by extending the basis functions e.g. with higher order polynomials, GPs can automatically detect any nonlinear as well as ns effects without the need of explicitly defining basis functions. By definition, the prior probability density of GP function values f (X) = (f (x 1), f (x 2), ⋯, f (x N)) T for any finite number of fixed input covariates X = (x 1, x 2. x N) (where) is defined to have a joint multivariate Gaussian distributionwhere elements of the N -by- N covariance matrix are defined by the GP kernel function [K X, X (θ)] i, j = k (x i, x j | θ) with parameters θ. Mean in Eq. (1) can in general depend on X but zero mean is often assumed in practice. Covariance (also called the kernel function) of the normal distribution defines the smoothness of the function f, i.e. how fast the regression function can vary. Intuitively speaking, although GP is defined such that any finite-dimensional marginal has a Gaussian distribution, GP regression is a non-parametric method in the sense that the regression function f has no explicit parametric form. More formally, GP contains countably infinite many parameters that define the regression function, which are the function values f at all possible inputs. For a comprehensive introduction to GPs we refer the reader to the bookand the Methods section.

---

### Topological regression as an interpretable and efficient tool for quantitative structure-activity relationship modeling [^111KLPRX]. Nature Communications (2024). High credibility.

Neighborhood training model

Customarily, the weights are expressed as a deterministic function of the distances in the predictor space. In standard KNN regression, we assume that distance in the predictor space is proportional to the distance in the response space. In metric learning, a transformation of the predictor space is learned such that there is an approximate isometry between the transformed predictor space and the response space. In TR, we instead write a formal statistical model to connectwith the squared Euclidean distances in the predictor space in the following fashion:

We define the weightsand since we have I * and I to be disjoint and the responses could be assumed to be absolutely continuous, we can definewith the entries in, i.e.being real quantities. Define the squared Euclidean distance matrix in the predictor space as

We define a simple multivariate linear regression model connectingwith D X. Consider the m th row of. Observe that, this row consists of the weights used to express the m th response in I using all the responses in I *. We envision this row to be a set of repeated measurements taken on the m th point in I from the vantage points in I *. Thus, denoting the K elements in the m th row ofby, the corresponding row of predictors in D X by, and the matrix of regression coefficients bywe arrive at the following regression modelwith. Now assuming mutual independence across the N rows ofand since N > K (by construction), we can obtain the MLEs of B and Σ. Letanddenote their respective estimates. Then, for a new query point, we can computeand, using, obtain the predictions. However, observe that (1) requires (W 1, q u e r y, W 2, q u e r y, ⋯, W K, q u e r y) to generate a prediction for the query point, and simply exponentiating the output, of (6) will yield a biased estimate of W becausedue to Jensen's inequality. Therefore, we use the properties of the multivariate log-normal distribution to improve the estimate of W in the following way:

---

### Cox process representation and inference for stochastic reaction-diffusion processes [^114AorBc]. Nature Communications (2016). Medium credibility.

The reaction–diffusion master equation

Consider a system as in equation (13), but in an M -dimensional volume discretized into L cubic compartments of edge length h and volume h M. Denote as n = (n 1 1, …, n N 1, …, n 1 L, …, n N L) the state of the system, whereis the copy number of species X i in the l th compartment. Under well-mixed and dilute conditions in each compartment, the reaction dynamics in each compartment is governed by a corresponding chemical master equation as in equation (14). If we model diffusion of species X i between neighbouring compartments by linear reactions with rate constant d i = D i / h 2, where D i is the microscopic diffusion constant of species X i, the time evolution of the (single time) marginal probability distribution of the system obeys the RDME:

where f r (n l) is the propensity function of the r th reaction evaluated at the state vectorof the l th compartment, is a vector of length N × L with the entry corresponding to species X i in the l th compartment equal to 1 and all other entries zero, andis a vector of length N × L with the entries corresponding to the l th compartment equal to the r th row of the stoichiometric matrix S and zero otherwise.

Real-valued Poisson representation in space

We next apply the PR to the RDME in equations (23) and (24) after applying the mean-field approximations defined in the Results section to bimolecular reactions and reactions with two non-identical product molecules, and subsequently take the continuum limit. Consider first the diffusion term in equation (23). Since different species do not interact with each other if there are no chemical reactions happening, we can consider a system containing only a single species, say species X 1, for which equation (23) reduces to

where n = (n 1, …, n L), n m is the number of X 1 particles in the m th compartment, δ m is a vector with a one in the m th entry and zero otherwise, d is the diffusion constant of species X 1 and the sum over m runs over all neighbouring compartmentsof the l th compartment. For this system, the PR is real and deterministic, and we use the PR without any approximations. The corresponding Langevin equation reads

---

### The simplicity of protein sequence-function relationships [^11762ZNE]. Nature Communications (2024). High credibility.

This way of dissecting the sequence-function relationship gives RFA several desirable properties. First, RFA offers a maximally efficient description of the global sequence-function relationship. An RFA model truncated at any epistatic order captures the maximum amount of phenotypic variance that can be captured by any linear model of the same order (Supplementary Section 2.6). Consider all zero-order models, which predict the phenotype of every sequence by a single number. The RFA zero-order term is the mean phenotype of all sequences and is therefore the best predictor in the sense of minimizing the total squared error. The first-order RFA model predicts each variant's phenotype as the sum of the first-order effects of its constituent states and the global mean. This predictor again achieves the minimum total squared error among all possible first-order models and therefore explains the maximum possible amount of phenotypic variance. This property continues as the model order increases. To the greatest extent possible, RFA explains the sequence-function relationship by low-order causal factors, which are relatively few in number and apply most broadly, rather than by high-order factors, which at the limit explain every single data point as the result of a unique set of idiosyncratic causes.

Second, RFA is robust to measurement noise, because its terms are defined using average phenotypes over sets of genotypes. To illustrate this property, we simulated a genetic architecture in which phenotypic measurements are determined by up to fourth-order effects plus a moderate amount of measurement noise (Fig. 1c). The RFA terms computed from the simulated measurements accurately estimate the true effects; errors in the estimated terms are smaller than the noise in the individual phenotypic measurements, even for the highest-order terms. The fraction of phenotypic variance explained by the computed terms is also accurate.

Third, when data are partially sampled, RFA models can be accurately estimated by least-squares regression. When 50% of genotypes are missing from the simulated example, the estimated terms of the model and the variance partition are highly accurate (Fig. 1c, Supplementary Fig. 1). RFA can be accurately estimated by regression because its true terms minimize the sum of squared error across all genotypes, so least-squares estimates converge on the true values as long as noise and sampling are unbiased. Truncated models can be estimated accurately because the patterns of variation produced by the unmodeled higher-order interactions appear as noise around lower-order predictions, so they cannot be absorbed by the model (Supplementary Section 2.9).

---

### A novel oppositional binary crow search algorithm with optimal machine learning based postpartum hemorrhage prediction model [^117JzZxt]. BMC Pregnancy and Childbirth (2022). Medium credibility.

Whereasrepresent the opposite number and x ∈ R denotes a real number determined on range of x ∈[a, b]. While a = 0 and b = 1 Eq. (3) becomes

While there is a point P (x 1, x 2,… x n) in n dimension coordinate and x 1, x 2, …, x n ∈ R later, the opposite pointis determined as its coordinates:

In such cases, have 2 values, x represent initial arbitrary value in [a, b] anddenotes the opposite values of x. They calculate f (x)&in all the iterations of OBCSA, later, employ on the evaluation function g ifselect x or else selectConsequently, the f l would be in range: f l ∈[f l min, f l max]. The opposite numbercan be determined by:

Later, evaluate the fitness for the first f l value and the fitness forin all the iterations. When, they select f l, or elsewould be selected. The stages of presented method can be given in the following.

Step1: The count of crows is n c = 25, f l min = 0.1, f l max = 1.8, A P = 0.3, and the maximal number of iterations is t max = 100.

Step2: The position that represent the features are made by U (0, 1).

Step3: The fitness function (FF) can be determined by

Whereas C represent the classification performance, W represent the weighted factors in the range of zero and one, F all represent the overall amount of features and F sub signifies the length of elected feature.

Step4: The position of the crows are upgraded as Eq. (2)

Step5: Steps 3 & 4 are repetitive till a t max is attained.

---

### Quantifying unobserved protein-coding variants in human populations provides a roadmap for large-scale sequencing projects [^112aVx3J]. Nature Communications (2016). Medium credibility.

Methods

UnseenEst algorithm

UnseenEst uses the empirical SFS of a given class of variants in a cohort to estimate its frequency distribution in the population. The inputs into the algorithm are the number of alleles in the sample cohort, k, and the SFS, which is a set of counts, { F i }, where F i is the number of variants that are observed in exactly i out of the k alleles. A key challenge for the method is to accurately estimate the frequency distribution of variants that have empirical count of 0 (that is, they are not observed) in the cohort but are likely to have some small, non-zero frequency in the population.

More concretely, let X denote a discrete set of frequencies x in [0, 1] and let h (x) denote the fraction of all the variants with frequency x. UnseenEst estimates h (x) by finding the set of h (x) that jointly minimizes the value of the expression:

where bin(x, k, i) is the binomial probability of observing i heads in k independent flips of a coin with bias x. The intuition for minimizing this objective is as follows:is the number of variants that we expect to find in i out of k alleles in the cohort and F i is the empirical number of variants observed in i alleles. If h (x) is the true frequency distribution, then the expectation should be close to the empirical, which is why we want to find h (x) that minimizes the objective above. Given an estimate of the frequency distribution h (x), the expected number of unique variants in N alleles can be calculated by the formula.

The standard 3rd order jackknife estimator does not estimate the frequency distribution h (x). Instead, it directly estimates that the number of unique variants in N alleles is g 1 (N, k) F 1 +g 2 (N, k) F 2 +g 3 (N, k) F 3, where F 1, F 2, F 3 are the number of variants observed once, twice and three times in k alleles, and g 1, g 2, g 3 are specific functions of N and k derived from self-consistency requirements. Note that the confidence intervals of the jackknife in Supplementary Fig. 3 are very narrow because there is relatively little variation in the counts F 1, F 2, F 3, when the sample size is large.

---

### Restricting mutualistic partners to enforce trade reliance [^116EYwJr]. Nature Communications (2016). Medium credibility.

The focal fungus also acquires an amount P f of phosphorus and retains a fraction 1− q f of this, so that it has a total quantity of phosphorus P tf = P f (1− q f). Substituting the expressions for C tf and P tf, we have derived above into the fungus' fitness function given in the main text yields

We use equations (5a) and (5b) to calculate evolutionary maxima. Specifically, we calculate ESSs, strategies where deviations from population mean strategies leads to decreased fitness (mathematically, we calculate:andfor plants, andfor mycorrhizal fungi). We find that the fitness equilibrium is always unique given parameter values and that the second derivatives of fitness with respect to individual strategy are always negative (andfor plants, andfor mycorrhizal fungi). Therefore, the fitness equilibrium is a unique ESS. In this analysis, we do not consider whether the ESS is also convergence stable. The ESSs for plants depend on the strategies used by fungi, and the ESSs for fungi depend on the strategies used by plants. Hence, we need to find the ESSs for plants given fungal strategy, and the ESSs for fungi given plant strategy. Then, we can solve these four resulting expressions to find co-evolved ESSs in terms of model parameters only (x *, q p *, q f *, r*). We first calculate the equilibria of fitness for allocation to direct carbon uptake and trade in a focal plant, ∂w p / ∂x p = ∂w p / ∂q p = 0. At an ESS, any individual receives population mean fitness to first order, so we take the focal plant's strategy to be the population average. Substituting, we find that at equilibrium

---

### Representation of exposures in regression analysis and interpretation of regression coefficients: basic concepts and pitfalls [^114RikXr]. Nephrology, Dialysis, Transplantation (2014). Low credibility.

Regression models are being used to quantify the effect of an exposure on an outcome, while adjusting for potential confounders. While the type of regression model to be used is determined by the nature of the outcome variable, e.g. linear regression has to be applied for continuous outcome variables, all regression models can handle any kind of exposure variables. However, some fundamentals of representation of the exposure in a regression model and also some potential pitfalls have to be kept in mind in order to obtain meaningful interpretation of results. The objective of this educational paper was to illustrate these fundamentals and pitfalls, using various multiple regression models applied to data from a hypothetical cohort of 3000 patients with chronic kidney disease. In particular, we illustrate how to represent different types of exposure variables (binary, categorical with two or more categories and continuous), and how to interpret the regression coefficients in linear, logistic and Cox models. We also discuss the linearity assumption in these models, and show how wrongly assuming linearity may produce biased results and how flexible modelling using spline functions may provide better estimates.

---

### Fractional response analysis reveals logarithmic cytokine responses in cellular populations [^111X3kR9]. Nature Communications (2021). High credibility.

Formally, FRC scales as the log of the dosewhich given incremental approximation, ∆log(x) = log(x + ∆x) − log(x) ≈ ∆x/x, implies fold-change sensitivity in the populationwhich in the studied system universally describes dose–responses in populations across different cell types.

The FRA, therefore, condenses the description of the complex multivariate responses into a simple quantitative formula. Furthermore, FRA uncovered that the number of programmed response distributions, i.e. maximal value of FRC, and the fractional cell-to-cell heterogeneity structure are very similar for all cell types. This similarity indicates that the immune system may precisely control responses of fractions of cells rather than responses of individual cells. In multicellular organisms, a fraction of cells with a given response level is a biologically essential response variable. For example, the outcome of a viral infection in tissue depends on the number of NK cells with given response levels and induced cytotoxic activity. Our analysis revealed that in the studied system the fraction of cells that have responses in a specific range is not only tightly controlled in the population of a given cell type but is controlled in the same way across different cell types, as opposed to responses of individual cells that are largely heterogeneous within one cell type and across cell types. The role for controlling the fractions of cells with specific responses can, in principle, be tested by perturbing cell-to-cell heterogeneity through genetic manipulation and observing the phenotypic effects on the performance of the immune system.

---

### Vacuum-field-induced THz transport gap in a carbon nanotube quantum dot [^111rccYU]. Nature Communications (2021). High credibility.

For temperatures T much smaller than the cavity mode frequency f c a v, the cavity correlatorscan be calculated for a cavity in its fundamental state asBy substituting Eqs. (18)–(21) into Eq. (11) we obtain the expressionAbove, the lesser and greater Green's functions of the dot can be calculated by using the Keldysh description of quantum dot circuits as:andwith Γ = Γ L + Γ R. We assume that the levels(or ε d) are separated by a level spacing Δ. We have used equations (22), (23) and (24) to plot Fig. 3 d, e with Γ L (R) / f c a v = 0.1, x = 0.7, Δ/ f c a v = 0.6 and T / f c a v = 0.05. For Fig. 3 e we have used the same parameters and ε d / f c a v = 0.4. Note that for, one recovers the standard expression of the current through an independent dot, i.e. Due to Eq. (22), forsufficiently large, the dot conductance shows steps which are due to photo-assisted tunneling for e V = ± h f c a v, ± 2 h f c a v, ± 3 h f c a v. and a current suppressed quadratically inforwith small "subgap" resonances. When Δ ≫ V, k B T, Γ L (R), and when a single dot level d 0 is close to the reservoir Fermi energies for V = 0, the ratio between the subgap conductance G (V = 0) and the conductance G 1 for e V = h f c a v + 0 + can be expressed in the limit Γ L = Γ R = Γ/2 aswithFrom equation (22), for T = 0, x = 1/2 and Δ ≫ Γ, we can derive analytically the subgap conductance peak height to beand a gap edge at V ≈ ± f c a v with a magnitude. This allows us to get a ratio R between the latter and the former of. From Fig. 3 b, we estimate the experimental R to be 5 and therefore. The error bars oncorrespond to the estimated systematic errors in determining the parameters of the quantum dot.

---

### Multilayer block copolymer meshes by orthogonal self-assembly [^115La9sT]. Nature Communications (2016). Medium credibility.

The single chain partition function can be evaluated as follows

where q (r, s) is a restricted chain partition function (propagator) that could be calculated by solving a modified diffusion equation

subjected to the initial condition q (r, 0) = 1. As the two ends of the polymer are distinct, a complementary partition function q * (r, s) is defined similarly and satisfies the same modified diffusion equation with an initial condition q *(r, 1) = 1. Here, we utilized s as a chain contour variable in units of N. All lengths were expressed in units of the unperturbed radius-of-gyration of a polymer, R g = (Nb 2 /6) 1/2, where b is the statistical segment length. The solution to the modified diffusion equation was conducted by implementing the pseudo-spectral method. An iterative relaxation of the fields towards their saddle-point values was implemented following ref.

By evaluating q (r, s) and its complementary function, the segment volume fractions can be determined as follows

Here we considered a 2D rectangular domain of size (L x × L y = 24 × 14R g 2). The SCFT equations were solved on a grid that had an equal lattice spacing of 0.2R g in the x and y directions. The volume fraction of the polymer f A was set to 0.5, whereas the degree of incompatibility χN was 17. This resulted in a structure that had the form of stripes (2D projection of in-plane cylinders or out-of-plane lamellae). The sides of the computational cell had periodic boundary conditions simulating an unconfined system in the x direction. Confinement was applied to the top and bottom surfaces where both have equal affinity to block A.

---

### Different population dynamics in the supplementary motor area and motor cortex during reaching [^1138qCPj]. Nature Communications (2018). Medium credibility.

We consider the data matrix, A, where each column contains the firing rate of one neuron across times and conditions. We estimate the latent variables as a projection, X = AW T, where each column of X contains the values of one latent variable across times and conditions. The rows of the orthogonal matrix W are the "neural dimensions", found by minimizing a cost function f (W). Because the above hypothesis contains three components, we employ a tripartite cost function:The first term, f rec (W), is identical to the PCA cost function, and is small if the latent variables capture considerable variance (i.e. if the firing rates in A are accurately reconstructed by A rec = XW). The second two terms relate to the hypothesis that there exists condition-invariant structure in some dimensions and dynamical structure in other, orthogonal dimensions. If so, an appropriate W will result in latent variables X = [X invar, X dyn]. X invar are latent variables that vary with time but not condition. X dyn are latent variables whose evolution obeys linear dynamics. f invar (W) is small if X invar varies strongly with time but not condition. f dyn (W) is small if the fitis good for some choice of D. Equations for f rec (W), f invar (W), and f dyn (W) are provided in the Methods. By minimizing f (W) we ask the question: does there exist a projection of the population response that captures considerable variance and has the hypothesized condition-invariant and dynamical structure?

---

### Elacestrant (Orserdu) [^111yBi3v]. FDA (2024). Medium credibility.

The dosage of elacestrant PO for treatment of breast cancer in postmenopausal female adults with ESR1 mutation (advanced or metastatic, ER-positive, HER2-negative, progression after ≥ 1 line of endocrine therapy) is 345 mg PO daily until disease progression or unacceptable toxicity

---

### Fractional response analysis reveals logarithmic cytokine responses in cellular populations [^112b3Arv]. Nature Communications (2021). High credibility.

Fractional response curves

Outcomes of physiological processes, e.g. of inflammation or stress responses, depend on the number of cells with specific responses, rather than on their mean or median, which constitutes the fraction of cells with a given response as a biologically relevant variable. We proposed, therefore, to quantify dose–responses in terms of cellular fractions and show here how this can be achieved for multivariate data.

We first introduced the fractional response curve (FRC) that quantifies fractions of cells that exhibit different responses to a change in dose, or in fact any other experimental condition. For each subsequent dose, the increase of FRC reflects the fraction of cells that exhibit responses different from lower doses. Adding cumulatively distinct fractions results in counting the number of distinct response distributions.

For an illustration of FRC, in addition to the formal definition derived in Methods, we considered a simple hypothetical example involving one signaling effector and three doses, although the approach extends to a general multivariate scenario. Response distributions to three doses, x 1, x 2, x 3, which can be interpreted as control, intermediate, and high dose, are shown in Fig. 2a. When dose 1 was considered alone, fractions of cells with all possible responses sum up to 1 (Fig. 2b). Therefore, we defined the value of the FRC for dose 1 to be 1, and write r(x 1) = 1. We then asked what fraction of the cellular population exhibits different responses after the change from dose 1 to dose 2. The fraction of cells exhibiting different responses is equivalent to the overall increase in the frequency of responses (Fig. 2c, green region). The overall fractional increase, denoted as ∆ r, is calculated as the area of the green region, and ∆ r = 0.31, represents the 31% of the cellular population exhibiting different responses due to dose increase. Therefore, we defined the value of the FRC for dose 2 to be the sum of the previous value and the fractional increment, r (x 2) = r (x 1) + ∆ r = 1.31. When dose 3 was considered, the fraction of cells that exhibited different responses is again equivalent to the overall increase in the frequency of different responses, now compared with the two lower doses (Fig. 2d). As before, the overall increase, ∆ r, is equivalent to the area of the yellow region (Fig. 2d), with ∆r = 0.74, representing 74% of cells stimulated with dose 3 exhibiting responses different to populations stimulated with lower doses. Again, the value of the FRC for dose 3 was defined as the sum of the previous value and the fractional increment, r(x 3) = r(x 2) + ∆r = 2.05. Changes in the FRC show what fraction of cells exhibit different responses owing to the dose increase. Adding subsequent fractional increments, ∆r, leads to the value of FRC expressed in terms of the cumulative fraction of cells that exhibit different responses due to dose change.

---

### Recovery after stroke: not so proportional after all? [^114HEHc1]. Brain (2019). Medium credibility.

First, these studies should report r(X,Δ), r(X, Y), and σ Y /σ X, for those patients deemed to recover proportionally. Despite our concerns about r(X,Δ), we do learn something when r(X, Y) is strong, but r(X,Δ) is weak, as inresults discussed above, which appeared to be better explained by constant recovery than by proportional recovery.

Second, future studies should consider explicitly testing the hypothesis that recovery depends on baseline scores. These tests sensibly acknowledge that the null hypothesis is rarely r(X,Δ) ≈ 0 in these analyses. However, they do not address the proper measurement and interpretation of effect sizes, which is our primary concern here; somewhat paradoxically, this means that they may be less useful in larger samples than in smaller samples.

Those hypothesis tests will also all be confounded by ceiling effects. We recommend that future studies should measure the impact of such effects, perhaps by reporting the shapes of the distributions of X and Y (greater asymmetry implying more prominent ceiling effects). Future studies should also attempt to minimize ceiling effects. One approach might be to remove patients whose outcomes are at ceiling: though certainly inefficient, this does at least remove the spurious r(X,Δ) in our simulations of constant recovery (see above). However, it may be difficult to determine which patients to remove in practice; the Fugl-Meyer scale, for example, imposes item-level ceiling effects, which could distort σ Y /σ X well below the maximum score. A better, though also more complex alternative, may be to use assessment tools expressly designed to minimize ceiling effects, or to add such tools to those currently in use.

More generally, we may need to replace correlations with alternative methods, which can provide less ambiguous evidence for the proportional recovery rule. One principled alternative might use Bayesian model comparison to adjudicate between different forward or generative models of the data at hand: i.e. using the empirical data to quantify evidence for or against competing hypotheses about the nature of recovery, which may or may not be conserved across patients. We hope that this paper will encourage work to develop such methods, delivering better evidence for (or against) the proportional recovery rule.

---

### Fractional response analysis reveals logarithmic cytokine responses in cellular populations [^111LPMUV]. Nature Communications (2021). High credibility.

Calculation of typical fractions

The fractions of cells stimulated with dose i that have responses typical to dose j, v ij, can be easily calculated from data regardless of the number of doses and the type of experimental measurements. We have thatCalculation of typical fractions, v ij, with the above formula requires the possibility to examine the condition P (y | x j) > P (y | x k) for any experimentally observed response, y. The distributions P (y | x j) can be reconstructed from data using a variety of probability density estimators. The use of the available estimators, however, might be problematic for multivariate responses. We, therefore, propose a more convenient strategy. We replace the condition P (y | x j) > P (y | x k) with an equivalent condition that is computationally much simpler to evaluate. Precisely, we propose to use the Bayes formulaIf we set the equiprobable prior distribution, i.e. P (x j) = 1 /m, we have that P (y | x j) is proportional to P (x j | y) and the condition P (y | x j) > P (y | x k) is equivalent toThe above strategy allows avoiding estimation of the response distributions, P (y | x j), from data. For continuous and multivariate variable y the estimation of P (x j | y) is generally simpler than estimation of P (y | x j). Precisely, an estimatorof the distribution P (x j | y) can be built using a variety of Bayesian statistical learning methods. For simplicity and efficiency, here we propose to use logistic regression, which is known to work well in a range of applications. In principle, however, other classifiers could also be considered. The logistic regression estimators of P (x j | Y = y) arise from a simplifying assumption that log-ratio of probabilities, P (x j | Y = y) and P (x m | Y = y) is linear. Precisely, The above formulation allows fitting the logistic regression equations to experimental data, i.e. finding values of the parameters, α j and β j that best represent the data. The fitted logistic regression model allows assigning cellular responses to typical doses based on conditions given by Eq. 8. Formally, the fractions v ij defined by Eq. 6 are calculated aswhere n i is the number of cells measured for the dose x i, y l i denotes response of the l- th cell, andis equal 1 iffor anyand 0 otherwise.

---

### An additive gaussian process regression model for interpretable non-parametric analysis of longitudinal data [^1116tZh3]. Nature Communications (2019). High credibility.

Methods

Notation

We model target variables (gene/protein/bacteria/etc;) one at a time. Let us assume that there are P individuals and there are n i time-series measurements from the i th individual. The total number of data points is thus. We denote the target variable by a column vector y = (y 1, y 2. y N) T and the covariates by X = (x 1, x 2. x N), where x i = (x i 1, x i 2. x id) T is a d -dimensional column vector and d is the number of covariates. We denote the domain of the j th variable byand the joint domain of all covariates is. In general, we use a bold font letter to denote a vector, an uppercase letter to denote a matrix, and a lowercase letter to denote a scale value.

Gaussian process

GP can be seen as a distribution of nonlinear functions. For inputs, GP is defined aswhere μ (x) is the mean and k (x, x ′) is a positive-semidefinite kernel function that defines the covariance between any two realizations of f (x) and f (x ′) bywhich is called kernel for short. The mean is often assumed to be zero, i.e. and the kernel has parameters θ, i.e. k (x, x ′| θ). For any finite collection of inputs X = (x 1, x 2. x N), the function values f (X) = (f (x 1), f (x 2). f (x N)) T have joint multivariate Gaussian distributionwhere elements of the N -by- N covariance matrix are defined by the kernel [K X, X (θ)] i, j = k (x i, x j | θ).

We use the following hierarchical GP modelwhere π (ϕ) defines a prior for the kernel parameters (including), is the noise variance, and I is the N -by- N identity matrix. For a Gaussian noise model, we can marginalise f analytically

---

### Identification of signal bias in the variable flip angle method by linear display of the algebraic ernst equation [^115fFyKR]. Magnetic Resonance in Medicine (2011). Low credibility.

In contrast to Fig. 2a, the fitted lines at different TR could hardly be discerned when using the common linearization (Fig. 2c), because the slopes were close to one at short TR. The error bars were highly correlated. In particular for small flip angles (upper right), the error bars were almost parallel to the regression line, so the linear fit yielded a small error (about 10 −4) for the slope, exp(–TR/ T 1). This yielded consistent T 1 estimates, albeit with artificially low standard errors. In contrast to above, the standard errors increased with TR from 0.0002 ms to 0.0067 ms. The origin region of Fig. 2c is zoomed in Fig. 2D. The variation of the y -intercepts, A (1–exp(–TR/ T 1), reflects the influence of TR. Again, the errors of the fitted A were smaller than in Fig. 2a by more than a factor of 1000. Similar errors in T 1 and A were observed for both methods after application of weights to account for the covariance of x - and y -errors.

---

### Dose response to upper extremity stroke rehabilitation varies by individual: early indicators of treatment response [^1122spVC]. Stroke (2024). Medium credibility.

Fitting and Characterizing Dose-Response Trajectories

A negative exponential function (Eq. 1) fits the data much better than a linear function and accounts for the eventual asymptotic rehabilitation response. The dose-response trajectory was, thus, quantified using a negative exponential fit across all repetitions of each movement type for each participant using least squares regression. Outlier movements (beyond 3 SD of the negative exponential fit) were removed from our analyses. The R 2 coefficient and visual analysis of movement quality scatterplots were used to validate goodness of fit.

To quantify baseline movement quality, the movement quality index for each movement type was averaged for the lesser of the first 100 repetitions or the first 10% of repetitions. This averaging procedure was used in lieu of the y-intercept to guard against the possibility of initial learning effects (the first few movements performed while learning how to play the game may be of poorer quality, which could artificially lower the y-intercept and, hence, overestimate treatment change). The value of the best-fit line at the end of treatment reflected posttreatment movement quality. We predicted the magnitude of improvement in movement quality given sufficient dosing by subtracting the baseline movement quality from the horizontal asymptote of the negative exponential best fit.

Movements for which there were < 150 movement repetitions were excluded from the analysis. The 150-repetition threshold was chosen for the following 3 reasons: at least 15 repetitions would be available for baseline movement quality calculations, it excluded participants who completed almost no gaming motor practice outside of the clinic, and it produced relatively stable best-fit trajectories that corresponded closely with visual inspection.

---

### Thalamic circuits for independent control of prefrontal signal and noise [^115KccnU]. Nature (2021). Excellent credibility.

arose from the NMDA-Rs of the same population (for example, α 1 S 1 in equation (1)) and competing population (for example, α 2 S 2 in equation (2)), the AMPA receptor gating variables of the same population (for example, β 1 r 1 in equation (2)) and competing population (for example, β 2 r 2 in equation (2)), and external inputs (for example, in equation (2)). GABA receptor gating variables were also expressed in α i and β i to account for lateral inhibition. The synaptic parameter values are α 1 = 0.164 nA, α 2 = −0.022 nA, β 1 = 9.9 × 10 −4 nC, β 2 = −6.5 × 10 −5 nC. The external inputis due to a constant but noisy input, and a stimulus input.is described by an Orntein-Ulhenbeck process with mean I OU = 0.350 nA, noise σ OU = 0.015 nA, and time constant τ OU = 2 ms. = 0.017 nA under the presence of favoured input pulses, but 0 otherwise. Using change of variables, the transfer function can be written aswhere a, b, d were constants that depended on β 1, and f was a function of x i that depended on β 2. The expression ofare detailed in a previous study, but in brief, the transfer function results in a smooth and thresholded input–output response (Extended Data Fig. 8c, bottom). A choice was selected at the end of stimulus presentation, based on the population with higher decision variable (S 1, S 2). Stimulus inputs in general drove categorical, winner-take-all competitions such that the two decision variables were largely separated (with the loser decision variable near 0; Extended Data Fig. 8).

---

### The dynamics and geometry of choice in the premotor cortex [^114RAH8c]. Nature (2025). Excellent credibility.

The discretized latent trajectoryis obtained by marginalizing the continuous trajectoryover all latent paths connectingandduring each ISI. These marginalizations are implicit in the transition probability densitiesin equation (5). The transition probability densityaccounts for the drift and diffusion in the latent space and also for the absence of spikes during each interval between adjacent spike observations. This probability density satisfies a modified Fokker–Planck equation, which we derived previously:

Hereis the deterministic potential force, and the termleads to the probability decay due to spikes emitted by any neurons in the population, such thatincludes only trajectories consistent with no spikes emitted between t j −1 and t j. The solution of this equationpropagates the latent probability density forwards in time during each ISI. To model the reaction time task, we solved equation (6) with absorbing boundary conditions, which ensure that trajectories reaching a boundary before the trial end do not contribute to the likelihood. In addition, the absorption operatorin equation (5) enforces that the likelihood includes only trajectories terminating on the boundaries at the trial end time t E (Supplementary Methods 2.1). Together, these two conditions ensure that the likelihood includes only trajectories that reach one of the boundaries for the first time at the trial end time.

To fit the model to data, we derived analytical expressions for the gradients of the model likelihood with respect to each of the model components (Supplementary Methods 2.2). We computed functional derivatives of the likelihood with respect to latent dynamics as previously described. In this study, we computed functional derivatives of the likelihood with respect to tuning functions (Supplementary Methods 2.2). Instead of directly updating the functions Φ (x), p 0 (x) and f i (x) we, respectively, update the forceand auxiliary functions

The potential Φ (x) is obtained from F (x) viawhere we fixed the integration constant C to satisfy. As C is an arbitrary integration constant, a specific choice of C was not important as long as it was the same for all models. The initial state distribution p 0 (x) was obtained from the auxiliary function F 0 (x) in equation (7) via

---

### The biological basis of sex differences in athletic performance: consensus statement for the American College of Sports Medicine [^112Ws7eh]. Medicine and Science in Sports and Exercise (2023). High credibility.

New York City marathon — sex difference in first-place running velocity correlates with participation ratio, with a positive association between the ratio of men-to-women finishers and the sex difference in velocity (r = 0.58, r2 = 0.34, P < 0.001), a linear regression of y = 10.4 + 0.98x (n = 184 pairs), prediction that a ratio of 1 yields an 11.4% sex difference, and an averaged first-place sex difference of 16.9%; illustratively, a ratio of 6 means that for every female runner in a 10-yr age group, there were six male runners, and the dataset spans 31 yr.

---

### Protein expression in experimental malignant glioma varies over time and is altered by radiotherapy treatment [^117TCMau]. British Journal of Cancer (2006). Low credibility.

Partial least squares

Partial least squares is a multivariate regression method that relates the data matrix (X, descriptors) to a y response that can be either single (y) or multiple (Y). Partial least squares has proved to be a powerful tool for finding relationships between descriptor matrices and responses, especially when there are more variables than observations and the variables are collinear to each other and noisy. The PLS theory and methods discussed here concern single y responses. As in PCA, principal components are constructed to reduce the dimensions of X. In order to obtain the principal components, PLS maximises the covariance between the response variable y and a linear combination of the original variables t = Xw, where t is the score vector, X is the data matrix and w is the weight vector. A more detailed description of PLS can be found elsewhere. t = score vector for X; A = the number of PLS components; p = loading vector for X; c = loading vector for Y; E = residual matrix for X; F = residual matrix for Y; When using PLS for discriminant analysis (PLS-DA), dummy variables are used to describe the classes to which different samples can be assigned. This is performed by creating pairs of binary variables for each class, for example ones and zeros, where a one signifies that the object belongs to the class concerned, and a zero implies that it does not. With a PLS-DA model it is possible to predict whether or not an object belongs to a specific class considering the predicted class variable. A predicted value greater than 0.5 implies that the sample belongs to that class and a value lower than 0.5 implies that it does not belong to it.

---

### Determination of the effective local lethal function for the nanOx model [^117EgWbH]. Radiation Research (2020). Medium credibility.

NanOx is a biophysical model recently developed in the context of hadrontherapy to predict the cell survival probability from ionizing radiation. It postulates that this may be factorized into two independent terms describing the cell response to two classes of biological events that occur in the sequence of an irradiation: the local lethal events that occur at nanometric scale and can by themselves induce cell death, and the non-local lethal events that lead to cell death by an effect of accumulation and/or interaction at a larger scale. Here we address how local lethal events are modeled in terms of the inactivation of undifferentiated nanometric targets via an "effective local lethal function F ", which characterizes the response of each cell line to the spectra of "restricted specific energy". F is initially determined as a linear combination of basis functions. Then, a parametric expression is used to reproduce the function's main features, a threshold and a saturation, while at the same time reducing the number of free parameters. This strategy was applied to three cell lines in response to ions of different type and energy, which allows for benchmarking of the α(LET) curves predicted with both effective local lethal functions against the experimental data.

---

### Bayesian source separation for reference function determination in fMRI [^115Fe4GZ]. Magnetic Resonance in Medicine (2001). Low credibility.

In analyzing fMRI results, identification of significant activation in voxels is a crucial task. A standard method selects a "known" reference function and performs a regression of the time courses on it and a linear trend. Once the linear trend is found, the correlation between the assumed to be known reference function and the detrended observed time-course in each voxel is computed. But the most important question is: How does one choose the reference function? Here, a Bayesian source separation approach to determining the underlying reference function is described and applied to real fMRI data. This underlying reference function is the unobserved response due to the presentation of the experimental stimulus.

---

### Forecasting of the first hour aftershocks by means of the perceived magnitude [^112mHijK]. Nature Communications (2019). High credibility.

The parameter μ M in Eq. (3) represents the maximum value ofand we define it the perceived magnitude since it provides the magnitude of ground shaking felt close to the station. We find that the three mainshocks present roughly the same value of μ M, a result expected for the Kos and Lesvos earthquakes with comparable magnitudes and similar epicentral distance from the recording station. In the case of the Ischia earthquake, this result can be primarily attributed to the smaller epicentral distance from the recording station (see Table 1) but also to soil conditions at the seismic station location. Concerning the functional dependence of the scaling function F (x) in Eq. (3), the shape of the envelope function is dominated by different mechanisms such as source properties, attenuation, scattering effects, etc. The scaling collapse of Fig. 2c suggests that these features are, at a first approximation, incorporated in two seismic source-dependent parameters: the perceived magnitude μ M and the horizontal time rescaling τ M. This result is confirmed by the scaling collapse also obtained for the other mainshocks, when plottingas a function of (t − t 0)/ τ M (Fig. 3). In all cases we find that, for x < 10, F (x) ≃ log 10 (x) − q log 10 (x + x 0) with q ≃ 3.5 (continuous curve in Fig. 2c) and x 0 = (q − 1) q q /(1− q) ≃ 0.43 such that the maximum value of F (x) is equal to zero. We assume the same functional dependence of F (x) for all mainshocks in order to reduce the number of fitting parameters in our method. This fit corresponds to a power law decay of the amplitude of coda-waves of V (t)which anticipates the exponential decay at longer timescales. We wish to emphasize that our results are weakly affected by the specific choice of F (x) and we expect that other choices for F (x) lead to similar results.

---

### Estimating time and size of bioterror attack [^117HpbzQ]. Emerging Infectious Diseases (2004). Low credibility.

Appendix

We seek to estimate the initial attack size and the time of the attack from observed cases of infection in real time. The case report data are the (reporting-delay corrected) times at which cases have been reported. We intend this model to be applied once an attack has been discovered and assume that the agent is noncontagious (or in the case of a contagious agent, that no secondary transmission has occurred) and that any interventions mounted (such as vaccination or the administration of antimicrobial agents) have not yet had any effect on the early case reporting data. We define t j to be the time at which the j th case is observed and define the origin as the time at which the first case is observed (so t 1 = 0). The unknown time from the attack until the first case is observed is denoted by A > 0 (and thus the actual date of the attack is equal to – A), while the unknown number of persons infected in the attack is denoted by N > 1.

We treat A and N as random variables and assume that the attack is detected through the reporting of the first case at time t 1 = 0. At the time the attack is detected, we quantify our beliefs regarding the size of the attack by the prior probability distribution. Let X denote the symptom-free incubation time for the attack agent, with probability density f (x) and survivor functionIf n persons were actually infected in the attack, then the time from the attack until the first case is observed would equal the minimum of n independent incubation timesthus

Consequently, the probability that a units of time would pass before the attack would be detected by the first case equalsfrom which the conditional probability density function of A given an attack of size n follows as

Equation 2 implies that the joint prior distribution for the size and time of attack when the first case is observed is equal to

---

### Fractional response analysis reveals logarithmic cytokine responses in cellular populations [^115j8sqi]. Nature Communications (2021). High credibility.

Although we can now measure single-cell signaling responses with multivariate, high-throughput techniques our ability to interpret such measurements is still limited. Even interpretation of dose-response based on single-cell data is not straightforward: signaling responses can differ significantly between cells, encompass multiple signaling effectors, and have dynamic character. Here, we use probabilistic modeling and information-theory to introduce fractional response analysis (FRA), which quantifies changes in fractions of cells with given response levels. FRA can be universally performed for heterogeneous, multivariate, and dynamic measurements and, as we demonstrate, quantifies otherwise hidden patterns in single-cell data. In particular, we show that fractional responses to type I interferon in human peripheral blood mononuclear cells are very similar across different cell types, despite significant differences in mean or median responses and degrees of cell-to-cell heterogeneity. Further, we demonstrate that fractional responses to cytokines scale linearly with the log of the cytokine dose, which uncovers that heterogeneous cellular populations are sensitive to fold-changes in the dose, as opposed to additive changes.

---

### Evolution of dosage compensation under sexual selection differs between X and Z chromosomes [^117K1Tvd]. Nature Communications (2015). Medium credibility.

Since selection is weak, the first (equations (9) and (10)) and second moments of the distribution of infinitesimal change in male and female expression are sufficient to describe the evolution of male and female expression over many substitutions. In continuous time, and ignoring the time taken for segregation to occur and the time between mutations, we obtain that the random variables Z m (t) and Z f (t), which describe male and female expression at time t, satisfy the stochastic differential equation,

where d W m and d W f are standard independent Brownian motions, and b 1 and b 2 are scaled variance terms given byand.

Unless selection on male and female expression is very weak (very small N eX S m, N eX S f), and evolution is dominated by genetic drift, the qualitative features of the evolution of dosage compensation are captured by the expected trajectory of equations (11) and (12). Writing the expected male and female expression as z m and z f, we find that their evolution is given by equations (1) and (2), where time is rescaled according to τ = 2 μt /30. This scaling eases the comparison between X and Z expression when differences between mutation rates in the two systems are ignored.

The expected values for male and female expression through time, z m (τ) and z f (τ), may be found exactly by solving equations (1) and (2). If we assume that degradation of the Y-linked gene copy leads to an initial diminution in expression in males of − z 0, and female expression is unperturbed by degradation and remains at the ancestral optimal level zero, we have

where. Similar expressions for Z-linked genes can be found by replacing N eX by N eZ, and m subscripts by f and vice versa. Plots in Fig. 1 correspond to equations (13) and (14) with respect to t rather than τ to compare more easily the deterministic paths with the stochastic replicates of equations (11) and (12) that are shown in Fig. 2.

---

### Dominance vs epistasis: the biophysical origins and plasticity of genetic interactions within and between alleles [^115SLKER]. Nature Communications (2023). High credibility.

Model 1: protein folding

Phenotype is determined by the total concentration of folded protein. In this model, the protein of interest (X) expressed from each allele (α i, i ∈{1, 2} – one maternal and the other paternal copy respectively, and the alleles are allowed to be the wild type) has two configuration states: unfolded (X U, α i) and folded (X F, α i). The free energy difference between folded and unfolded protein states is ∆ G Folding, α i (kcal per mol). Mutations on each allele can affect folding energy (∆ G Folding), which is described as the sum of wild-type folding energy and the energy differences (mutations) ∆ G Folding, wt + ∆∆ G Folding, α i. Equilibrium between the two states follows Eq. (10).

In the above and following equations, R is the gas constant (R = 1.98 × 10 −3 kcal per mol), T is the absolute temperature for 37 °C (310.15 Kelvin) and the wild-type ∆ G Folding, wt is set to −2 kcal per mol unless stated otherwise.

The total concentration of the protein (X T) follows Eq. (11).

Expression levels from each allele were considered to be equal and therefore, [X T,1] = [X T,2] = 0.5 [X T] in all our models. Using Eqs. (10) and (11) with [X T] as a constant, we can calculate the functional molecule [X F, α i] as a function of energy terms and total protein concentration in the following way:

---

### Evolution of dosage compensation under sexual selection differs between X and Z chromosomes [^1158UjZ8]. Nature Communications (2015). Medium credibility.

The probability of fixation determines the expected change in expression over one particular substitution in each sex. If the mutant is lost, expression does not change, if the mutant is fixed, male and female expression become z m + δ m and z f +2 δ f, respectively (Table 2). To calculate the expected change in expression, we consider all possible mutational sizes and their frequencies,

where 3 N is the total number of genes in the population, μ is the mutation rate and f (δ m, δ f) is the probability density function for the effects of mutants on male and female expression. We assume that f (δ m, δ f) is a bivariate normal distribution with mean (0, 0) and covariance matrix

In this case, equations (6) and (7) read

and the second moments of the change in male and female expressions are approximately E[(Δ z m) 2 | z m, z f] = E[(Δ z f) 2 | z m, z f] = σ 2, and E[(Δ z m)(Δ z f)| z m, z f] = ρ σ 2. For simplicity, we set σ 2 = 0.1. Setting different values for σ 2 changes the impact of mutations, but since it does so with the same intensity in males and females, it does not change our results. However, increasing σ 2 increases the overall rate of evolution.

---

### Allergen immunotherapy: a practice parameter third update [^114juksf]. The Journal of Allergy and Clinical Immunology (2011). Medium credibility.

Allergen extract dilution calculations — use V1 x C1 = V2 x C2 to prepare target concentrations and express wt/vol as percentages; for example, preparing V1 = 5 mL at C1 = 1:200 from C2 = 1:10 uses V2 = (V1 x C1)/C2, yielding V2 = 0.025/0.1 = 0.25, and percent expressions include 1:10 = 10%, 1:20 = 5%, and 1:40 = 2.5%.

---

### Roflumilast (Zoryve) [^116oLut2]. FDA (2024). Medium credibility.

The dosage of roflumilast TOP for treatment of plaque psoriasis in adults is 1 application(s) TOP daily (0.3% cream or foam)

---

### The limits of normal approximation for adult height [^112ZoSZo]. European Journal of Human Genetics (2021). Medium credibility.

Adult height inspired the first biometrical and quantitative genetic studies and is a test-case trait for understanding heritability. The studies of height led to formulation of the classical polygenic model, that has a profound influence on the way we view and analyse complex traits. An essential part of the classical model is an assumption of additivity of effects and normality of the distribution of the residuals. However, it may be expected that the normal approximation will become insufficient in bigger studies. Here, we demonstrate that when the height of hundreds of thousands of individuals is analysed, the model complexity needs to be increased to include non-additive interactions between sex, environment and genes. Alternatively, the use of log-normal approximation allowed us to still use the additive effects model. These findings are important for future genetic and methodologic studies that make use of adult height as an exemplar trait.

---

### Finding spatially variable ligand-receptor interactions with functional support from downstream genes [^112g8Bes]. Nature Communications (2025). High credibility.

Identifying spatially variable LRI candidates

SPIDER incorporates six state-of-the-art methods for spatial variance evaluation. From the profile matrix and coordinates of the abstract interfaces, denoted as Y and Z to ease the notation, we have, whereis the number of ligand-receptor pairs and y p denotes the profile of LRI p across all abstract interfaces. SPIDER measures the spatial-induced variance of y p with several formulations below.

Gaussian process (GP) regression is a common approach for spatial regression; under the GP model, an LRI signal y p follows a normal distribution with the mean of μ ⋅ 1 and a covariance matrix from two terms - a non-spatial noise term ψ I and a spatial term formulated by a covariance function σ 2 Q (ϕ). In the spatial variance term, Q (ϕ) captures the spatial correlation between abstract interfaces under the lengthscale ϕ controlling the decay rate of correlation with distance. The parameters are inferred by maximizing the log-likelihood, with which we obtain the log-likelihood ratio (LLR) compared to the null hypothesis of no spatial covariance.

Assuming that the LLR follows a χ 2 distribution, we obtain the corresponding p -value and q -value for y p. We can also rank LRI with the fraction of spatial variance (FSV) defined in SpatialDE. For the above process, we apply the implementation in SpatialDE and SOMDEusing a squared exponential covariance function.

Furthermore, we incorporate two variations of the GP regression model (Supplementary Method 1.3.1). Replacing the full covariance matrix with a nearest-neighbor approximation accelerates the model fitting process to scale linearly with the number of abstract interfaces, instead of cubically when using the full covariance matrix; the subsequent estimation of LLR, p -value, and q -value resembles the previous method. We apply the implementation in nnSVG. To integrate a variety of covariance functions, we incorporate all possible covariance functions with a generalized linear mixed model. For an efficient model fitting process, we utilize an extended score test that only estimates the model under the null hypothesis as implemented in SpatialDE2. Assuming that the sum of the score vector for the variance components follows a mixture of χ 2 distributions, we obtain the corresponding p -value and q -value for y p.

---

### Racial disparities in infant mortality: what has birth weight got to do with it and how large is it? [^1143rLfw]. BMC Pregnancy and Childbirth (2010). Low credibility.

The likelihood function for the basic birth weight (x) only CDDmlr model (i.e. CDDmlr without any exogenous covariate) of infant mortality (y) is formally defined as a product of the conditional mortality submodel f 2 (y | x; θ, β) and the birth weight density submodel f 1 (x; θ):

In the case of two truncated Gaussian subpopulations, the birth weight density submodel f 1 (x; θ) is given by

π s, the mixing proportion, is defined as the proportion of births belonging to the less numerous of the two subpopulations, that is, the secondary subpopulation (s, "compromised" subpopulation) as opposed to the primary subpopulation (p, "normal" subpopulation). The reparameterization of π s (Eq. 3) transforms the 0 and 1 bounds on π s to minus and plus infinity, respectively. For i = s and p, represents the Gaussian density, truncated at 500 grams, with mean μ i and variance. The conditional mortality submodel f 2 (y | x; θ, β) with two subpopulations is given by

where q s (x; θ) is the probability that an infant with birth weight x belongs to the s subpopulation. For i = s and p,

The birth weight density submodel f 1 (x; θ) (Eq. 2) determines that

Overall, there are 11 parameters, five defining the birth weight distribution, and six defining the subpopulation-specific mortalities.

In this study, the basic CDDmlr model is extended in two ways. First, we have used European American births as the default and defined the African American "race" effect as an indicator variable (z) on each of the 11 parameters in the basic CDDmlr model. Second, for i = s and p, standardized birth weight (, i.e. x is standardized according to the respective subpopulation mean and standard deviation) is used in the corresponding logistic regression function. Thus

---

### SNMMI procedure standard / EANM practice guideline for diuretic renal scintigraphy in adults with suspected upper urinary tract obstruction 1.0 [^115hq8Vr]. Seminars in Nuclear Medicine (2018). Medium credibility.

Relative function — relative uptake is usually measured by placing an ROI over each kidney and integrating counts in the renal ROI for 1–2, 1–2.5, or 2–3 minutes after injection or by using the Rutland-Patlak plot; for the integral method or when furosemide is given at the beginning of the study (F = 0), the 1–2 or 1–2.5-minute periods are preferred, and measurement should occur before any activity drains into the ureter or bladder.

---

### A cochlear-bone wave can yield a hearing sensation as well as otoacoustic emission [^112tyGCr]. Nature Communications (2014). Medium credibility.

The coefficients G 1 (k) and G 2 (k) are defined as follows:

Here we have used the abbreviation L (k) = [2 i ω ρ (F 1 + F 2) w bm +3 F 1 F 2 Z bm (x)]/[3 A 1 A 2 Z bm (x)] with F 1/2 = A 1/2 k 2 − cω 2 ρ. The dispersion relation L(k) = 0 has been derived from the eigenvalues of the matrix equation (10).

The Green's functions for bone stimulation can be derived analogously. Assume that both cochlear chambers, at a certain longitudinal location x 0, are sinusoidally compressed and expanded:

We make the following ansatz for the Greens functions:

which yields the amplitude equations

The solutions are

with L (k) as given above. In the symmetric case of equal chamber areas, A 1 = A 2, we obtain W 1 (k) = W 2 (k). No basilar-membrane displacement then arises, because the pressures in both chambers are equal.

When attempting to compute the integral in the ansatz for the Green's functions (equations (25) and (29)), we encounter a problem: the integrand has a singularity at the wave vectors k for which L (k) = 0, that is, at those wave vectors that obey the dispersion relation. However, we can employ the residue theorem of complex analysis to compute the integrals. Indeed, for propagation apical of the generation site, that is at a location x < x 0, we can close the contour in the upper-half plane because the integrand there is exponentially suppressed. The integral then only involves a contribution from the poles in the upper-half plane. In the case of basilar-membrane stimulation, we obtain a contribution proportional to. The pressures p 1/2 (− k bm, ω, x 0) represent the pressures of the basilar membrane mode in the two chambers

Analogous results can be obtained for the cochlear-bone wave with k cb (x).

In the opposite case, for a cochlear location basal to the generation site, x > x 0, the integration path can be closed in the lower-half plane.

---

### Control of finite critical behaviour in a small-scale social system [^1167cLr3]. Nature Communications (2017). Medium credibility.

Independent model inference

The independent model consists of individuals participating in conflict randomly, with the frequencies of individual appearance equal to their empirically measured, heterogeneous values f i = 〈 x i 〉. Naively, this can be written as a relative negative log likelihood, and is equivalent to a maximum entropy model that matches only the frequencies f i. (The relative negative log likelihood L (x) of state x is related to the likelihood p (x) by p (x) = exp(− L (x))/ Z, where Z is a normalization constant set by the constraint that the sum of likelihoods over all states is one. In statistical physics, Z is the partition function and L (x) is proportional to the free energy of state x.)

However, as detailed in 'Operational definitions' in the Methods section, a fight was operationalized for these analyses as involving two or more socially mature individuals. (Observed fights that involved only juveniles and 0 or 1 mature individuals are therefore excluded.) Correspondingly, we forbid our models from producing fights of size smaller than two. We treat this as an additional constraint on the model. The resulting maximum entropy model is then the one in which the likelihood of states with fewer than two individuals present is taken to zero. This corresponds to a relative negative log likelihood

where Θ(z) is 0 when z ≤ 0 and 1 when z > 0.

In the unconstrained case (α = 0), we can easily solve for h i:

We must now solve numerically for h i to match the empirically measured f i = 〈 x i 〉 = 〈 x i 〉 α →∞. To accomplish this, note that the unconstrained model will have modified statistics:

where f 0 is the frequency of size zero fights, f i 1 is the frequency of size one fights consisting solely of individual i, andis the overall frequency of size one fights (all measured in the unconstrained model). In terms of unconstrained individual frequencies, these are

We use an iterative procedure to solve equation (7) for 〈 x i 〉 α = 0, which are then used in equation (6) to find the fields. Finally, samples from the independent model (equation (4)) are produced by sampling using equation (5) and simply discarding samples in which fewer than two individuals appear. For our data, this results in discarding about 17% of samples produced with equation (5).

---

### Input-output maps are strongly biased towards simple outputs [^11354s9E]. Nature Communications (2018). Medium credibility.

Many systems in nature can be described using discrete input-output maps. Without knowing details about a map, there may seem to be no a priori reason to expect that a randomly chosen input would be more likely to generate one output over another. Here, by extending fundamental results from algorithmic information theory, we show instead that for many real-world maps, the a priori probability P(x) that randomly sampled inputs generate a particular output x decays exponentially with the approximate Kolmogorov complexity [Formula: see text] of that output. These input-output maps are biased towards simplicity. We derive an upper bound P(x)≲[Formula: see text], which is tight for most inputs. The constants a and b, as well as many properties of P(x), can be predicted with minimal knowledge of the map. We explore this strong bias towards simple outputs in systems ranging from the folding of RNA secondary structures to systems of coupled ordinary differential equations to a stochastic financial trading model.

---

### Decimeter-depth and polarization addressable color 3D meta-holography [^112NWi2P]. Nature Communications (2024). High credibility.

Fig. 2
Computation strategy of the meta-hologram.

Then Eqs. (3) and (4) can be obtained by solving the Helmholtz equation:whereis the transfer function andis the phase of the transfer function. λ is the wavelength. So, the complex amplitude distributions T LCP and T RCP of the holographic plane corresponding to the two polarization states can be obtained by calculating their respective corresponding initial light fields:whererepresents Fourier transform, andrepresents inverse Fourier transform, T 1 (x, y; d 0) and T 2 (x, y; d 0) represent the complex amplitude distributions of 3D objects I and II, respectively. H LCP (f x, f y; z) and H RCP (f x, f y; z) represent the transfer function for different polarization states.

When calculating the hologram, all the light fields, spectra and transfer functions are discretely sampled with equal grids, and the Fourier transform and inverse transform of the continuous space involved in Eqs. (5) and (6) are also replaced by Fast Fourier transform and inverse Fast Fourier transform calculation, respectively. The under-sampling operation in the discrete sampling process causes errors, which will lead to spectrum aliasing, and finally the reconstructed light field of large depth holography will be seriously affected. The local signal frequency of the transfer function M f can be expressed as follows:

In the sampling of the transfer function, the sampling frequency Δ f x needs to satisfy the Nyquist sampling theorem to avoid the aliasing error:

However, in the calculation of meta-holography, the sampling frequency is usually determined by the following equation:where N is the number of pixels in the x direction on the meta-hologram. Therefore, in order to achieve large depth meta-holography, the transfer function itself can only be band-limited to meet the requirements of Nyquist sampling theorem.

---

### Inferring histology-associated gene expression gradients in spatial transcriptomic studies [^115XSXbL]. Nature Communications (2024). High credibility.

Representative examples of either hypothesis as well as for either screening approach, are displayed in Supplementary Fig. 8a, b. To be able to adopt either of these hypotheses, we posit that, if the inferred gradient in question stems from a gene whose inferred expression gradient is dependent on the spatial trajectory or the spatial annotation, it should not merely consist of randomly scattered expression values. Instead, it should display a discernible degree of gradual expression change forming a recognizable pattern. Thus, the smoother the inferred gradient, the less random it is, and vice versa. Assuming this relationship, we quantify the degree of randomness of a gradient using its total variation (TV). This metric is calculated by considering the absolute differences between adjacent expression estimates, as displayed in Supplementary Fig. 8c. The total variation is calculated according to the formula:where
TV is the total variation.
n is the number of expression estimates in the gradient.
yi represents the gene expression value at expression estimate i.

| y i+1 − y i | calculates the absolute difference between the gene expression values of adjacent expression estimates.

To assess the effectiveness of the total variation in capturing the percentage of noise or randomness introduced into a gradient, we leveraged the simulated dataset discussed above, where a certain degree of noise was introduced per simulation, represented by randomly generated expression values. Each resulting simulation was a combination of pattern-specific expression (Supplementary Figs. 9 c, 10c) and randomly generated expression, based on different noise types (Supplementary Figs. 9 d–f, 10d–f). We examined the relationship between the total variation and the degree of randomness across different underlying patterns and various types of noise. Our findings consistently demonstrated a strong linear relationship between the total variation and noise ratio across all simulation modalities using a resolution of 100 μm. The high C values of 0.77–0.92 and 0.78–0.91 in the noise types equally distributed and combined (which we consider the most realistic manifestation of noise in real-life data) indicate that this metric effectively quantifies the degree of randomness.

---

### Transient power-law behaviour following induction distinguishes between competing models of stochastic gene expression [^111GPGgB]. Nature Communications (2025). High credibility.

Inferring the number of inactive gene states is difficult from steady-state data

In Fig. 2 the steady-state mRNA count distributions of thousands of rate parameter sets for the N = 3, 4, 5 state models are compared with those of their effective telegraph models (computed using the previously detailed three-step procedure). Note that to ensure biological relevance, the rate parameter ranges for each model were chosen to comfortably cover the bounds of the telegraph model rate parameters estimated using a range of eukaryotic data in various published studies (see Table 1 of ref.for these estimates and see Supplementary Note 2 for further details on generating the rate parameter sets).

Fig. 2
The steady-state mRNA count distribution of the N -state model can often be well fit by an effective telegraph model.

Thousands of rate parameter sets of the N -state model (N = 3, 4, 5) were sampled in the range relevant for eukaryotic gene expression. For each parameter set the mapping in Eq. (2) was used to obtain the rate parameters of an effective telegraph model. A – C P-P plots comparing the cumulative distribution function (CDF) of the N -state and effective telegraph models. These are constructed from the 500 rate parameter sets with the largest Wasserstein distance (WD) values between the distributions of the two models. Deviations from the line y = x (red) indicate differences between the distributions. D – G Typical steady-state mRNA count distributions of the 5-state (solid blue lines) and effective telegraph models (histograms) for four classes of distributions (Shape I: Unimodal with Fano factor < 2; Shape II: Unimodal with Fano factor ≥ 2; Shape III: Bimodal with one peak at zero and another at a non-zero value; Shape IV: Bimodal with peaks at non-zero values. H Boxplots displaying summary statistics of the WD between the steady-state distribution of mRNA counts of the N -state and effective telegraph model. The horizontal line and the box show the median and the interquartile range, respectively; whiskers extend to the 10th and 90th percentiles; sample sizes given in Supplementary Table 2. I – K Bar charts displaying the percentage of each distribution shape sampled across the parameter space. The agreement between the N -state and its effective telegraph model becomes worse as N increases and is worst for Shape IV because this shape is qualitatively impossible to capture by the telegraph model. See Supplementary Note 2 for implementation details.

---

### Allometric cascade as a unifying principle of body mass effects on metabolism [^111JPwdx]. Nature (2002). Excellent credibility.

The power function of basal metabolic rate scaling is expressed as aM(b), where a corresponds to a scaling constant (intercept), M is body mass, and b is the scaling exponent. The 3/4 power law (the best-fit b value for mammals) was developed from Kleiber's original analysis and, since then, most workers have searched for a single cause to explain the observed allometry. Here we present a multiple-causes model of allometry, where the exponent b is the sum of the influences of multiple contributors to metabolism and control. The relative strength of each contributor, with its own characteristic exponent value, is determined by the control contribution. To illustrate its use, we apply this model to maximum versus basal metabolic rates to explain the differing scaling behaviour of these two biological states in mammals. The main difference in scaling is that, for the basal metabolic rate, the O(2) delivery steps contribute almost nothing to the global b scaling exponent, whereas for the maximum metabolic rate, the O(2) delivery steps significantly increase the global b value.

---

### Evaluation of the evenness score in next-generation sequencing [^111oWZQR]. Journal of Human Genetics (2016). Low credibility.

The evenness score (E) in next-generation sequencing (NGS) quantifies the homogeneity in coverage of the NGS targets. Here I clarify the mathematical description of E, which is 1 minus the integral from 0 to 1 over the cumulative distribution function F(x) of the normalized coverage x, where normalization means division by the mean, and derive a computationally more efficient formula; that is, 1 minus the integral from 0 to 1 over the probability density distribution f(x) times 1-x. An analogous formula for empirical coverage data is provided as well as fast R command line scripts. This new formula allows for a general comparison of E with the coefficient of variation (= standard deviation σ of normalized data) which is the conventional measure of the relative width of a distribution. For symmetrical distributions, including the Gaussian, E can be predicted closely as 1-σ(2)/2⩾E⩾1-σ/2 with σ ≤ 1 owing to normalization and symmetry. In case of the log-normal distribution as a typical representative of positively skewed biological data, the analysis yields E≈exp(-σ*/2) with σ*(2) = ln(σ(2)+1) up to large σ (≤ 3), and E≈1-F(exp(-1)) for very large σ (⩾2.5). In the latter kind of rather uneven coverage, E can provide direct information on the fraction of well-covered targets that is not immediately delivered by the normalized σ. Otherwise, E does not appear to have major advantages over σ or over a simple score exp(-σ) based on it. Actually, exp(-σ) exploits a much larger part of its range for the evaluation of realistic NGS outputs.

---

### What are the determinants of gene expression levels and breadths in the human genome? [^1128r2Aj]. Human Molecular Genetics (2012). Low credibility.

Using this new approach, we identify which genomic traits are significant determinants of gene expression levels and breadths. Our statistical models, using information on genomic traits alone, can predict substantial amount of variation found in the levels and breadths of gene expression in the human and mouse genomes. Furthermore, we identify genomic traits that are more strongly associated with one aspect of gene expression than with the other. We also show that some genomic features exhibit species-specific patterns of associations with gene expression traits. Our study provides valuable insights into the molecular mechanisms underlying the regulation of gene expression. In addition, our method is highly applicable to many other questions in biology, to disentangle effects of different factors on biologically correlated traits.

---

### Standards of care in diabetes – 2025 [^1177SxqY]. Diabetes Care (2025). High credibility.

Regarding specific circumstances for diabetic nephropathy, more specifically with respect to pediatric patients, ADA 2025 guidelines recommend to determine the eGFR at the time of diagnosis and annually thereafter.

---

### Different population dynamics in the supplementary motor area and motor cortex during reaching [^112SyGVv]. Nature Communications (2018). Medium credibility.

Hypothesis-guided dimensionality reduction

Dimensionality reduction began by formatting neural (or muscle) responses as a matrix, A, where each column contains the responses of one neuron, concatenated across all times and conditions. A is thus of size CT × N where C is the number of conditions, T is the number of time points and N is the number of neurons. We also consider A, where each element describes the derivative of the firing rate for the corresponding condition, time and neuron. We seek a CT × K matrix, X, where each column describes one of K latent variables (K was set to six for all analyses). X is found via projection: X = AW T, where W is a K × N orthogonal matrix. Each latent variable is thus a weighted sum of individual-neuron responses, with the weights defining K dimensions in neural space. Those weights were found by optimizing a cost function: with the constraint that W is orthonormal. We used a tripartite cost function: f (W) = f rec (W)+ f invar (W)+ f dyn (W), with each term corresponding to one aspect of the guiding hypothesis.

The first term, f rec (W), is identical to the cost function used by PCA, and encourages the dimensions in W to capture variance in A, such that A can be reconstructed reasonably accurately from X. Because W is an orthogonal matrix, A rec = XW = AW T W is the optimal linear reconstruction of A (in terms of minimizing mean squared error) from X. Thus. The squared Frobenius norm, is the sum of the squares of the individual elements. f rec (W) is small if A can be reasonably well reconstructed from X. PCA can be thought of as a special case of HDR, where only the term f rec (W) is used. This corresponds to the 0th order hypothesis that the largest signals (those that most dominate the responses of single neurons) are important.

---

### Cell cycle and growth response of CHO cells to X-irradiation: threshold-free repair at low doses [^115Ke7Gr]. International Journal of Radiation Oncology, Biology, Physics (2001). Low credibility.

Purpose

To test the hypothesis of a threshold for induced repair of DNA damage (IR) and, secondarily, of hyperradiosensitivity (HRS) to low-dose X-irradiation.

Methods and Materials

Exponentially growing Chinese hamster ovary cells (CHO) were X-irradiated with doses from 0.2 to 8 Gy. Survival data were established by conventional colony-forming assay and flow-cytometric population counting. The early cell cycle response to radiation was studied based on DNA-profiles and bromodeoxyuridine pulse-labeling experiments.

Results

Colony-forming data were consistent with HRS. However, these data were of low statistic significance. Population counting provided highly reproducible survival curves that were in perfect accord with the linear-quadratic (LQ) model. The dominant cell cycle reaction was a dose-dependent delay of G2 M and late S-phase.

Conclusion

There was no evidence for a threshold of IR and for low-dose HRS in X-irradiated CHO cells. It is suggested that DNA damage repair activity is constitutively expressed during S-phase and is additionally induced in a dose-dependent and threshold-free manner in late S-phase and G2. The resulting survival is precisely described by the LQ model.

---

### Statistical testing in transcriptomic-neuroimaging studies: a how-to and evaluation of methods assessing spatial and gene specificity [^113tVSVr]. Human Brain Mapping (2022). Medium credibility.

What is however not yet answered by means of spatially constrained null models is the importance of evaluating whether an observed correlating transcriptomic‐neuroimaging pattern goes beyond effects that one can expect from taking any other gene or gene‐set from the data, that is, to what extent is our effect really unique to our gene(s)‐of‐interest X or are we looking at a much more general effect? To address the notion of "gene specificity", statistical comparisons of our effect for gene X to effects derived from other genes are needed. Several options are proposed, the most simple one being to compare the effect of gene X against a null distribution of effects based on selecting random genes from the pool of all N ≈ 20,000 genes (Romme et al.) (Figure 1c). Such a "random gene" model may indeed provide an important first indication of whether or not our observed association with Y is relatively unique for our gene(s)‐of‐interest X or whether it can easily be found for many other genes (resulting in a pseudo p > .05). The random‐gene null model can serve as a first simple evaluation of whether or not our observed effect has some level of specificity or not.

---

### Dynamics of lineage commitment revealed by single-cell transcriptomics of differentiating embryonic stem cells [^117W6XC8]. Nature Communications (2017). Medium credibility.

Quantification of the flow cytometry experiments

The distribution of cells in the space of CD24 and PDGFRA expression was modeled by the sum of 4 bivariate normal distributions. This model has in principle 19 free parameters (8 for the means, 8 for the standard deviations and 3 for the size of the relative contributions). To ensure robust fitting to the date we reduced the number of parameters to 9 by keeping the standard deviations constant and only allowing 4 different values for the means.

N(x, y, µ x, µ y, σ x, σ y) is a bivariate normal distribution in x and y (PDGFRA and CD24 expression, respectively) with mean (µ x, µ y) and standard deviation (σ x, σ y). This model was fit to a reference data set (typically untreated control cells after 96 h of RA exposure) by maximizing the log-likelihood −log(p). To subsequently calculate the size of the fractions f i for a particular sample we first calculated the probabilities that the expression values (x, y) found in a particular cell were drawn from one the 4 normal distributions N(x, y, µ x, µ y, σ x, σ y). The cell was then ascribed to the distribution from which it was most likely drawn.

---

### Warped linear mixed models for the genetic analysis of transformed phenotypes [^1145eEh1]. Nature Communications (2014). Medium credibility.

Linear mixed models (LMMs) are a powerful and established tool for studying genotype-phenotype relationships. A limitation of the LMM is that the model assumes Gaussian distributed residuals, a requirement that rarely holds in practice. Violations of this assumption can lead to false conclusions and loss in power. To mitigate this problem, it is common practice to pre-process the phenotypic values to make them as Gaussian as possible, for instance by applying logarithmic or other nonlinear transformations. Unfortunately, different phenotypes require different transformations, and choosing an appropriate transformation is challenging and subjective. Here we present an extension of the LMM that estimates an optimal transformation from the observed data. In simulations and applications to real data from human, mouse and yeast, we show that using transformations inferred by our model increases power in genome-wide association studies and increases the accuracy of heritability estimation and phenotype prediction.

---

### Clotrimazole [^115eJtJD]. FDA. Low credibility.

The dosage of clotrimazole OTIC for treatment of otomycosis in adults is 1 vial OTIC BID for 14 days (1%/0.17 mL)

---

### An alternative to the breeder's and lande's equations [^1129RvLg]. G3 (2014). Low credibility.

The breeder's equation is a cornerstone of quantitative genetics, widely used in evolutionary modeling. Noting the mean phenotype in parental, selected parents, and the progeny by E(Z0), E(ZW), and E(Z1), this equation relates response to selection R = E(Z1) - E(Z0) to the selection differential S = E(ZW) - E(Z0) through a simple proportionality relation R = h(2)S, where the heritability coefficient h(2) is a simple function of genotype and environment factors variance. The validity of this relation relies strongly on the normal (Gaussian) distribution of the parent genotype, which is an unobservable quantity and cannot be ascertained. In contrast, we show here that if the fitness (or selection) function is Gaussian with mean μ, an alternative, exact linear equation of the form R' = j(2)S' can be derived, regardless of the parental genotype distribution. Here R' = E(Z1) - μ and S' = E(ZW) - μ stand for the mean phenotypic lag with respect to the mean of the fitness function in the offspring and selected populations. The proportionality coefficient j(2) is a simple function of selection function and environment factors variance, but does not contain the genotype variance. To demonstrate this, we derive the exact functional relation between the mean phenotype in the selected and the offspring population and deduce all cases that lead to a linear relation between them. These results generalize naturally to the concept of G matrix and the multivariate Lande's equation Δ(z) = GP(-1)S. The linearity coefficient of the alternative equation are not changed by Gaussian selection.

---

### The 2020 Canadian Cardiovascular Society / Canadian Heart Rhythm Society comprehensive guidelines for the management of atrial fibrillation [^115FLPdu]. The Canadian Journal of Cardiology (2020). High credibility.

Regarding diagnostic investigations for atrial fibrillation, more specifically with respect to initial evaluation, CCS/CHRS 2020 guidelines recommend to identify and treat reversible and secondary causes of AF.

---

### Recommendations for a standardized pulmonary function report. An official American Thoracic Society technical statement [^116VXEZR]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Spirometry reporting specifies that numerical values are given for the FEV1, the FVC, and the FEV1/FVC ratio; the latter should be reported as a decimal fraction and the space for percent predicted value left blank, and if bronchodilators are given the LLN column need not be repeated with absolute and percent change given only for FEV1 and FVC. Other numerical values such as the forced inspiratory flow at 75% of FVC (FEF75%) and FEF25–75% are not recommended for routine use. Graph requirements include that for the volume–time curve the volume scale should be at least 10 mm/L, the time scale at least 20 mm/s, and 1 second prior to the start of expiration should be displayed; on the flow–volume plot the flow display should be at least 5 l/min/L/s, and the ratio of flow to volume should be 2 L/s to 1 L, and linear and log scales where values are plotted as z-scores relative to the predicted value (z = 0) give an intuitive sense of severity.