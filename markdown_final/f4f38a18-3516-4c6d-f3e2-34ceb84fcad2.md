# Medical Question & Answer

**Sample ID**: f4f38a18-3516-4c6d-f3e2-34ceb84fcad2
**Dataset Index**: 320889

---

## Question

What is the maximum number of natural numbers not exceeding 2016 that can be marked so that the product of any two marked numbers is a perfect square?

#

---

## Answer

> Let's see… What do we have here? The user is asking for the maximum number of natural numbers not exceeding 2016 that can be marked so that the product of any two marked numbers is a perfect square. Let's break this down step-by-step. First, I need to think about the algebraic condition that makes the product of two numbers a perfect square. Then, I should classify numbers by their square-free parts and determine how many numbers share each square-free part up to 2016. Next, I will identify which square-free classes are compatible with each other and which are not. Finally, I will compute the maximum size by selecting the largest compatible class and verify the result with a concrete example.

> Let me first confirm the core condition: the product of two natural numbers a and b is a perfect square if and only if, when we write a = s·k² and b = s·m² with s square-free, the square-free parts are equal, i.e., s_a = s_b. Wait, let me verify that. If a = s·k² and b = s·m², then ab = s²(km)², which is indeed a perfect square. Conversely, if ab is a perfect square, then the square-free parts must match so that the exponents of all primes in ab are even, which forces s_a = s_b. So, compatibility within a marked set requires that all marked numbers share the same square-free part.

> Hold on, let's not jump to conclusions about mixing different square-free parts. I initially wondered if we could mix classes cleverly, but wait, if I pick a with square-free part s and b with square-free part t where s ≠ t, then ab has square-free part st, which is not 1, so ab cannot be a perfect square. Therefore, the marked set must be entirely contained within a single square-free class to satisfy the pairwise product condition.

> Next, I should review how to decompose numbers into square-free part and square factor. For any natural number n ≤ 2016, I can write n = s·k² where s is square-free and k is a positive integer. The square-free part s ranges over integers whose prime factors all appear to the first power, and the square factor k² captures the even exponents. I need to count, for each square-free s, how many k satisfy s·k² ≤ 2016.

> I will now examine which square-free classes are actually represented up to 2016. Let me think about the smallest and largest square-free numbers and the density. Square-free numbers include 1, 2, 3, 5, 6, 7, 10, 11, 13, 14, 15, 17, 19, 21, 22, 23, 26, 29, 30, 31, 33, 34, 35, 37, 38, 39, 41, 42, 43, 46, 47, and so on. I should confirm that there are indeed many square-free numbers below 2016, and that for each such s, the count of k with s·k² ≤ 2016 varies with s.

> Let me consider the class with square-free part 1, i.e., perfect squares. The numbers are 1², 2², 3²,…, 44², since 44² = 1936 ≤ 2016 and 45² = 2025 > 2016. So there are 44 perfect squares in this class. I should double-check that 44 is correct: yes, 44² = 1936 ≤ 2016, and 45² exceeds 2016, so the count is 44.

> Now, I need to check whether any other square-free class can yield more than 44 numbers. For a general square-free s, the largest k such that s·k² ≤ 2016 is floor(√(2016/s)). As s increases, this upper bound decreases, so the size of the class tends to shrink. For example, s = 2 gives floor(√(2016/2)) = floor(√1008) ≈ 31.7, so 31 numbers; s = 3 gives floor(√(2016/3)) = floor(√672) ≈ 25.9, so 25 numbers; s = 5 gives floor(√(2016/5)) = floor(√403.2) ≈ 20.07, so 20 numbers. I should confirm these counts: for s = 2, k goes up to 31, yielding 31 numbers; for s = 3, k up to 25, yielding 25 numbers; for s = 5, k up to 20, yielding 20 numbers. All are less than 44, and larger s will generally yield even smaller classes.

> Hmm, wait a minute, I should verify whether there might be a larger class for some intermediate s. Let me consider s = 6: floor(√(2016/6)) = floor(√336) ≈ 18.33, so 18 numbers; s = 7: floor(√(2016/7)) ≈ 16.96, so 16 numbers; s = 10: floor(√(2016/10)) ≈ 14.19, so 14 numbers. The trend is downward, reinforcing that the maximum occurs when s = 1.

> Therefore, the largest compatible class is the set of perfect squares up to 2016, which contains 44 numbers. I should confirm that this set indeed satisfies the pairwise product condition: if a = k² and b = m², then ab = (km)², which is a perfect square, so the condition is satisfied. Conversely, any marked set with more than 44 numbers would have to include numbers from different square-free classes, which would violate the pairwise product condition, as shown earlier.

> Final answer: The maximum number of natural numbers not exceeding 2016 that can be marked so that the product of any two marked numbers is a perfect square is 44, achieved by marking the perfect squares 1, 4, 9,…, 1936.

---

The maximum number of natural numbers not exceeding 2016 that can be marked such that the product of any two marked numbers is a perfect square is **44**. This is achieved by selecting all perfect squares up to 2016, i.e. (1^2, 2^2, \\ldots, 44^2), since the product of any two squares is a square. No larger set is possible because any non-square would introduce a non-square product with at least one other element, violating the condition.

---

## Mathematical reasoning

### Perfect squares property

Perfect squares have the form (n = k^2) for some integer (k). The product of two perfect squares is itself a perfect square: ((a^2) \\cdot (b^2) = (ab)^2). Thus, a set of perfect squares satisfies the condition that the product of any two elements is a perfect square.

---

### Identifying perfect squares up to 2016

We need to find all perfect squares (k^2) such that (k^2 \\leq 2016). Solving (k^2 \\leq 2016) gives (k \\leq \\sqrt{2016} \\approx 44.9), so the largest integer (k) is 44. Therefore, the perfect squares up to 2016 are (1^2, 2^2, \\ldots, 44^2), totaling **44 numbers**.

---

### Verification of the condition

Let (S = {1^2, 2^2, \\ldots, 44^2}). For any (a, b \\in S), we have (a = m^2) and (b = n^2) for some integers (m, n). Then (a \\cdot b = m^2 \\cdot n^2 = (mn)^2), which is a perfect square. Thus, the set (S) satisfies the required condition.

---

### Proof that 44 is the maximum

Suppose there exists a larger set (T) with more than 44 elements that satisfies the condition. Then (T) must contain at least one non-square, say (x). For any square (y = k^2) in (T), the product (x \\cdot y = x \\cdot k^2) must be a perfect square. This implies (x) must be a perfect square, contradicting our assumption that (x) is a non-square. Therefore, **no larger set** can satisfy the condition.

---

## Conclusion

The maximum number of natural numbers not exceeding 2016 that can be marked such that the product of any two marked numbers is a perfect square is **44**, achieved by selecting all perfect squares up to 2016.

---

## References

### Current sample size conventions: flaws, harms, and alternatives [^f34a2bb9]. BMC Medicine (2010). Low credibility.

Value of information methods

Many methods have already been described in the statistical literature for choosing the sample size that maximizes the expected value of the information produced minus the total cost of the study. See for an early discussion, for recent examples, and the introduction of for additional references. These require projecting both value and cost at various different sample sizes, including quantifying cost and value on the same scale (note, however, that this could be avoided by instead maximizing value divided by total cost). They also require formally specifying uncertainty about the state of nature; although this can be criticized as being subjective, it improves vastly on the usual conventional approach of assuming that one particular guess is accurate. These methods can require considerable effort and technical expertise, but they can also produce the sort of thorough and directly meaningful assessment that should be required to justify studies that are very expensive or that put many people at risk.

Simple choices based on cost or feasibility

Recent work has justified two simple choices that are based only on costs, with no need to quantify projected value or current uncertainty about the topic being studied. Because costs can generally be more accurately projected than the inputs for conventional calculations, this avoids the inherent inaccuracy that besets the conventional approach. One choice, called n min, is the sample size that minimizes the total cost per subject studied. This is guaranteed to be more cost-efficient (produce a better ratio of projected value to cost) than any larger sample size. It therefore cannot be validly criticized as inadequate. The other, called n root, is the sample size that minimizes the total cost divided by the square root of sample size. This is smaller than n min and is most justifiable for innovative studies where very little is already known about the issue to be studied, in which case it is also guaranteed to be more cost efficient than any larger sample size. An interactive spreadsheet that facilitates identification of n min and n root is provided as Additional file 2.

A common pragmatic strategy is to use the maximum sample size that is reasonably feasible. When sample size is constrained by cost barriers, such as exhausting the pool of the most easily studied subjects, this strategy may closely approximate use of n min and therefore share its justification. When constraints imposed by funders determine feasibility, doing the maximum possible within those constraints is a sensible choice.

---

### Abstract representations emerge naturally in neural networks trained to perform multiple tasks [^472ba8eb]. Nature Communications (2023). High credibility.

Next, we quantify how the level of abstraction developed in the representation layer depends on the number of classification tasks used to train the model (Fig. 3 c). For each number of classification tasks, we train 10 multi-tasking models to characterize how the metrics depend on random initial conditions. As the number of classification tasks P exceeds the number of latent variables D, both the classification and regression generalization metrics saturate to near their maximum possible values (classifier generalization metric: exceeds 90% correct with 8 tasks; regression generalization metric: exceeds r 2 = 0.8 with 9 tasks; Fig. 3 c, right of the gray line). Saturation of the classifier generalization metric indicates that the broad organization of the latent variables is perfectly preserved; while saturation of the regression generalization metric indicates that even the magnitude information that the multi-tasking model did not receive supervised information about is preserved and represented in a fully abstract format. Importantly, both the training and testing set split and the classification boundary for the classifier generalization metric are randomly selected — they are not the same as classification tasks used in training.

The multi-tasking model also reduces the number of samples required to both learn and generalize novel tasks. For a multi-tasking model trained to perform P = 10 tasks with D = 5 latent variables, we show how the performance of a novel classification task depends on the number of samples. We compare this performance to a lower bound (Fig. 3 f, dark gray), from when the task is learned from the standard input representation; as well as an upper bound (Fig. 3 f, light gray), from when the task is learned directly from the latent variables. The performance of the multi-tasking model nearly saturates this upper bound (Fig. 3 f, left). Next, we perform a similar novel task analysis, but where the novel task is learned from one half of the stimulus space and is tested in the other half – just like our generalization analysis above (and schematized in Fig. 1 c, top). We compare the same lower and upper bound as before and show that, again, the multi-tasking model representation nearly saturates the upper bound (Fig. 3 d, right). Thus, not only does the multi-tasking model produce representations with good generalization properties, it also produces representations that lend themselves to the rapid (i.e. few sample) learning of novel tasks.

---

### Universal and uniquely human factors in spontaneous number perception [^5cea3268]. Nature Communications (2017). Medium credibility.

Some researchers have suggested that a combination of non-numeric dimensions is used to quantify sets of objects, instead of representing number directly. To test this we implemented a regression analysis, which included the combination of variables suggested (convex hull, density, cumulative surface area and average diameter) and also included number (equation (12)) on our results from Experiment 1. Again group was entered as a random effects term. If number is represented via these alternative dimensions then we should find no significant effect of number when this collection of variables is included in the model. This is not what we found. Instead, number was a significant predictor over and above all other measures (β number = 1.05, P < 0.001, n = 4,785). This persistent effect of number even when other variables are controlled shows that the effect of number cannot be accounted for by alternative variables. Additionally, the effect of number was significantly greater than any of the variables tested (z -tests: Number versus Convex Hull: z = 7.39, P < 0.001, n = 4,785; Number versus Density: z = 2.79, P < 0.01, n = 4,785; Number versus Area: z = 2.15, P < 0.05, n = 4,785; Number versus Dot Diameter: z = 5.51, P < 0.001, n = 4,785). The results suggest that numerical information is perceived directly, rather than indirectly through the representation of other quantitative dimensions.

---

### Number of samples-hypothesis testing [^69e3a758]. Nuclear Medicine and Biology (2008). Low credibility.

This note reviews "further discussions on choosing the number of animals for an experiment". Particular emphasis will be given to the table therein.

---

### What is Ne, anyway? [^8a832536]. The Journal of Heredity (2022). Medium credibility.

Few doubt that effective population size (Ne) is one of the most important parameters in evolutionary biology, but how many can say they really understand the concept? Ne is the evolutionary analog of the number of individuals (or adults) in the population, N. Whereas ecological consequences of population size depend on N, evolutionary consequences (rates of loss of genetic diversity and increase in inbreeding; relative effectiveness of selection) depend on Ne. Formal definitions typically relate effective size to a key population genetic parameter, such as loss of heterozygosity or variance in allele frequency. However, for practical application to real populations, it is more useful to define Ne in terms of 3 demographic parameters: number of potential parents (adult N), and mean and variance in offspring number. Defined this way, Ne determines the rate of random genetic drift across the entire genome in the offspring generation. Other evolutionary forces (mutation, migration, selection)-together with factors such as variation in recombination rate-can also affect genetic variation, and this leads to heterogeneity across the genome in observed rates of genetic change. For some, it has been convenient to interpret this heterogeneity in terms of heterogeneity in Ne, but unfortunately, this has muddled the concepts of genetic drift and effective population size. A commonly repeated misconception is that Ne is the number of parents that actually contribute genes to the next generation (NP). In reality, NP can be smaller or larger than Ne, and the NP/Ne ratio depends on the sex ratio, the mean and variance in offspring number, and whether inbreeding or variance Ne is of interest.

---

### The snm procedure guideline for general imaging 6.0 [^638aaec4]. SNMMI (2010). Medium credibility.

SNM Procedure Guideline for General Imaging — matrix size and pixel depth in nuclear medicine acquisitions notes that matrix size is almost always a power of 2 with typical values 64 x 64, 128 x 128, 256 x 256 and 512 x 512, and that non-square matrix sizes also exist for whole-body studies. Each pixel can be represented with a single byte (pixel values ranging from 0 to 255 counts) or with 16 bit words (pixel values ranging up to a maximum of 32k or 64k). Overflow occurs when the number of counts recorded at some given position exceeds the maximum number of counts, and overflow is more likely to occur when using a byte matrix.

---

### Universal and uniquely human factors in spontaneous number perception [^7b54d8ff]. Nature Communications (2017). Medium credibility.

Once subjects were trained to accurately categorize the training stimuli, a small percentage of 'probe' stimuli were added in with the presentation of 'standard' training stimuli. On these probe trials, number and cumulative area were systematically uncorrelated so that responses to these stimuli would show which dimension subjects spontaneously used (Fig. 2). On probe trials, all responses were treated as correct and positively reinforced. This procedure allowed us to precisely measure the contribution of each dimension to subjects' underlying categorization of the quantities. If number was the primary basis of subjects' categorization judgments during training, then as number of dots increases, the percentage of trials where subjects select the 'more' category should also increase, regardless of the cumulative area of the dots. In contrast if surface area was the primary basis of categorization then subjects' choice of the 'more' category will increase with the cumulative surface area values of the sets. If both dimensions were used relatively equally, subjects' category choices would be modulated to a similar degree by the number and cumulative surface area values. We found that all groups had a bias to use the numerical dimension of the stimuli regardless of age, culture or species. The number bias was greatly enhanced in humans compared to non-human primates and correlated with age and math education in humans.

---

### Why is a small sample size not enough? [^1f4c7fd4]. The Oncologist (2024). Medium credibility.

Background

Clinical studies are often limited by resources available, which results in constraints on sample size. We use simulated data to illustrate study implications when the sample size is too small.

Methods and Results

Using 2 theoretical populations each with N = 1000, we randomly sample 10 from each population and conduct a statistical comparison, to help make a conclusion about whether the 2 populations are different. This exercise is repeated for a total of 4 studies: 2 concluded that the 2 populations are statistically significantly different, while 2 showed no statistically significant difference.

Conclusions

Our simulated examples demonstrate that sample sizes play important roles in clinical research. The results and conclusions, in terms of estimates of means, medians, Pearson correlations, chi-square test, and P values, are unreliable with small samples.

---

### Response of polygenic traits under stabilizing selection and mutation when loci have unequal effects [^82a8ea8f]. G3 (2015). Low credibility.

Dynamics when most effects are small

As the distribution of quantitative trait loci measured in experimental and natural populations find most effects to be small, it is important to study this situation in detail. Here we have obtained analytical expressions for the dynamics by assuming the genetic variance to be constant. Although the fact that the variance does not change much in time when most effects are small was observed numerically in, an explanation of this behavior was not provided. Here, as explained in Appendix A, it is a good approximation to assume the variance to be time-independent provided the product of the initial values of the mean deviation and skewness is small.

In the absence of mutations, have considered the effect of background with a time-independent genetic variance on the frequency at a single focal locus. Their results match the ones obtained here using the short-term dynamics model with directional selection only; in particular, (11) and (12) match the results (21) and (25) of, respectively, on identifying their parametersand a withand γ from this study.

Our basic result concerning the dynamics of the phenotypic mean is that it relaxes over a time scale that is inversely proportional to the initial variance. Because the variance is of order ℓ, we thus have the important result that the mean approaches the optimum faster if a larger number of loci is involved. Moreover, this time depends nonmonotonically on the shape parameter of the gamma distribution. Note that the phenotypic mean deviation relaxes to zero when the phenotypic optimum is far below the upper boundon the phenotypic mean. However, when the phenotypic optimum exceeds the maximum typical value of the mean, such that the mean deviation remains substantially different from zero at late times, (10b) shows that all higher cumulants vanish at the end of the phase of directional selection.

---

### Number needed to treat (or harm) [^d0b683ae]. World Journal of Surgery (2005). Low credibility.

The effect of a treatment versus controls may be expressed in relative or absolute terms. For rational decision-making, absolute measures are more meaningful. The number needed to treat, the reciprocal of the absolute risk reduction, is a powerful estimate of the effect of a treatment. It is particularly useful because it takes into account the underlying risk (what would happen without the intervention?). The number needed to treat tells us not only whether a treatment works but how well it works. Thus, it informs health care professionals about the effort needed to achieve a particular outcome. A number needed to treat should be accompanied by information about the experimental intervention, the control intervention against which the experimental intervention has been tested, the length of the observation period, the underlying risk of the study population, and an exact definition of the endpoint. A 95% confidence interval around the point estimate should be calculated. An isolated number needed to treat is rarely appropriate to summarize the usefulness of an intervention; multiple numbers needed to treat for benefit and harm are more helpful. Absolute risk reduction and number needed to treat should become standard summary estimates in randomized controlled trials.

---

### The cost of large numbers of hypothesis tests on power, effect size and sample size [^4b1c0c24]. Molecular Psychiatry (2012). Low credibility.

Results

For a fixed targeted effect size and fixed sample size, power decreases as the number of tests and corresponding critical value increase (Table 1, Figure 2a). If the power for a single test is 80%, the power is approximately 50% for 10; 10% for 1000; and 1% for 100 000 Bonferroni-adjusted tests. To avoid a drop in nominal power without increasing sample size, an investigator may target larger effect sizes (Table 1, Figure 2b) using equation (1). For one million tests, the effect size multiplier is 2.25 at 80% power and 2.08 at 90% power. Suppose it has been determined that a sample of 100 yields 80% power to detect a difference in group means when the mean in group 1 is 10 and the mean in group 2 is 12. The original sample size of 100 would also yield 80% power to detect a mean difference of 2.25 × 2 = 4.50, with Bonferroni adjustment for one million tests. The effect size multiplier works on the scale of the underlying Normally distributed test statistic. For example, effect size multiplication for an OR should be carried out after conversion to the natural log scale. Suppose 500 total cases and controls provide 90% power to detect an OR = 1.2, the effect size multiplier for 90% power and 1000 tests is 1.65. Since log e (1.2) = 0.18, the same sample of 500 yields 90% power in 1000 tests to detect a log e (OR) = 1.65 × 0.18 = 0.30 or, equivalently, an OR = exp(0.30) = 1.35.

---

### Optimizing natural fertility: a committee opinion [^098c7f49]. Fertility and Sterility (2013). Medium credibility.

Natural fertility — intercourse frequency: Cycle fecundity was similar with intercourse that occurred daily, every other day, and even every 3 days in the fertile window, but was lowest when intercourse occurred only once in the fertile window. Couples should be informed that reproductive efficiency increases with the frequency of intercourse and is highest when intercourse occurs every 1 to 2 days during the fertile window. Intercourse more frequently than every 1 to 2 days is not associated with lower fecundity, and couples should not be advised to limit the frequency of intercourse when trying to achieve pregnancy.

---

### Global potential for natural regeneration in deforested tropical regions [^ff04e9bc]. Nature (2024). Excellent credibility.

Modelling approach

The statistical modelling and validation is based on a set of six million random points generated within the regeneration and non-regeneration spatial domains defined above, stratified so that each level of the dependent variable was equally represented but with no other form of spatial stratification. For each of these random points, we used geographical information systems to extract values for a range of biophysical and socioeconomic variables (see the 'Predictor variables' section). We based the spatial predictions on the model containing biophysical variables only (the rationale is provided in Supplementary Information 1).

We used randomForeststo fit models of natural regeneration occurrence and estimate spatially explicit predictions of the potential for natural regeneration in the near future (2030), assuming that overall conditions from 2000 to 2016 apply to future scenarios. We adopted a variable-selection procedure to improve parsimony and reduce the high computational burden of making predictions at 30-m resolution over large scales. RandomForests involves stochastic fitting processes, so two models fit to the same data may differ. We therefore fit ten models with all biophysical covariates by randomly selecting 500,000 training records (balanced with respect to the levels of the dependent variable) from a pool of six million records. The variables were ordered by the mean decrease in accuracy resulting from their omission, and the overall ordering of variable importance was based on summing the rank position of each variable among each of these ten models. Following a previous study, we then iteratively fit new models adding one additional variable in order of importance each time to identify the point at which the addition of new variables does not improve model accuracy. The final model was fit using only those variables with one million records. Model validation was based on an independent set of random points that was balanced with respect to the dependent variable. A concern with this approach to validation is that it may overestimate model performance, as it does not account for autocorrelation. We evaluated this effect by quantifying the distance between the validation and training locations and assessing model performance in 1-km distance increments from the training locations.

---

### Using the number needed to treat in clinical practice [^646ae153]. Archives of Physical Medicine and Rehabilitation (2004). Low credibility.

The number needed to treat (NNT) is gaining attention as a method of reporting the results of clinical trails with dichotomous outcome measures. The NNT is defined as the number of patients who would need to be treated, on average, with a specific intervention to prevent 1 additional bad outcome or to achieve 1 desirable outcome in a given time period. Because it reports outcomes in terms of patient numbers, it is extremely useful to clinicians for making decisions about the effort expended with a particular intervention to achieve a single positive outcome. This special communication describes the NNT statistic and its utility for choosing clinical interventions.

---

### Dynamics of ranking [^58e0eeab]. Nature Communications (2022). High credibility.

Virtually anything can be and is ranked; people, institutions, countries, words, genes. Rankings reduce complex systems to ordered lists, reflecting the ability of their elements to perform relevant functions, and are being used from socioeconomic policy to knowledge extraction. A century of research has found regularities when temporal rank data is aggregated. Far less is known, however, about how rankings change in time. Here we explore the dynamics of 30 rankings in natural, social, economic, and infrastructural systems, comprising millions of elements and timescales from minutes to centuries. We find that the flux of new elements determines the stability of a ranking: for high flux only the top of the list is stable, otherwise top and bottom are equally stable. We show that two basic mechanisms - displacement and replacement of elements - capture empirical ranking dynamics. The model uncovers two regimes of behavior; fast and large rank changes, or slow diffusion. Our results indicate that the balance between robustness and adaptability in ranked systems might be governed by simple random processes irrespective of system details.

---

### Clinical decision rules: how to use them [^62ab325a]. Archives of Disease in Childhood: Education and Practice Edition (2010). Low credibility.

The first of this pair of papers outlined what a clinical decision rule is and how one should be created. This section examines how to use a rule, by checking that it is likely to work (examining how it has been validated), understanding what the various numbers that tell us about "accuracy" mean, and considers some practical aspects of how clinicians think and work in order to make that information usable on the front lines.

---

### What counts? Sources of knowledge in children's acquisition of the successor function [^bf36c023]. Child Development (2021). Medium credibility.

Although many U.S. children can count sets by 4 years, it is not until 5½-6 years that they understand how counting relates to number-that is, that adding 1 to a set necessitates counting up one number. This study examined two knowledge sources that 3½- to 6-year-olds (N = 136) may leverage to acquire this "successor function": (a) mastery of productive rules governing count list generation; and (b) training with "+1" math facts. Both productive counting and "+1" math facts were related to understanding that adding 1 to sets entails counting up one number in the count list; however, even children with robust successor knowledge struggled with its arithmetic expression, suggesting they do not generalize the successor function from "+1" math facts.

---

### Graphs [^c6c6d5a7]. Chest (2006). Low credibility.

Two rules of good graphs are presented and explicated.

---

### Development and external validation of Indian population-specific garbhini-GA2 model for estimating gestational age in second and third trimesters [^b85120b4]. The Lancet Regional Health: Southeast Asia (2024). High credibility.

Development of population-specific gestational dating model

We employed multiple approaches to establish the relationship between foetal biometric variables and GA. These included Random Forest and Gradient Boosting machine learning methods implemented in R and multivariable polynomial regression developed using an optimisation technique inspired by the process of natural selection and genetics called "Genetic Algorithm" implemented in MATLAB (Supplementary Methods). We decided to implement these methods because they were the most appropriate for achieving our objectives, which involved identifying the most essential population-specific characteristics to determine GA accurately. Furthermore, these methods are well-suited for addressing the expected collinearity in the data, as we were working with variables associated with foetal growth, which were likely to be correlated and also capture if any non-linear relationships existed between these features. Random Forest and Gradient Boosting models were developed after undergoing hyperparameter tuning, which involved finding the optimal hyperparameter settings. This included parameters such as the number of decision trees and the maximum number of features considered for node splitting in the case of Random Forest and parameters like the number of boosting rounds, the maximum tree depth, and others for Gradient Boosting. The developed models were evaluated using the TEST and VALIDATION datasets (Supplementary Methods; Figure S3).

In the genetic algorithm method, we generated several candidate multivariate polynomial equations using the combinations of the USG variables in the TRAINING set, including their natural logarithms and square root forms (Table S3). Due to the stochastic nature of genetic algorithms, an optimal solution was not guaranteed. Therefore, the algorithm was run ten times independently to ensure reproducibility and consistency. We selected those equations that resolved in more than 5 out of 10 trials. These selected equations were evaluated using the TEST and VALIDATION datasets. The best-performing multivariate polynomial equation was named the Garbhini-GA2 formula.

---

### To portray clonal evolution in blood cancer, count your stem cells [^90574c79]. Blood (2021). Medium credibility.

Clonal evolution, the process of expansion and diversification of mutated cells, plays an important role in cancer development, resistance, and relapse. Although clonal evolution is most often conceived of as driven by natural selection, recent studies uncovered that neutral evolution shapes clonal evolution in a significant proportion of solid cancers. In hematological malignancies, the interplay between neutral evolution and natural selection is also disputed. Because natural selection selects cells with a greater fitness, providing a growth advantage to some cells relative to others, the architecture of clonal evolution serves as indirect evidence to distinguish natural selection from neutral evolution and has been associated with different prognoses for the patient. Linear architecture, when the new mutant clone grows within the previous one, is characteristic of hematological malignancies and is typically interpreted as being driven by natural selection. Here, we discuss the role of natural selection and neutral evolution in the production of linear clonal architectures in hematological malignancies. Although it is tempting to attribute linear evolution to natural selection, we argue that a lower number of contributing stem cells accompanied by genetic drift can also result in a linear pattern of evolution, as illustrated by simulations of clonal evolution in hematopoietic stem cells. The number of stem cells contributing to long-term clonal evolution is not known in the pathological context, and we advocate that estimating these numbers in the context of cancer and aging is crucial to parsing out neutral evolution from natural selection, 2 processes that require different therapeutic strategies.

---

### Number needed to treat and number needed to harm are not the best way to report and assess the results of randomised clinical trials [^f243221a]. British Journal of Haematology (2009). Low credibility.

The inverse of the difference between rates, called the 'number needed to treat' (NNT), was suggested 20 years ago as a good way to present the results of comparisons of success or failure under different therapies. Such comparisons usually arise in randomised controlled trials and meta-analysis. This article reviews the claims made about this statistic, and the problems associated with it. Methods that have been proposed for confidence intervals are evaluated, and shown to be erroneous. We suggest that giving the baseline risk, and the difference in success or event rates, the 'absolute risk reduction', is preferable to the number needed to treat, for both theoretical and practical reasons.

---

### The role of evolution in the emergence of infectious diseases [^f965afb1]. Nature (2003). Excellent credibility.

It is unclear when, where and how novel pathogens such as human immunodeficiency virus (HIV), monkeypox and severe acute respiratory syndrome (SARS) will cross the barriers that separate their natural reservoirs from human populations and ignite the epidemic spread of novel infectious diseases. New pathogens are believed to emerge from animal reservoirs when ecological changes increase the pathogen's opportunities to enter the human population and to generate subsequent human-to-human transmission. Effective human-to-human transmission requires that the pathogen's basic reproductive number, R(0), should exceed one, where R(0) is the average number of secondary infections arising from one infected individual in a completely susceptible population. However, an increase in R(0), even when insufficient to generate an epidemic, nonetheless increases the number of subsequently infected individuals. Here we show that, as a consequence of this, the probability of pathogen evolution to R(0) > 1 and subsequent disease emergence can increase markedly.

---

### Natural genetic variation underlying tiller development in barley (L) [^89bfeb2e]. G3 (2020). Medium credibility.

Figure 3
Minor trade-offs between tiller number and other traits in 2015. A) Percent variance associated with predictor variables in multiple linear regression models of maximum tiller number in 2-rows and 6-rows in 2015. White numbers on bars represent percent variance associated with individual predictor variables (< 2% if number is not shown). Traits shaded in red and blue are positively and negatively associated with tiller number, respectively. Numbers beside bars are total percent variance (R 2) represented by the entire model. B) (Left) Representative density plot illustrating assignment of 2-row and 6-row lines into percentile groups based on maximum and productive tiller number. (Right) Comparison of tiller number in percentile groups based on maximum and productive tiller number. C) Box plots showing traits that were significantly different between percentile groups based on maximum and productive tiller number. D) Box plots of percent productive tillers (tillers that survive and form grain-bearing spikes). Diamonds on box plots represent mean trait values, and letters indicate whether groups were significantly different (Tukey Test, FDR-adjusted p-value, 0.01).

---

### Global potential for natural regeneration in deforested tropical regions [^e3a6982b]. Nature (2024). Excellent credibility.

Tropical natural regeneration potential

We build on the methods used for a recently published pantropical remote sensing analysisthat identified 31.6 ± 11.9 Mha of natural regrowth in 4.78 million patches globally between 2000 and 2012 that persisted to 2016 (hereafter, 2000–2016), with an average patch area of 1.2 ha, to assess the scale and locations where natural regeneration has potential in tropical forest biomes (± 25° latitude). The analysisdistinguished areas of natural regrowth from plantations on the basis of extensive training data and ground-truth information; patches of natural regrowth were defined as at least 0.45 ha in area with vegetation taller than 5 m in height.

Based on a sample of 5.4 Mha of natural regrowth detected previously, we used machine learning methods to distinguish areas in which natural regeneration did or did not occur across the global tropics as a function of a suite of geospatial, biophysical and socioeconomic variables. To predict potential tropical forest regeneration across Earth, we selected predictor variables spanning local (site) and landscape scales (from 30 m to country level) that are known to influence the potential for tropical forest regrowth. These biophysical and socioeconomic variables include distance to nearest tree cover; local forest density in a 1 km 2 area; land cover; 12 soil metrics reflecting conditions in the top 30 cm of soil; 19 bioclimatic variables reduced to 5 principal component axes; slope; net primary productivity; monthly average of burned area due to wildfires over the period of 2001–2017; distance to water; population density; gross domestic product; human development index; road density; distance to urban areas; and protected area status (Supplementary Table 2). Spatially explicit values of predictor variables were then used to model the relative potential for natural regeneration in the present (2015) and in the near future (2030), assuming that overall conditions from 2000 to 2016 apply to future scenarios (Fig. 1). For reporting purposes, we translate the resulting continuous potential for natural regeneration value (0–1) to area-based values by multiplying the relative potential for natural regeneration by the area of each pixel, resulting in a weighted-area value. We provide a sensitivity analysis to this calculation where area is regarded as cells that have greater than 50% potential (Supplementary Table 4). The continuous potential for natural regeneration can be interpreted as the probability of natural regeneration per pixel; the weighted-area value is therefore the expected area of natural regeneration per pixel.

---

### A DNA-based system for selecting and displaying the combined result of two input variables [^325e4170]. Nature Communications (2015). Medium credibility.

Discussion

In this work we have demonstrated examples of a DNA-based lookup table exemplified by the selection of results of a multiplication. It should, however, be clear that we could have chosen any operation that requires two inputs and results in a one- or two-digit output. Since the method can differentiate the order of the inputs, that is, sequence-wise 1 × 2 is different from 2 × 1, the method could also be used for subtraction. In more general terms, it is a method to retrieve a relation between individuals in populations X and Y that has been encoded in the result library. It would also be possible for one combination of inputs X and Y to have selected more results. Since each of inputs X and Y are encoded by 8 nt, the input libraries can principally have 65,536 different members each and the result library can principally have 4.29 billion members. Synthesizing this number of sequences is obviously unrealistic, and cross-reactions between sequences and self-complementarity would be an increasing problem the closer the library numbers come to the principal limits. However, the method has sufficient diversity to cover synthetically realistic numbers of inputs and results.

The translation is also a selection process; however, in contrast to the result selection where two inputs are converted into one result, the translation segregates the result in an array of data that are required for displaying the result. We have demonstrated this concept for the seven-segmented displays, and the mechanism by which the result is translated resembles an electronic calculator. In the translation the result is converted into a binary code that addresses the individual lines of the display (Table 1).

Most of DNA-based logic systems are binary and thus also require binary inputs where a DNA strand represents 0 or 1. The translation process described here would enable the conversion of higher-order information in a DNA input strand to an array of binary numbers.

As the complexity of oligonucleotide-based technologies such as biosensing and regulating systems in synthetic biology increases, more complex and multiplexed output information of such systems becomes available. The method presented here is an example of a method to combine pieces of information encoded in DNA strands and process it to a common result that is displayed in an immediately readable format. For example, two disease-marker oligonucleotide inputs of biologic origin could potentially be combined on a template via the four-way junction method and only if both markers were present they would select a solution from a solution library.

---

### Optimizing natural fertility: a committee opinion [^894bdac0]. Fertility and Sterility (2013). Medium credibility.

Age-related fertility — summary statement: The chances of pregnancy and live birth for women significantly decrease after age 35, as the risks of aneuploidy and miscarriage increase.

---

### Natural genetic variation underlying tiller development in barley (L) [^1b2c8eea]. G3 (2020). Medium credibility.

We considered the possibility that larger trade-offs or trade-offs that were not indicated by correlations or MLR modeling could be identified by comparing traits in lines with extremely different tillering capacities. Therefore, 2-rows and 6-rows were split into 10 th and 90 th percentile groups based on maximum and productive tiller number (Figure 3B). Despite at least 2.5-fold or higher change in average tiller number between percentile groups (Figure 3B), very few traits were significantly different between percentile groups. Stem diameter was lower in high tillering 6-rows (90 th percentile, maximum and productive) than low tillering 6-rows (10 th percentile, maximum and productive) but was not significantly different between high and low tillering 2-rows (Figure 3C). Fifty kernel weight was also lower and lodging severity increased in high tillering 6-rows (90 th percentile, maximum) than low tillering 6-rows (10 th percentile, maximum), but they were not significantly different between high and low tillering 2-rows (Figure 3C). Interestingly, the trend in percent productive tillers between percentiles based on maximum tiller number was reversed in percentiles based on productive tiller number (Figure 3D). This suggests that tiller survival had a major impact on final productive tiller number in 2015 and that variation in tiller survival may alleviate trade-offs between tiller number and other traits. Overall, our results suggest that trade-offs between tiller number and other traits were very minor and were slightly more pronounced in 6-rows than 2-rows, but, in general, there were no major trade-offs between tiller number and other traits independent of spike row-type.

Results from our study and others show that tiller number is consistently lower in 6-rows than 2-rows, possibly due to a trade-off with seeds per spike, which is inherently higher in 6-rows (Figure S2B). However, there was no evidence from our study that more seeds per spike within 2-row or 6-row groups was associated with lower tiller number. Overall, results from this study indicated that trade-offs between tiller number and seeds per spike probably only exist if the difference in seeds per spike is very large, as it is between 2-rows and 6-rows. Similarly, other studies reported no or low association between tiller number and yield-related traits.

---

### The good, the bad and the ugly: what do we really do when we identify the best and the worst organisations? [^16985458]. BMJ Quality & Safety (2024). High credibility.

While many statistical methods exist for identifying the best and worst performers, differences between methods largely concern adjustment for differences in the served populations. We do not address the issue of case-mix or population adjustment but focus on statistical methods used after adjustment. Most performance classification methods belong to one of three categories. The first is simple ranking, where organisations' data are taken at face value and performance classification is based on where an organisation sits in a ranked list of eligible organisations. Simple ranking will preferentially select organisations with a smaller sample size as being the best and worst performers and is suboptimal. Best and worst performers based on simple ranking will differ according to the distribution of the number of observations used for each organisation (figure 1). Although this approach has been used historically, most contemporary examples are found in media reporting or local reporting.

Figure 1
The influence of varying numbers of observations. The above figures show simulated organisational scores on an imagined metric plotted against the number of observations used in each organisation. As expected, there is more variation (scatter) when the number of observations is small, and the influence of chance is high. A wide range of numbers of observations is used in (a), and (b) a restricted range (100 < n < 150) is considered. In each case, the lowest 2.5% of scores, based on simple ranking, are plotted in red. In (a), there is a tendency to identify organisations with fewer observations. Importantly, the organisations flagged as poor performers are not the same when we consider a restricted range of numbers of observations (b). The organisation highlighted with a large blue dot is not flagged in (a) but is flagged in (b). Thus, it is hard to make general statements about the outcome of using simple ranking, as it depends on the range of sample sizes involved. In contrast, z-score and overdispersed z-score-based methods depend only on the sample size of the organisation under consideration. In both panels, the organisation highlighted in blue is flagged as a poor performer using an overdispersed z-score method, but not flagged using a standard z-score based method. Such methods are consistent and can be shown to depend only on reliability (box 1). With a continuous metric, reliability can be thought of as a vertical slice through a funnel plot as indicated for the example above.

---

### The impact of reproductive factors on the metabolic profile of females from menarche to menopause [^7819a2ce]. Nature Communications (2024). High credibility.

Parity

Genetic instruments were selected from a GWAS of number of children ever born, as a proxy of parity, which included 785,604 men and women of European ancestry from 45 studies (Supplementary data 14). Number of children ever born was treated as a continuous measure and included both parous and nulliparous women. Linear regression models were used to estimate the association between genetic variants and number of children ever born adjusting for principal components of ancestry, birth year, its square and cubic, to control for non-linear birth cohort effects. Family-based studies controlled for family structure or excluded relatives. The sex-combined metanalysis also included interactions of birth year and its polynomials with sex. For our analyses, we used the 32 independent SNPs reported by the GWAS to be strongly associated with number of children ever born (P -value < 5*10–8) in either the sex-combined (28 SNPs) or female-specific (4 SNPs) metanalyses and summary association data from the female-specific metanalyses. The GWAS included UK Biobank (maximum estimated sample overlap: 14%).

Age at natural menopause

Genetic instruments were selected from a GWAS of age at natural menopause conducted in 201,323 women of European ancestry (Supplementary data 14). Linear regression models were used to estimate the association between genetic variants and age at natural menopause (in years). For our analyses, we selected 290 SNPs reported by the GWAS to be strongly associated with age at natural menopause (P -value < 5*10–8). Where available, we used association data from the sample combining discovery and replication stages (N = 496,151). Given the age at menarche GWAS included UK Biobank participants (maximum estimated sample overlap: 13% considering the GWAS combined discovery and replication samples), we have also selected an additional set of age at natural menopause-associated genetic variants (N = 42 SNPs) using data from a previous GWAS that did not include UK Biobank (details in 'Sensitivity analyses' below and Supplementary data 14).

---

### The use of plots in orthopaedic literature [^95fed5f5]. The Bone & Joint Journal (2015). Low credibility.

Plots are an elegant and effective way to represent data. At their best they encourage the reader and promote comprehension. A graphical representation can give a far more intuitive feel to the pattern of results in the study than a list of numerical data, or the result of a statistical calculation. The temptation to exaggerate differences or relationships between variables by using broken axes, overlaid axes, or inconsistent scaling between plots should be avoided. A plot should be self-explanatory and not complicated. It should make good use of the available space. The axes should be scaled appropriately and labelled with an appropriate dimension. Plots are recognised statistical methods of presenting data and usually require specialised statistical software to create them. The statistical analysis and methods to generate the plots are as important as the methodology of the study itself. The software, including dates and version numbers, as well as statistical tests should be appropriately referenced. Following some of the guidance provided in this article will enhance a manuscript.

---

### Natural genetic variation underlying tiller development in barley (L) [^a8f7064b]. G3 (2020). Medium credibility.

Results And Discussion

Tiller number in the two- and six-row diversity panel

In 2014 and 2015, 761 lines were grown in the field, and data were collected for weekly and productive tiller number, days to heading, plant height, stem diameter, leaf width (2015 only), seeds per spike, fifty kernel weight, and lodging (2015 only) (Table S3). Fifty-four lines that were grown in 2014 and 2015 were also grown in 2016 in three complete, randomized blocks, and data for weekly and productive tiller number, days to heading, plant height, seeds per spike, and fifty kernel weight were collected (Table S3). Phenotypic data were analyzed in all lines and in subsets of lines based on spike row-type and PPD-H1 alleles. Tiller number data from 2014 and 2015 are summarized in Table 1, and all trait data from all years are summarized in Table S5.

Genetic variance for tiller number was significant (p-value < 0.0001) in 2014, 2015, and 2016 for most time points (Table 1, Table S5). In both years for all line subsets, variance was highest for maximum tiller number and tiller number measured at later time points (5-7WPE), and it decreased for productive tiller number (Table 1). Tiller number at 6WPE, the time point at which maximum tiller number occurred on average for all lines, also had the highest heritability estimate (0.53) of all tiller counts. Decreased heritability from 6WPE to productive tiller number was likely due to variability in tiller survival, which appears to be strongly influenced by environment as genetic variance for percent productive tillers was not significant (Table S5). Heritability estimates for tillering traits were lower than other traits measured (Table S5).

---

### Reconstructing propagation networks with natural diversity and identifying hidden sources [^d752aa57]. Nature Communications (2014). Medium credibility.

Although the number of base strings is relatively small compared with the network size, we need a set of strings at different time with respect to a base string to formulate the mathematical framework for reconstruction. We study how the length of time series affects the accuracy of reconstruction. Figure 3a, b show the success rate as a function of the relative length n t of time series for SIS and CP dynamics on both homogeneous and heterogeneous networks, where n t is the total length of time series from the beginning of the spreading process divided by the network size N. The results demonstrate that even for very small values of n t, most links can already be identified, as reflected by the high values of the success rate shown. Figure 3c, d show the minimum lengthrequired to achieve at least 95% success rate for different network sizes. For both SIS and CP dynamics on different networks, decreases considerably as N is increased. This seemingly counterintuitive result is due to the fact that different base strings can share strings at different times to enable reconstruction. In general, as N is increased, will increase accordingly. However, a particular string can belong to different base strings with respect to the threshold Δ, accounting for the slight increase in the absolute length of the time series (see Supplementary Fig. 4 and Supplementary Note 5) and the reduction in(see Supplementary Note 3 on the method to choose base and subordinate strings). The dependence of the success rate on the average node degree ‹ k › for SIS and CP on different networks has been investigated as well (see Supplementary Fig. 5 and Supplementary Note 5). The results in Figs 2 and 3, Supplementary Figs 2–5 and Table 1 demonstrate the high accuracy and efficiency of our reconstruction method based on small amounts of data.

---

### Quantifying information accumulation encoded in the dynamics of biochemical signaling [^4060ed48]. Nature Communications (2021). High credibility.

Results

Toward quantifying information accumulation encoded in the dynamics of biochemical signaling molecules

Cellular signaling molecules often encode information about the biological condition via the temporal trajectories, or dynamics, of their activities (Fig. 1a, upper). The MI between categorical stimulus conditions and response trajectories quantifies the extent of stimulus-distinction over time (Fig. 1a, lower), and the maximum MI for certain stimulus conditions represents the maximum capacity of discriminating these conditions. For example, bits of maximum MI reveal perfect distinction on M different stimulus conditions. The rate or temporal profile with which cells acquire the information to discriminate specific stimulus conditions remains largely unexplored.

Fig. 1
Toward quantifying information accumulation encoded in the dynamics of biochemical signaling molecules.

a (Upper) When cells encounter a stimulus, the responses of a major signaling molecule transmit information to the nucleus so that cells respond appropriately. The temporal trajectories of single-cell signaling response can have rich dynamics, such as the two exemplified trajectories under each separately added stimulus. (Lower) The maximum mutual information (MI) between the responses and the stimulus conditions quantifies the number of conditions that can be effectively distinguished. If the information is perfectly transmitted, it would reachbits for M distinct stimulus conditions. The information transmits over time, and the temporal profile of cells acquiring the information for stimuli-discrimination remains to be explored. b An overview of the three representative approaches of quantifying the mutual information. c The workflow of the present approach. The measured trajectory ensemble under each stimulus condition can be learned by training a time-inhomogeneous Markov model or a hidden Markov model (circles are hidden states and squares are emission states in the model schematic). Under the optimal number of states, such stochastic models enable to infer the trajectory probabilities and develop the dynamical mutual information (dMI), which quantifies the mutual information encoded in signaling dynamics cumulatively.

---

### Natural genetic variation underlying tiller development in barley (L) [^5f1d10c7]. G3 (2020). Medium credibility.

Tiller number was compared using data for the 54 lines (27 2-rows and 27 6-rows) grown in all three years. Due to waterlogging in the field early in development in 2014, the onset of tiller development was delayed and maximum and productive tiller number was much lower than 2015 and 2016 (Figure 1A, B). By 2WPE in 2014, 25.4% of all lines grown had not yet developed at least one tiller per plant on average, whereas all lines grown in 2015 had developed at least one tiller per plant by 2WPE. Maximum tiller number was not significantly different between 2015 and 2016, but productive tiller number was lower in 2016 than 2015 due to lower tiller survival (Figure 1B). Despite differences between years, they all followed a similar trend where average tiller number increased linearly until 5WPE, after which it either slowed or began decreasing (Figure 1A).

Figure 1
Overview of tiller development (tillering) and other traits. A) Progression of average tiller number throughout the growing season for 54 lines grown in 2014, 2015, and 2016. B) Box plots summarizing tillering traits for 54 lines grown in all three years. Diamonds represent mean trait values, and letters indicate whether groups are significantly different based on FDR-adjusted p-values from ANOVA in conjunction with Tukey Test. C) Box plots summarizing non-tillering traits show similar relationship between years as average productive tiller number.

Average plant height, stem diameter (measured in 2014 and 2015 only), seeds-per-spike, and fifty kernel weight followed a similar trend as productive tiller number across the three years, where trait values were highest in 2015 and lowest in 2014 (Figure 1C). In years when plants developed more productive tillers on average, they were also taller with thicker stems, more seeds per spike, and heavier seeds on average (Figure 1C), indicating that productive tiller number is correlated with overall plant fitness.

---

### Optimizing natural fertility: a committee opinion [^63aa3a95]. Fertility and Sterility (2013). Medium credibility.

Clinical pregnancy probability — cycle day and age: The probability of clinical pregnancy increases from 3.2% on cycle day 8 to 9.4% on cycle day 12 and decreases to less than 2% on cycle day 21, and the likelihood of success decreases with increasing age.

---

### Methodology for ACOEM's occupational medicine practice guidelines-2025 revision [^5b3929ab]. ACOEM (2025). High credibility.

Newcastle - Ottawa Quality Assessment Scale — case control studies scoring is defined as follows: A study can be awarded a maximum of one star for each numbered item within the Selection, Exposure, and Outcome categories, with a maximum of two stars for Comparability and a maximum of 9 points overall. Under Selection, items include 1) Is the case definition adequate? with options a) yes, with independent validation; b) yes, e.g. record linkage or based on self reports; c) no description; 2) Representativeness of the cases with options a) consecutive or obviously representative series of cases; b) potential for selection biases or not stated; and 3) Selection of Controls with options a) community controls; b) hospital controls; c) no description.

---

### Natural genetic variation underlying tiller development in barley (L) [^cfd9f701]. G3 (2020). Medium credibility.

Days to heading and spike row type were associated with a high proportion of variance in tiller number models (Figure 2A). Together they were associated with 28% of the total variance in maximum tiller number models and 12% of the total variance in productive tiller number models (Figure 2A). Interestingly, a very small proportion of variance in the productive tiller number model was associated with days to heading (1.9%) (Figure 2A), probably due to variability in tiller survival between lines. Furthermore, average differences in tiller survival represented by percent productive tillers between 2-rows and 6-rows (Figure S2A) could explain why seeds per spike accounted for a larger proportion of variance in productive tiller number than maximum tiller number.

Principal coordinates (PCo) analysis based on tiller number throughout development and productive tiller number also indicated that a large proportion of variance in tiller number was associated with days to heading and spike row-type. Groups based on spike row-type and days to heading were more strongly correlated than any other single trait with PCo1 (R = 0.59, P < 2.2e-16), which accounted for 86% of the total variance in the PCo model (Figure 2B). Furthermore, although 6-rows produced fewer tillers on average than 2-rows, maximum tiller number in late heading 6-rows (> 60 days) was not significantly different from earlier heading 2-rows (< 60 days), indicating that high tiller number can be achieved in late heading 6-rows (Figure 2C).

Trade-offs between tillering and other traits

Tiller number and other traits were compared to evaluate trade-offs associated with high tiller number. Because spike row-type is correlated with tiller number and other traits, trade-offs were assessed separately in 2-row and 6-row subsets and using 2015 data only for the same reasons as previously described. Results of MLR modeling indicated minor trade-offs between tiller number and other vegetative traits. Leaf width and stem diameter were associated with a significant proportion of variance in productive and maximum tiller number MLR models (Figure 3A), and their coefficients were consistently negative, indicating a tendency for leaf width and stem diameter to decrease as tiller number increased. Both traits were also weakly, negatively correlated with productive tiller number (Table 2 and Table S6).

---

### Natural language processing models reveal neural dynamics of human conversation [^346bb012]. Nature Communications (2025). High credibility.

In addition, to examine whether the neural-NLP correlation is modulated by only short or long durations of speech before transitions, we categorized word sequences into short or long durations using a 50-percentile threshold. Even after this separation, we found a significantly higher percentage of responding channels for both groups than chance (T -test, p < 2 × 10 −14 for both speaking and listening), suggesting that the correlation is not caused by either short or long durations.

To address potential biases due to unequal word counts between speech production and comprehension, we matched the number of words in each condition for each participant by setting a cutoff equal to the minimum word count for speaking and listening. We found that there was a slight numerical difference in the percentage of significant channels between speech production (10%) and comprehension (13%) when word counts were matched, with conditions a substantially higher percentage of significant channels compared to chance (Chi-square proportion test, statistic > 100, p < 2 × 10 −23 for both conditions, Fig. S16). These findings are largely consistent with our main analysis without word number matching (compared to 11% for production and 15% comprehension), indicating that the variation in word number does not drive the apparent difference between channels responding to speaking and listening.

Finally, to ensure the observed percentage of correlated channels was not limited only to Pearson correlations, we conducted additional control analysis by performing non-parametric hypothesis tests for rank correlations. Specifically, we used the Kendall Tau test and Spearman R to examine the correlations between channels and NLP embeddings. Using the Kendall Tau test, we found there were 11.3% channels responding to NLP during language production, whereas there were 12.6% to comprehension (Chi-square proportion test, statistic = 150.0, p = 1.1 × 10 −36 for speaking; statistic = 230.1, p = 5.5 × 10 −52 for listen). A similar percentage of channels were selected using the Spearman R test (speaking: 11.8%, statistic = 187.7, p = 1.0 × 10 −42; listening: 12.9%, statistic = 253.0, p = 5.8 × 10 −57). These together, indicate that the channel-NLP correlations were not limited to specific statistical methods, and were generalizable to various methods for correlation tests.

---

### MIRD pamphlet no. 27: MIRDcell V3, a revised software tool for multicellular dosimetry and bioeffect modeling [^95177843]. Journal of Nuclear Medicine (2022). Medium credibility.

FIGURE 4.
Complex radiobiologic parameters.

From the "Multicellular Geometry" tab, the "3-D Cluster" tab is selected and the radius of the cluster is set to 125 μm. The distance between cells is adjusted until the number of cells matches the experimental observations (described in the "Complex Radiobiological Parameters" section). A drug penetration depth of 12 μm is set, and a radial exponential activity distribution is selected from the drop-down labeling-method menu; the exponential factor is set to 0.4. Since the drug penetrates to only a single cell layer (∼12 μm), the selection of the activity distribution has minimal effect on the rest of the cluster. The time-integrated activity coefficient is set to T p /ln(2) = 1.11 h (where T p is the physical half-life of the radionuclide). Even though what ultimately matters is the product of the time-integrated activity coefficient and the maximum mean activity per cell, which provides the mean number of decays per cell (after correcting h to s), it is helpful to know the time-integrated activity coefficient for reasonability checks. The percentage of cells that are labeled in MIRDcell is set to 100%. The maximum mean activity per cell (all cells) (Bq) is adjusted until the maximum mean absorbed dose to cells in the MIRDcell SF curve matches the maximum average dose (Gy) given in Figure 6 of Kennel et al. When the "Compute" button is clicked the first time, an error message will pop up indicating that 100% of the cells cannot be labeled because this number exceeds the number of cells within the drug penetration depth; the percentage of labeled cells will be automatically set to the maximum number allowed when the error message is accepted. Therefore, a good rule is to let the program decide the percentage of labeled cells when a drug penetration depth is specified. Alternatively, if there is a specific desired percentage, the value can be set before the "Compute" button is clicked. Once the model is run, the SF as a function of different domains can be visualized under the "SF Curve" tab. The maximum mean activity per cell that matched the desired absorbed dose was 0.02 Bq (Fig. 5). The radial activity histograms and tomographic sections (Fig. 6) of the selected cell cluster geometry are displayed in the "Radial Histogram" and "3-D Slice" tabs, respectively. Tomographic sections of each cell layer can be displayed by specifying the value, in terms of cell diameters, in the box labeled "Axial Height." Alternatively, the sections can be scrolled through using the wheel of the mouse.

---

### Current sample size conventions: flaws, harms, and alternatives [^8e4148bc]. BMC Medicine (2010). Low credibility.

Background

The belief remains widespread that medical research studies must have statistical power of at least 80% in order to be scientifically sound, and peer reviewers often question whether power is high enough.

Discussion

This requirement and the methods for meeting it have severe flaws. Notably, the true nature of how sample size influences a study's projected scientific or practical value precludes any meaningful blanket designation of < 80% power as "inadequate". In addition, standard calculations are inherently unreliable, and focusing only on power neglects a completed study's most important results: estimates and confidence intervals. Current conventions harm the research process in many ways: promoting misinterpretation of completed studies, eroding scientific integrity, giving reviewers arbitrary power, inhibiting innovation, perverting ethical standards, wasting effort, and wasting money. Medical research would benefit from alternative approaches, including established value of information methods, simple choices based on cost or feasibility that have recently been justified, sensitivity analyses that examine a meaningful array of possible findings, and following previous analogous studies. To promote more rational approaches, research training should cover the issues presented here, peer reviewers should be extremely careful before raising issues of "inadequate" sample size, and reports of completed studies should not discuss power.

Summary

Common conventions and expectations concerning sample size are deeply flawed, cause serious harm to the research process, and should be replaced by more rational alternatives.

---

### Sample size estimation: current practice and considerations for original investigations in MRI technical development studies [^ff5f6795]. Magnetic Resonance in Medicine (2021). Medium credibility.

Purpose

To investigate and to provide guidance for sample size selection based on the current practice in MR technical development studies in which healthy volunteers are examined.

Methods

All original articles published in Magnetic Resonance in Medicine between 2017 and 2019 were investigated and categorized according to technique, anatomical region, and magnetic field strength. The number of examined healthy volunteers (ie, the sample size) was collected and evaluated, whereas the number of patients was not considered. Papers solely measuring patients, animals, phantoms, specimens, or studies using existing data, for example, from an open databank, or consisting only of theoretical work or simulations were excluded.

Results

The median sample size of the 882 included studies was 6. There were some peaks in the sample size distribution (eg, 1, 5, and 10). In 49.9%, 82.1%, and 95.6% of the studies, the sample size was smaller or equal to 5, 10, and 20, respectively.

Conclusion

We observed a large variance in sample sizes reflecting the variety of studies published in Magnetic Resonance in Medicine. Therefore, it can be concluded that it is current practice to balance the need for statistical power with the demand to minimize experiments involving healthy humans, often by choosing small sample sizes between 1 and 10. Naturally, this observation does not release an investigator from ensuring that sufficient data are acquired to reach statistical conclusions.

---

### How do you design randomised trials for smaller populations? A framework [^9ac3f0db]. BMC Medicine (2016). Low credibility.

Exploring less common approaches to reducing sample size

We now consider some less standard approaches to bringing the sample size requirements closer to the numbers it is feasible to recruit in a reasonable time frame.

Step 3: Relaxing α by a small amount, beyond traditional values

The much-criticised 5% significance level is used widely in much applied scientific research, but is an arbitrary figure. It is extremely rare for clinical trials to use any other level. It may be argued that this convention has been adopted as a compromise between erroneously concluding a new treatment is more efficacious and undertaking a trial of an achievable size and length. Settings where traditionally sized trials are not possible may be just the area where researchers start to break this convention, for good reason.

In considering the type I error, it is critical to consider the question: 'What are the consequences of erroneously deciding to use a new treatment routinely if it is truly not better?'

Taking the societal perspective as before, we might consider the probability of making a type I error, thus erroneously burdening patients with treatments that do not improve outcomes, or even worsen them, while potentially imposing unnecessary toxicity.

First, for conditions where there are only enough patients available to run one modestly sized randomised trial in a reasonable time frame, research progress will be relatively slow, and making a type I error may be less of a concern than a type II error. In contrast, making several type I errors in a common disease could lead in practice to patients taking several ineffective treatments; for a disease area where only one trial can run at any given time, the overall burden on patients is potentially taking one ineffective treatment that does not work.

Thus, if we take the societal perspective with the trials in Table 1 then, if each trial was analysed with α = 0.05 and we see (hypothetically) 40% positive results, then the expected number of false positive trials is given in the final column. We also assumed 10% and 70% positive results, with qualitatively similar conclusions.

---

### Skillful multiyear predictions of ocean acidification in the California current system [^980bad1e]. Nature Communications (2020). High credibility.

We compute the anomaly correlation coefficient (ACC) via a Pearson product-moment correlation to quantify the linear association between predicted and target anomalies (where the target is either the model reconstruction or the observational product). If the predictions perfectly match the sign and phase of the anomalies, the ACC has a maximum value of 1. If they are exactly out of phase, it has a minimum value of −1. The ACC is a function of lead time, :Where F ′ is the forecast anomaly, O ′ is the verification field anomaly, and the ACC is calculated over the initializations spanning 1954–2017 (N = 64) relative to the reconstruction and CESM-LE, and over initializations covering 1990–2005 (N = 16) relative to the JMA observational product. We quantify statistical significance in the ACC using a t test at the 95% confidence level with the null hypothesis that the two time series being compared are uncorrelated. We follow the methodology of Bretherton et al. using the effective sample size in t tests to account for autocorrelation in the two time series being correlated:Where N is the true sample size andandare the lag 1 autocorrelation coefficients for the forecast and verification data. We assess statistical significance between two ACCs (e.g. between that of the initialized forecast and a simple persistence forecast for the same lead time) using a z test at the 95% confidence level with the null hypothesis that the two correlation coefficients are not different.

To quantify the magnitude of forecast error, or the accuracy in our forecasts, we use the normalized mean absolute error(NMAE), which is the MAE normalized by the interannual standard deviation of the verification data. The NMAE is 0 for perfect forecasts, < 1 when the forecast error falls within the variability of the verification data, and increases as the forecast error surpasses the variability of the verification data. MAE is used instead of bias metrics such as the root mean square error (RMSE), as it is a more accurate assessment of bias in climate simulations. Where N is the number of initializations andis the standard deviation of the verification data over the verification window.

---

### Neighbourhood level real-time forecasting of dengue cases in tropical urban Singapore [^abf44f56]. BMC Medicine (2018). Low credibility.

For each forecast window (from k = 1 to 12 weeks), a separate LASSO sub-model was developed, which used data available at the time of the forecast only. Each LASSO sub-model is as follows:where y t, i is the number of cases (natural log-transformed, with 1 added to avoid logging 0) in neighbourhood i in week t. The terms, andare the square, cubic and square root of the number of cases. Similarly, n t, i, 1 and n t, i, 2 are the total number of cases (similarly, natural log-transformed, with 1 added to avoid logging 0) from all neighbourhoods whose centroids are within 1 km radius and within a ring from 1 km to 2 km from the centroid of neighbourhood i, in week t, respectively. W t, i, c represents the climatic variable (average, minimum and maximum temperature, and average relative humidity) at time t in neighbourhood i. T t, i measures the number of cases moving into neighbourhood i in week t, derived from a one-time telco dataset on the movement of users. A t, i measures average building age in neighbourhood i in week t. N t is the national total number of cases (natural log-transformed, with 1 added) in week t. V i and U i measure the vegetation and connectivity index of neighbourhood i. Detailed information on the type of each set of variables are documented in Additional file 3: Table S1. Covariates in the LASSO regression were z -scored prior to estimation and the coefficients were rescaled afterwards.

Parameter estimation was subject to the LASSO constraint: Ten-fold cross validation was performed and the constraint term that optimised the out-of-sample performance was chosen as the optimal p for the forecast model.

As the models were built separately for each forecast window, the variables included in the final forecast model and their lags and parameter magnitude and sign may differ substantially.

LASSO models were built using all the data from the training dataset, which comprised information from 2010 to 2015. Out-of-sample validation was performed on the testing dataset consisting of data from 2016.

---

### Research techniques made simple: sample size Estimation and power calculation [^b2b6b836]. The Journal of Investigative Dermatology (2018). Low credibility.

Sample size and power calculations help determine if a study is feasible based on a priori assumptions about the study results and available resources. Trade-offs must be made between the probability of observing the true effect and the probability of type I errors (α, false positive) and type II errors (β, false negative). Calculations require specification of the null hypothesis, the alternative hypothesis, type of outcome measure and statistical test, α level, β, effect size, and variability (if applicable). Because the choice of these parameters may be quite arbitrary in some cases, one approach is to calculate the sample size or power over a range of plausible parameters before selecting the final sample size or power. Considerations that should be taken into account could include correction for nonadherence of the participants, adjustment for multiple comparisons, or innovative study designs.

---

### Plants capable of selfing are more likely to become naturalized [^898f6c8a]. Nature Communications (2016). Medium credibility.

As in the previous models, we separately analysed naturalization incidence (whether the species have been recorded as naturalized outside their native ranges), and for the subset of only the naturalized species, their naturalization extent (the number of regions where they are naturalized). For the analysis of the binary variable naturalization incidence, we used a diagonally weighted least square estimation method with robust standard errors. For the analysis of naturalization extent, we log-transformed the number of regions where the species is native to and the number of regions, where the species is naturalized, and used a maximum likelihood estimation method with robust standard errors (MLR). To facilitate comparisons among the different path coefficients, we standardized all continuous variables to a mean of zero and s.d. of 1. We assessed model fit using a relative measure, the Comparative Fit Index, and an absolute measure, the root mean square error of approximation. A Comparative Fit Index > 0.95 and a root mean square error of approximation < 0.06 are considered good. In addition, we reported R 2 for the endogenous variables naturalization incidence and naturalization extent. We did the path analysis using the 'lavaan' packagein R. The results remained qualitatively similar when we used one average index per species (Supplementary Fig. 5).

Data availability

Data for this article, including four selfing-ability indices and information on data sources, species name, life history, native range and global naturalization as assessed based on the Global Naturalized Alien Flora database may be found in the Supplementary Information (Supplementary Data 1). Fruit set and seed production data in pollinator exclusion, self-pollination and cross-pollination treatments are available from the corresponding author upon request.

---

### Deep learning driven biosynthetic pathways navigation for natural products with bioNavi-NP [^76b3c282]. Nature Communications (2022). High credibility.

Data set

MetaNetXintegrated a number of major resources of metabolites and biochemical reactions including MetaCyc, KEGG, The SEED, Rhea, BiGGand so on. However, the information regarding reaction directionality and compartmentalization were generally disregarded, which are important to predict the precursors of the NPs. So we first extracted reactions directly from MetaCyc (version 23.5) pathway ontology and KEGG (accessed in January, 2021) pathway maps, where the "irrelevant" agents such as cofactors and minor substrates or products were removed. These reactions were excluded from MetaNetX (version 4.1) dataset, and following the previous work, we derived pairs of precursors and metabolites where the maximum common substructure exceeds 40% of the atoms of the metabolite for the rest of the reactions. Then, multi-product reactions were decomposed to multiple mono-product reactions with the same substrates and the cofactors were removed. Compounds containing Coenzyme A (CoA) play an important role in the biosynthesis of natural products especially polyketides and lipids. Although CoA is a complex structure that contains 84 heavy atoms, it is usually not involved in the reaction center, so the CoAs in the molecules were replaced by "" in the SMILES format and restored when the results were output. Considering that the stereochemistry is often poorly annotated in the structure databases, we firstly checked the structures in our reaction dataset. The potential stereochemical centers exist in 15,372 out of 20,710 structures, and 10,909 (71.0%) of them are completely annotated with the stereochemistry, 12,763 (83.0%) are annotated with at least half of the stereochemistry. Thus the stereochemistry was preserved in our dataset. The resulting BioChem dataset consists of 33,710 unique pairs of precursors and metabolites, 1000 of which were selected as the test set and another 1000 as the validation set (see Supplementary Fig. 2). Additionally, in order to learn more syntactic rules for chemical reactions, we screened reactions with components similar to natural products from the organic chemistry dataset for model pre-training. Specifically, we used molecular ECFP4 fingerprintsto select the reactions with a maximum similarity of ≥ 0.8 (calculated by RDKit) between any component (reactants and/or products) of each reaction in the USPTO database and the NPs from the DNPdatabase (version 27.2), resulting in 60,000 chemical reaction equations. The USPTO_NPL dataset was merged with the BioChem dataset to train the single-step model.

---

### Multi-condensate state as a functional strategy to optimize the cell signaling output [^ae4481a7]. Nature Communications (2024). High credibility.

Now we consider a hemispheroidal cluster which has HemiSphericity. Supplementary Fig. 6 showed how to create hemispheroidal clusters with a shape parameter, where. For a perfect hemisphere, With lowering the value ofone can create clusters with arbitrary HemiSphericity, such that,

For a fixed number of Arp2/3, due to higher surface-to-volume ratio (Supplementary Fig. 7), surface density of Arp2/3 goes down for lower hS (Supplementary Text 3). Figure 4D reflects this trend where the same number of Arp2/3 is recruited onto the clusters with varying hS.

We next want to investigate the causal factor underlying the greater response coming from the MCS. Figure 3 informed us that if we split a large cluster into four smaller clusters and distribute N Arp2/3 equally into each cluster (such that each smaller cluster retains N/4), the four-cluster scenario produces a higher F-actin response compared to the single large cluster. Although the volumes of clusters are the same in both cases, the surface-to-volume ratio increases as we split one hemisphere into n smaller ones. To probe this effect further, we first derive an expression for local F production (F per cluster in the n-cluster system) (Supplementary Text 4), WhereWhere

The shape parameter, decides the HemiSphericity of the cluster. For a perfect hemisphere, Using Eq. (5), we compute the local F as a function of cluster number (n). Figure 4E shows a rather interesting trend. As we increase n, initially the drop in per-cluster F is minimal. Consequently, on the cumulative global scale (Fig. 4F), we have a gradual increase in F production. We note the logarithmic scale along both the axes. As we go to large cluster numbers, beyond a certain value, local F starts to drop quickly which also drags down global production. The differential rate at which local F drops as a function of n yields the most fascinating aspect of the global F trend — an optimal cluster number (n optimal). Figure 4F suggests that there exists a multi-cluster configuration where the F-actin production, on the global or system level, is maximum. This behavior is consistent with a wide range of hS. Although, with lower hS, the maximal level of global F gradually drops and n optimal shifts towards lower values (Fig. 4F).

---

### Juno spacecraft gravity measurements provide evidence for normal modes of jupiter [^5f37f190]. Nature Communications (2022). High credibility.

Model selection: AIC

We performed the task of model selection using the AIC. This well-established statistical criterion is based on the use of entropy as a measure of information and provides a quantitative value of the information loss when a given data fitting model is selected. It selects models using their likelihood L in fitting the Doppler data while favoring low complexity models, expressed by the associated regression-effective degree of freedom k:

AIC consists of a trade-off between the goodness of fit and the simplicity of the model itself. Its minimization requires the maximization of the (log-) likelihood function of the model to the data, while keeping the solution's degrees of freedom to minimum (low value of k). See dedicated subsection in Methods for how to compute the number of degrees of freedom of a given dynamical solution. In the case of a least square estimation filter, by substituting the corresponding likelihood function, the AIC can be computed as:With n obs being the number of Doppler data points, RSS obs their root-sum-square, normalized with the noise standard deviation, and C a constant which depends only on the number of data points (which is unchanged in the different solutions we aim to compare).

Given the possibility of incurring into a solution with a large difference between estimated and model amplitudes for the normal modes, the likelihood is constructed as the product of the data-driven and amplitude-based likelihoods. The additional, amplitude-based, likelihood term is:

here n modes is the number of modes estimated (for all the arcs), RSS modes the root-sum-square of the difference between the model and estimated amplitudes of the modes, normalized to the uncertainty of the mode amplitude, and = RSS modes / n modes is the maximum likelihood estimate of the variance of a model's residuals distribution. This term becomes important when the data are not compatible with the amplitude of the modes we try to impose. The final AIC function reads:

Since only differences in AIC among different solutions are important, we select the constant C to have AIC equal to zero for the best solution found in the analysis. Gravity solutions with a small AIC value are selected as the best models. Generally, solutions having ΔAIC larger than ~500 still show signature in the Doppler data of a few passes.

---

### Current sample size conventions: flaws, harms, and alternatives [^2ab4a8e1]. BMC Medicine (2010). Low credibility.

Sensitivity analysis

Sample size planning involves considerable uncertainty, and a simple and familiar way of assessing uncertainty is with sensitivity analyses: examining how results change under different assumptions. I propose a framework, illustrated in Table 1, for presenting a useful array of possibilities for a study's most important products, the estimated effect and its confidence interval. This is based on varying 1) assumptions that determine the precision of estimates and 2) the observed effect size. Together with discussion of the potential value of each resulting outcome, this provides an informal assessment of the value of the information that may result. This is less systematic than the value of information methods mentioned above, but it covers a range of likely scenarios, avoids technical difficulties of the mathematically formalized methods, and focuses on particular concrete results, which allows reviewers to easily assess the claimed potential value. Because the table entries show the information that would be used in a systematic review, the potential value can be discussed in terms of how it would modify a recent review or contribute to future ones, if those are deemed to be the most important considerations.

Table 1
Sample layout of sensitivity analysis.

Shown are possible study results with a given sample size (935 per group, based on the vitamin study discussed above), for a yes or no outcome. Rows have differing assumptions concerning precision of the estimates, ranging from high precision (top row) to low precision (bottom row). For a continuous outcome, the rows would instead be for optimistic (small), expected, and pessimistic (large) standard deviations.

The entries in the table are exactly the key results that interpretation should focus on when the study is completed, so this properly aligns planning with eventual use. The middle row can be a best guess such as would be used for conventional calculations; the other rows should reflect a reasonable range of uncertainty, which will depend on what is already known about the topic being studied. For the columns, inclusion of the intermediate case is important, because this will often include the most problematic or disappointing potential results. The vitamin study paired a safe and inexpensive intervention with a severe outcome, so even results in the middle column would be regarded as encouraging; the actual completed study landed essentially in Box 7, which should have been interpreted as very encouraging even though not definitive. Boxes 8 and 9 will usually be the least useful, but as noted above (False assurance), the risk of disappointing results is always present and should not be considered a flaw in study design.

---

### Selective fishing induces density-dependent growth [^2560991d]. Nature Communications (2014). Medium credibility.

Over the last decades, views on fisheries management have oscillated between alarm and trust in management progress. The predominant policy for remedying the world fishing crisis aims at maximum sustainable yield (MSY) by adjusting gear selectivity and fishing effort. Here we report a case study on how striving for higher yields from the Eastern Baltic cod stock by increasing selectivity has become exceedingly detrimental for its productivity. Although there is a successive increase in numbers of undersized fish, growth potential is severely reduced, and fishing mortality in fishable size has increased. Once density-dependent growth is introduced, the process is self-enforcing as long as the recruitment remains stable. Our findings suggest that policies focusing on maximum yield while targeting greater sizes are risky and should instead prioritize catch rates over yield. Disregarding the underlying population structure may jeopardize stock productivity, with dire consequences for the fishing industry and ecosystem structure and function.

---

### Single photon emission computed tomography (SPECT) myocardial perfusion imaging guidelines: instrumentation, acquisition, processing, and interpretation [^c6205197]. Journal of Nuclear Cardiology (2018). Medium credibility.

Computed tomography (CT) calibration and uniformity for SPECT/CT attenuation correction require daily and periodic checks with defined HU tolerances. The CT calibration is checked daily with a water-filled cylinder, usually 24 cm in diameter provided by the manufacturer, and if the error is greater than 5 to 10 HU (i.e., different than 0 HU), the CT system is considered to be out of calibration; an air calibration (blank-scan) can be tried and, if unresolved, the manufacturer's field service engineer must be called, and calibration must be checked by a qualified person on an annual basis or after any major repair or calibration. For field uniformity, small square or circular ROIs are placed at 90° intervals near the edge of the cylinder image and compared with the center, and the maximum difference in mean region CT number should not exceed 5 HU or the manufacturer's specified value, since nonuniformities greater than this may affect SPECT AC based on the CT image.

---

### Super-emitters in natural gas infrastructure are caused by abnormal process conditions [^6eca421e]. Nature Communications (2017). Medium credibility.

Methods for compression systems

Supplementary Fig. 11 shows a flow chart summarizing the methodology followed to estimate emissions from compressors. There are four major sources of emissions related to compression systems on production sites: fugitive emissions, engine exhaust emissions, compressor start-ups and compressor blowdowns.

To determine the total number of compressors at natural gas production sites in the region, we derived from the GHGRPan activity factor (for the Barnett Shale) of 0.14 compressors per well. For a total number of 25,700 wells in the region we estimated 3,600 compressors. We determined a distribution for the number of compressors per production site using the Texas Commission on Environmental Quality (TCEQ)'s 2009 Barnett Shale Special Emissions Inventory (BSEI). For the subset of sites in the inventory that reported at least one compressor, the average was 1.2 compressors per site with a range of 1 to 5. We used this distribution to assign a specific number of compressors to randomly selected sites until we reached the total count of compressors in the region. The number of compressors at each site never exceeded the well count. For each site, we then estimated total compressor emissions (considering fugitives, engine exhaust, start-ups and blowdowns).

---

### Measuring the burden of hundreds of bioBricks defines an evolutionary limit on constructability in synthetic biology [^ebec7d3b]. Nature Communications (2024). High credibility.

Burden analysis

To analyze the burden assay data for one plate, we first subtracted the average values of all media blanks from the OD and fluorescence measurements. Next, to deal with well-to-well variation in background levels, we shifted the values to force the means of the points over the first hour of measurements for each strain to match the grand mean for those data points over all replicates of that strain. We then fit growth rates using nonlinear least-squares regression to an exponential model: C (t) = C 0 e rt. We assumed that OD is directly proportional to the number of cells at a given time, C (t). C 0 is the initial number of cells, and r is the specific growth rate. We fit C 0 and r for all sets of nine consecutive measurements (a 90-min window in the standard assay) after the OD exceeded 0.03 and recorded the largest value of r as the maximum specific growth rate for that strain. To determine the fluorescent protein (e.g. GFP) production rate per cell, p, we repeated this procedure while fitting fluorescence values to the equation: F (t) = F 0 + C 0 (p/ r) (e rt − 1). F 0 is the initial fluorescence and F(t) is the fluorescence at time t. This equation is derived by integrating the relationship dF / dt = p C(t). We fit F 0 and p in this model to the data while keeping C 0 and r fixed to the values determined from the OD curve fit for the corresponding time window. Again, we recorded the largest value of p across all time points as the maximum fluorescent protein production rate.

---

### Graphs, tables, and figures in scientific publications: the good, the bad, and how not to be the latter [^a3a1cb03]. The Journal of Hand Surgery (2012). Low credibility.

Graphs, figures, and tables can save readers time and energy, aid their understanding of an article, and reduce the word count of the main text. However, many graphics submitted to and published in scientific journals fail to meet their potential and include mistakes that jeopardize their clarity. Many formats are available for presenting data, as well as a variety of techniques for enhancing interpretability. When the appropriate format is used to depict data, it conveys the greatest amount of information in the clearest fashion, complements the text, and deepens readers' understanding. The aims of this article are to draw attention to the necessity of well-constructed graphs, tables, and figures in scientific publications, and to show how to create them.

---

### 111In-leukocyte scintigraphy for suspected infection / inflammation 3.0 [^7c3eefef]. SNMMI (2024). High credibility.

111In-leukocyte quality control includes determining labeling efficiency by recentrifugation (approximately 450 g for 5 min) of washed cells resuspended in 5 mL of buffered saline, with the leukocyte pellet resuspended in 5 mL of cell-free plasma; leukocyte clumping may be checked on a hemocytometer slide; and the average number of cells per 50-micron (small) square is then determined with No. cells/cm3 (mL) = average number of cells/small square x (2 x 10^6).

---

### Sample sizes based on three popular indices of risks [^4a61a58f]. General Psychiatry (2018). Low credibility.

Sample size justification is a very crucial part in the design of clinical trials. In this paper, the authors derive a new formula to calculate the sample size for a binary outcome given one of the three popular indices of risk difference. The sample size based on the absolute difference is the fundamental one, which can be easily used to derive sample size given the risk ratio or OR.

---

### Too many digits: the presentation of numerical data [^a41454ae]. Archives of Disease in Childhood (2015). Low credibility.

Thus a decimal places rule that ignores significant digits does not work. But equally, and perhaps surprisingly, a significant digits rule that ignores decimal places does not always work either. Reporting risk ratios to three significant digits for example leads to the largest ratio below 1 being reported as 0.999 and the smallest above 1 as 1.01, with three and two decimal places, respectively. This is clearly unsatisfactory as they differ in precision by a factor of ten. In this instance a combination of significant digits and decimal places, the rule of four, works best: round the risk ratio to two significant digits if the leading non-zero digit is four or more, otherwise round to three.

The rule of four gives three decimal places for risk ratios from 0.040 to 0.399, two from 0.40 to 3.99 and one from 4.0 to 39.9. Applying it to the example of 22.68 above gives 22.7 (95% CI 7.5 to 74). Alternatively one can apply the rule with one less significant digit, giving 23 with CI 8 to 70.

Another example is the reporting of test statistics such as t or F. Specifying one decimal place would permit say t = 30.1, where 30 is clearly sufficient as it is so highly significant. Conversely specifying two significant digits would permit t = −0.13, where again the extra precision is irrelevant as it is far from significant. A suitable rule specifies up to one decimal place and up to two significant digits.

When comparing group means or percentages in tables, rounding should not blur the differences between them. This is the basis for the Hopkins two digits rule, whereby the mean has enough decimal places to ensure two significant digits for the SD. An analogous rule for percentages might be to use enough decimal places to ensure two significant digits for the range of values across groups, eg, if the range is 10% or more use whole numbers, if less than 1% use two decimal places, and otherwise one. In practice percentages are usually given along with their corresponding frequencies, so precision is less critical as the exact values can be calculated.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^5f767c11]. CDC (2011). Medium credibility.

Table 2 — additional community viral load (VL) measures — includes a cumulative measure defined as "Sum of VLs for all cases" that reflects "The number of virus particles in the population who have a VL; affected by the number of cases; reflects both prevalence of HIV cases and burden of viremia." A suppression proportion is defined by "The numerator is the number of cases whose most recent VL was suppressed; the denominator is the number of cases with a VL" and interpreted as "Maximal Virologic Suppression, or the percent virologically suppressed reflects ART uptake, adherence, and effectiveness in clinical settings; a quality of HIV care measure; in jurisdictions offering universal treatment, an important marker of uptake of universal ART guidelines." Calculation timing guidance notes "Time needs to be defined for each calculation, usually a 12-month period. Pooling over a few years can increase VL completeness but is harder to interpret." The abbreviation is specified as "ART, antiretroviral treatment."

---

### The basic reproduction number (R) of measles: a systematic review [^f3ec52f0]. The Lancet: Infectious Diseases (2017). Medium credibility.

The basic reproduction number, R nought (R 0), is defined as the average number of secondary cases of an infectious disease arising from a typical case in a totally susceptible population, and can be estimated in populations if pre-existing immunity can be accounted for in the calculation. R 0 determines the herd immunity threshold and therefore the immunisation coverage required to achieve elimination of an infectious disease. As R 0 increases, higher immunisation coverage is required to achieve herd immunity. In July, 2010, a panel of experts convened by WHO concluded that measles can and should be eradicated. Despite the existence of an effective vaccine, regions have had varying success in measles control, in part because measles is one of the most contagious infections. For measles, R 0 is often cited to be 12–18, which means that each person with measles would, on average, infect 12–18 other people in a totally susceptible population. We did a systematic review to find studies reporting rigorous estimates and determinants of measles R 0. Studies were included if they were a primary source of R 0, addressed pre-existing immunity, and accounted for pre-existing immunity in their calculation of R 0. A search of key databases was done in January, 2015, and repeated in November, 2016, and yielded 10883 unique citations. After screening for relevancy and quality, 18 studies met inclusion criteria, providing 58 R 0 estimates. We calculated median measles R 0 values stratified by key covariates. We found that R 0 estimates vary more than the often cited range of 12–18. Our results highlight the importance of countries calculating R 0 using locally derived data or, if this is not possible, using parameter estimates from similar settings. Additional data and agreed review methods are needed to strengthen the evidence base for measles elimination modelling.

---

### Committee opinion no. 680 summary: the use and development of checklists in obstetrics and gynecology [^6d342952]. Obstetrics and Gynecology (2016). Medium credibility.

A Checklist for Checklists — development, drafting, and validation prompts provide concrete design and testing considerations. Development prompts include "Do you have clear, concise objectives for your checklist?" Drafting prompts ask whether a checklist will "Utilize natural breaks in workflow (pause points)?", "Fit on one page?", and "Is the font: Sans serif? Upper and lower case text? Large enough to be read easily? Dark on a light background?", and whether "Are there fewer than 10 items per pause point?" and "Is the date of creation (or revision) clearly marked?" Validation prompts include "Trialed the checklist with frontline users (either in a real or simulated situation)?", "Modified the checklist in response to repeated trials?", and whether it will "Fit the flow of work?" and "Detect errors at a time when they can still be corrected?", with attention to planning for future revision; additionally, "Please note: A checklist is NOT a teaching tool or algorithm."

---

### In search of a' good number': knowledge controversy and population estimates in the endgame of hepatitis C elimination [^2ad24e88]. BMJ Global Health (2024). High credibility.

Narratives of care

Narratives of scale intersect with those of care. The downsizing of the population estimate is accounted as a lessening which is not merely felt in numerical terms but as a form of devaluing and discounting in other ways too:

For people living with hepatitis C, all of a sudden it's not as a big a problem, which for some people is a good thing, but for other people potentially minimises what they are experiencing. This has been done and all of a sudden there is less people. We've just changed the number of people overnight. As an individual, it feels like you've either just been taken out and tossed aside, or you know, put back in. (P5)

In this narrative, there are fewer people, and people who matter have gone 'missing': '40 000 people seemed to disappear from the prevalence number overnight, and there has been some questions raised about where did those people go' (P5). This is the narrative that envisages the estimate as an absolute, as a number that represents actual individuals. In this articulation, the number is not a mere estimate, and moreover, it is more-than a number. When translated as not 'just a number', 'It is people that we're talking about, the numbers reflect people' (P16). 'People see this as people, they see it as themselves' (P28). This is why getting the number 'right', 'accurate', 'correct' and 'true' is also a concern of those working face-to-face with affected people (P20). In this narrative, latitude and fluidity in estimation are part of the problem and cause of the controversy rather than part of the defence which seeks to moderate it. Because 'the number really matters' in representing lived experience — of people, care and treatment — the claim that estimates afford numbers a wide latitude that 'does not matter' in practice is contested. The contestation here then, is the navigation between an estimate that is also a number: The changing number is 'not really real, but real' at the same time. For instance, it is not that there is a misunderstanding that estimates are 'the result of an equation' that do not necessarily represent 'individual people' — the science is relatively well understood — but that this is how they are felt in experiential terms. As commented:

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^b1513dd5]. CDC (2011). Medium credibility.

Table 1b — Minimum sample size for detecting geometric mean (GM) ratio k with α = 0.05 and W = 0.9 — presents required sample sizes by standard deviation (S) strata and defines parameters; α is significance level, W is power, GM is geometric mean, and S is standard deviation, noting that 1.2 is the standard deviation of national viral load (VL) data. Each jurisdiction will need to assess the standard deviation of their local VL data and then determine the appropriate sample size needed to assess VL, and if the sample size is inadequate to meet the recommended case inclusion criterion, an alternate method may need to be used, such as combining multiple years of data. For k = 3, sample sizes across S = 1, 1.1, 1.2, 1.3, 1.4, 1.5 are 75, 91, 108, 127, 147, 169, and Table 1b uses power = 90%; jurisdictions may also explore differences in means of viral loads, including categorical differences in the proportion with undetectable or very low VL.

---

### The rate and effects of spontaneous mutation on fitness traits in the social amoeba, dictyostelium discoideum [^f583307b]. G3 (2013). Low credibility.

Second, some traits may be under stabilizing selection in nature because of their effects on spore production. The number of FBs and the number of spores are positively correlated (Table 4). However, if the relationship between spore number (i.e. fitness) and FB has a nonlinear component, that could indicate stabilizing selection. In that situation, we would expect significant nonlinearity, with an optimum close to the ancestral value. Put another way, mutations that increase or decrease the number of FBs relative to the ancestor may all decrease spore number. To examine this hypothesis, a nonlinear regression between spore number and FB number was calculated by fitting a cubic relationship. This analysis indicated a significant nonlinearity, with a local optimum occurring at a relative fitness of 1.06 (A). A similar nonlinear regression analysis was also performed for spores per FB, which is also positively correlated with spore number (Table 4). Again there was significant nonlinearity, with a maximum at 1.21 (B). These results suggest that both of these fitness components may be under stabilizing selection, which would imply that mutations that sufficiently increase the value of one of these fitness components are not in fact beneficial overall. None of the others fitness components exhibited significant nonlinearity in a regression with spore number. This supports the idea that it is stabilizing selection on spores per FB, and total number of FBs, that causes the high frequency of "beneficial" mutations that increase these trait values.

Third, the high frequency of trait-increasing mutations may indicate the trait is not actually a fitness component. If correct, the ancestor's particular trait value has little predictive power regarding the effects of mutations. This is because there has been no evolutionary history of selection removing only those mutations that reduce the trait value, as there has for directionally selected fitness traits, which enriches them among random mutations. For the remaining two components exhibiting high estimates of the frequency of trait-increasing mutations, the distance that a slug travels and the relative competitive ability, is this a likely explanation? Slug migration is thought to be an important component of fitness in D. discoideum. Slug migration allows D. discoideum to move to regions of more abundant resources. It also assists in long-range spore dispersal and may assist in predator protection (f1;). The fact that the sizeable majority of mutations reduced slug distance supports the notion that slug migration distance being at least partially under direct selection in nature.

---

### Feasible future global scenarios for human life evaluations [^c6444068]. Nature Communications (2019). High credibility.

Methods

Survey data

Following recent World Happiness Reports, we include a measure of income along with variables representing healthy life expectancy (life expectancy), perceived level of corruption (corruption), freedom to choose (freedom), prevalence of donating to others (giving), and availability of informal social support (social support). The English wording of the life evaluation question is "Please imagine a ladder, with steps numbered from 0 at the bottom to 10 at the top. The top of the ladder represents the best possible life for you and the bottom of the ladder represents the worst possible life for you. On which step of the ladder would you say you personally feel you stand at this time?" The social support variable is the national average of dichotomous responses to "If you were in trouble, do you have relatives or friends you can count on to help you whenever you need them, or not?" The freedom variable is the national average of dichotomous responses to "Are you satisfied or dissatisfied with your freedom to choose what you do with your life?" The corruption variable is the national average of dichotomous answers to two questions: "Is corruption widespread throughout the government or not?" and "Is corruption widespread within businesses or not?" In order to isolate the prevalence for giving from the variation in financial capacities, national mean responses to "Have you donated money to a charity in the past month?" are regressed on GDP per capita, and the residual becomes the Giving variable. Further descriptions and important notes for these variables are given in the statistical appendix for Chapter 2 of the 2017 report. Our measure of income is the natural logarithm of internationally comparable (i.e. purchasing power parity) GDP per person; this measure has a relatively good linear fit with life evaluations, for example, ref. By considering these measures at the aggregate (country) level, we best capture the sum of individual effects and those which come from public goods (at national and subsidiary scales) and from social and contextual effects at all spatial scales up to the country level. Our use of country-level data is motivated by the assumption that externalities (e.g. positional effects, or public goods) are likely to be large within countries, but are likely to be relatively small between countries.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^dee6cacc]. CDC (2011). Medium credibility.

Community viral load sample size calculations — sample size to detect differences in geometric mean (GM) viral load depends on power and standard deviation (S). The table specifies "α = 0.05 and W = 0.8," with S columns 1 to 1.5; for k = 3 the minimum sample sizes by S are 54, 66, 78, 92, 106, and 122, and "1.2 is the standard deviation of national VL data," so at S = 1.2 the needed sample size is 78.

---

### Recommendations for prevention and control of influenza in children, 2023–2024 [^9e7d9395]. Pediatrics (2023). High credibility.

Table 4 — Recommended seasonal influenza vaccines for children and adolescents (United States, 2023–2024) lists product, age group, presentation with antigen content, recommended dose, and Thimerosal Mercury Content† (µg Hg/0.5-mL Dose). Examples include: Fluarix Quadrivalent (GlaxoSmithKline) for ≥ 6 mo as a 0.5-mL prefilled syringe (15 µg/0.5 mL) with recommended dose 0.5 mL and Thimerosal Mercury Content† 0; Fluzone Quadrivalent (Sanofi Pasteur) for ≥ 6–35 mo as a 0.5-mL single-dose vial* (15 µg/0.5 mL) with recommended dose 0.25 or 0.5 mL and Thimerosal Mercury Content† (µg Hg/0.5-mL Dose) 0, and for 6–35 mo as a 5.0-mL multidose vial* (15 µg/0.5 mL) with recommended dose 0.25 or 0.5 mL and Thimerosal Mercury Content† (µg Hg/0.5-mL Dose) 25; Flucelvax Quadrivalent (Seqirus) ≥ 6 mo in a 5.0-mL multidose vial* (15 µg/0.5 mL) with recommended dose 0.5 mL and Thimerosal Mercury Content† (µg Hg/0.5-mL Dose) 25; Flublok Quadrivalent (Sanofi Pasteur) for ≥ 18 y as a 0.5-mL prefilled syringe (45 µg/0.5 mL) with recommended dose 0.5 mL and Thimerosal Mercury Content† 0; and FluMist Quadrivalent (AstraZeneca) for 2–49 y as a 0.2-mL prefilled intranasal sprayer (virus dose: 10^6.5–7.5 FFU/0.2 mL) with recommended dose 0.2 mL and Thimerosal Mercury Content† 0. Handling notes state: "A total of 0.25 mL drawn from a single or multidose vial is an acceptable dose for children 6 to 35 months of age," "For vaccines that include a multidose vial presentation, the maximum number of doses withdrawn should not exceed the number specified in the package insert (eg, 10 doses for Fluzone, 20 doses for Afluria). Residual product should be discarded," and "Single-dose vials should be used for only 1 dose (0.25 mL or 0.5 mL). Residual product remaining in the vial should be discarded."

---

### Trimming the genomic fat: minimising and re-functionalising genomes using synthetic biology [^655d724b]. Nature Communications (2023). High credibility.

Introduction

Modern genomes represent the culmination of ~3.8 billion years of life's evolution on Earth and encode the stunning complexity we observe in the biosphere. Despite ~70 years of molecular biology research, we are still yet to understand precisely how this complexity is encoded in a genome. Our complete understanding is hindered by two major factors. Firstly, genomes have evolved to facilitate reproduction and survival in diverse environments, through incompletely characterised mechanisms, and thus appear to be incredibly complex to us. Furthermore, under laboratory conditions, a large proportion of genes are individually non-essential –, yet essential when removed combinatorially. Adding to these challenges is the fact that a significant proportion of genes in any given genome have functions that are yet to be defined. For example, in the genome of Escherichia coli, only 48.9% of genes have been characterized, while in the genome of Saccharomyces cerevisiae, over 1000 of the ~6000 genes have unknown functions. To explain this phenomenon, it is hypothesised that genes with unknown functions are either redundant, or their functions are not needed in the lab conditions, and are only important under specific conditions. Secondly, genetic interactions and regulations are overwhelmingly complex, and emergent phenomena are difficult to define in well-studied organisms, let alone rationally designed ones. As biotechnological capabilities advance, scientists are not only working to understand the biology, but are also increasingly ambitious in engineering biological systems to solve existing problems. Synthetic biology is a young interdisciplinary field that combines biology with cutting-edge engineering techniques and can benefit agricultural, manufacturing, fuel, environmental and medical sectors. With advances in synthetic biology, complex heterologous pathways have been engineered for wide applications, including the production of value-added molecules, the utilisation of inexpensive nutrition sources and the detection of pollutants or diseases. However, the complexity of biological systems has hindered our ability to modify an existing genome, which might result in genetic incompatibility, instability of the heterologous pathways, and low product yields due to competition for cellular resources. Unexpected results are often obtained after rational engineering, and thus require a laborious trial and error process. As whole-genome synthesis becomes achievable and cheaper, one solution is to unlock a more complete understanding of biology at the genomic level by construction of minimal genomes. Theoretically, a minimal genome consists of the smallest possible number of genes required to support a living cell under a defined set of conditions. Minimal genomes are therefore almost as difficult to define as they are to create, since a minimal gene set will vary according to the environment and construction method. In practice, genome minimisation is more commonly aimed at building a genome with a reduced set of genes relative to its wild-type counterpart, rather than the absolute lowest number of genes. These genomes are intended to be easier to understand and engineer, have fewer uncharacterised genetic elements, and less complex regulatory networks.

---

### Recommendations for prevention and control of influenza in children, 2015–2016 [^2c61e3c9]. Pediatrics (2015). Medium credibility.

Egg-independent influenza vaccine technologies and product selection — 2015–2016: Two trivalent influenza vaccines manufactured using technologies that do not use eggs will be available for people 18 years or older during the 2015–2016 season — cell culture–based inactivated influenza vaccine (ccIIV3) and recombinant influenza vaccine (RIV3); for ccIIV3, only trace amounts of ovalbumin are detectable and egg allergy is not listed as a contraindication, and for RIV3, it is licensed for people 18 years or older and there are no egg proteins in this version of influenza vaccine. Vaccination should not be delayed to obtain a specific product.

---

### A general strategy for diversifying complex natural products to polycyclic scaffolds with medium-sized rings [^b15c000f]. Nature Communications (2019). High credibility.

The interrogation of complex biological pathways demands diverse small molecule tool compounds, which can often lead to important therapeutics for the treatment of human diseases. Since natural products are the most valuable source for the discovery of therapeutics, the derivatization of natural products has been extensively investigated to generate molecules for biological screenings. However, most previous approaches only modified a limited number of functional groups, which resulted in a limited number of skeleta. Here we show a general strategy for the preparation of a library of complex small molecules by combining state-of-the-art chemistry - the site-selective oxidation of C-H bonds - with reactions that expand rigid, small rings in polycyclic steroids to medium-sized rings. This library occupies a unique chemical space compared to selected diverse reference compounds. The diversification strategy developed herein for steroids can also be expanded to other types of natural products.

---

### How many days was that? We' re still not sure, but we' re asking the question better! [^b41c7375]. Medicine and Science in Sports and Exercise (2008). Low credibility.

Unreliable measures limit the ability to detect relationships with other variables. Day-to-day variability in measurement is a source of unreliability. Studies vary substantially in numbers of days needed to reliably assess physical activity. The required numbers of days has probably been underestimated due to violations of the assumption of compound symmetry in using the intraclass correlation. Collecting many days of data become unfeasible in real-world situations. The current dilemma could be solved by adopting distribution correction techniques from nutrition or gaining more information on the measurement model with generalizability studies. This would partition the variance into sources of error that could be minimized. More precise estimates of numbers of days to reliably assess physical activity will likely vary by purpose of the study, type of instrument, and characteristics of the sample. This work remains to be done.

---

### Modeling and forecasting the total number of cases and deaths due to pandemic [^4243a391]. Journal of Medical Virology (2022). Medium credibility.

The COVID-19 pandemic has appeared as the predominant disease of the 21st century at the end of 2019 and was a drastic start with thousands of casualties and the COVID-19 victims in 2020. Due to the drastic effect, COVID-19 scientists are trying to work on pandemic diseases and Governments are interested in the development of methodologies that will minimize the losses and speed up the process of cure by providing vaccines and treatment for such pandemics. The development of a new vaccine for any pandemic requires long in vitro and in vivo trials to use. Thus the strategies require understanding how the pandemic is spreading in terms of affected cases and casualties occurring from this disease, here we developed a forecasting model that can predict the no of cases and deaths due to pandemic and that can help the researcher, government, and other stakeholders to devise their strategies so that the damages can be minimized. This model can also be used for the judicial distribution of resources as it provides the estimates of the number of casualties and number of deaths with high accuracy, Government and policymakers on the basis of forecasted value can plan in a better way. The model efficiency is discussed on the basis of the available dataset of John Hopkins University repository in the period when the disease was first reported in the six countries till the mid of May 2020, the model was developed on the basis of this data, and then it is tested by forecasting the no of deaths and cases for next 7 days, where the proposed strategy provided excellent forecasting. The forecast models are developed for six countries including Pakistan, India, Afghanistan, Iran, Italy, and China using polynomial regression of degrees 3–5. But the models are analyzed up to the 6th-degree and the suitable models are selected based on higher adjusted R-square (R 2) and lower root-mean-square error and the mean absolute percentage error (MAPE). The values of R 2 are greater than 99% for all countries other than China whereas for China this R 2 was 97%. The high values of R 2 and Low value of MAPE statistics increase the validity of proposed models to forecast the total no cases and total no of deaths in all countries. Iran, Italy, and Afghanistan also show a mild decreasing trend but the number of cases is far higher than the decrease percentage. Although India is expected to have a consistent result, more or less it depicts some other biasing factors which should be figured out in separate research.

---

### A framework to predict the price of energy for the end-users with applications to monetary and energy policies [^5b5689a1]. Nature Communications (2021). High credibility.

Weight prediction results

The validity of our proposed methodology is tested over a period of 174 months from January 2006 to June 2020, by comparing the predicted value of the monthly weight of each product's demand with its actual, known value. For this comparison, the sum of the squared prediction error for each month is computed over the testing period (see "Methods") and the results in the form of an average sum of squared error, minimum sum of squared error, and maximum sum of squared error are summarized in Table 1. It should be noted that the number of months to be compared decreases as the year of prediction increases. For example, the predictions of the second year require the predicted weights of the first year, so there are less actual monthly values to compare.

Table 1
Weight prediction results up to 4 years from January 2006 to June 2020.

As shown in Table 1, the predictive ability of our proposed methodology is quite remarkable since the reported error values are extremely low. This is true even if we consider the square root of the average sum of the squared errors which is 1.8808%, 2.0874%, 2.2329%, and 2.3641% for the first, second, third, and fourth year, respectively. The very low predictive error (2.3641%) in the case of the fourth year, where only unknown (predicted) values have been used, is of significant importance. As expected, the average sum of the squared error increases, as the year of prediction increases due to the decreasing number of months with known values. The accuracy of the proposed methodology is also verified from the fact that even the maximum sum of squared error over the tested period is rather low, i.e. 0.006685 or 8.1764% when we consider the square root of the sum of the squared errors.

This excellent predictive ability along with the unique inherent characteristics of EPIC that captures both the demands and the prices of the products over the entire energy landscape in the United States justifies our opinion that EPIC is the ideal tool for designing, assessing, and optimizing various policy decisions of public interest. Two prime, representative policy case studies are presented in the next sections.

---

### Scale-free networks are rare [^0cde0f97]. Nature Communications (2019). High credibility.

Robustness analysis

In order to assess the dependence of these results on the evaluation scheme itself, we conduct a series of robustness tests.

Specifically, we test whether the above results hold qualitatively when (i) we consider only network data sets that are naturally simple (unweighted, undirected, monoplex, and no multi-edges); (ii) we remove the power-law with cutoff from the set of alternative distributions; (iii) we lower the percentage thresholds for all categories to allow admission if any one constituent simple graph satisfies the requirements; and (iv) we analyze the scaling behavior of the degree distribution's first and second moment ratio. Details for each of these tests, and two others, are given in Supplementary Note 5. We also test whether the evaluation scheme correctly classifies four different types of synthetic networks with known structure, both scale free and non-scale free. Details and results for these tests are given in Supplementary Note 6.

The first test evaluates whether the extension of the scale-free hypothesis to non-simple networks and the corresponding graph-simplification procedure biases the results. The second evaluates whether the presence of finite-size effects drives the lack of evidence for scale-free distributions. Applied to the corpus, each test produces qualitatively similar results as the primary evaluation scheme (see Supplementary Note 5, and Supplementary Fig. 4), indicating that the lack of empirical evidence for scale-free networks is not driven by these particular choices in the evaluation scheme itself.

The third considers a "most permissive" parameterization, which evaluates the impact of our requirements that a minimum percentage of degree sequences satisfy the constraints of a category. Under this test, we specifically examine how the evidence changes if we instead require that only one degree sequence satisfies the given requirements. That is, this test lowers the threshold for each category to be maximally permissive: if scale-free structure exists in any associated degree sequence, the network data set is counted as falling into the corresponding category.

---

### Summary benchmarks-full set – 2024 [^6f68d3ad]. AAO (2024). High credibility.

Preferred Practice Pattern (PPP) guidelines — use and ethics note states that "The PPPs are intended to serve as guides in patient care, with greatest emphasis on technical aspects," that "true medical excellence is achieved only when skills are applied in a such a manner that the patients' needs are the foremost consideration," and that "The Academy is available to assist members in resolving ethical dilemmas that arise in the course of practice."

---

### Neural-like computing with populations of superparamagnetic basis functions [^2062628a]. Nature Communications (2018). Medium credibility.

If the gripper strikes in the down zone, the opposite is implemented.

The key advantage of this learning rule is its simplicity: there is no need to perform a precise measurement of the error (here distance between the gripper and the object) as required by most learning methods in the literature. Note that the proposed system is independent of this learning rule and that different algorithms could be used to perform more complex tasks. Figure 3b shows that the distance between the object and the gripper is progressively decreased through repeated learning steps. After 3000 learning steps, the mean error is below 2.5% of the range: learning is successful. As can be seen in Fig. 3c the grasping error decreases as the number of junctions in the input population increases. The precision of the result indeed improves as the population grows, better approximating an ideal, infinite basis set. Figure 3c also demonstrates that transfer of information between populations of different sizes can be achieved, allowing changes of basis if needed.

The example of the gripper in Fig. 3 shows how we can transfer information without degradation from one population to a different one performing a basis change. Now we show that our system and our simple learning procedure can also transform information during the transfer between populations, in other words, realize more complex functions than the identity of Fig. 3. In Fig. 4a, we illustrate increasingly more complicated transformations: linear but not identity (double), square, inverse, and sine of the stimulus. Each can be learned with excellent precision, similar to the identity.

Fig. 4
Evaluation of stochastic population coding with superparamagnetic tunnel junctions. a Performance of several transformations, including non-linear. The 2 inputs label corresponds to transformation from polar to Cartesian coordinates. The series label corresponds to two transformations in series implementing the function sin 2 (x). b Distance to target versus variability of the energy barrier (bottom axis) and variability of the natural frequency (top axis). The experimental variability is indicated in red. c Distance to target for different times of observation during which switching rates are recorded, leading to different energy dissipated by the junctions (see Methods). Longer acquisition time allows better precision of the transformation, but leads to higher energy consumption. Each population is composed of 100 junctions and 3000 learning steps are used. Each data point corresponds to the average over 50 trials and the error bar to the associated standard deviation

---

### Estimating the syphilis incidence and diagnosis rate in Japan: a mathematical modelling study [^e07d869d]. Sexually Transmitted Infections (2020). Medium credibility.

Objectives

The reported number of syphilis cases is increasing in Japan. In this study, we aimed to estimate both the incidence of infection and diagnosis-and-reporting rate using a mathematical model that captures the time course of infection.

Methods

We analysed yearly notification data from 1961 to 2016 and stratified according to clinical stage and sex. We built a compartmental ordinary differential equations model to describe the natural epidemiological course of syphilis in which the incidence of infection and diagnosis-and-reporting rate were included as time-varying parameters that we estimated via the maximum likelihood method.

Results

From 2008 to 2016, the estimated number of new syphilis cases among men and women was 1070 (95% CI 1037 to 1104) and 302 (95% CI 287 to 318), respectively, which was increased from the previous period (1999–2007) with 269 (95% CI 256 to 282) and 71 (95% CI 64 to 78) cases, respectively. The diagnosis-and-reporting rate did not vary greatly over time. We estimated the number of undiagnosed syphilis cases from either incubating or early clinical (ie, primary or secondary) infections in 2016 to be 262 (95% CI 249 to 275) and 79 (95% CI 74 to 84) for men and women, respectively.

Conclusions

The recent increase in reported cases of syphilis is owing to an increase in the number of infections. We found no evidence of time-dependent improvement in detection or reporting.

---

### Understanding statistical terms: 2 [^351bd399]. Drug and Therapeutics Bulletin (2009). Low credibility.

An increasing number of statistical terms appear in journal articles and other medical information. A working knowledge of these is essential in assessing clinical evidence. With this in mind, we are producing a series of explanatory articles covering various statistical terms and their uses. This, the second article in the series, will focus on some of the most common terms used in reporting the results of randomised controlled trials.1.

---

### Optimizing natural fertility: a committee opinion [^a88a4233]. Fertility and Sterility (2013). Medium credibility.

Frequency of intercourse — effects of abstinence duration on semen parameters: Whereas abstinence intervals greater than 5 days may adversely affect sperm counts, abstinence intervals as short as 2 days are associated with normal sperm densities. A widely held misconception is that frequent ejaculations decrease male fertility. In men with normal semen quality, sperm concentrations and motility remained normal, even with daily ejaculation, and in men with oligozoospermia, sperm concentration and motility may be highest with daily ejaculation. After longer abstinence intervals of 10 days or more, semen parameters begin to deteriorate.

---

### Why population attributable fractions can sum to more than one [^48796442]. American Journal of Preventive Medicine (2004). Low credibility.

Background

Population attributable fractions (PAFs) are useful for estimating the proportion of disease cases that could be prevented if risk factors were reduced or eliminated. For diseases with multiple risk factors, PAFs of individual risk factors can sum to more than 1, a result suggesting the impossible situation in which more than 100% of cases are preventable.

Methods

A hypothetical example in which risk factors for a disease were eliminated in different sequences was analyzed to show why PAFs can sum to more than 1.

Results

PAF estimates assume each risk factor is the first to be eliminated, thereby describing mutually exclusive scenarios that are illogical to sum, except under special circumstances. PAFs can sum to more than 1 because some individuals with more than one risk factor can have disease prevented in more than one way, and the prevented cases of these individuals could be counted more than once. Upper and lower limits of sequential attributable fractions (SAFs) can be calculated to describe the maximum and minimum proportions of the original number of disease cases that would be prevented if a particular risk factor were eliminated.

Conclusions

Improved descriptions of the assumptions that underlie the PAF calculations, use of SAF limits, or multivariable PAFs would help avoid unrealistic estimates of the disease burden that would be prevented after resources are expended to reduce or eliminate multiple risk factors.

---

### An experimental test of noncontextuality without unphysical idealizations [^b2fd1de5]. Nature Communications (2016). Medium credibility.

Preparations are represented by vectors of raw data specifying the relative frequencies of outcomes for each measurement, uncertainties on which are calculated assuming Poissonian uncertainty in the photon counts. For each run, the raw data is fit to a set of states and effects in a GPT in which three binary-outcome measurements are tomographically complete. This is done using a total weighted least-squares method. The average χ 2 over the 100 runs is 3.9 ± 0.3, agreeing with the expected value of 4, and indicating that the model fits the data well (see Supplementary Note 4, Supplementary Data 1 and 2, and Supplementary Software 1). The fit returns a 4 × 8 matrix that serves to define the 8 GPT states and 4 GPT effects, which are our estimates of the primary preparations and measurements. The column of this matrix associated to the t, b preparation, which we denote, specifies our estimate of the probabilities assigned by the primary preparationto outcome '0' of each of the primary measurements. The raw and primary data are compared in Fig. 3. The probabilities are indistinguishable on this scale. We plot the probabilities for P 1, P 2, and P 3 in Fig. 4a on a much finer scale. We then see that the primary data are within error of the raw data, as expected given the high quality of the fit to the GPT. However, the operational equivalences of equations (2) and (4) are not satisfied by our estimates of the primary preparations and measurements, illustrating the need for secondary procedures.

We define the six secondary preparations as probabilistic mixtures of the eight primaries: where theare the weights in the mixture. We maximizeover validsubject to the constraint of equation (4), that is,(a linear program). A high value of C P ensures that each of the six secondary preparations is close to its corresponding primary. Averaging over 100 runs, we find C P = 0.9969 ± 0.0001, close to the maximum of 1. An analogous linear program to select secondary measurements yields similar results. Supplementary Tables 3 and 4 display the weights that define each secondary preparation and measurement, averaged over 100 runs. Figure 3 also displays the outcome probabilities for the secondary procedures, confirming that they are close to ideal. Figure 4 demonstrates how our construction enforces the operational equivalences.

---

### The PRISMA statement for reporting systematic reviews and meta-analyses of studies that evaluate healthcare interventions: explanation and elaboration [^0502a522]. BMJ (2009). Excellent credibility.

Some studies are published more than once. Duplicate publications may be difficult to ascertain, and their inclusion may introduce bias. We advise authors to describe any steps they used to avoid double counting and piece together data from multiple reports of the same study (such as juxtaposing author names, treatment comparisons, sample sizes, or outcomes). We also advise authors to indicate whether all reports on a study were considered, as inconsistencies may reveal important limitations. For example, a review of multiple publications of drug trials showed that reported study characteristics may differ from report to report, including the description of the design, number of patients analysed, chosen significance level, and outcomes. Authors ideally should present any algorithm that they used to select data from overlapping reports and any efforts they used to solve logical inconsistencies across reports.

Item 11: Data items

List and define all variables for which data were sought (such as PICOS, funding sources) and any assumptions and simplifications made.

Examples "Information was extracted from each included trial on: (1) characteristics of trial participants (including age, stage and severity of disease, and method of diagnosis), and the trial's inclusion and exclusion criteria; (2) type of intervention (including type, dose, duration and frequency of the NSAID [non-steroidal anti-inflammatory drug]; versus placebo or versus the type, dose, duration and frequency of another NSAID; or versus another pain management drug; or versus no treatment); (3) type of outcome measure (including the level of pain reduction, improvement in quality of life score (using a validated scale), effect on daily activities, absence from work or school, length of follow up, unintended effects of treatment, number of women requiring more invasive treatment)."

---

### Pediatric radiopharmaceutical administered doses: 2010 North American Consensus guidelines [^817d51e1]. Journal of Nuclear Medicine (2011). Medium credibility.

Maximum administered activity guidance — Many North American hospitals used a fixed maximum administered activity approximately 70 times the recommended weight-based administered activity (examples are 370 MBq (10 mCi) for 123I-MIBG and 555 MBq (15 mCi) for 99mTc-MDP), and the consensus states: For patients who weigh more than 70 kg, it is recommended that the maximum administered activity not exceed the product of the patient's weight (kg) and the recommended weight-based administered activity; some practitioners may choose to set a fixed maximum administered activity equal to 70 times the recommended weight-based administered activity, for example, approximately 370 MBq (10 mCi) for 18F-FDG body imaging.

---

### American Thoracic Society / centers for disease control and prevention / Infectious Diseases Society of America: controlling tuberculosis in the United States [^e3207a60]. American Journal of Respiratory and Critical Care Medicine (2005). Medium credibility.

Grading system for ranking recommendations — ATS/CDC/IDSA TB control statement defines how recommendations are rated: "The recommendations contained in this statement… were rated for their strength by use of a letter grade and for the quality of the evidence… by use of a Roman numeral (Table 1)." Strength A is "Highly recommended in all circumstances," B is "Recommended; implementation might be dependent on resource availability," and C is "Might be considered under exceptional circumstances." Quality of evidence I is "Evidence from at least one randomized, controlled trial," II is "Evidence from (1) at least one well-designed clinical trial, without randomization; (2) cohort or case-control analytic studies; (3) multiple time-series; or (4) dramatic results from uncontrolled experiments," and III is "Evidence from opinions of respected authorities, on the basis of cumulative public health experience, descriptive studies, or reports of expert committees."

---

### Summary benchmarks-full set – 2024 [^e2c40ddc]. AAO (2024). High credibility.

Preferred Practice Pattern (PPP) guidelines — GRADE recommendation categories are specified as: "Strong recommendation (SR): Used when the desirable effects of an intervention clearly outweigh the undesirable effects or clearly do not" and "Discretionary recommendation (DR): Used when the trade-offs are less certain — either because of low-quality evidence or because evidence suggests that desirable and undesirable effects are closely balanced."

---

### Data-driven discovery of dimensionless numbers and governing laws from scarce measurements [^8fa447ff]. Nature Communications (2022). High credibility.

We design an iterative two-level optimization scheme to determine the two sets of unknown parameters in the regression problem, namely the basis coefficients γ and polynomial coefficients β. The optimization scheme includes multiple iterative steps. At each step, we adjust the first-level basis coefficients γ while holding the second-level polynomial coefficients β constant, and then optimize the second-level polynomial coefficients β while keeping the first-level basis coefficients γ constant. This process is repeated until the result is converged, that is, the values of γ and β remain unchanged. There are several advantages to the proposed two-level approach over a single-level approach that combines the two sets of unknowns together during optimization. We can use different optimization methods and parameters (such as the learning rate) for these two-level models to significantly improve the efficiency of the optimization. More importantly, we can utilize physical insights to inform the learning process. The first-level basis coefficients γ have a clear physical meaning, which is related to the powers that produce the dimensionless number. Thus, those values have to be rational numbers to maintain dimensional invariance. Moreover, their typical range is limited. It is worth noting that the absolute values of the coefficients in most of the dimensionless numbers and scaling laws are less than four. To leverage those physical insights or constraints, we design several methods for optimizing the first-level basis coefficients, including a simple grid search (used in this section) and a much more efficient pattern search (Section 4.2 of the SI). For the second-level coefficients, we conduct multiple standard representation learning methods, including the polynomial regression used in this section, tree-based extreme gradient boosting (XGBoost) used in the section "Porosity formation in 3D printing of metals", and general gradient descent method (Section 4.1 of the SI). Details on the two-level optimization framework are provided in Section 4 of the SI.

---

### Number needed to treat (NNT) in clinical literature: an appraisal [^bb555026]. BMC Medicine (2017). Low credibility.

Background

The number needed to treat (NNT) is an absolute effect measure that has been used to assess beneficial and harmful effects of medical interventions. Several methods can be used to calculate NNTs, and they should be applied depending on the different study characteristics, such as the design and type of variable used to measure outcomes. Whether or not the most recommended methods have been applied to calculate NNTs in studies published in the medical literature is yet to be determined. The aim of this study is to assess whether the methods used to calculate NNTs in studies published in medical journals are in line with basic methodological recommendations.

Methods

The top 25 high-impact factor journals in the "General and/or Internal Medicine" category were screened to identify studies assessing pharmacological interventions and reporting NNTs. Studies were categorized according to their design and the type of variables. NNTs were assessed for completeness (baseline risk, time horizon, and confidence intervals [CIs]). The methods used for calculating NNTs in selected studies were compared to basic methodological recommendations published in the literature. Data were analyzed using descriptive statistics.

Results

The search returned 138 citations, of which 51 were selected. Most were meta-analyses (n = 23, 45.1%), followed by clinical trials (n = 17, 33.3%), cohort (n = 9, 17.6%), and case-control studies (n = 2, 3.9%). Binary variables were more common (n = 41, 80.4%) than time-to-event (n = 10, 19.6%) outcomes. Twenty-six studies (51.0%) reported only NNT to benefit (NNTB), 14 (27.5%) reported both NNTB and NNT to harm (NNTH), and 11 (21.6%) reported only NNTH. Baseline risk (n = 37, 72.5%), time horizon (n = 38, 74.5%), and CI (n = 32, 62.7%) for NNTs were not always reported. Basic methodological recommendations to calculate NNTs were not followed in 15 studies (29.4%). The proportion of studies applying non-recommended methods was particularly high for meta-analyses (n = 13, 56.5%).

Conclusions

A considerable proportion of studies, particularly meta-analyses, applied methods that are not in line with basic methodological recommendations. Despite their usefulness in assisting clinical decisions, NNTs are uninterpretable if incompletely reported, and they may be misleading if calculating methods are inadequate to study designs and variables under evaluation. Further research is needed to confirm the present findings.

---

### Recommendations for prevention and control of influenza in children, 2024–2025: technical report [^3d54425c]. Pediatrics (2024). High credibility.

Inactivated influenza vaccine (IIV) administration — multidose vials and prefilled syringes — requires adherence to product-specific instructions: "careful attention should be paid to ensure that each product is used according to its approved age indication, dosing, and volume of administration"; for multidose vials, "the maximum number of doses drawn from a multidose vial is specified in the package insert and should not be exceeded; residual product must be discarded regardless of the remaining volume in the vial." A "0.5 mL prefilled syringe of any IIV should not be split into 2 separate 0.25 mL doses," and "if a prefilled syringe of Fluzone is used for a child younger than 36 months, the dose volume will be 0.5 mL per dose."

---

### Exponential concentration in quantum kernel methods [^128b9a13]. Nature Communications (2024). High credibility.

The power of kernel-based learning methods stems from the fact that they map data fromto a higher-dimensional feature space (in this case the 2 n -dimensional Hilbert space) where inner products are taken and a decision boundary such as a support vector machine can be trained. Notably, thanks to the Representer Theorem, the optimal kernel-based model is guaranteed to be expressed as a linear combination of the kernels evaluated over the training dataset (see Chapter 5 in ref.). More concretely, for a kernel-based QML model h a depends on the input data through the inner product between states. We have that the optimal solution is given bywhere. Additionally, if the lossis appropriately chosen, then the loss landscape can be guaranteed to be convex. It follows that by constructing the Gram matrix K whose entries are kernels over training input pairs, where, the optimal parameters a opt can be found by solving the convex optimization problem in Eq. (1). Thus if the Gram matrix can be calculated exactly, kernel-based methods are perfectly trainable.

As an example, in kernel ridge regression, we consider a square loss functionwith a regularization λ and a norm in a feature space. The optimal parameters can be analytically shown to be of the formwhere y is a training label vector with the i th component y i. Another common example is a support vector machine where we consider a binary classification problem with the corresponding labels y ∈ { − 1, + 1}. Using a hinge loss function with no regularization, i.e. the optimization problem in Eq. (1) can be reformulated assubject to 0 ≤ a (i) for all i. Assuming that the Gram matrix K can be accurately and efficiently obtained, solving for the optimal parameters can done with a number of iterations in.

---

### Forecasting global road traffic injury mortality for 2030 [^65585a50]. Injury Prevention (2020). Medium credibility.

Objective

To forecast the number and rate of deaths from road traffic injuries (RTI) in the world in 2030.

Methods

This study was a secondary analysis of annual country-level data of RTI mortality rates for 1990–2017 in the Global Burden of Disease (GBD) 2017 Study, population projection for 2030, gross domestic product (GDP) per capita for 1990–2030 and average years of schooling among people aged 15 years+ for 1990–2030. We developed up to 6884 combinations of forecasting models for each subgroup stratified by country, sex and mode of transport using linear and squared year, GDP per capita and average years of schooling as potential predictors. We conducted a fixed-size, rolling window out-of-sample forecast to choose the best combination for each subgroup. In the validation, we used the data for 1990–2002, 1991–2003 and 1992–2004 (fit periods) to forecast mortality rates in 2015, 2016 and 2017 (test periods), respectively. We applied the selected combination of models to the data for 1990–2017 to forecast the mortality rate in 2030 for each subgroup. To forecast the number of deaths, we multiplied the forecasted mortality rates by the corresponding population projection.

Results

During the test periods, the selected combination of models produced the number of deaths that is higher than that estimated in the GBD Study by 5.1% collectively. Our model resulted in 1.225million deaths and 14.3 deaths per 100000 population in 2030, which were 1% and 12% less than those for 2017 in the GBD Study, respectively.

Conclusions

The world needs to accelerate its efforts towards achieving the Decade of Action for Road Safety goal and the Sustainable Development Goals target.

---

### Historical and projected future range sizes of the world's mammals, birds, and amphibians [^4992a9b0]. Nature Communications (2020). High credibility.

Fig. 4
Method of estimating potential natural and actual range for the example of the bat-eared fox (Otocyon megalotis) in the year 1900.

Here, for visualisation purposes, cropland, pasture, and urban areas were aggregated into one map; in reality, our method checks each of them separately against species' artificial habitat preferences.

In a second step, we refined the derived species-specific maps of the proportion of 0.5° grid cells contained in species' extents of occurrence by combining them with species-specific biome requirements and maps of global biome distributions. Species-specific biome requirements (data II) include one or more habitat categories (cf. Supplementary Table 1), in which each species is known to occur. A species was estimated as being present in a grid cell contained in its previously derived extent of occurrence under the potential natural biome at a given point in time if the species' list of habitat categories contained the local (i.e.g.rid cell-specific) potential natural biome at the relevant time (data III; see above). This required matching IUCN habitat categories with the biome categories of the Biome4 vegetation model, which was done as shown in Supplementary Table 1. In this way, we subset extents of occurrences by only retaining grid cells where the natural biome type is included in a species' list of suitable habitat categories. The result of this step represents a species' estimated potential natural habitat range (i.e. in the hypothetical absence of anthropogenic land use) at a given point in time.

In a third step, we estimated actual habitat ranges by including maps of global land use through time. Each species' actual habitat range at a given time was derived by removing any unsuitable anthropogenic land from the previously estimated potential natural range. Historical and projected future land use maps (data IV; see above) provide the fraction of each grid cell that is occupied by cropland, pasture or urban areas. These data were combined with information on which of these three artificial land cover types, if any, species can occur in, which is also included in the list of species' biome requirements (data II). This allowed us, for each grid cell contained in a species' potential natural range at a given time, to estimate the proportion of the grid cell that contained suitable habitat. A species' actual habitat range size was then obtained as the sum of the areas of the remaining suitable habitat from all relevant grid cells.

---

### Global evidence of positive biodiversity effects on spatial ecosystem stability in natural grasslands [^c66a1608]. Nature Communications (2019). High credibility.

We also used structural equation models to test the causal relationships between species richness, productivity, and spatial variability of productivity. The 9640 plots were grouped into 47 groups by species richness level as in Fig. 6. For each plot group, four variables were calculated: species richness, mean productivity, productivity variability across plots (measured as either SD or CV), and number of plots. Number of plots was chosen as an independent variable because productivity variability across plots may be associated with the number of plots at a given species richness: the larger the number of plots, the greater the likelihood of extreme productivity values, and the greater the variability in plot productivity. Productivity variability was always selected as a response variable with no effect on other variables. Three competing models (Supplementary Fig. 4) were set up: richness and mean productivity were both independent variables (Model a), and either richness or productivity was an independent variable and the other an intermediate variable (Model b and Model c, respectively). Maximum likelihood was used to estimate model parameters. The three competing models were then compared by using four absolute fit indices (, GFI, AGFI, RMSEA), five relative fit indices (NFI, RFI, IFI, TLI, CFI), and three parsimonious fit indices (NC, AIC, CAIC; see Supplementary Table 2 for details). Model b was the best-fitting model (P = 0.353; Fig. 7 and Supplementary Table 2).

---

### Elective female genital cosmetic surgery: ACOG committee opinion, number 795 [^2996ee1b]. Obstetrics and Gynecology (2020). High credibility.

Variability of female genitalia measurements — selected normative values from Table 2 (Mean [in mm], Standard Deviation, Minimum [in mm], Maximum [in mm]) include: Width of clitoris — Mean 4.62, Standard Deviation 2.538, Minimum 1, Maximum 22; Length of clitoris — Mean 6.89, Standard Deviation 4.965, Minimum 0.5, Maximum 34; Length of labia minora (right) — Mean 42.1, Standard Deviation 16.35, Minimum 6, Maximum 100; Width of labia minora (right) — Mean 13.4, Standard Deviation 7.875, Minimum 2, Maximum 61.

---

### The cost of large numbers of hypothesis tests on power, effect size and sample size [^9df69432]. Molecular Psychiatry (2012). Low credibility.

To compensate for a greater number of tests, a more realistic strategy may be to increase the sample size (equation 2). Table 1 and Figure 2c give sample sizes needed to maintain the original power at the original targeted effect size. For one million tests, the sample size multiplier is 5.06 for 80% power and 4.33 for 90% power, using equation (2). In the first example above, 506 subjects would be sufficient to reach 80% power to detect that the means differ by 2 in one million Bonferroni-adjusted tests. Although it might appear counterintuitive, the sample size multiplier is smaller for 90% power because the initial sample size is larger. In the same example, 132 subjects would be needed to reach 90% power for one test. For 90% power for one million tests, 4.33 × 132 = 572 subjects are needed. Noting the nearly linear relationship in Figure 2c, we also obtained an approximate rule-of-thumb for the sample size multiplier by fitting zero-intercept linear regression models to the results in Figure 2c. The estimated slopes show that m is approximately, where γ = 1.2 for 50% power, 0.68 for 80% power, 0.55 for 90% power and 0.38 for 99% power.

The rate at which the critical value and, consequently, the effect size and sample size multipliers increase becomes slower and slower as the number of tests becomes larger (Figure 2d), owing to the exponential decline in the tails of the Normal density. For example, effect size multipliers for one million vs ten million tests at 80% power are 2.25 and 2.39, respectively. Sample size multipliers are 5.06 and 5.71. At 80% power, ten million tests require only a 6% increase in the targeted effect size or a 13% increase in the sample size when compared to one million tests. In contrast, 10 tests require a 30% increase in the targeted effect size or a 70% increase in the sample size as compared with a single test. For 80% power and one billion or one trillion tests, the required sample sizes, respectively, are approximately 7 or 9 times that needed for a single test. See Table 1 for some numerical results and the provided Excel calculator (Supplementary Table 1) to explore unreported results and specific study designs.

---

### Disproportionate declines of formerly abundant species underlie insect loss [^715ce79f]. Nature (2024). Excellent credibility.

The community data available to us can be divided into three types, with different processing requirements:
The study provided one species-by site-by-year matrix. We used this directly to calculate all biodiversity metrics for analysis.
Species-by-site by-year data were provided together with an account of the sampling effort (number of samples underlying each row is reported, but no raw data were provided per individual sample). Here, we subsampled the years with higher trapping effort to provide the expected community composition if the number of samples had been the same as in the year with the lowest sample size. We repeated this subsampling procedure 100 times, and calculated the means of each biodiversity metric, to be used as model input. In some cases, it was clear that subsampling to the lowest sample size would lead to a large loss of data, and in such cases, it was more economical to discard a year than lose a large amount of data. A practical example: if year 1 has 300 individuals in 6 traps and year 10 has 500 individuals in 4 traps, we subsampled 200 individuals (4/6 of 300) from year 1, so that each year has the number of individuals equivalent to 4 traps. We repeated this resampling 100 times to derive 100 hypothetical community compositions, and calculated the biodiversity metrics and population abundances for each iteration.
Exact data were provided per sampling event (number of individuals per species per trap/site per date), but the number of traps, dates or periods sampled is variable among years (owing to trap malfunctions or changes in sampling design or sampling frequency). Here, we first divided the years into monthly or bimonthly periods, and then decided whether we could maximize data use by including more months or more years (that is, whether to exclude periods that were not sampled each year, or exclude years that did not have samples in each period). The aim here was to maximize data use, so that we could attain the highest quality of data for a maximum number of years. For each period, we then subsampled the minimum number of samples observed in any year from the available samples, and added these together to form one yearly sample. This was repeated 100 times, and the biodiversity metrics were calculated each time. The mean values of these 100 iterations were used for model input. A practical example: ten pitfall traps per site were usually emptied weekly, but sometimes fortnightly, the trapping period changed from year-round at the start to May–September in later years, and sometimes samples were missing owing to flooding or trampling by livestock. Here, we first discarded all months other than May–September. Then we allocated five monthly periods and calculated the number of sampling days (sum of sampling period × number of traps) for each month. We then drew random samples (without replacement) from the available samples per month, until the total number of sampling days added up to the minimum number sampling days observed. We added the months up to form one yearly sample and calculated our metrics. This was repeated 100 times.

---