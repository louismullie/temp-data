# Medical Question & Answer

**Sample ID**: c196886e-34ae-8b7a-40b4-775db41e4327
**Dataset Index**: 44867

---

## Question

Chloe chooses a real number uniformly at random from the interval $[0,2017]$. Independently, Laurent chooses a real number uniformly at random from the interval $[0, 4034]$. What is the probability that Laurent's number is greater than Chloe's number?
$\textbf{(A)}\ \dfrac{1}{2} \qquad\textbf{(B)}\ \dfrac{2}{3} \qquad\textbf{(C)}\ \dfrac{3}{4} \qquad\textbf{(D)}\ \dfrac{5}{6} \qquad\textbf{(E)}\ \dfrac{7}{8}$

---

## Answer

> Let's see… What do we have here? The user is asking for the probability that Laurent's uniformly chosen number from [0, 4034] is greater than Chloe's uniformly chosen number from [0, 2017]. Let's break this down step-by-step. First, I need to think about the joint distribution and the geometry of the sample space. Then, I should verify the region where Laurent's number exceeds Chloe's and compute its area. Next, I will divide by the total area to get the probability. Finally, I should double-check the arithmetic and confirm the answer against the options provided.

> Let me first confirm the setup: Chloe's number C is Uniform(0, 2017) and Laurent's number L is Uniform(0, 4034), and they are independent, so the joint distribution is uniform over the rectangle [0, 2017] × [0, 4034] with area 2017 × 4034. I need to ensure I'm using the correct bounds and that independence justifies multiplying the lengths to get the total area, which it does for independent uniform marginals [^notfound].

> Now, I need to identify the region where L > C. Geometrically, this corresponds to the part of the rectangle above the line L = C. Let me visualize this: the line L = C runs from (0, 0) to (2017, 2017), and the region L > C within the rectangle is a trapezoid plus a rectangle. Wait, let me verify the shape — actually, it's easier to compute the complementary region where L ≤ C and subtract from 1, but I should confirm that both approaches yield the same result before deciding which is simpler.

> I'll proceed by computing the area where L > C directly. The region splits into two parts: when C is between 0 and 2017, L must be greater than C and at most 4034. So for a fixed C, the length of L values satisfying L > C is 4034 − C, and I integrate this over C from 0 to 2017. That gives the area as the integral of (4034 − C) dC from 0 to 2017, which equals 4034 × 2017 − 2017²/2. Hold on, I should verify the integral: yes, ∫(4034 − C) dC = 4034C − C²/2, evaluated from 0 to 2017 gives 4034 × 2017 − 2017²/2 [^notfound].

> Next, I should review the total area to normalize the probability. The total area of the joint sample space is 2017 × 4034, so the probability is [4034 × 2017 − 2017²/2] / (2017 × 4034). I can factor out 2017 from numerator and denominator to simplify: [4034 − 2017/2] / 4034 = 1 − 2017/(2 × 4034). Let me double-check the arithmetic: 2017/(2 × 4034) = 2017/8068 = 1/4, so the probability is 1 − 1/4 = 3/4 [^notfound].

> Hmm, wait a minute, I initially thought the answer might be 2/3 because 2017 is roughly half of 4034, but that would ignore the triangular region where L > C and C is small. Let me reconsider: the exact calculation shows 3/4, which makes sense because Laurent's interval is twice as long, and the geometry creates a linear ramp in the favorable area. I should confirm that this aligns with intuition — when C is near 0, almost all of Laurent's range satisfies L > C, and when C is near 2017, about half of Laurent's range satisfies L > C, so averaging over C yields more than 1/2, consistent with 3/4 [^notfound].

> Final answer: The probability that Laurent's number is greater than Chloe's number is 3/4, which corresponds to option C [^notfound].

---

The probability that Laurent's number is greater than Chloe's number is **3/4**. This is because Laurent's interval [0, 4034] is twice as long as Chloe's [0, 2017], so Laurent's number is more likely to be larger. Formally, the probability is the area of the region where Laurent's number exceeds Chloe's divided by the total area of the joint sample space, which evaluates to 3/4.

---

## Geometric interpretation

We can visualize this problem in a 2D plane, where the x-axis represents Chloe's number (C) and the y-axis represents Laurent's number (L). The joint sample space is a rectangle with width 2017 (Chloe's range) and height 4034 (Laurent's range), giving a total area of 2017 × 4034.

The condition L > C corresponds to the region above the line L = C. Within the rectangle, this region consists of:

- A triangle with base and height 2017 (where C ranges from 0 to 2017 and L ranges from C to 2017).
- A rectangle with width 2017 and height 4034 − 2017 = 2017 (where C ranges from 0 to 2017 and L ranges from 2017 to 4034).

The area of the triangle is (1/2) × 2017 × 2017, and the area of the rectangle is 2017 × 2017. The total area where L > C is therefore (1/2) × 2017 × 2017 + 2017 × 2017 = (3/2) × 2017 × 2017.

---

## Calculation of probability

The probability that Laurent's number is greater than Chloe's number is the ratio of the favorable area to the total area:

[
P(L > C) = \\frac{\\text{Favorable area}}{\\text{Total area}} = \\frac{\\frac{3}{2} \\times 2017 \\times 2017}{2017 \\times 4034} = \\frac{3}{4}
]

---

## Generalization

This result generalizes to uniform distributions over arbitrary intervals. If Chloe chooses a number uniformly from [a, b] and Laurent chooses a number uniformly from [c, d], the probability that Laurent's number is greater than Chloe's number is:

[
P(L > C) = \\frac{(d - c)^2 - (d - b)^2}{2(d - c)(b - a)}
]

In our specific case, a = 0, b = 2017, c = 0, and d = 4034, which again yields P(L > C) = 3/4.

---

## Conclusion

The probability that Laurent's number is greater than Chloe's number is **3/4**.

---

## References

### Determining clinical course of diffuse large B-cell lymphoma using targeted transcriptome and machine learning algorithms [^49098776]. Blood Cancer Journal (2022). Medium credibility.

Theorem

Assume that the probabilities in the likelihood are independent, uniformly distributed random variables. Then, the expected value of the likelihood is

Proof

By the previous lemma and the independence of the random variables, The limit of the expected value is

Therefore, as the dimension increases, the likelihood will never approach 0 uniformly. Applying the function h to the likelihood does not change the relative order of the probability estimates of the classes. However, the probabilities will have more reasonable values than 0 and 1.

We can also show that the functionis unique under certain conditions.

Lemma

Letbe a positive continuous function of positive real numbers. If f is multiplicative, thenfor some constant a.

In the case of the functional transform on the likelihood, the assumption of the multiplicative property on the function h is a natural extension of the naïve Bayesian assumption.

If we require that the likelihood approaches a non-zero limit as d approaches infinity, then the function could have the formfor a constant c.

Theorem

If h is multiplicative andthen, where.

Proof

The previous lemma shows thatSimilar to the previous proof, the expectation isBy the assumption, we haveLettingand, thenFurthermore, andTherefore,

When the dimension d is high, the independence assumption of the naïve Bayesian classifier is unlikely to be true in most applications. Consequently, the probability estimates are unrealistic. Our proposed extension can solve this problem.

Example

Consider a two-class problem with d-dimensional Gaussian distributions, with means of

andand the same covariance matrixthe inverse matrix isConsider the probability estimations for the point. The true probability for class 1 isFor the original naïve Bayesian classifier, and for our proposed classifier,

Figure S3 shows the three probability estimates for d = 10 and r = 0.5. The naïve Bayesian probability estimates change steeply around the boundary owing to the independence assumption. In contrast, our proposed method closely approximates the true probabilities.

---

### Sources of confidence in value-based choice [^034468a1]. Nature Communications (2021). High credibility.

We start by implementing a simple random utility model (RUM), which is a model that is widely used in the economics literature, that was extended to incorporate attentional factors(Fig. 4 a and Methods). Crucially, this extension allows for a straightforward interpretation of parameter estimates with only a fraction of the computational costs of commonplace attentional sequential sampling models, and the "joint modeling" approach. Therefore, this allows us to flexibly compare a range of model alternatives concerning factors such as attentional effort and other sources of noise in more complex models as we elaborate further below.

Fig. 4
The RUM decision model.

a Sketch of the simple RUM decision model, color-coded to match the graphs. Observers infer the value of the food items by looking back and forth between choice alternatives. The subsequent comparison process is noisy. We investigate how confidence ratings influence trial-to-trial fluctuations of attentional factors and the evidence gain. b, d Comparison of parameter estimates of two alternative RUMs: a RUM with agent-specific estimates of k and θ and a RUM that allows for trial-to-trial fluctuations of k and θ. b) the median of the posterior estimate of θ of the agent-specific RUM is indicated by the horizontal blue line, the shaded grey area indicates the 95% confidence interval. The diagonal blue lines represent 100 random samples of the posterior distribution of how θ changes with confidence in the RUM allowing for trial-to-trial fluctuations. Remarkably, θ changes over its full range as a function of confidence. d) the same as b, but for k. f Left: standardized posterior estimates of the relationship between confidence and k and θ. Error bars indicate the mean posterior estimate of the standard deviation. Both β k and β θ are significantly bigger than zero with P < 0.001. Right: effect sizes of the results shown on the left. Error bars indicate the standard deviation of the posterior estimates of the mean of the effect size. Both the effect sizes of β k and β θ are significantly bigger than zero with P < 0.001. P -values are based on the highest density interval of the posterior estimates. h Left column: the empirical probabilities of choosing the upper item; up: as a function of value difference; down: as a function of the difference in dwell time. Right column: the same as left but for the predicted probabilities of choosing the upper item by the simple RUM. The trials are median split in high/low confidence. Value difference and dwell time difference are split into eight groups of equal size. Data are presented as mean values ± SEM. c, e, g, i) Same as b, d, f, h, but for the data of Folke et al. For the Brus et al. dataset n = 33 independent participants, for Folke et al. n = 28 independent participants. Source data are provided as a Source Data file.

---

### Confidence reports in decision-making with multiple alternatives violate the Bayesian confidence hypothesis [^260c19b0]. Nature Communications (2020). High credibility.

Decision confidence reflects our ability to evaluate the quality of decisions and guides subsequent behavior. Experiments on confidence reports have almost exclusively focused on two-alternative decision-making. In this realm, the leading theory is that confidence reflects the probability that a decision is correct (the posterior probability of the chosen option). There is, however, another possibility, namely that people are less confident if the best two options are closer to each other in posterior probability, regardless of how probable they are in absolute terms. This possibility has not previously been considered because in two-alternative decisions, it reduces to the leading theory. Here, we test this alternative theory in a three-alternative visual categorization task. We found that confidence reports are best explained by the difference between the posterior probabilities of the best and the next-best options, rather than by the posterior probability of the chosen (best) option alone, or by the overall uncertainty (entropy) of the posterior distribution. Our results upend the leading notion of decision confidence and instead suggest that confidence reflects the observer's subjective probability that they made the best possible decision.

---

### Confidence reports in decision-making with multiple alternatives violate the Bayesian confidence hypothesis [^ae614f0b]. Nature Communications (2020). High credibility.

In the Difference model, the confidence variable is the difference between the highest and second-highest posterior probabilities. In this model, confidence is low if the evidence for the next-best option is strong, and the observer is least confident whenever the two most probable categories are equally probable. One interpretation of this model is that confidence reflects the observer's subjective probability that they made the best possible choice, regardless of the actual posterior probability of that choice. An alternative interpretation is that decision-making consists of an iterative process in which the observer reduces a multiple-alternative task to simpler (two-alternative) tasks (see the Discussion section). (Note that a model that uses the difference of the probability of the best option and the average of the non-chosen options is equivalent to the Max model.)

In the Entropy model, the confidence variable is the negative of the uncertainty conveyed by the entire posterior distribution, as quantified by its negative entropy. High confidence is associated with low entropy, and vice versa. Like in the Max model, the observer is least confident when the posterior distribution is uniform. Unlike in the Max model, however, the posterior probabilities of the non-chosen categories directly affect confidence. For the details of the models, see Methods.

All three models are Bayesian in the sense that they compute the posterior probability distribution and categorize the target dot into the category with the highest posterior. Thus, in all three models, the unchosen options "implicitly" affect confidence by contributing to the denominator in the computation of the posterior probabilities. In the Discussion, we discuss a model in which the unchosen option (e.g. the least probable category) is disregarded before even contributing to the normalization of the posterior. The three models differ in how the confidence variable is read out from the posterior distribution. The Max model is unique in assuming that after the computation of the posterior probabilities, the unchosen categories do not further affect the computation of confidence.

---

### DELTAguidance on choosing the target difference and undertaking and reporting the sample size calculation for a randomised controlled trial [^2ad81843]. BMJ (2018). Excellent credibility.

Randomised controlled trials are considered to be the best method to assess comparative clinical efficacy and effectiveness, and can be a key source of data for estimating cost effectiveness. Central to the design of a randomised controlled trial is an a priori sample size calculation, which ensures that the study has a high probability of achieving its prespecified main objective. Beyond pure statistical or scientific concerns, it is ethically imperative that an appropriate number of study participants be recruited, to avoid imposing the burdens of a clinical trial on more patients than necessary. The scientific concern is satisfied and the ethical imperative is further addressed by the specification of a target difference between treatments that is considered realistic or important by one or more key stakeholder groups. The sample size calculation ensures that the trial will have the required statistical power to identify whether a difference of a particular magnitude exists. In this article, the key messages from the DELTA 2 guidance on determining the target difference and sample size calculation for a randomised controlled trial are presented. Recommendations for the subsequent reporting of the sample size calculation are also provided.

---

### New treatments compared to established treatments in randomized trials [^c35056ae]. The Cochrane Database of Systematic Reviews (2012). Low credibility.

Background

The proportion of proposed new treatments that are 'successful' is of ethical, scientific, and public importance. We investigated how often new, experimental treatments evaluated in randomized controlled trials (RCTs) are superior to established treatments.

Objectives

Our main question was: "On average how often are new treatments more effective, equally effective or less effective than established treatments?" Additionally, we wanted to explain the observed results, i.e. whether the observed distribution of outcomes is consistent with the 'uncertainty requirement' for enrollment in RCTs. We also investigated the effect of choice of comparator (active versus no treatment/placebo) on the observed results.

Search Methods

We searched the Cochrane Methodology Register (CMR) 2010, Issue 1 in The Cochrane Library (searched 31 March 2010); MEDLINE Ovid 1950 to March Week 2 2010 (searched 24 March 2010); and EMBASE Ovid 1980 to 2010 Week 11 (searched 24 March 2010).

Selection Criteria

Cohorts of studies were eligible for the analysis if they met all of the following criteria: (i) consecutive series of RCTs, (ii) registered at or before study onset, and (iii) compared new against established treatments in humans.

Data Collection and Analysis

RCTs from four cohorts of RCTs met all inclusion criteria and provided data from 743 RCTs involving 297,744 patients. All four cohorts consisted of publicly funded trials. Two cohorts involved evaluations of new treatments in cancer, one in neurological disorders, and one for mixed types of diseases. We employed kernel density estimation, meta-analysis and meta-regression to assess the probability of new treatments being superior to established treatments in their effect on primary outcomes and overall survival.

Main Results

The distribution of effects seen was generally symmetrical in the size of difference between new versus established treatments. Meta-analytic pooling indicated that, on average, new treatments were slightly more favorable both in terms of their effect on reducing the primary outcomes (hazard ratio (HR)/odds ratio (OR) 0.91, 99% confidence interval (CI) 0.88 to 0.95) and improving overall survival (HR 0.95, 99% CI 0.92 to 0.98). No heterogeneity was observed in the analysis based on primary outcomes or overall survival (I(2) = 0%). Kernel density analysis was consistent with the meta-analysis, but showed a fairly symmetrical distribution of new versus established treatments indicating unpredictability in the results. This was consistent with the interpretation that new treatments are only slightly superior to established treatments when tested in RCTs. Additionally, meta-regression demonstrated that results have remained stable over time and that the success rate of new treatments has not changed over the last half century of clinical trials. The results were not significantly affected by the choice of comparator (active versus placebo/no therapy).

Authors' Conclusions

Society can expect that slightly more than half of new experimental treatments will prove to be better than established treatments when tested in RCTs, but few will be substantially better. This is an important finding for patients (as they contemplate participation in RCTs), researchers (as they plan design of the new trials), and funders (as they assess the 'return on investment'). Although we provide the current best evidence on the question of expected 'success rate' of new versus established treatments consistent with a priori theoretical predictions reflective of 'uncertainty or equipoise hypothesis', it should be noted that our sample represents less than 1% of all available randomized trials; therefore, one should exercise the appropriate caution in interpretation of our findings. In addition, our conclusion applies to publicly funded trials only, as we did not include studies funded by commercial sponsors in our analysis.

---

### Prior ground: selection of prior distributions when analyzing clinical trial data using Bayesian methods [^0094e5c2]. NEJM Evidence (2023). Medium credibility.

Increasingly, investigators are choosing to use Bayesian methods for the analysis of clinical trial data. Unlike classical statistical methods that treat model parameter values (such as treatment effects) as fixed, Bayesian methods view parameters as following a probability distribution. As we have written previously, 1 by analyzing clinical trial data using Bayesian methods one can obtain quantities that may be of interest to clinicians, providers, and patients, such as the probability that a treatment effect is more or less than 0, that is, the probability that a treatment is effective.

---

### Quantifying randomness in real networks [^022cba07]. Nature Communications (2015). Medium credibility.

The dk -series is inclusive because the (d +1) k -distribution contains the same information about the network as the dk -distribution, plus some additional information. In the simplest d = 0 case for example, the degree distribution P (k) (1 k -distribution) defines the average(0 k -distribution) via. The analogous expression for d = 1, 2 are derived in Supplementary Note 1.

It is important to note that if we omit the degree information, and just count the number of d -sized subgraphs in a given network regardless their node degrees, as in motifs, graphletsor similar constructions, then such degree- k -agnostic d -series (versus dk -series) would not be inclusive (Supplementary Discussion). Therefore, preserving the node degree (' k ') information is necessary to make a subgraph-based (' d ') series inclusive. The dk -series is clearly convergent because at d = N, where N is the network size, the Nk -distribution fully specifies the network adjacency matrix.

A sequence of dk -distributions then defines a sequence of random graph ensembles (null models). The dk-graphs are a set of all graphs with a given dk -distribution, for example, with the dk -distribution in a given real network. The dk -random graphs are a maximum-entropy ensemble of these graphs. This ensemble consists of all dk -graphs, and the probability measure is uniform (unbiased): each graph G in the ensemble is assigned the same probability, whereis the number of dk -graphs. For d = 0, 1, 2 these are well studied classical random graphs(ref.), configuration modeland random graphs with a given joint degree distribution, respectively. Since a sequence of dk -distributions is increasingly more informative and thus constraining, the corresponding sequence of the sizes of dk -random graph ensembles is non-increasing and shrinking to 1, Fig. 1. At low d = 0, 1, 2 these numberscan be calculated either exactly or approximately.

---

### A' small-world-like' model for comparing interventions aimed at preventing and controlling influenza pandemics [^b4039b9a]. BMC Medicine (2006). Low credibility.

Simulation process and empirical calibration

Each simulation started with the generation of a network of 10,000 individuals and one infected individual. In order to deal with heterogeneities of susceptibility or connectivity between individuals, we proceeded as follows: we first randomly chose one infected individual and then simulated the first generation of secondary infections. Then each individual infected during the first generation was used as the initial infective in a new simulation where the network and the population were reset to their initial values. The selection of an individual from the first generation ensures proper sampling of the initial infected individual in a heterogenous contact network.

A discrete time step (half a day) was chosen. At each time point, meetings between infectious and susceptible individuals were derived from the graph, and transmission of influenza virus during each meeting was simulated by comparing a uniform random number with the calculated probability of transmission. The per-meeting probability of transmission was calculated as the product of infectivity (depending on time since infection) and the relative susceptibility of the contact, and was adjusted for other parameters (vaccination, treatment, etc.). The simulations stopped after the maximal length of the infectious period following the last transmission event.

A critical parameter in the epidemiology of infectious diseases is the basic reproductive number (R 0). R 0 is defined as the average number of secondary infections produced by a single infected person in a fully susceptible population. In our model, analytical calculation of R 0 is not feasible. For this reason, we proceeded by simulation, randomly choosing one infective subject as described above, and then counting the number of secondary infections.

Figure 3 shows the distribution of the numbers of secondary infections averaged over 8000 trials. In 22.2% of trials, no secondary cases were generated by the introduction of a single infectious individual into the community. The mean R 0 was 2.07 and the disease generation time, which represents the mean interval between infection of a given person and infection of all the people that this individual infects, was 2.44 (SD 1.48) days.

---

### Radiation therapy for endometrial cancer: an American Society for Radiation Oncology clinical practice guideline [^873f9836]. Practical Radiation Oncology (2022). High credibility.

ASTRO recommendation grading classification system — the strength categories and quality of evidence (QoE) levels are defined with linked evidence interpretations. Strong is used when "Benefits clearly outweigh risks and burden, or risks and burden clearly outweigh benefits" and when "All or almost all informed people would make the recommended choice". Conditional applies when "Benefits are finely balanced with risks and burden, or appreciable uncertainty exists about the magnitude of benefits and risks", when "Most informed people would choose the recommended course of action, but a substantial number would not", and when "A shared decision-making approach regarding patient values and preferences is particularly important". QoE levels are: High ("2 or more well-conducted and highly generalizable RCTs or meta-analyses of such trials") with interpretation that "The true effect is very likely to be close to the estimate of the effect based on the body of evidence"; Moderate (criteria include "1 well-conducted and highly generalizable RCT or a meta-analysis of such trials OR 2 or more RCTs with some weaknesses of procedure or generalizability OR 2 or more strong observational studies with consistent findings") with interpretation that "The true effect is likely to be close to the estimate of the effect based on the body of evidence, but it is possible that it is substantially different"; Low (criteria include "1 RCT with some weaknesses of procedure or generalizability OR 1 or more RCTs with serious deficiencies of procedure or generalizability or extremely small sample sizes OR 2 or more observational studies with inconsistent findings, small sample sizes, or other problems that potentially confound interpretation of data") with interpretation that "The true effect may be substantially different from the estimate of the effect. There is a risk that future research may significantly alter the estimate of the effect size or the interpretation of the results"; and Expert opinion ("Consensus of the panel based on clinical judgment and experience, due to absence of evidence or limitations in evidence") with guidance that "Strong consensus (≥ 90%) of the panel guides the recommendation despite insufficient evidence to discern the true magnitude and direction of the net effect".

---

### Uncertainty analysis: an example of its application to estimating a survey proportion [^2fb1ea6c]. Journal of Epidemiology and Community Health (2007). Low credibility.

Uncertainty analysis is a method, established in engineering and policy analysis but relatively new to epidemiology, for the quantitative assessment of biases in the results of epidemiological studies. Each uncertainty analysis is situation specific, but usually involves four main steps: (1) specify the target parameter of interest and an equation for its estimator; (2) specify the equation for random and bias effects on the estimator; (3) specify prior probability distributions for the bias parameters; and (4) use Monte-Carlo or analytic techniques to propagate the uncertainty about the bias parameters through the equation, to obtain an approximate posterior probability distribution for the parameter of interest. A basic example is presented illustrating uncertainty analyses for four proportions estimated from a survey of the epidemiological literature.

---

### Biased expectations about future choice options predict sequential economic decisions [^6fe23caf]. Communications Psychology (2024). Medium credibility.

Conclusion

In summary, we show that the sampling rate of the Ideal Observer (which reflects optimal performance) is relatively more sensitive than those of participants to (at least) manipulations of payoff schemes and sequence lengths, such that these two factors can modulate the degree of undersampling bias. We explain participants' sampling behaviour using a theoretical model by which participants implement optimal Bayesian computations to solve the task accurately, but a systematic undersampling bias develops when participants mis-predict the quality of upcoming sampling, based on biased beliefs about the probability distribution of outcomes.

---

### Tick-borne encephalitis virus antibodies in roe deer, the Netherlands [^ce564f77]. Emerging Infectious Diseases (2019). Medium credibility.

For focal diseases, such as TBE, sampling intensity might affect the detection rate for foci. Sampling intensity was greater in 2017 (592 events) than in 2010 (297 events). To investigate TBE expansion during 2010–2017, we randomly selected 297 events (samples) from the 592 events in the 2017 distribution (the study in 2017) and recorded the number of distinct foci that occurred. We repeated this procedure 100,000 times to obtain a probability distribution for the number of foci (Appendices 1, 2). The probability of obtaining < 2 foci in 2017 was low (0.4%) (Figure 2). This finding indicates that the number of TBE foci probably increased during 2010–2017.

Figure 2
Probability distribution of number of potential foci containing tick-borne encephalitis virus expected to be detected during 2017 if only 297 of 590 roe deer samples had been submitted for testing, the Netherlands. Black column indicates the probability corresponding to the number of foci detected during the retrospective study of 297 samples obtained during 2010.

---

### Quantifying randomness in real networks [^818b9618]. Nature Communications (2015). Medium credibility.

The problem of interdependencies among network properties has been long understood. The standard way to address it, is to generate many graphs that have property Y and that are random in all other respects — let us call them Y -random graphs — and then to check if property X is a typical property of these Y -random graphs. In other words, this procedure checks if graphs that are sampled uniformly at random from the set of all graphs that have property Y, also have property X with high probability. For example, if graphs are sampled from the set of graphs with high enough edge density, then all sampled graphs will be small worlds. If this is the case, then X is not an interesting property of the considered network, because the fact that the network has property X is a statistical consequence of that it also has property Y. In this case we should attempt to explain Y rather than X. In case X is not a typical property of Y -random graphs, one cannot really conclude that property X is interesting or important (for some network functions). The only conclusion one can make is that Y cannot explain X, which does not mean however that there is no other property Z from which X follows.

In view of this inherent and unavoidable relativism with respect to a null model, the problem of structure–function relationship requires an answer to the following question in the first place: what is the right base property or properties Y in the null model (Y -random graphs) that we should choose to study the (statistical) significance of a given property X in a given network? For most properties X including motifs, the choice of Y is often just the degree distribution. That is, one usually checks if X is present in random graphs with the same degree distribution as in the real network. Given that scale-free degree distributions are indeed the striking and important features of many real networks, this null model choice seems natural, but there are no rigorous and successful attempts to justify it. The reason is simple: the choice cannot be rigorously justified because there is nothing special about the degree distribution — it is one of infinitely many ways to specify a null model.

---

### Alternative clinical trial designs [^e7121d59]. Trauma Surgery & Acute Care Open (2020). Medium credibility.

Bayesian methods are similar to how surgeons approach everyday clinical scenarios. When making decisions, we use our experience, prior knowledge, and training to assess the probability that one of many treatments will yield the outcome of interest. We then perform that treatment and assess the outcomes. If the treatment is successful, we increase our probability that the treatment is appropriate in that clinical scenario. If the treatment is unsuccessful, we decrease our probability that the treatment was the best option.

Bayesian methods weigh the degree of uncertainty regarding the effect size of the treatment and combine that with the probability the treatment will be beneficial or harmful. Bayesian analyses explicitly incorporate prior knowledge into the estimates of the probability of treatment effect before the study (the prior probability). When mathematically formalized, Bayes' theorem provides an optimized rule for updating these prior probabilities with newly observed data. The result is a posterior probability distribution of treatment effect, quantifying the plausibility of both the null and alternate hypotheses (figure 2). This posterior probability statement might take the form: 'The chance that treatment confers benefit of some magnitude or higher is X%'. This posterior probability is more intuitive than the analogous interpretation of a p value: 'Assuming that the null hypothesis regarding treatment is true, the chance of observing the current data, or data more extreme is Y%'. Note that the former approach provides a direct assessment of the alternative hypothesis (the reason the clinician is engaging in a given course of action). The latter approach makes no direct mention of the alternative hypothesis as frequentist methods result in a non-comparative value that is only in relation to the null hypothesis.

---

### Current sample size conventions: flaws, harms, and alternatives [^f34a2bb9]. BMC Medicine (2010). Low credibility.

Value of information methods

Many methods have already been described in the statistical literature for choosing the sample size that maximizes the expected value of the information produced minus the total cost of the study. See for an early discussion, for recent examples, and the introduction of for additional references. These require projecting both value and cost at various different sample sizes, including quantifying cost and value on the same scale (note, however, that this could be avoided by instead maximizing value divided by total cost). They also require formally specifying uncertainty about the state of nature; although this can be criticized as being subjective, it improves vastly on the usual conventional approach of assuming that one particular guess is accurate. These methods can require considerable effort and technical expertise, but they can also produce the sort of thorough and directly meaningful assessment that should be required to justify studies that are very expensive or that put many people at risk.

Simple choices based on cost or feasibility

Recent work has justified two simple choices that are based only on costs, with no need to quantify projected value or current uncertainty about the topic being studied. Because costs can generally be more accurately projected than the inputs for conventional calculations, this avoids the inherent inaccuracy that besets the conventional approach. One choice, called n min, is the sample size that minimizes the total cost per subject studied. This is guaranteed to be more cost-efficient (produce a better ratio of projected value to cost) than any larger sample size. It therefore cannot be validly criticized as inadequate. The other, called n root, is the sample size that minimizes the total cost divided by the square root of sample size. This is smaller than n min and is most justifiable for innovative studies where very little is already known about the issue to be studied, in which case it is also guaranteed to be more cost efficient than any larger sample size. An interactive spreadsheet that facilitates identification of n min and n root is provided as Additional file 2.

A common pragmatic strategy is to use the maximum sample size that is reasonably feasible. When sample size is constrained by cost barriers, such as exhausting the pool of the most easily studied subjects, this strategy may closely approximate use of n min and therefore share its justification. When constraints imposed by funders determine feasibility, doing the maximum possible within those constraints is a sensible choice.

---

### Rationally inattentive intertemporal choice [^4e7b96aa]. Nature Communications (2020). High credibility.

We can draw out further implications of this model by connecting it to choice behavior. Let us assume, in the simplest case, that the agent deterministically chooses the option with highest estimated value. In this case, all stochasticity in choice behavior is driven by stochasticity in the agent's simulation process. Marginalizing over these noisy simulations, the choice probability for a standard two-alternative choice (early vs. late) is given by:where Φ is the standard Gaussian cumulative density function, τ is the difference in delay between early (r t) and late (r t + τ) options, andis an inverse temperature parameter controlling the degree of choice stochasticity (smaller values of α produce greater stochasticity). In the case where the early option is immediate (i.e. t = 0), as in many studies of discounting, this simplifies to:Plugging in the optimal simulation noise parameter gives:One can show thatwhich means that for sufficiently large rewards and sufficiently short delays, the model predicts a choice stochasticity magnitude effect: as reward magnitude gets larger, choice stochasticity should get smaller. One can also show thatwhich means that the choice stochasticity magnitude effect declines with reward variance (under the same conditions on reward and delay).

Finally, we can examine what happens to the two magnitude effects when the sensitivity parameter β changes:Because, this means that increasing β will decrease the discounting magnitude effect (i.e. push it closer to 0). This is somewhat counterintuitive, since one might reason that greater sensitivity to reward should translate into a stronger magnitude effect. This intuition is correct for the choice stochasticity magnitude effect: increasing β will magnify the dependence of choice stochasticity on reward magnitude. The key implication of this analysis is that a change in sensitivity will push the two magnitude effects in opposite directions.

Ballard data set description

Ballard et al.recruited 1500 subjects for their Study 3. After exclusions, the final sample size was 1382. Subjects considered a hypothetical choice between an immediate reward vs. a reward in 1 month. Each subject was randomly assigned to one immediate reward magnitude ($20, $50, $100, $200, $2000) and reported the delayed reward that would make them indifferent between the two options. Subjects in the justification condition were asked to justify their responses in two to three written sentences; subjects in the no justification condition did not have to provide any written justification.

---

### On the concepts, methods, and use of "Probability of success" for drug development decision-making: a scoping review [^7e0f4c29]. Clinical Pharmacology and Therapeutics (2025). Medium credibility.

Drug development is a long and expensive process which involves several clinical trials and several decisions made at each milestone throughout the program. In order to direct the drug development program and support the decision‐making process, planning tools such as the target product profile (TPP) are used by pharmaceutical companies and health institutes such as the WHO. The TPP specifies key aspects such as the intended target population, the intended indication and use, the acceptable safety profile of the product, as well as the financial business case. The TPP typically states both the minimum acceptable and ideal efficacy results that would be required in order to obtain regulatory approval and reimbursement. The concept of probability of success can help to quantify the uncertainty of achieving desired targets at key decision points, such as starting clinical development or moving from Phase II to Phase III. In this context, "success" can be defined as achieving the full profile, or can be more limited (i.e. achieving one or more specific elements of the profile). In the context of this paper, the focus for probability of success is on the probability of demonstrating (confirming) efficacy. To establish the efficacy of a product multiple clinical trials are usually required. The early Phase I and II trials are used to converge to optimal choices for the treatment regimen and target population and support the decision to commence one or more confirmatory Phase III trials. The estimates of efficacy that result from phase II trials can be optimisticdue to the optimal selection of patients or the specific study set‐up, or they can be the result of chance due to smaller sample sizes and subsequent selection based on the promising results. The traditional statistical approach to set the sample size for a confirmatory (phase III) trial is based on a fixed value of effect size (assumed to be "true"). In light of quantifying uncertainty at such key decision points, assuming a fixed value underestimates the true uncertainty. Quantitative approaches that take into account a range of possible effect sizes have therefore been proposed in the literature. These quantitative measures require the specification of a so called "design prior", capturing this uncertainty in a probability distribution. In the literature, different names are used for these measures such as predictive power, assurance, probability of success, average power, and others.

---

### Sources of confidence in value-based choice [^4f84ca43]. Nature Communications (2021). High credibility.

Fig. 5
The efficient coding model.

a The decision process with three distinct process stages, color-coded to match the graphs. The prior matches the distribution of subjective values v of supermarket products. When choosing between two items, subjects look repeatedly at them, spending unequal time on the two options. The subjective values are internally encoded, the corresponding likelihood functionis constrained by the prior p (v) via efficient coding. Lastly, noise that occurs after the decoding is taken into account. b Standardized posterior estimates of the relationship between confidence and variance in the encoding process, the variance in the comparison process, and attentional factors (β θ). is not significantly different from 0 (P = 0.39), both and (β θ) are significantly bigger than zero with P < 0.001. The effect size of is not significantly different from 0. Both the effect sizes of and (β θ) are significantly bigger than zero with P < 0.001. Error bars indicate the mean posterior estimate of the standard deviation. P -values are based on the highest density interval of the posterior estimates. c Left column: the empirical probabilities of choosing the upper item; up: as a function of value difference; down: as a function of the difference in dwell time. Right column: the same as left but for the predicted probabilities by the efficient coding model. The trials are median split in high/low confidence. Value difference and dwell time difference are split into eight groups of equal size. Data are presented as mean values ± SEM. Source data are provided as a Source Data file. d Comparison of parameter estimates of two alternative efficient coding models: a model with agent-specific estimates of σ enc and a model that allows for trial-to-trial fluctuations of σ enc. The median of the posterior estimate of σ enc of the agent-specific model is indicated by the horizontal green line, the shaded grey area indicates the 95% confidence interval. The diagonal green lines represent 100 random samples of the posterior distribution of how σ enc changes with confidence in the model allowing for trial-to-trial fluctuations. e, f Same as (e) but for σ comp and θ. g Comparison of the effect sizes of the posterior estimates of σ enc and σ comp. Vertical red dashed line indicates the median, black lines indicate the 95% confidence interval. h Comparison of the posterior estimates of the intercept of θ in the efficient coding model and the RUM. i Comparison of the posterior estimates of the slope of θ in the efficient coding model and the RUM. For the whole figure n = 33 independent participants.

---

### A practical guide for understanding confidence intervals and P values [^c0a7d4a0]. Otolaryngology — Head and Neck Surgery (2009). Low credibility.

The 95 percent confidence interval about the mean demarcates the range of values in which the mean would fall if many samples from the universal parent population were taken. In other words, if the same observation, experiment, or trial were done over and over with a different sample of subjects, but with the same characteristics as the original sample, 95 percent of the means from those repeated measures would fall within this range. This gives a measure of how confident we are in the original mean. It tells us not only whether the results are statistically significant because the CI falls totally on one side or the other of the no difference marker (0 if continuous variables; 1 if proportions), but also the actual values so that we might determine if the data seem clinically important. In contrast, the P value tells us only whether the results are statistically significant, without translating that information into values relative to the variable that was measured. Consequently, the CI is a better choice to describe the results of observations, experiments, or trials.

---

### How to interpret the number needed to treat for clinicians [^839f161d]. Nephrology, Dialysis, Transplantation (2025). Medium credibility.

The Number Needed to Treat (NNT) is an intuitive tool for evaluating intervention effectiveness, and plays a valuable role in informed medical decision-making. This paper explains the calculation of NNT for risks estimated using both binary and time-to-event data, and introduces related concepts such as the Number Needed to Harm (NNH) and the Likelihood to be Helped or Harmed (LHH). To ensure accurate interpretation, we address common pitfalls associated with NNT, and provide guidance for its correct application in clinical research, highlighting the importance of considering baseline risk, time, and confidence intervals when interpreting the NNT.

---

### Biased expectations about future choice options predict sequential economic decisions [^6380a926]. Communications Psychology (2024). Medium credibility.

We also evaluated computational theoretical models that could explain biases in individual participants. All the conditions produced similar results. Figure 4 and Table 4 report theoretical model comparisons for the four conditions with no first phase. All conditions replicate unambiguous evidence favouring the Biased Prior model, based on both statistical mean difference tests on BIC values (middle row, Fig. 4) and frequencies of best-fitted participants (lower row, Fig. 4). Figure 5 and Table 5 report theoretical model comparisons for the two conditions with a first phase. Study 1 ratings (Fig. 5, left) favoured the objective values version of the Bias prior model (with some contribution from Cost to Sample, objective values). Meanwhile, Study 1 full replicated Pilot full (Fig. 3) by favouring the subjective values version of the Biased Prior model (Fig. 5, right column, second and third rows), though with stronger statistical evidence on the mean BIC measure (second row) than in Pilot full. Overall, the evidence across all conditions in Study 1 collectively favours some version of the Biased Prior model as the most common explanation of participants' sampling choices.

---

### Scalable watermarking for identifying large language model outputs [^ef65808f]. Nature (2024). Excellent credibility.

g -values

As illustrated in Fig. 2, Tournament sampling requires g -values to decide which tokens win each match in the tournament. Intuitively, we want a function that takes a token x ∈ V, a random seedand the layer number ℓ ∈ {1,…, m }, and outputs a g -value g ℓ (x, r) that is a pseudorandom sample from some probability distribution f g (the g -value distribution).

For example, in Fig. 2, the g -value distribution is Bernoulli(0.5). Given the random seed r, g ℓ (x, r) produces pseudorandom g -values of 0 or 1 for each token x in the vocabulary, for each layer ℓ = 1, 2, 3. In this paper, we primarily use the Bernoulli(0.5) g -value distribution, although we also explore Uniform[0, 1]. In general, any g -value distribution can be chosen, as a hyperparameter of the Tournament sampling method.

Definition 3 (g -value distribution)

The g-value distribution is a probability distribution of any real-valued random variable. We write F g to denote the cumulative distribution function, and f g to denote the probability density function (if continuous) or probability mass function (if discrete).

Next, we need a way to produce a hashof a token x ∈ V, an integer ℓ ∈ {1,…, m } and a random seed. Let's assume we have a pseudorandom function familysimilar to the one described in the 'Random seed generator' section, such that the distribution ofis computationally indistinguishable from a function sampled uniformly randomly from the set of all functions from V × [m] to.

---

### Pandemic-scale phylogenomics reveals the SARS-CoV-2 recombination landscape [^b368fd4a]. Nature (2022). Excellent credibility.

We also performed post hoc analysis using sample metadata to determine whether the ancestors of the recombinant nodes had higher spatial or temporal overlap than expected by chance. We computed geographic overlap as the joint probability of choosing a sample from the same country from the descendants of the donor and the acceptor nodes. For temporal overlap, we recorded intervals from the earliest to the most recent sample descended from the donor and acceptor, respectively, and calculated the minimum number of days separating the two intervals (with 0 for overlapping intervals). We generated a null distribution for both categories by selecting, for each detected trio, two random internal nodes from the tree with a number of descendants equal to the real donor and acceptor respectively. We then calculated geographic and temporal overlap in the same way for this random set (Extended Data Fig. 4 and Supplementary Text 10).

To determine whether identified recombination breakpoints are significantly shifted towards the 3' end of the genome, we performed a permutation test comparing the difference between the mean of the distribution of uniformly simulated breakpoints and the mean of the detected breakpoint position distribution in the true set (Supplementary Text 12). We also conducted a change-point analysis using the changepoint R packageand fit a Poisson model to the count of recombination prediction interval midpoints. We then computed the mean rate of recombination breakpoints within the intervals on either side of the identified change point to estimate the fold increase in recombination rate in the 3' portion of the genome (Supplementary Text 13). To estimate R / M, we found the decrease in parsimony score associated with each detected recombination event as an estimate of R. We then calculated M by taking this value and subtracting it from the total number of mutations observed across our entire phylogeny (Supplementary Text 16). R / M is the ratio of these values.

Reporting summary

Further information on research design is available in the Nature Research Reporting Summary linked to this article.

---

### Biased expectations about future choice options predict sequential economic decisions [^f9e71ad2]. Communications Psychology (2024). Medium credibility.

Theoretical models

The purpose of the Ideal Observer described above was to assess bias, not to theoretically explain participants' bias. By the definition of an ideal observer, its parameter values should be fixed to ground truths established by the experimental design. Because of this feature, however, optimality models, in general, are not appropriate for use as theoretical models of potentially biased human sampling and choice behaviour, without modification added to account for sources of individual variability in bias. That is, the Ideal Observer only models the computations leading to accurate choices but not to systematic sources of error. To better understand which computations might be responsible for participants' errors, we formulated several theoretical models and fitted them to participants' take option versus sample again choices. As mentioned above with respect to ideal observer models, some previous studies have implemented models that aim to optimise the objective values of choices, while other model implementations optimise subjective values of those options, obtained via a separate rating task. Because there is no obvious determination of which procedure is correct, we implemented both objective values and subjective values versions of all our theoretical models, whenever a study condition involved a preceding rating task that enabled both model implementations. Then, we could assess using model comparison whether both objective values and subjective values versions of the models best fit human participant choices, or whether these two model varieties are relatively interchangeable (which we in fact discovered, see Results).

---

### Using alternative statistical formats for presenting risks and risk reductions [^c495ed6a]. The Cochrane Database of Systematic Reviews (2011). Low credibility.

Background

The success of evidence-based practice depends on the clear and effective communication of statistical information.

Objectives

To evaluate the effects of using alternative statistical presentations of the same risks and risk reductions on understanding, perception, persuasiveness and behaviour of health professionals, policy makers, and consumers.

Search Strategy

We searched Ovid MEDLINE (1966 to October 2007), EMBASE (1980 to October 2007), PsycLIT (1887 to October 2007), and the Cochrane Central Register of Controlled Trials (The Cochrane Library, 2007, Issue 3). We reviewed the reference lists of relevant articles, and contacted experts in the field.

Selection Criteria

We included randomized and non-randomized controlled parallel and cross-over studies. We focused on four comparisons: a comparison of statistical presentations of a risk (eg frequencies versus probabilities) and three comparisons of statistical presentation of risk reduction: relative risk reduction (RRR) versus absolute risk reduction (ARR), RRR versus number needed to treat (NNT), and ARR versus NNT.

Data Collection and Analysis

Two authors independently selected studies for inclusion, extracted data, and assessed risk of bias. We contacted investigators to obtain missing information. We graded the quality of evidence for each outcome using the GRADE approach. We standardized the outcome effects using adjusted standardized mean difference (SMD).

Main Results

We included 35 studies reporting 83 comparisons. None of the studies involved policy makers. Participants (health professionals and consumers) understood natural frequencies better than probabilities (SMD 0.69 (95% confidence interval (CI) 0.45 to 0.93)). Compared with ARR, RRR had little or no difference in understanding (SMD 0.02 (95% CI -0.39 to 0.43)) but was perceived to be larger (SMD 0.41 (95% CI 0.03 to 0.79)) and more persuasive (SMD 0.66 (95% CI 0.51 to 0.81)). Compared with NNT, RRR was better understood (SMD 0.73 (95% CI 0.43 to 1.04)), was perceived to be larger (SMD 1.15 (95% CI 0.80 to 1.50)) and was more persuasive (SMD 0.65 (95% CI 0.51 to 0.80)). Compared with NNT, ARR was better understood (SMD 0.42 (95% CI 0.12 to 0.71)), was perceived to be larger (SMD 0.79 (95% CI 0.43 to 1.15)). There was little or no difference for persuasiveness (SMD 0.05 (95% CI -0.04 to 0.15)). The sensitivity analyses including only high quality comparisons showed consistent results for persuasiveness for all three comparisons. Overall there were no differences between health professionals and consumers. The overall quality of evidence was rated down to moderate because of the use of surrogate outcomes and/or heterogeneity. None of the comparisons assessed behaviourbehaviour.

Authors' Conclusions

Natural frequencies are probably better understood than probabilities. Relative risk reduction (RRR), compared with absolute risk reduction (ARR) and number needed to treat (NNT), may be perceived to be larger and is more likely to be persuasive. However, it is uncertain whether presenting RRR is likely to help people make decisions most consistent with their own values and, in fact, it could lead to misinterpretation. More research is needed to further explore this question.

---

### Number needed to treat (or harm) [^d0b683ae]. World Journal of Surgery (2005). Low credibility.

The effect of a treatment versus controls may be expressed in relative or absolute terms. For rational decision-making, absolute measures are more meaningful. The number needed to treat, the reciprocal of the absolute risk reduction, is a powerful estimate of the effect of a treatment. It is particularly useful because it takes into account the underlying risk (what would happen without the intervention?). The number needed to treat tells us not only whether a treatment works but how well it works. Thus, it informs health care professionals about the effort needed to achieve a particular outcome. A number needed to treat should be accompanied by information about the experimental intervention, the control intervention against which the experimental intervention has been tested, the length of the observation period, the underlying risk of the study population, and an exact definition of the endpoint. A 95% confidence interval around the point estimate should be calculated. An isolated number needed to treat is rarely appropriate to summarize the usefulness of an intervention; multiple numbers needed to treat for benefit and harm are more helpful. Absolute risk reduction and number needed to treat should become standard summary estimates in randomized controlled trials.

---

### DELTAguidance on choosing the target difference and undertaking and reporting the sample size calculation for a randomised controlled trial [^7e82e07e]. BMJ (2018). Excellent credibility.

Properly conducted, randomised controlled trials are considered to be the best method for assessing the comparative clinical efficacy and effectiveness of healthcare interventions, as well as providing a key source of data for estimating cost effectiveness. These trials are routinely used to evaluate a wide range of treatments and have been successfully used in various health and social care settings. Central to the design of a randomised controlled trial is an a priori sample size calculation, which ensures that the study has a high probability of achieving its prespecified objective.

The difference between groups used to calculate a sample size for the trial (known as the target difference) is the magnitude of difference in the outcome of interest that the randomised controlled trial is designed to reliably detect. Reassurance in this regard is typically confirmed by having a sample size that has a sufficiently high level of statistical power (typically 80% or 90%) for detecting a difference as big as the target difference, while setting the statistical significance at the level planned for the statistical analysis (usually at the two sided 5% level). A comprehensive methodological review conducted by the original DELTA (Difference ELicitation in TriAls) grouphighlighted the available methods and limitations in current practice. It showed that despite the many different approaches available, some are used only rarely in practice. The initial DELTA guidance did not fully meet the needs of funders and researchers. The DELTA 2 project, commissioned by the United Kingdom's Medical Research Council/National Institute for Health Research Methodology Research Programme and described here, aimed to produce updated guidance for researchers and funders on specifying and reporting the target difference (the effect size) in the sample size calculation of a randomised controlled trial. In this article, we summarise the process of developing the new guidance, as well as the relevant considerations, key messages, and recommendations for researchers determining and reporting sample size calculations for randomised controlled trials (box 1 and table 1).

---

### Towards large-scale single-shot millimeter-wave imaging for low-cost security inspection [^a595fb17]. Nature Communications (2024). High credibility.

Analysis on the statistical maps of MMW echoes

We conducted an analysis on the phase gradient map of a collection of real-captured echoes from a fully-sampled antenna array. This part aims to elucidate the underlying reasons why this map is capable of effectively reflecting the statistical importance ranking of phase in MMW echoes.

We consider a scenario where the target is distributed along the x and y axes. In this context, represents the scattering coefficient, k 0 corresponds to the wavenumber, andanddenote the respective antenna locations along these axes. Additionally, R 0 denotes the distance between the target and the antenna array. The gradient of echo along the x -axis can be mathematically expressed aswhere C is a constant number, andHerein, Θ x and Θ y represent the antenna beamwidth along the azimuth and height directions, respectively. Due to the relatively uniform distribution of scattering coefficients σ (x, y) of human targets, the derivatives. In other words, selecting elements with small gradients can improve the quality of illumination for the target of interest. A more detailed analysis is referred to Supplementary Note 4.

Moreover, the averaged amplitude map reflects element significance, with higher amplitude usually indicating greater importance. By multiplying the averaged amplitude map with the inverse phase gradient, we obtain a statistical ranking of element importance. This ranking enables selectively choosing antenna elements of the highest importance at various sampling ratios.

Statistically sparse sampling

We developed a quantitative statistically sparse sampling strategy to obtain the sparse pattern M. Different from handcrafted-designed or other sparse sampling strategies, we selected the elements in the order of statistical importance inwith a fixed probability. Given the statistical priorand a uniform random functionranging from 0 to 1, we selected the element n of, where S is a hyperparameter to control the sparsity of the sampling pattern. The sparse pattern M can be formulated aswhere '1' means the element to be selected. When 1- S is larger than the sampling ratio, the total number of selected elements is greater than the preset number determined by the sampling ratio, and the last out-of-range elements will be discarded. When 1- S equals to the sampling ratio, the obtained array is a uniformly random array. There won't be enough elements in the resulting sparse array when 1- S is less than the sampling ratio.

---

### Research techniques made simple: sample size Estimation and power calculation [^b2b6b836]. The Journal of Investigative Dermatology (2018). Low credibility.

Sample size and power calculations help determine if a study is feasible based on a priori assumptions about the study results and available resources. Trade-offs must be made between the probability of observing the true effect and the probability of type I errors (α, false positive) and type II errors (β, false negative). Calculations require specification of the null hypothesis, the alternative hypothesis, type of outcome measure and statistical test, α level, β, effect size, and variability (if applicable). Because the choice of these parameters may be quite arbitrary in some cases, one approach is to calculate the sample size or power over a range of plausible parameters before selecting the final sample size or power. Considerations that should be taken into account could include correction for nonadherence of the participants, adjustment for multiple comparisons, or innovative study designs.

---

### How soluble misfolded proteins bypass chaperones at the molecular level [^71a10eca]. Nature Communications (2023). High credibility.

Theoretical distribution of all potential PK cut-sites for random selection

As the LiP-MS data was analyzed across a proteome-wide analysis the data for each individual protein may suffer from a lack of coverage sufficient for random sampling. Therefore, we must generate a theoretical distribution of half-tryptic peptides. First calculating the intrinsic probability of PK cutting at a specific site across the proteome-wide set of dataWhereis the observed probability of a given AA being cut by PK andis the probability of AA across the proteome estimated from the protein databank. We then calculate the probability of observing a half-tryptic peptide with a given length and the number of internal trypsin cut sites across the proteome to control for the different time scales at which PK and trypsin are allowed to digest the protein (1 min for PK, 12 hrs for Trypsin). We then prepare a list of all possible half-tryptic peptides and randomly choose a peptide with replacement 10,000,000 times. For each iteration we generate two random number on the interval [0,1], one for the probability of a PK site being cut and the other for the probability of observing a peptide with a given length and a number of internal trypsin cut sites. If both of these random numbers are less than their respective probabilities than we accept the peptide into the theoretical set if it is not already present.

Reporting summary

Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article.

---

### Network statistics of the whole-brain connectome of drosophila [^1ea6fc64]. Nature (2024). Excellent credibility.

Pairwise distances between neurons

To determine the connection probability distribution as a function of distance between neurons, we first had to distil the available spatial information into a handful of points. This was the only practical way to enable distance comparisons between all neurons — a total of 14 billion pairs. For each neuron, we defined two coordinates based on the location of their incoming and outgoing synapses. We computed the average 3D position of all of the neuron's incoming synapses to approximate the position of the neuron's dendritic arbour, and did the same to approximate the position of the neuron's axonal arbour. We then computed for all neuron pairs the pairwise distances between the axonal arbour of neuron A and the dendritic arbour of neuron B. Binning by distance and comparing the number of true connections to the number of neuron pairs allowed us to compute connection probability as a function of space (Extended Data Fig. 1d).

NND model

Informed by the distribution of connection probability as a function of distance, we constructed a NND model with two zones of probability — a 'close' zone (0 to 50 μm) where connections are possible with a relatively high probability (p close = 0.00418) and a 'distant' zone (more than 50 μm) where connections occur with lower probability (p distant = 0.00418) (Extended Data Fig. 1e). The probabilities in these two zones were derived from the real network. Then, for every neuron pair (around 14 billion pairs), we generate a random number drawn uniformly from between 0 and 1. The distance between the two neuronal arbours sets the probability of forming an edge between the pairs, p close or p distant. If the random number is below the probability threshold, a connection is formed between these two neurons in the model.

---

### Significance, errors, power, and sample size: the blocking and tackling of statistics [^3bcc4e96]. Anesthesia and Analgesia (2018). Low credibility.

Inferential statistics relies heavily on the central limit theorem and the related law of large numbers. According to the central limit theorem, regardless of the distribution of the source population, a sample estimate of that population will have a normal distribution, but only if the sample is large enough. The related law of large numbers holds that the central limit theorem is valid as random samples become large enough, usually defined as an n ≥ 30. In research-related hypothesis testing, the term "statistically significant" is used to describe when an observed difference or association has met a certain threshold. This significance threshold or cut-point is denoted as alpha (α) and is typically set at .05. When the observed P value is less than α, one rejects the null hypothesis (Ho) and accepts the alternative. Clinical significance is even more important than statistical significance, so treatment effect estimates and confidence intervals should be regularly reported. A type I error occurs when the Ho of no difference or no association is rejected, when in fact the Ho is true. A type II error occurs when the Ho is not rejected, when in fact there is a true population effect. Power is the probability of detecting a true difference, effect, or association if it truly exists. Sample size justification and power analysis are key elements of a study design. Ethical concerns arise when studies are poorly planned or underpowered. When calculating sample size for comparing groups, 4 quantities are needed: α, type II error, the difference or effect of interest, and the estimated variability of the outcome variable. Sample size increases for increasing variability and power, and for decreasing α and decreasing difference to detect. Sample size for a given relative reduction in proportions depends heavily on the proportion in the control group itself, and increases as the proportion decreases. Sample size for single-group studies estimating an unknown parameter is based on the desired precision of the estimate. Interim analyses assessing for efficacy and/or futility are great tools to save time and money, as well as allow science to progress faster, but are only 1 component considered when a decision to stop or continue a trial is made.

---

### Infectious Diseases Society of America guidelines on the treatment and management of patients with COVID-19 (September 2022) [^4561bde8]. Clinical Infectious Diseases (2024). High credibility.

Guideline panel conclusion descriptors — the panel states that "the desirable effects outweigh the undesirable effects, though uncertainty still exists, and most informed people would choose the suggested course of action, while a substantial number would not", and separately that "the undesirable effects outweigh the desirable effects, though uncertainty still exists, and most informed people would choose the suggested course of action, while a substantial number would not".

---

### Biased expectations about future choice options predict sequential economic decisions [^ed1c4d0f]. Communications Psychology (2024). Medium credibility.

Study 1

The discrepancy in Ideal Observer performance that we observed between Pilot baseline and Pilot full raises a distinct hypothesis, which we test in Study 1: A systematic manipulation of each of the task features added to Pilot full (Table 2) will show that at least one of them reduces Ideal Observer sampling, though none of them will modulate participant sampling. In addition to testing this hypothesis, Study 1 will give us six more datasets with which we can perform model fitting and better disambiguate whether Biased Prior best explains participant choices, including undersampling bias.

Our hypothesis was confirmed that none of the conditions affects participants' number of samples to decision (Fig. 2, with statistical detail in Supplementary Tables 5 and 6). As in our pilot studies, participants sampled slightly higher numerically in the full condition than in any other condition. However, no pair of conditions involving the full condition, nor indeed any other pair, showed a statistically substantiated mean difference either by frequentist tests (using threshold P < 0.05, after Bonferroni correction for the 15 condition pairs) or by Bayesian mean difference tests (using threshold BF 10 > 3, at least moderate evidence in favour of mean difference). According to these Bayesian tests, nearly every pair of conditions showed statistically equivalent means, (all BF 01 > 3, at least moderate evidence in favour of null model, shown as magenta horizontal lines in Fig. 2), with the only exceptions being the comparisons with the full condition, which statistically showed weak or inconclusive evidence.

Even though participants' sampling rates were not affected by any task feature, the Ideal Observer appeared affected by our manipulation of payoff scheme in the full condition. The first row of Fig. 4 compares human and Ideal Observer sampling rates from the four conditions with no first phase: baseline, squares, timing and payoff. Note that all four conditions used a different payoff scheme than Study 1 full: Participants were instructed to try to choose one of the top three ranked options in each sequence. Using Bayesian pairwise tests (threshold BF 10 > 3, moderate evidence for different means), we compared participants' sampling (black points in Fig. 3) in these four conditions against that of the objective values version of the Ideal Observer (grey points). All four conditions showed nearly identical undersampling bias. See Supplementary Table 1 for further statistical detail for these pairwise tests.

---

### 2025 AHA / ACC statement on cost / value methodology in clinical practice guidelines (update from 2014 statement): a report of the American college of cardiology / American Heart Association joint committee on clinical practice guidelines [^63fa1003]. Journal of the American College of Cardiology (2025). High credibility.

Cost-effectiveness thresholds for cardiovascular cost-effectiveness analyses (CEAs) — The 2014 ACC/AHA cost-value methodology statement provided thresholds to identify high-, intermediate-, and low-value care based on U.S. per capita gross domestic product, and the writing committee determined that an intervention with an ICER < $50 000 per QALY gained should be considered high value, $50 000 to < $150 000 (1–3× per capita GDP) per QALY gained should be considered intermediate value, and ≥ $150 000 per QALY gained should be considered low value. WHO-origin GDP benchmarks suggested an intervention that costs less than 3 times the national annual GDP per capita per disability-adjusted life year averted should be considered cost-effective and that costs less than 1 times should be considered highly cost-effective, but critics note they "set such a low bar for cost effectiveness that very few interventions with evidence of efficacy can be ruled out"; WHO experts now also advise against per capita GDP-based thresholds and suggest considering cost-effectiveness alongside other contextual information in a transparent multicomponent decision process. Reflecting these concerns, the writing committee reviewed alternative approaches to incorporating CEA results into guidelines, including relying on US-based studies for identifying a cost-effectiveness threshold or thresholds for cardiovascular CEAs, and Vanness et al used a health opportunity cost approach estimating that for every $1 000 000 increase in health care expenditures, 1869 persons become uninsured, resulting in 81 QALYs lost due to mortality and 15 QALYs lost due to morbidity, implying a cost-effectiveness threshold of $104 000 per QALY ($51 000 to $209 000 per QALY) in 2019 US dollars; in the probabilistic analysis, 40% of the simulations suggested the threshold was less than $100 000 per QALY.

---

### Guidelines for validation of next-generation sequencing-based oncology panels: a joint consensus recommendation of the Association for Molecular Pathology and college of American pathologists [^01148c3a]. The Journal of Molecular Diagnostics (2017). Medium credibility.

Nonparametric tolerance intervals for non-normal distributions: The above estimate of the tolerance interval would only be applicable to a population that is normally distributed, but when the underlying population is often not normal (eg, when there is a natural boundary that the data cannot exceed (ie, 0% or 100%)) it is helpful to define tolerance intervals using nonparametric methods; the one-sided nonparametric tolerance interval can be determined by finding the value for k that satisfies the cumulative binomial equation, where CL is the confidence level (eg, 0.95), and by setting k = 0 (ie, 0 failures) the formula can be simplified.

---

### How do you design randomised trials for smaller populations? A framework [^a886d5cf]. BMC Medicine (2016). Low credibility.

Changing the primary outcome to something more information heavy

Statistically speaking, the best primary outcome to use is the one with the greatest information content; that is, the one which minimises the variance of the treatment effect relative to the magnitude of the treatment effect. In terms of information content, there is generally a hierarchy for outcome measures with continuous outcomes tending to hold most information, followed by, in order, time-to-event, ordinal and finally binary outcome measures. From a statistical perspective, it is, thus, sensible to use the most information-rich primary outcome available. It is always costly in terms of sample size to split continuous or ordered outcome data into two categories.

Clearly the primary outcome measure must be important from the perspective of both patients and treating clinicians: the practical convenience of needing fewer patients should not determine the choice of outcome unless candidate outcome measures are considered relevant for decision-making for all interested parties, including patients, clinicians, relevant health authorities and, potentially, regulators.

It is also important to consider any other studies in the field or closely related areas, so that common outcome measures might be measured in all studies to facilitate the synthesis of evidence.

---

### Investigating suspected cancer clusters and responding to community concerns: guidelines from CDC and the council of state and territorial epidemiologists [^591fb3f1]. MMWR: Recommendations and Reports (2013). Medium credibility.

CDC/CSTE suspected cancer clusters — beta, power, and power analysis: Power, or 1- β (beta), is the probability of rejecting the null hypothesis when it is actually false, and responders often use a beta value of 0.20 or less (or equivalently a power of 0.8 or more); power analysis is useful for determining minimum sample size and whether the number of cases is sufficient, usually a power of 0.8 or greater.

---

### FedECA: federated external control arms for causal inference with time-to-event data in distributed settings [^8156a2cc]. Nature Communications (2025). High credibility.

Synthetic data generating model of time-to-event outcome

To illustrate the performance of our proposed FL implementation, we rely on simulations with synthetic data. We simulate covariates and related time-to-event outcomes respecting the proportional hazards (PH) assumption, with the baseline hazard function derived from a Weibull distribution. For simplicity, we assume a constant treatment effect across the population. The data generation process consists of several consecutive steps that we describe below, assuming our target is a dataset with p covariates and n samples.

First, a design matrixis drawn from a multivariate normal distribution to obtain (baseline) observations for n individuals described by p covariates. The covariance matrix Σ is taken to be a Toeplitz matrix such that the covariances between pairs (X (i), X (j)) of covariates decay geometrically. In other words, for a fixed ρ > 0, we have cov(X (i), X (j)) = ρ ∣ i − j ∣. Such a covariance matrix implies a locally and hierarchically grouped structure underlying the covariates, which we choose to mimic the potentially complex structure of real-world data. To reflect the varying correlations of the covariates with the outcome of interest, the coefficients β i of the linear combination used to build the hazard ratio are drawn from a standard normal distribution.

In the context of clinical trials with external control arms, which implies non-randomized treatment allocation, we simulate the treatment allocation in such a way that it depends on the covariates. More precisely, we introduce the treatment allocation variable A that follows a Bernoulli distribution, where the probability of being treated (the propensity score) q depends on a linear combination of the covariates, connected by a logit link function g. The coefficients α i of the linear combination are drawn from a uniform distribution, where the range k ≥ 0 is symmetric around 0 and is normalized by the number of covariates. The degree of influence of the covariates on A can be regulated by adjusting the value of k. The greater the value of k, the stronger the influence, and therefore the lower the degree of overlap between the distributions of propensity scores of the treated and (external) control groups. Conversely, k = 0 removes the dependence, leading to a randomized treatment allocation.

---

### Applying network theory to epidemics: control measures for Mycoplasma pneumoniae outbreaks [^eccef985]. Emerging Infectious Diseases (2003). Low credibility.

Appendix A

Probability Generating Functions

Let Ρ χ be the normalized probability that a randomly chosen caregiver is working in k wards and q χ the probability that a randomly chosen ward has k caregivers working in it. We define probability generating functions (pgfs) for these degree distributions thus:

Since Ρ χ and q χ are each properly normalized probability distributions, and g 0 (1) = 1. The generating functions contain all the same information as the probability distributions but in a form that will be more convenient for our purposes. We can always recover the probability distributions again by differentiation.

If we assume that each of W wards has on average μ w caregivers working in it, and each of C caregivers interact with μ c wards on average, then, ƒ′ 0 (1) = μ c and g′ 0 (1) = μ w. (In general, the moments of the probability distributions are given by derivatives of the generating functions evaluated at one.)

Suppose we now choose a caregiver at random and follow an edge to a ward in which the caregiver works. The pgf for the number of caregivers working this ward is. Hence the distribution of caregivers working in this ward other than the originally selected caregiver is described by.

Likewise, if we start from a specific ward and choose a random caregiver working in that ward, then the number of other wards in which the caregiver works is given by.

---

### Biased expectations about future choice options predict sequential economic decisions [^9b0d2bac]. Communications Psychology (2024). Medium credibility.

Next, the model works backwards through the sequence, iteratively using the aforementioned formula forwhen computing each respective action value Q for taking the option and declining the option for each t. Whenever the reward value of taking the current option is considered, the reward function R assigns reward values to options based on their ranks. h represents the relative rank of the current option.

In contrast, the reward value of sampling again is simply the cost to sample C.

This customisable R function allowed us to examine how the Ideal Observer changes its sampling strategy under the different reward payoff schemes used in our studies. Pilot full, Study 1 full, Study 2 and both conditions in Study 3 all involved instructing participants to try to choose the best price possible. In study conditions using these instructions, we implemented a continuous payoff function (resembling that of the classic Gilbert & Mosteller formulation), in which the relative rank of each choice would be rewarded commensurate with the value of its associated option. In Pilot baseline and the baseline, squares, timing, and prior conditions of Study 1, we adapted the payoff scheme to match participants' instructions that they would be paid £0.12 for the best rank, £0.08 for the second-best rank, £0.04 for the third best rank and £0 for any other ranks. Lastly, in the payoff condition of Study 1, we programmed the reward payoff function to match participants' reward of 5 stars for the best rank, 3 stars for the second-best rank, one star for the third-best rank and zero stars for any other ranks.

Another feature added to our implementation of the Ideal Observer, compared to the Gilbert & Mosteller base model, is the ability to update the model's generating distribution from its experience with new samples in a Bayesian fashion, instead of this generating distribution being specified in advance and then fixed throughout the paradigm. This Bayesian version of the optimality model treats option values as samples from a Gaussian distribution with a normal-inverse- χ2 prior. The prior distribution is initialised before experiencing any options with four parameters: the prior mean μ 0, the degrees of freedom of the prior mean κ, the prior variance σ 2 0 and the degrees of freedom of the prior variance ν. The μ 0 and σ 2 0 parameters of this prior distribution are then updated by the model following presentation of each newly sampled option value as each sequence progresses.

---

### Deep learning segmentation of chromogenic dye RNAscope from breast cancer tissue [^1e69deaa]. Journal of Imaging Informatics in Medicine (2025). Medium credibility.

Finally, random perturbations are applied to the dot to represent the variation in real dots more accurately. Each cardinal direction from the centre dot is randomly stretched by 0–3 pixels as shown in Fig. 9, the entire dot is scaled by an x and y factor (described in the following sentences), some random noise which is randomly selected from 0.975–1.025 for each pixel is multiplied onto each colour channel, and lastly the dot is rotated by a random integer angle (0°- 359°). The scale factor is calculated as a random number from 0.25–0.5 separately for the x and y axis, with a single random numbercalculated according to Eq. 1 added to both. The reason for adding a squared uniform random value (which will most frequently be a low value) to both dimensions is to represent the low frequency of cases where a substantially larger dot appear in real images. Some examples of the resulting dots can be seen in Fig. 10. Since the characteristics of dots within a patch tend to be similar, the generated dots are designed to follow a similar pattern. Therefore, some values for parameters involved in creating these dots are controlled separately for each patch. Although the primary colour is randomly selected from yellow to dark brown (as previously defined), each patch is only allowed to select from a randomly selected, continuous 20% of this range. There is a 25% chance of the secondary (dark-centred) dot type being present in the patch; otherwise, there will be none at all. If the patch is selected to have these secondary dots, the probability of each generated dot being of the secondary type is randomly selected to be 0–50%. To represent how most patches have either few dots (0–5) or many dots (20+), the number of dots to add for a patch, n, is calculated according to formula 2 which gives an exponential scale from 2 to 64. The dots are placed at random x and y co-ordinates on the patch, with the random x and y values being recalculated until the co-ordinate is at least 3 pixels away from any other existing dot.

Fig. 11
The validation intersection over union during training of the deep learning network with different loss functions

Table 1
The best-scores of the deep learning network when trained using each loss function

---

### The net chance of a longer survival as a patient-oriented measure of treatment benefit in randomized clinical trials [^dedfe16d]. JAMA Oncology (2016). Medium credibility.

Importance

Time to events, or survival end points, are common end points in randomized clinical trials. They are usually analyzed under the assumption of proportional hazards, and the treatment effect is reported as a hazard ratio, which is neither an intuitive measure nor a meaningful one if the assumption of proportional hazards is not met.

Objective

To demonstrate that a different measure of treatment effect, called the net chance of a longer survival, is a meaningful measure of treatment effect in clinical trials whether or not the assumption of proportional hazards is met.

Design

In this simulation study, the net chance of a longer survival by at least m months, where m months is considered clinically worthwhile and relevant to the patient, was calculated as the probability that a random patient in the treatment group has a longer survival by at least m months than does a random patient in the control group minus the probability of the opposite situation. The net chance of a longer survival is equal to zero if treatment does not differ from control and ranges from -100% if all patients in the control group fare better than all patients in the treatment group up to 100% in the opposite situation. We simulated data sets for realistic trials under various scenarios of proportional and nonproportional survival hazards and plotted the Kaplan-Meier survival curves as well as the net chance of a longer survival as a function of m. Data analysis was performed from August 14 to 18, 2015.

Main Outcomes and Measures

The net chance of a longer survival calculated for values of m ranging from 0 to 40 months.

Results

When hazards are proportional, the net chance of a longer survival approaches zero as m increases. The net chance of a longer survival (Δ) was 13% (95% CI, 6.5%-19.4%; P < .001) when any survival difference was considered clinically relevant (m = 0 months). When survival differences larger than 20 months were considered relevant (m = 20), the net chance of a longer survival was very close to zero (Δ[20] = 0.5%; 95% CI, -0.1% to 1.1%; P = 0.09). In contrast, when treatment effects are delayed or when some patients are cured by treatment, the net chance of a longer survival benefit remains high and tends to the cure rate. For crossing hazards, the Δ was negative (Δ = -6.9%; 95% CI, -14.0% to -0.5%; P = 0.047). However when large survival differences were considered (m = 20), the Δ(m) was positive (Δ[20] = 8.9%; 95% CI, 6.7%-11.1%; P < .001).

Conclusions and Relevance

The net chance of a longer survival is useful whether or not the assumption of proportional hazards is met in the analysis of survival end points and may be helpful as a measure of treatment benefit that has direct relevance to patients and health care professionals.

---

### Detecting heritable phenotypes without a model using fast permutation testing for heritability and set-tests [^ec429053]. Nature Communications (2018). Medium credibility.

Reducing the number of sampled permutations using SAMC

To cope with the major computational hurdle of permutation testing, we use an efficient p -value evaluation procedure based on the Stochastic Approximation Markov Chain Monte Carlo (SAMC) algorithm. A description of the SAMC algorithm and its tuning is given in Supplementary Note 4.

In summary, let the proposal distribution q (π t, τ) define the probability of choosing a new permutation τ, given that the current permutation is π t. Let. Let D + 1 be the number of intervals in the partitioning of [0,1]. For a permutation π ∈ S n, let J (π) be the index of the interval in whichfalls. Letbe the logarithm of our current estimates of partition sizes, up to a multiplicative (in log scale, additive) constant. The algorithm is:
Initialize a uniform estimate.
Choose a random initial permutation π 1.
For t = 1,…, T (or until convergence): Simulate a sample π t + 1 by a single Metropolis-Hastings update, as follows: i. Generate τ according to the proposal distribution q (π t, τ). ii. Calculate the ratio iii. Accept the proposed move with a probability of min (1, r). If accepted, set π t +1 = τ. Otherwise, set π t +1 = π t. Update the estimates: For i = 1,…, D + 1, set, where γ (t) is called the gain factor and is defined as γ (t) = t 0 /max(t 0, t).
Return.

---

### Representation of probabilistic outcomes during risky decision-making [^ef9fce1c]. Nature Communications (2020). High credibility.

Autocorrelation

To study the temporal structure of the outcome representations, we computed the autocorrelation of the decoded probabilities. To assess whether they differed from chance, we compared the autocorrelation against a null distribution created at each time lag from the 100 permuted classifiers. Specifically, the likelihood of the autocorrelation under the null distribution at a given time point was approximated to the relative number of permutations that resulted in a more extreme (two-sided) value for the autocorrelation. Clusters were defined as the sets of consecutive time points for which the log-likelihood (LL) was larger than 3, and cluster size was quantified as the sum of the LL of all the points in the cluster. We performed group-level statistics at the cluster-level with a non-parametric permutation test, and report only the clusters that were bigger than the biggest cluster found in 95% of analyses with the permuted classifiers. Note that this test controls the false positive rate across the entire time interval; the location of clusters is reported for illustration only. Next, we collapsed the decoded probabilities into the most likely represented outcome (i.e. P if p (P) > p chance; N if p (P) < p chance, where p chance was determined at the participant level by the relative number of occurrences of positive and negative outcomes in the training set). We then took the resulting set of epochs of steady representation (i.e. the time interval during which the most likely represented outcome did not change) and computed the distribution of their duration. This distribution was tested against the same distribution computed from the permuted classifiers. Statistical difference was tested with a two-sample Kolmogorov–Smirnov test. We also tested the average number of transitions from one outcome representation to the other (i.e. number of epochs) against the number of transitions predicted under the null distribution obtained from the permuted classifiers. The p -value was computed as the proportion of more extreme results from the permuted classifiers.

---

### Biased expectations about future choice options predict sequential economic decisions [^a5d9b491]. Communications Psychology (2024). Medium credibility.

The first row of Fig. 5 shows comparisons of participant and Ideal Observer sampling in the two conditions with an initial rating phase: ratings and full. Further statistical detail for these comparisons is found in Supplementary Table 1. Study 1 ratings (left column), a condition in which the top three ranks are rewarded, showed robust participant undersampling compared to both versions of Ideal Observer. In contrast, in the full condition (right column), there was inconclusive statistical evidence for any undersampling compared to the subjective values version of the Ideal Observer. Relative to the objective value version, some statistically significant undersampling survived, though its effect size Cohen's d = –0.61 was cut in half compared its Study 1 ratings counterpart Cohen's d = –1.72. Thus, compared to all other Study 1 conditions, only the Study 1 full condition showed some degree of reduced sampling by the Ideal Observer. Note that Study 1 full is also the only condition in Study 1 where participants and the Ideal Observer were instructed to maximise the option value of their choices, instead of using a scheme that rewards only the top three ranked options.

---

### Probabilistic medical predictions of large language models [^8f868866]. NPJ Digital Medicine (2024). Medium credibility.

Large Language Models (LLMs) have shown promise in clinical applications through prompt engineering, allowing flexible clinical predictions. However, they struggle to produce reliable prediction probabilities, which are crucial for transparency and decision-making. While explicit prompts can lead LLMs to generate probability estimates, their numerical reasoning limitations raise concerns about reliability. We compared explicit probabilities from text generation to implicit probabilities derived from the likelihood of predicting the correct label token. Across six advanced open-source LLMs and five medical datasets, explicit probabilities consistently underperformed implicit probabilities in discrimination, precision, and recall. This discrepancy is more pronounced with smaller LLMs and imbalanced datasets, highlighting the need for cautious interpretation, improved probability estimation methods, and further research for clinical use of LLMs.

---

### Graphical analysis for phenome-wide causal discovery in genotyped population-scale biobanks [^6c8e1d23]. Nature Communications (2021). High credibility.

Fig. 3
Mean number of discoveries and empirical false discovery rates (FDR) of Mendelian randomization methods in simulated data from graphs with 15 continuous traits.

The underlying causal diagram was generated such that the expected in- and out-going degrees of the traits were 1.5. All simulated graphs contained cycles. For each trait we added between 10 and 20 binary instruments (uniformly, i.i.d). To add horizontal pleiotropy, for each instrument we decided whether it is horizontally pleiotropic or not with probability p pleio, and if so, we added between 1 and 10 links into additional traits (uniformly, iid). When generating datasets, the traits had standard normal noise, causal quantities were randomly and uniformly sampled such that their absolute value was between 0.1 and 0.9, and binary instruments were generated randomly with a probability between 0.05 and 0.4. The plots show the mean results of the simulations for different p pleio values (e.g. the mean of the empirical FDR over the simulated graphs). Discoveries from each statistical test were done at a 10% significance level after adjusting for FDR using the BY algorithm. When two methods have a similar empirical FDR, greater number of predictions correspond to greater power. a Results with p 1 = 0.001 and p 2 = 0.01. b Results with p 1 = 1 × 10 −05 and p 2 = 0.001. MR-Egger is not presented as it consistently had greater empirical FDR values than the other methods.

---

### Natural statistics support a rational account of confidence biases [^03397678]. Nature Communications (2023). High credibility.

Previous work has sought to understand decision confidence as a prediction of the probability that a decision will be correct, leading to debate over whether these predictions are optimal, and whether they rely on the same decision variable as decisions themselves. This work has generally relied on idealized, low-dimensional models, necessitating strong assumptions about the representations over which confidence is computed. To address this, we used deep neural networks to develop a model of decision confidence that operates directly over high-dimensional, naturalistic stimuli. The model accounts for a number of puzzling dissociations between decisions and confidence, reveals a rational explanation of these dissociations in terms of optimization for the statistics of sensory inputs, and makes the surprising prediction that, despite these dissociations, decisions and confidence depend on a common decision variable.

---

### More efficient, smaller multicancer screening trials [^925c1362]. Journal of the National Cancer Institute (2025). Medium credibility.

Sample-size calculations

Sample-size calculations for targeted and intended-effect analysis require an additional parameter over the traditional approach. In practice, one is likely to have information about event rates and the test-positivity rate from prior studies. The additional parameter needed is the rate in individuals who test negative or positive, or their ratio, or the screening effect in individuals who test positive. We suggest that trials are planned based on what is known about the test sensitivity and specificity in order to estimate the probability of having an event given test positive. One might also consider an adaptive trial design, with re-estimation of sample size based on data arising from the earliest recruits.

To help choose the most appropriate design, researchers can estimate "the proportion of participants dying from cancer who have a positive study sample" to consider using a targeted analysis over the traditional analysis, and "the probability of an individual with a positive test dying from cancer (during the study follow-up)" to consider the intended-effect analysis rather than the targeted analysis. A referee suggested that one might consider a hybrid design when not everyone is tested but a (stratified) sample of the control arm is taken in addition to all those individuals with an event. Without stratification, one must test a large proportion of the population to achieve meaningful gains over intended-effect analysis, making the rationale for (random) sampling weak compared with targeted or full intended-effect analysis. Further work to elucidate the potential benefit of stratified sampling and scenarios when it might be worthwhile would be helpful.

---

### Improving reproducibility of differentially expressed genes in single-cell transcriptomic studies of neurodegenerative diseases through meta-analysis [^50395dfe]. Nature Communications (2025). High credibility.

The AUC represents the probability that the model, if given a randomly chosen positive and negative example, will rank the positive higher than the negative, with 1.0 being a perfect score and 0.5 being the lowest score. This metric allows us to compare the performance of different models, assuming the datasets are roughly balanced between cases and controls. The Irwin-Hall distribution is the theoretical null distribution for the SumRank statistic, because it assumes that the genes in each study are uniformly distributed and each study is independent of the other, and the Irwin-Hall distribution is the sum of independent, uniformly distributed random variables. We thus initially obtain p values for each gene using an Irwin-Hall distribution (two-sided) as implemented in the unified version 1.1.6package, dirwin.hall function, with the number of datasets as the number of uniform distributions specified. However, it is possible that genes are not uniformly distributed, given the complexities of gene expression, and we also choose only a subset of datasets for each gene, so for both of these reasons, the distribution will deviate from Irwin-Hall. We thus calibrated the p values by permutations (see below).

Merge meta-analysis

After quality control, the Seurat objects for each dataset were first subsetted to the relevant cell type and then merged using the Seurat merge function with all settings at default. The count matrices for the merged objects had 1 added to them (for a pseudocount) and were then converted to DESeq dataset types with the DESeqDataSetFromMatrix command with design = ~Diagnosis+Dataset, to provide some accounting for dataset-specific batch effects (this design regresses out dataset-specific artifacts). DESeq2 differential expression was then performed, and results were extracted for the Diagnosis variable (p values and log2 fold-changes for each gene).

---

### Heuristics in risky decision-making relate to preferential representation of information [^43e6061b]. Nature Communications (2024). High credibility.

In a typical risky-choice task, individuals choose between a safe option with a known, fixed outcome, and a gamble option which can lead probabilistically to one of two possible outcomes. Normative choice in such settings requires evaluating the gamble by summing the utility of each uncertain outcome, weighted by its probability, and comparing this expected utility to the utility of a known safe option –. One explanation for deviations from normativity, as well as variability, is the need for individuals to employ heuristics that reduce the computational burden entailed in this rational approach to choice –. Whereas the normative choice strategy requires independent consideration of each possible task outcome, individuals can reduce the number of outcomes they consider through preferential reliance on a particular type of information during evaluation. For example, individuals could prioritize probability information, and selectively ignore the safe outcome as well as the unlikely gamble outcome, leading to a decision solely based on whether the more likely gamble outcome is attractive. Alternatively, they could prioritize reward information, and solely represent outcomes useful for comparison along this dimension.

We hypothesized that prioritization of distinct types of information during choice evaluation – and more specifically preferential representation of outcome stimuli relevant for comparing choices alongside the type of information prioritized – would explain heuristic weightings of probability and reward information. We leveraged individual differences in heuristic reliance on reward or probability information in choice behavior and examined whether this variability related to inter-participant variability in a disposition to represent outcomes which support a prioritization of a one or the other type of information. If heuristic reliance on probability or reward information in behavior is related to prioritization of probability or reward information during choice evaluation, then we would expect individuals who weigh probability or reward information more in choice to preferentially represent outcome stimuli useful for comparing choices according to that information dimension. At a higher level, we sought to determine whether the outcomes that an individual tends to consider when deciding underpin the type of information their choices reflect a heuristic reliance upon.

---

### Reciprocity of social influence [^7bc552ea]. Nature Communications (2018). Medium credibility.

The second choice of the partner was computed differently for susceptible and insusceptible partners. For experiment 1, the influence that the insusceptible partner took from the participants in each trial was chosen with a probability of 0.65 from a uniform distribution on the interval [0, 0.2], with a probability of 0.2 randomly from a uniform distribution on the interval [0.3, 0.7], and with a probability of 0.15 randomly from a uniform distribution on the interval [0.7, 0.9]. For experiment 2, the influence that the insusceptible partner took from the participants was chosen randomly from a uniform distribution on the interval [0, 0.2]. For the susceptible partner, in all experiments, the influence was chosen with a probability of 0.5 randomly from a uniform distribution on the interval [0.7, 1], with a probability of 0.2 randomly from a uniform distribution on the interval [0.3, 0.7], and with a probability of 0.3 randomly from a uniform distribution on the interval [0, 0.3]. In the transition block, the influence of the partner was a linear interpolation between the susceptible and the insusceptible partner:where inf s and inf ins were the influences of the susceptible and insusceptible partners, respectively (as explained above). λ gradually increased with time from 0 at the beginning to 1 at the end of the transition block for a transition from the susceptible to the insusceptible condition. For the transition from the insusceptible to the susceptible condition, λ decreased gradually from 1 to 0.

In experiment 1, on average, the advice that the partners took from the participants was 0.3 and 0.55 in the insusceptible and susceptible conditions, respectively. In experiment 2A, on average, the advice that the partner took from the participants was 0.07 and 0.5 in the insusceptible and susceptible conditions, respectively. The second choice of the partners in experiment 2B was designed exactly the same as in experiment 2A and the average advice that the partner took in the insusceptible and the susceptible condition was identical to experiment 2A.

---

### Biased expectations about future choice options predict sequential economic decisions [^49921c9c]. Communications Psychology (2024). Medium credibility.

Here, we set the prior values of μ and σ 2 in two possible ways: objective value and subjective values versions. In some previous studies of optimal stopping for price decisions, the mean and variance of the generating distribution has been fixed in advance by the mean and variance of the distribution of objective prices. We implemented an objective values version of the Ideal Observer in this way for all the study conditions reported herein. This objective values procedure for the Ideal Observer assumes that the raw prices can be treated as a proxy for participants' subjective value of the prices, so an Ideal Observer that optimises only the raw prices when making decisions would therefore be an appropriate basis for comparison with participants. However, we also had direct access to participants' subjective values of options in some conditions (Pilot full, Study 1 full condition, Study 1 ratings condition, Study 2 and both sequence length conditions of Study 3), due to the presence of the initial rating phase, and so we could also build a subjective values version of the Ideal Observer. This second way of computing the Ideal Observer assumes that participants' subjective valuation of prices may not necessarily exactly equal the raw price values, especially in their scaling, which may be relevant to full information problems. We used each participants' individualised ratings (subjective valuations) of the prices as option values input to the subjective values version of the Ideal Observer, and we used the mean and variance of individual participants' ratings distributions when initialising the prior of the generating distribution of the Ideal Observer.

Because conditions with an initial rating phase had objective and subjective values versions of the Ideal Observer, with each version providing separate optimality estimates, we were able to test the hypotheses that the use of objective or subjective values when modelling (a) affects the strategy taken by the optimality model and (b) changes the assessment of participant bias. We ensured for both objective values and subjective values versions of the models that better options were always more positively-valued such that the models were always solving a maximisation problem. We further ensured that estimated parameters for both objective values and subjective values versions of the models would be on the same scales by reflecting the objective prices around their mean. Then we rescaled those values to span 1 (the highest/worst price) to 100 (the best price). These reflected and rescaled objective values were then used in objective values models when computing the prior generating distribution, and when inputting price values to the model as option values. Subjective values were already rated by participants on this same 1 to 100 scale.

---

### Biased expectations about future choice options predict sequential economic decisions [^9fd74e4a]. Communications Psychology (2024). Medium credibility.

Theoretical modelling

We were able to theoretically explain, in terms of a computational mechanism, participants' sampling bias. Our model fits suggest that participants' reluctance to increase sampling rates when it is optimal to do so arises because participants expect future option values to be lower on average than the ground truth. Participants of course still make some suboptimal decisions in full conditions, where undersampling bias was reduced or absent, and our data suggests that mis-specified prior expectations may best account for erroneous decisions when they occur in these conditions as well. We should note, however, that the Cost to Sample model was the best-fitting model and may explain suboptimal decisions for many participants. In the Cost to Sample model, participants perceive sampling itself to be intrinsically costly or rewarding, even though there were no ground truth extrinsic costs or rewards associated with sample choices. In the case of undersampling, participants using this strategy would settle for an earlier option in part because continued sampling would be perceived as aversive. Indeed, all three of our models accurately predicted participants' mean sampling rates (Figs. 3–7). The framework we promote here, therefore — using an optimality model to explain accurate performance and then parameterising it to account for systematic bias — appears to produce models that predict participant behaviour with reasonable accuracy. It is certainly possible that different participants within the same sample might adopt any of these strategies, even if the Biased Prior strategy might be the most common.

---

### Herbal remedy clinical trials in the media: a comparison with the coverage of conventional pharmaceuticals [^a863d464]. BMC Medicine (2008). Low credibility.

Appendix 1: Coding frame for clinical trials

1. Basic information

a. Trial number

b. Type of clinical trial (Pharmaceutical = 1; CAM = 2)

c. Journal name

d. Year

e. Type of institution where research conducted (lead or corresponding author)

Not specified 0

University/hospital 1

Not-for-profit organization 3

Mixed 4

Private 5

Government 6

2. Contents

a. Medical condition

Not specified 0

Type of medical condition

b. Dose(s) specified (0 = no; 1 = yes)

c. Location of clinical trial

Not specified 0

US 1

Canada 7

UK 2

Europe 3

Australia 4

South America 5

Asia 6

International 8

Mid East/Africa 9

d. Is the article framed as a controversy? (no) 1

If yes, is the report

balanced 2

or imbalanced 3

e. Type of main benefit

Not specified 0

None (stated that there is no benefit) 1

Basic research 2

Improved health/treatment 3

Decreased side-effects 4

General safety 5

Increased autonomy/empowerment 6

Spiritual, moral, ethical 7

Environmental/ecological/nature 8

Economic 9

Improved quality of life 10

Other (specify) 11

f. Likelihood of benefit

Not specified 0

High 1

Moderate 2

Low 3

No benefit (stated) 4

Mentioned but not quantified 5

g. Total number of benefits mentioned number

h. Type of main risk/cost

Not specified 0

None (stated that there is no risk) 1

Basic research 2

Health 3

Increased side-effects 4

General safety 5

Decreased autonomy/empowerment 6

Spiritual, moral, ethical 7

Environmental/ecological/nature 8

Economic 9

Decreased quality of life 10

Other (specify) 11

i. Likelihood of risk/cost

Not specified 0

High 1

Moderate 2

Low 3

No risk/cost (stated) 4

Mentioned but not quantified 5

j. Total number of risks/costs mentioned number

3. Quality of clinical trial

a. Was the study described as randomized (this includes the use of words such as randomly, random and randomization? (no = 0; yes = 1)

---

### Heuristics in risky decision-making relate to preferential representation of information [^3734478d]. Nature Communications (2024). High credibility.

Sampling models

We additionally fit two sampling models. According to our sampling models, the participant uses importance sampling to estimate the difference in utility between accepting and rejecting the gamble outcome. Both models assume participants first select a number of samples to take, which we assume is drawn from an ordered probit distribution,.sets the center of the distribution and is a free parameter. The scale parameter, is set to 2. Following this, the participant drawssamples where each sample corresponds to eitheror, from the distribution, which is defined below. Givensamples, the participant computes an estimate of the value difference between the gamble option and safe option:

reflects the importance weights,.is defined the same as it is for the prospect theory models, with two free parameters, and.is the number of points paired with the outcome that was drawn on sample.

The participant's probability of accepting is then 1 if > 0, 0 if < 0 and.5 if = 0. We defineas a function of the number of samples taken, and the number of samples drawn as,

Then the probability of acceptance then marginalizes over the number of samples taken, S, as well as the number of samples drawn as O1:where we took the maximum number of samples, to be 7. Here, we assume the number of samples taken, S, is selected from an Ordered Probit distribution, with scale parameter, c = 2, and center parameter, n, a free parameter.

We considered two sampling models, which differ with regards to the sampling distribution. For probability sampling. For utility weighted sampling. Both models have a 3 free parameters: and.

Model fitting procedure

For each participant, we estimated the free parameters of each model by maximizing the likelihood of choices, jointly with group-level distributions over the entire population using an Expectation Maximization (EM) procedure. Models were compared by computing the integrated Bayesian information criterion over the entire group of participants for each model. In order to compare model predictions to data points, we computed for each trial, for each participant, the probability of acceptance under that participant's best fitting parameters.

---

### Number needed to treat and number needed to harm are not the best way to report and assess the results of randomised clinical trials [^f243221a]. British Journal of Haematology (2009). Low credibility.

The inverse of the difference between rates, called the 'number needed to treat' (NNT), was suggested 20 years ago as a good way to present the results of comparisons of success or failure under different therapies. Such comparisons usually arise in randomised controlled trials and meta-analysis. This article reviews the claims made about this statistic, and the problems associated with it. Methods that have been proposed for confidence intervals are evaluated, and shown to be erroneous. We suggest that giving the baseline risk, and the difference in success or event rates, the 'absolute risk reduction', is preferable to the number needed to treat, for both theoretical and practical reasons.

---

### Predicting what will happen when we act. what counts for warrant? [^58aa35d9]. Preventive Medicine (2011). Low credibility.

To what extent do the results of randomized controlled trials inform our predictions about the effectiveness of potential policy interventions? This crucial question is often overlooked in discussions about evidence-based policy. The view I defend is that the arguments that lead from the claim that a program works somewhere to a prediction about the effectiveness of this program as it will be implemented here rests on many premises, most of which cannot be justified by the results of randomized controlled trials. Randomized controlled trials only provide indirect evidence for effectiveness, and we need much more than just randomized- controlled-trial results to make reliable predictions.

---

### Interpreting the results of noninferiority trials-a review [^e5d4a296]. British Journal of Cancer (2022). Medium credibility.

To illustrate this point, consider a conventional treatment that historically reduced 5-year recurrence from 30 to 20%, and suppose in the noninferiority trial, that the new treatment leads to 4% more recurrence than the conventional treatment. If, as before, the conventional treatment results in 20% recurrence, this 4% increase translates to a 20% relative (6% absolute) decrease in 5-year recurrence compared with no treatment (assuming recurrence with no treatment would still be 30%). However, if due to changes in other aspects of usual care or the study population, the conventional treatment leads to a 5-year recurrence of just 8% in the new trial and 12% on the new treatment, the question of what the recurrence rate would have been with no treatment becomes critical. If one assumes the benefit of conventional treatment is proportional to the risk of recurrence, then recurrence with no treatment would again be 50% greater, corresponding to 12% and indicating no effect of the new treatment. If, instead, one assumes the absolute benefit of conventional treatment is preserved, recurrence with no treatment would have been 18% (8% + (30–20%)), considerably more than 12% on the new treatment. For simplicity we have illustrated these issues in terms of the point estimates, but in practice it is essential to use the 95% confidence intervals for the treatment effects.

---

### Biased expectations about future choice options predict sequential economic decisions [^3c61900f]. Communications Psychology (2024). Medium credibility.

Methods specific to Study 3

Study 3 once again implemented the full condition, but this time manipulating sequence length. As participants in our preceding studies in this did not change their sampling rates to a statistically detectable degree, our goal for Study 3 was to test whether participants would do so at all. Costa and Averbeckpreviously showed that sequence length both increased participants' sampling rates and increased the size of their undersampling bias, compared to the Ideal Observer, and we attempted to replicate those findings here.

Study 3 was preregistered atin April 2023, with the data collected shortly thereafter. We enrolled 140 participants from the UK using Prolific. Fifty percent random assignment of participants to each group yielded 65 participants with 14 options and 75 participants with 10 options (which is a slight deviation from our pre-registered plan of 70 participants per group). As explained in the pre-registration, the sample size was intended to double that of Costa & Averbeck(who used a more powerful repeated-measures design and who were able to use more trials per participant in-lab, while we needed a shorter online study). The procedures were identical to Study 2, using the same jsPsych code, merely changing the sequence length of the optimal stopping phase of the study. The averages (over participants) of the Pearson's r values computed between the two phase 1 ratings to each price were 0.88 for the 10 option condition and.84 for the 14 option condition.

Ideal observer optimality model

To analyse the optimal stopping task data in all these studies, we compared the number of options our participants sampled before choosing an option to that of the Ideal Observer. The Ideal Observer is a benchmark of optimality, for which performance is Bayes-optimal. This finite-horizon, discrete-time, Markov decision process model has been used in previous studies. The Bayesian version of the optimality model for the full information problem builds on the classic Gilbert and Mosteller model. Models try to predict upcoming option values, with these expectations derived from the model's belief about the distribution from which future options are assumed to be generated (i.e. the generating distribution). More precisely, the utility u for the state s at sample t is the maximal action value Q, out of the available actions a in A. These action values in turn depend on the reward values r and the probabilities of outcomes j of subsequent states (i.e. the generating distribution), weighted by their utilities.

---

### Biased expectations about future choice options predict sequential economic decisions [^17735a25]. Communications Psychology (2024). Medium credibility.

We also considered objective and subjective values versions of the Cost to Sample model. These use the Ideal Observer for the full information problem described above as a base, while also assuming that participants' otherwise rational Bayesian computations can be biased by a free parameter value. In the case of the Cost to Sample models, the fitted parameter to account for such bias was the cost to sample value C (See computation ofin the Ideal Observer Optimality Model section above). In the Cost to Sample model, participants would undersample if they intrinsically perceive sampling as costly and so adopt a negatively valued C, whereas they would oversample if they perceive sampling as rewarding as so adopt a positive C. We initialised model fitting with a starting C value of 0 (i.e. the optimal value) and, during fitting, bounded C to be between –1 and 1 (The payoffs we used during model fitting were scaled to be between 0 and 1 and C values are specified on that scale).

We used a similar approach when building the subjective and objective values versions of the Biased Prior model. In this model, we added a new free parameter to, the mean of the prior generating distribution. Negative values of this parameter can bias an agent to compute pessimistic estimates of future option values by shifting the prior mean (i.e. expectation) to be lower. This can lead to undersampling by making the current option appear more appealing compared to the artificially deflated expectation of option values resulting from continued sampling. We initialised model fitting with a starting value of 0 (i.e. the optimal value) and the biased prior parameter was bounded during fitting to be between –100 and 100 (when compared to option values scaled between 0 and 100).

Reporting summary

Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article.

---

### Clinical policy: critical issues related to opioids in adult patients presenting to the emergency department [^815bca18]. Annals of Emergency Medicine (2020). High credibility.

Opioid detoxification — systematic review of randomized controlled trials (RCTs) comparing buprenorphine, methadone, clonidine, and lofexidine — used a "mixed treatment comparison approach" with WinBUGS and 80,000 MCMC simulations, with the main outcome being "completion of treatment". Across 23 RCTs with data on 2,112 patients, buprenorphine was more effective than clonidine (OR 3.95; 95% credible interval 2.01 to 7.46) but not than lofexidine (OR 2.64; 95% credible interval 0.9 to 7.5); buprenorphine may be more effective than methadone (OR 1.64; 95% credible interval 0.68 to 3.79), while methadone was more effective than clonidine (OR 2.42; 95% credible interval 1.07 to 5.37) and not necessarily more effective than lofexidine (OR 1.62; 95% credible interval 0.6 to 4.58). Probability ranking favored buprenorphine (85%) over methadone (12.1%), lofexidine (2.6%), and clonidine (0.01%); comparison between buprenorphine and methadone did not show a statistically significant difference. Limitations noted included that RCT settings were not specified and there was "no mention of heterogeneity measurement/sensitivity analyses".

---

### Structured connectivity in cerebellar inhibitory networks [^3fd070df]. Neuron (2014). Low credibility.

Connectivity at the Pair Level Appears Mostly Random

Is the connectivity between interneurons random on the level of individual pairs? To answer this question, we first calculated the overall probabilities for each type of connection between pairs in the data. The probability of a pair having no chemical or electrical connection was p = 0.340; electrical only p = 0.295; chemical only p = 0.214; dual chemical and electrical p = 0.121; bidirectional chemical p = 0.024; and bidirectional chemical with electrical p = 0.005. To test whether these results are consistent with the null hypothesis ("connectivity is random"), it was necessary to generate synthetic connectivity data defined as random and compare it to the real data. Any significant difference would disprove the null hypothesis and show nonrandom features of connectivity. We can formulate two sets of predictions for the pairwise connection probabilities, both based on random statistics. The first one only assumes that all chemical and electrical connections are made independently of each other with the average connection probabilities p E = 0.42 and p C = 0.20 (Figure 3 A, top; Supplemental Experimental Procedures). It represents a simple model of locally uniform random synaptic connectivity between pairs of cells. We name this first model the "uniform random" model. The second, more complex model also assumes that all connections are made independently of each other, but the probability of a connection depends on the intersomatic distance in xy and z (Figure 3 A, bottom). We constructed the model of distance dependence using the distributions observed in the data (Figures 2 A, 2B, S2 D, and S2E). We call this second model the "nonuniform random" model. In addition, we also tested two random models that include the position of the cells in the molecular layer (ML) as a parameter (Figure S3). The probabilities of the different connection types between pairs predicted by the two models (Figure 3 B; light and dark gray bars) were compared to the data (green bars, n = 420 pairs). For most of the connection types the ratio of the predicted to the actual connection probability is not significantly different from 1. The occurrence of fully connected (bidirectional chemical and electrical) pairs is significantly lower than predicted by both random models (p = 0.046 and 0.004 for the uniform and nonuniform random predictions, respectively; though the difference is not significant when including ML position in the random model, Figures S4 A and S4B). The occurrence of bidirectional chemical connections at the random level is in contrast to excitatory connections between layer 5 pyramidal cells, where they are overrepresented. In addition, the number of dual connections is at the level expected if electrical and chemical synapses are formed independently of each other. Thus, the fact that only small differences were observed compared to the predictions appears to suggest that random connectivity is an adequate model at the pair level for these interneuron networks.

---

### Outcome – adaptive randomization: is it useful? [^67702d35]. Journal of Clinical Oncology (2011). Low credibility.

Outcome-adaptive randomization is one of the possible elements of an adaptive trial design in which the ratio of patients randomly assigned to the experimental treatment arm versus the control treatment arm changes from 1:1 over time to randomly assigning a higher proportion of patients to the arm that is doing better. Outcome-adaptive randomization has intuitive appeal in that, on average, a higher proportion of patients will be treated on the better treatment arm (if there is one). In both the randomized phase II and phase III settings with a short-term binary outcome, we compare outcome-adaptive randomization with designs that use 1:1 and 2:1 fixed-ratio randomizations (in the latter, twice as many patients are randomly assigned to the experimental treatment arm). The comparisons are done in terms of required sample sizes, the numbers and proportions of patients having an inferior outcome, and we restrict attention to the situation in which one treatment arm is a control treatment (rather than the less common situation of two experimental treatments without a control treatment). With no differential patient accrual rates because of the trial design, we find no benefits to outcome-adaptive randomization over 1:1 randomization, and we recommend the latter. If it is thought that the patient accrual rates will be substantially higher because of the possibility of a higher proportion of patients being randomly assigned to the experimental treatment (because the trial will be more attractive to patients and clinicians), we recommend using a fixed 2:1 randomization instead of an outcome-adaptive randomization.

---

### Simulations suggest a constrictive force is required for Gram-negative bacterial cell division [^f8a0799b]. Nature Communications (2019). High credibility.

One endopeptidase exists in each enzyme complex to cleave existing peptide crosslinks. In our previous simulations, when an endopeptidase diffused across a crosslink, the enzyme cleaved the crosslink with a probability of 0.1. To speed up our current simulations, every 10 time steps, if the distance from the endopeptidase to a crosslink is within 3.0 nm, the enzyme captures then cleaves the crosslink. If there are multiple crosslinks within this reaction distance, the probability of crosslink i being chosen is calculated aswhere d i is the distance from the endopeptidase to crosslink i.

There are three transpeptidases in each complex, one crosslinking the two new strands to one another and the other two crosslinking the pair to the existing network (Supplementary Figure 1). Previously, the probability of a transpeptidase capturing a peptide of a PG bead at a distance d was given as P tp = (1 − d / d 0) 2, where d 0 = 2.0 nm was the reaction distance. To speed up the current simulations, d 0 is increased to 3.0 nm. Note that increasing the modeled rates of the enzymes did not change the principles driving cell wall remodeling in our simulations.

To constrain the enzymes into a complex, they were tethered such that a force, F ez = − k ez (d ez − D 0), was applied to draw two enzymes closer together if their distance d ez became larger than D 0 = 1.0 nm. The force constant was chosen to be k ez = 10 pN/nm. Since in real cells, enzymes are in the periplasm, we introduced a force normal to the surface, F surf = − k surf (d s − d s0), on an enzyme if it moved a distance d s larger than d s0 = 3.0 nm away from the surface. The force constant k surf was chosen as 500 pN/nm.

To model enzyme diffusion, a random force was applied to each enzyme every time step. Using the Box–Muller transformation, two random numbers from a Gaussian distribution were generated as:andwhere u 1 and u 2 are two random numbers from a uniform 0–1 distribution. For N enzymes, 3 N Gaussian random numbers were generated and then scaled by a force constant of 500 pN to obtain 3 N Cartesian components of the random forces.

---

### Understanding randomised controlled trials [^1055da10]. Archives of Disease in Childhood (2005). Low credibility.

The hierarchy of evidence in assessing the effectiveness of interventions or treatments is explained, and the gold standard for evaluating the effectiveness of interventions, the randomised controlled trial, is discussed. Issues that need to be considered during the critical appraisal of randomised controlled trials, such as assessing the validity of trial methodology and the magnitude and precision of the treatment effect, and deciding on the applicability of research results, are discussed. Important terminologies such as randomisation, allocation concealment, blinding, intention to treat, p values, and confidence intervals are explained.

---

### On the concepts, methods, and use of "Probability of success" for drug development decision-making: a scoping review [^204e00cc]. Clinical Pharmacology and Therapeutics (2025). Medium credibility.

Additional considerations

We identified a number of articles that addressed other important topics related to the calculation of PoS, while these methods are considered to be outside the scope of our own review. We provide a summary of these topics and invite interested readers to directly consult these articles.

As an extension of PoS, Liualso considers the variance parameterin addition to the unknown treatment effectand includes the prior distribution for both parameters Prior. This approach was referred to as "extended Bayesian Expected Power" (eBEP). Alhussain et al.have also developed methods to elicit uncertainty about variances from experts.

A couple of articles also explored methods for including uncertainty in the PoS estimate. Wang et al.presented a bootstrap approach to quantify the uncertainty in the PoS estimate by a confidence interval and Rufibach et al.also discussed a sensitivity interval or confidence interval for PoS.

Alt et al.and Zhangextended the calculation of PoS into the multivariate setting, with Alt et al.focusing on the PoS of a phase III trial with multiple outcomes and Zhangconsidering the joint PoS of multiple phase III trials.

Sucombines Bayesian and likelihood approach in defining the design prior based on phase II data. They introduced a user‐specified parameter that controls the degree of the trade‐off between Bayesian and likelihood with the parameter's two extreme values representing fully Bayesian and fully likelihood methods, respectively.

Finally, an important consideration in the context of PoS is how to use the result. Because PoS is on a continuous scale, this value does not give a clear go/no‐go decision on whether to proceed with a phase III trial. Several articles also referred to potential decision criteria based on PoS. Sabin et al.discussed the selection of appropriate decision criterion on the PoS. Graham et al.used their approach and defined that they need to observe a PoS value of at least 0.6 for a "go" decision of a study of combination therapy. Crisp et al.also focused on the doses that gave a PoS value of approximately 60%, however they also advise that a threshold of PoS should not be used to make a decision to proceed with the next trial. Instead, other aspects including the cost of study, risk discharged if successful, portfolio fit, competitor landscape, opportunity cost, and unmet patient need will all need to be considered. These topics are also beyond the scope of this paper; for a review on holistic go/no‐go decision‐making, we refer the reader to Jiang et al.

---

### Alternative clinical trial designs [^ea3d7982]. Trauma Surgery & Acute Care Open (2020). Medium credibility.

Figure 2
Graphical representation of a hypothetical study using Bayesian methods. Consider a trial of two treatments in which the rate of mortality was 34% (112/331) in treatment A and 40% (134/333) in treatment B. Frequentist inference would provide a risk ratio of 0.84 (95% CI 0.69 to 1.03, p = 0.09). The result would be stated that no statistically significant difference between these two interventions was observed. In contrast, a Bayesian analysis, using a vague, neutral prior, would provide a risk ratio of 0.86% and 95% credible interval of 0.70–1.04. Plotting this posterior distribution would result in an area under the curve to the left of 1 (ie, decreased mortality) of 94% of the entire distribution. The result would be stated as such: there was a 94% probability that treatment A reduced mortality compared with treatment B.

In addition to providing clinically applicable answers to the question posed, Bayesian methods have other advantages in clinical trials. First, the degree of uncertainty of the treatment effect is built into the posterior probability. Second, prior knowledge is explicitly included into the estimates of the probability of treatment effect. This inclusion allows for iterative updating of the posterior probability. Third, interim analyses under the frequentist method inflate the overall type I error rate requiring sequential methods to address ('alpha spending'). Bayesian methods have no such penalty and allow for a monitoring schedule at any stage and with any cohort size.

The main disadvantage to using Bayesian methods is the determination of the prior probability. When no data exist, the choice of a prior can be subjective and greatly affect the posterior probability. Ideally, the prior is based on already published high-quality studies when available. In scenarios where no prior data exist, conservative analyses using neutral priors with a range limited to plausible effect sizes may be used, another advantage of Bayesian methods. Alternatively, Bayesian methods permit a sensitivity analysis whereby a range of plausible prior probabilities are provided and the clinician can consider the prior they think credible.

---

### A co-design framework of neural networks and quantum circuits towards quantum advantage [^aa9b9d37]. Nature Communications (2021). High credibility.

Neural computation P-LYR

An m -input neural computation component is illustrated in Fig. 4 c, where m -input data I 0, I 1, ⋯, I m −1 and m corresponding weights w 0, w 1, ⋯, w m −1 are given. Input data I i is a real number ranging from 0 to 1, while weight w i is a {−1, +1} binary number. Neural computation in P-LYR is composed of 4 operations: (i) R: this operation converts a real number p k of input I k to a two-point distributed random variable x k, where P { x k = −1} = p k and P { x k = +1} = 1 − p k, as shown in Fig. 4 b. For example, we treat the input I 0 's real value of p 0 as the probability of x 0 that outcomes −1 while q 0 = 1 − p 0 as the probability that outcomes +1. (ii) C: this operation calculates y as the average sum of weighted inputs, where the weighted input is the product of a converted input (say x k) and its corresponding weight (i.e. w k). Since x k is a two-point random variable, whose values are −1 and +1 and the weights are binary values of −1 and +1, if w k = − 1, w k ⋅ x k will lead to the swap of probabilities P { x k = −1} and P { x k = + 1} in x k. (iii) A: we consider the quadratic function as the nonlinear activation function in this work, and A operation outputs y 2 where y is a random variable. (iv) E: this operation converts the random variable y 2 to 0–1 real number by taking its expectation. It will be passed to batch normalization to be further used as the input to the next layer.

---

### The geometric nature of weights in real complex networks [^72348287]. Nature Communications (2017). Medium credibility.

Since triangles are a reflection of the triangle inequality in the underlying metric space, we expect nodes forming triangles to be close to one another. Thus, the higher average normalized weight observed on triangles strongly suggests a metric nature of weights, which is not a trivial consequence of the relation between weights and topology. This leads us to formulate the hypothesis that the same underlying metric space ruling the network topology — inducing the existence of strong clustering as a reflection of the triangle inequality in the underlying geometry — is also inducing the observed correlation between ω norm and m. To prove this, we develop a realistic model of geometric weighted random networks, which allows us to estimate the coupling between weights and geometry in real networks.

A geometric model of weighted networks

Many models have been proposed to generate weighted networks. Among them, growing network modelsand the maximum-entropy class of models. However, none of them is general enough to reproduce simultaneously the topology and weighted structure of real weighted complex networks. We introduce a new model based on a class of random networks with hidden variables embedded in a metric spacethat overcomes these limitations. In this model, N nodes are uniformly distributed with constant density δ in a D -dimensional homogeneous and isotropic metric space (Supplementary Methods), and are assigned a hidden variable κ according to the probability density function (pdf) ρ (κ). Two nodes with hidden variables κ and κ ′ separated by a metric distance d are connected with a probability

where μ > 0 is a free parameter fixing the average degree and p (χ) is an arbitrary positive function taking values within the interval (0, 1). The free parameter μ can be chosen such that(κ) = κ. Hence, κ corresponds to the expected degree of nodes, so the degree distribution can be specified through the pdf ρ (κ), regardless of the specific form of p (χ) (Supplementary Methods). The freedom in the choice of p (χ) allows us to tune the level of coupling between the topology of the networks and the metric space, which in turn allows us to control many properties such as the clustering coefficient and the navigability.

---

### Sample sizes based on three popular indices of risks [^4a61a58f]. General Psychiatry (2018). Low credibility.

Sample size justification is a very crucial part in the design of clinical trials. In this paper, the authors derive a new formula to calculate the sample size for a binary outcome given one of the three popular indices of risk difference. The sample size based on the absolute difference is the fundamental one, which can be easily used to derive sample size given the risk ratio or OR.

---

### Worth adapting? Revisiting the usefulness of outcome-adaptive randomization [^b5bf7e3b]. Clinical Cancer Research (2012). Low credibility.

Outcome-adaptive randomization allocates more patients to the better treatments as the information accumulates in the trial. Is it worth it to apply outcome-adaptive randomization in clinical trials? Different views permeate the medical and statistical communities. We provide additional insights to the question by conducting extensive simulation studies. Trials are designed to maintain the type I error rate, achieve a specified power, and provide better treatment to patients. Generally speaking, equal randomization requires a smaller sample size and yields a smaller number of nonresponders than adaptive randomization by controlling type I and type II errors. Conversely, adaptive randomization produces a higher overall response rate than equal randomization with or without expanding the trial to the same maximum sample size. When there are substantial treatment differences, adaptive randomization can yield a higher overall response rate as well as a lower average sample size and a smaller number of nonresponders. Similar results are found for the survival endpoint. The differences between adaptive randomization and equal randomization quickly diminish with early stopping of a trial due to efficacy or futility. In summary, equal randomization maintains balanced allocation throughout the trial and reaches the specified statistical power with a smaller number of patients in the trial. If the trial's results are positive, equal randomization may lead to early approval of the treatment. Adaptive randomization focuses on treating patients best in the trial. Adaptive randomization may be preferred when the difference in efficacy between treatments is large or when the number of patients available is limited.

---

### Changes in semantic memory structure support successful problem-solving and analogical transfer [^5231072f]. Communications Psychology (2024). Medium credibility.

Introduction

In our daily life, we constantly deal with problems, ranging from the most mundane (e.g. what to cook for dinner given the ingredients at our disposal), to professional activities (e.g. how to reorganize our current plans to meet a new deadline), up to major societal challenges (e.g. how to find innovative solutions against global warming). How do we find new solutions to problems? While the ability to solve problems is a critical skill for adapting to new situations and innovating, the mechanisms underlying the problem-solving process remain largely unknown.

Among the new problems we face each day, some are well-defined (e.g. playing a jigsaw puzzle). The initial state (i.e. the number of independent pieces) and goal state (i.e. assembling the pieces so it looks like the picture model) are clear, and the solver can apply a set of operations (i.e. interlocking the pieces as a function of their shape) to reach the goal. However, for many of our problems (e.g. organizing work activities during the COVID-19 pandemic), the problem space is ambiguous. No heuristics or existing rules could be applied to transform the initial state into the goal state. Such "ill-defined" problemsthus require additional mental processes, which have been tightly linked to creative thinking –. Ill-defined problem-solving (or creative problem-solving) is often referred to as insight solving, where the solution comes to mind suddenly and effortlessly, with a "Eureka" phenomenon –. According to the Representational Change Theory, solving such problems involves restructuring the initial problem mental representational space, which presumably entails combining elements related to the problem in a new way. In theory, restructuring allows one to change perspective, reframe the problem, or escape its implicitly imposed constraints, leading to creative associations. For instance, consider the following problem: "A man walks into a bar and asks for a glass of water. The bartender points a shotgun at the man. The man says, 'Thank you', and walks out". The problem is ill-defined because the path to finding the solution is to be discovered, and the goal state is vague. Solving this problem first requires asking the right question: in which context would a shotgun and a glass of water help somebody? Rather than relying on obvious associations (e.g. a glass of water is related to thirst), solvers must fill the missing link between the relevant elements of the problem (a shotgun induces fear, and fear can be a remedy for hiccups, as can drinking a glass of water). Hence, restructuring the initial representation of a given problem would allow one to see this link and find its solution.

---

### 2025 AHA / ACC statement on cost / value methodology in clinical practice guidelines (update from 2014 statement): a report of the American college of cardiology / American Heart Association joint committee on clinical practice guidelines [^68e2e74a]. Journal of the American College of Cardiology (2025). High credibility.

Cost-effectiveness plane (Figure 1) — when a new intervention produces less health and lower costs compared with the comparator, it may be considered cost-effective if the incremental cost-effectiveness ratio (ICER) is greater than the cost-effectiveness threshold; however, this situation does not arise often because patients, clinicians, and health systems rarely accept worse health outcomes even if it reduces spending, and the figure includes labels "Intervention is cost-saving ("dominant") or always acceptable" and "Intervention is inferior ("dominated") or never acceptable".

---

### Summary benchmarks-full set – 2024 [^e2c40ddc]. AAO (2024). High credibility.

Preferred Practice Pattern (PPP) guidelines — GRADE recommendation categories are specified as: "Strong recommendation (SR): Used when the desirable effects of an intervention clearly outweigh the undesirable effects or clearly do not" and "Discretionary recommendation (DR): Used when the trade-offs are less certain — either because of low-quality evidence or because evidence suggests that desirable and undesirable effects are closely balanced".

---

### Clustering knowledge and dispersing abilities enhances collective problem solving in a network [^9f543c96]. Nature Communications (2019). High credibility.

The severity of this problem's complexity is not additive or linear in nature, but interdependent on all of the sub-problems that need to be solved for this to work. Furthermore, adopting a solution might have unintended, if not non-linear, consequences to the system's overall outcome that were not obvious or foreseeable. For instance, allowing students to use their mobile phones in schools might seem like a good idea to improve class engagement through other media, but an unintended consequence might be distraction or discouraging deeper critical thinking. The objective of the NK space is to capture the complex interaction among activities that yield performance. The NK problem space's popularity in modeling human decision-making stems from its verisimilitude with the complex and multidimensional problems that face problem-solving tasks, and because researchers can easily generate a large number of statistically similar problem spaces for robustness checks. (We note, however, in order to make sure our results are not an artifact of the idiosyncrasies of the NK problem space, we replicated all of our results in another rugged problem space, the Traveling Salesperson Problem — findings available in Supplemental Methods in Supplementary Fig. 2 through 9 and 11).

---

### DELTAguidance on choosing the target difference and undertaking and reporting the sample size calculation for a randomised controlled trial [^dc7137af]. BMJ (2018). Excellent credibility.

The required sample size is very sensitive to the target difference. Under the conventional approach, halving the target difference quadruples the sample size for a two arm, 1:1, parallel group superiority trial with a continuous outcome. Appropriate sample size formulas vary depending on the proposed trial design and statistical analysis, although the overall approach is consistent. In more complex scenarios, simulations can be used but the same general principles hold. It is prudent to undertake sensitivity calculations to assess the potential effect of misspecification of key assumptions (such as the control response rate for a binary outcome or the anticipated variance of a continuous outcome).

The sample size calculation and the target difference, if well specified, help provide reassurance that the trial is likely to detect a difference at least as large as the target difference in terms of comparing the primary outcome between treatments. Failure to clarify sufficiently what is important and realistic at the design stage can lead to subsequent sample size revisions, or an unnecessarily inconclusive trial due to lack of statistical precision or ambiguous interpretation of the findings. When specifying the target difference with a definitive trial in mind, the following guidance should be considered.

---

### X chromosome inactivation in the human placenta is patchy and distinct from adult tissues [^6a557f9b]. HGG Advances (2022). Medium credibility.

Determining which X chromosome is inactivated

At the whole X chromosome level, to determine whether the same X chromosome is inactivated at the two extraction sites for each placenta sample, we employed a phasing strategy on the X chromosome by defining that the biased alleles (the alleles with higher counts) are on the same haplotype. We restricted this analysis to contain only heterozygous and expressed variants that are shared between the two extraction sites. For consistency, we defined extraction site A to be the site with more expressed variants where the unphased allele balance is greater than 0.8. We defined the activated X chromosome to consist of alleles where its allele balance (the ratio between this allele's read count and the total read count) is greater than 0.8. In cases where the allele balance is less than 0.8, we picked an allele at random with equal probability between the reference allele and the alternate allele. Then, we computed the allele balance (defined as the ratio between the biased allele's count and total count) for the alleles on the same X chromosome. We called this phased allele balance. At each site, we computed the median phased allele balance to use as summary statistics. To compute phased allele balance, we followed this procedure:
1 Find expressed variants that are shared between site A and site B.
2 Using the shared expressed variants between site A and site B, tabulate the number of expressed variants that exhibit biased expression (i.e. unphased allele balance is greater than 0.8).
3 Pick a site to base phasing from based on the number of expressed variants with biased expression. For example, if site A has 50 expressed variants with biased expression and site B has 60 such variants, phasing is based on site B.
4 Phasing strategy (using site B to base phasing): the expressed haplotype is generated by the following calculation: For each expressed variant that is shared between site A and site B, pick the allele with allele balance greater than 0.8 to be on the expressed haplotype. If allele balance is less than 0.8, choose an allele at random with equal probability.

---

### 2025 AHA / ACC statement on cost / value methodology in clinical practice guidelines (update from 2014 statement): a report of the American college of cardiology / American Heart Association joint committee on clinical practice guidelines [^09bf3bfd]. Journal of the American College of Cardiology (2025). High credibility.

Cost-effectiveness plane — decision logic for new interventions is described by explicit ICER threshold labels and dominance definitions. When a new intervention is more effective and less costly, the 'Intervention is cost-saving ("dominant") or always acceptable', whereas when it is less effective and more costly, the 'Intervention is inferior ("dominated") or never acceptable'. For cases where the new intervention is more costly and more effective, 'ICER ≥ C-E threshold Intervention is not cost-effective' and 'ICER < C-E threshold Intervention is cost-effective'. In the less costly but less effective quadrant, labels specify 'ICER ≤ C-E threshold Intervention is not cost-effective' and 'ICER > C-E threshold Intervention is cost-effective', and the caption notes that when a new intervention produces less health and lower costs, 'it may be considered cost-effective if the ICER is greater than the cost-effectiveness threshold'.

---

### Simulated anthrax attacks and syndromic surveillance [^bb422b35]. Emerging Infectious Diseases (2005). Low credibility.

The Sverdlovsk outbreak generated no physician visits on day 1; the number of visits increased until day 9 and then decreased. We created a cumulative distribution of the probability of a visit for respiratory symptoms each day from day 0 to day 30. We did not run the simulation beyond day 30 because detecting an outbreak with our system would not be beneficial at that point. To prevent a continuous signal pattern, we introduced variation by using a Poisson distribution consistent with the cumulative distribution. This distribution simulates the natural variation that would be expected in such scenarios.

We used the following approach. First, we created a cumulative distribution of the respiratory visits expected each day from 1 through 30. For example, the cumulative distribution was 0.0 for day 1, 0.01 (0.00 + 0.01) for day 2, 0.03 (0.00 + 0.01 + 0.02) for day 3, etc. Second, we assigned a random number from 0.0 to 1.0 from a uniform distribution as each randomly created day for the number of infections. Third, if the random number generated was between the minimum cumulative range for a day and the maximum cumulative range for a day, we then produced a new visit from the infection. All calculations were rounded down to the nearest integer. The effect is shown in Table 2, which shows how many visits occurred during the simulations from 1 zip code on day 6. Of the 1,000 simulations, no visits occur 123 times on day 6. Four times, however, 8 visits occur. Most of the time 1, 2, or 3 visits occur on day 6.

Table 2
Visit distribution of 1,000 simulations at a 40% infection rate for day 6 from infection for zip code 55125 (St. Paul, Minnesota)

Each of the 1,000 simulations at the given infection rate was randomly assigned (with replacement) to an attack date. The additional cases were added to the historic data based on the date randomly chosen for each iteration, which created 1,000 new files.

The expected number of visits added for a specific release can be expressed aswhere n = the number of zip codes in the outbreak, m i = number of mall visitors in zip code i, h i = number of HealthPartners patients in zip code i, d = infection rate of the simulation, and pop i = population of zip code i. The actual number of added cases was random and followed Poisson distribution centered on the expected count.

---

### EHR-safe: generating high-fidelity and privacy-preserving synthetic electronic health records [^575a88af]. NPJ Digital Medicine (2023). Medium credibility.

Table 2
Fidelity results with utility metrics.

(Upper) Downstream task performance with four different predictive models and two different settings (train on real vs. train on synthetic) on MIMIC-III and eICU datasets. Performance is evaluated on the original test sets. The best performance in each column is shown in bold. (Lower) The average absolute performance difference (in terms of AUC/AP) between training on real vs. synthetic data and the corresponding p -values (computed by one sample T-test) for predicting mortality and gender with random subsets of features.

Additionally, we evaluate the utility of the synthetic data with a random subset of features and multiple target variables. The goal is to evaluate the predictive capability of each dataset regardless of which features and targets are being used. We choose random subsets with 30 features and two target variables (mortality and gender) and test the hypothesis that the performance difference between the trained models by original and synthetic data is greater than X. In a practical setting, the choice of X would enable data owners to define a constraint on the acceptable fidelity of synthetic data. We report results with X = 0.04 for illustrative purposes. We obtain the p -value (computed by one sample T-test) that allows us to reject this hypothesis. As can be seen in Table 2, for MIMIC-III mortality prediction, we can reject the hypothesis that AUC difference is greater than 0.04 with p -value smaller than 0.01 (average AUC difference is 0.009). For eICU gender prediction, we achieve 0.019 average AUC difference with p -value smaller than 0.001.

Privacy

Unlike de-identified data, there is no straightforward one-to-one mapping between real and synthetic data (generated from random vectors). However, there may be some indirect privacy leakage risks built on correlations between the synthetic data and partial information from real data. We consider three different privacy attacks that represent known approaches that adversaries may apply to de-anonymize private data (details are provided in Fig. 2 and Supplementary Information):
Membership inference attack: The adversary explores the probability of data being a member of the training data used for training the synthetic data generation model.
Re-identification attack: The adversary explores the probability of some features being re-identified using synthetic data and matching to the training data.
Attribute inference attack: The adversary predicts the value of sensitive features using synthetic data.

---

### Concurrence of form and function in developing networks and its role in synaptic pruning [^7796ccd1]. Nature Communications (2018). Medium credibility.

Monte Carlo simulations

The initial conditions for the neural states are random. For the network topology we can draw an initial degree sequence from some distribution p (k, t = 0), and then place edges between nodes i and j with a probability proportional to k i (0) k j (0), as in the configuration model. Time evolution is then accomplished in practice via computer simulations as follows. First, the number of links to be created and destroyed is chosen according to two Poisson distributions with means Nu (κ) and Nd (κ), respectively. Then, as many times as needed according to this draw, we choose a node i with probability π (I i) to be assigned a new edge, to another node randomly chosen; and similarly we choose a new node j according to η (I j) to lose an edge from one of its neighbours, randomly chosen. This procedure uses the BKL algorithm to assure proper evolution towards stationarity. This way, each node can then gain (or lose) an edge via two paths: either through the process with probability π (I i) for a gain (or η (I i) for a loss), or when it is randomly connected to (or disconnected from) an already chosen node. Therefore, the effective values of the second factors in Eq. 1 areand, where the 1/2 factor is included to assure normalization.

---

### Quantifying risks and interventions that have affected the burden of diarrhoea among children younger than 5 years: an analysis of the global burden of disease study 2017 [^f334f006]. The Lancet: Infectious Diseases (2020). High credibility.

Table
Mortality from diarrhoeal diseases and associated risk factors by GBD super-region and region, 2017

Figure 1
The diarrhoea mortality rate among children younger than 5 years by country, 1990 and 2017

Data are under-5 diarrhoea mortality rate (95% uncertainty interval) in 1990 (gray points) and in 2017 (coloured points). The colours indicate the Global Burden of Diseases, Injuries, and Risk Factors Study super region. Countries are ordered by increasing mortality rate in 2017.

Figure 2
Maps of diarrhoea mortality rate per 100 000 among children younger than 5 years, 1990–2017

(A) Diarrhoea mortality rate per 100 000 children younger than 5 years in 2017. (B) Relative percent difference and (C) absolute difference in diarrhoea mortality rate among children younger than 5 years between 2017 and 1990. (D) Ratio of observed-to-predicted diarrhoea mortality rate per 100 000 (predicted on the basis of the observed change in SDI between 1990 and 2017) in 2017. ATG = Antigua and Barbuda. FSM = Federated States of Micronesia. Isl = Islands. LCA = Saint Lucia. SDI = Socio-demographic Index. TLS = Timor-Leste. TTO = Trinidad and Tobago. VCT = Saint Vincent and the Grenadines.

---

### Brief resolved unexplained events (formerly apparent life-threatening events) and evaluation of lower-risk infants [^7e69974f]. Pediatrics (2016). Medium credibility.

Guideline definitions for key action statements (Table 4) — a strong recommendation is defined as "A particular action is favored because anticipated benefits clearly exceed harms (or vice versa) and quality of evidence is excellent or unobtainable", with the implication "Clinicians should follow a strong recommendation unless a clear and compelling rationale for an alternative approach is present". A moderate recommendation is defined as "A particular action is favored because anticipated benefits clearly exceed harms (or vice versa) and the quality of evidence is good but not excellent (or is unobtainable)", implying "Clinicians would be prudent to follow a moderate recommendation but should remain alert to new information and sensitive to patient preferences". A weak recommendation (based on low-quality evidence) is defined as "A particular action is favored because anticipated benefits clearly exceed harms (or vice versa), but the quality of evidence is weak", with the implication "Clinicians would be prudent to follow a weak recommendation but should remain alert to new information and very sensitive to patient preferences". A weak recommendation (based on balance of benefits and harms) is defined as "Weak recommendation is provided when the aggregate database shows evidence of both benefit and harm that appear to be similar in magnitude for any available courses of action", implying "Clinicians should consider the options in their decision-making, but patient preference may have a substantial role".

---

### DELTAguidance on choosing the target difference and undertaking and reporting the sample size calculation for a randomised controlled trial [^d8f39fa0]. BMJ (2018). Excellent credibility.

Box 1
DELTA 2 recommendations for researchers undertaking a sample size calculation and choosing the target difference

Begin by searching for relevant literature to inform the specification of the target difference. Relevant literature can: relate to a candidate primary outcome or the comparison of interest, and; inform what is an important or realistic difference for that outcome, comparison, and population.
Candidate primary outcomes should be considered in turn, and the corresponding sample size explored. Where multiple candidate outcomes are considered, the choice of the primary outcome and target difference should be based on consideration of the views of relevant stakeholder groups (eg, patients), as well as the practicality of undertaking such a study with the required sample size. The choice should not be based solely on which outcome yields the minimum sample size. Ideally, the final sample size will be sufficient for all key outcomes, although this is not always practical.
The importance of observing a particular magnitude of a difference in an outcome, with the exception of mortality and other serious adverse events, cannot be presumed to be self evident. Therefore, the target difference for all other outcomes needs additional justification to infer importance to a stakeholder group.
The target difference for a definitive trial (eg, phase III) should be one considered to be important to at least one key stakeholder group.
The target difference does not necessarily have to be the minimum value that would be considered important if a larger difference is considered a realistic possibility or would be necessary to alter practice.
Where additional research is needed to inform what would be an important difference, the anchor and opinion seeking methods are to be favoured. The distribution method should not be used. Specifying the target difference based solely on a standardised effect size approach should be considered a last resort, although it may be helpful as a secondary approach.
Where additional research is needed to inform what would be a realistic difference, the opinion seeking and the review of the evidence base methods are recommended. Pilot trials are typically too small to inform what would be a realistic difference and primarily address other aspects of trial design and conduct.
Use existing studies to inform the value of key nuisance parameters that are part of the sample size calculation. For example, a pilot trial can be used to inform the choice of the standard deviation value for a continuous outcome and the control group proportion for a binary outcome, along with other relevant inputs such as the amount of missing outcome data.
Sensitivity analyses, which consider the effect of uncertainty around key inputs (eg, the target difference and the control group proportion for a binary outcome) used in the sample size calculation, should be carried out.
Specification of the sample size calculation, including the target difference, should be reported according to the guidance for reporting items (see table 1) when preparing key trial documents (grant applications, protocols, and result manuscripts).

---

### Understanding the number needed to treat [^d75bb01d]. Journal of Pain and Symptom Management (2024). Medium credibility.

The number needed to treat (NNT) is the inverse of the absolute risk difference, which is used as a secondary outcome to clinical trials as a measure relevant to a positive trial, supplementing statistical significance. The NNT requires dichotomous outcomes and is influenced by the baseline disease or symptom severity, the particular population, the type and intensity of the interventional, the duration of treatment, the time period to assessment of response, and the comparator response. Confidence intervals should always accompany NNT for the precision of its estimate. In this review, three meta-analyses are reviewed, which included the NNT in the analysis of response.

---

### Hourly step recommendations to achieve daily goals for working and older adults [^588d8957]. Communications Medicine (2024). Medium credibility.

Figure 3 shows the predicted mean step counts and 95% CI for the rest of the day, given the step counts accumulated from 6 p.m. to 11 p.m. adjusting for age group, BMI group and sex. Overall, there was little difference in the conditional predicted mean step counts for the rest of the days between the weekdays and the weekends. On weekdays, participants who accumulated 6000 steps by 6 p.m. had a 0.692 probability of achieving 10,000 steps (all the goals) by the end of the day (refer to the Goal 3 diagonal line in Fig. 3, Table 3, and Supplementary Table S2). Similarly, participants who had accumulated 7000 steps by 7 p.m. 8000 steps by 8 p.m. 8500 steps by 9 p.m. and 9500 steps by 10 p.m. achieved 10,000 mean steps by the end of the day with a probability of 0.910, 0.982, 0.700, and 1, respectively. On weekends, participants who had accumulated 6500 steps by 6 p.m. 7000 steps by 7 p.m. 8000 steps by 8 p.m. 8500 steps by 9 p.m. and 9500 steps by 10 p.m. achieved 10,000 mean steps by the end of the day with probability 0.852, 0.583, 0.947, 0.717 and 1, respectively. The step counts required to achieve 5000 and 7500 steps by the end of the day can be found in Supplementary Tables S3 and S4, respectively.

---

### Incorporating economic evidence in clinical guidelines: a framework from the clinical guidelines committee of the American College of Physicians [^e53f1cf5]. Annals of Internal Medicine (2025). High credibility.

GRADE Evidence-to-Decision summary of judgments — A sample blank framework outlines judgment scale options, including "No clinically meaningful", "Small", "Medium", "Large", "Varies", "Uncertain", and "No included studies". Type of recommendation categories listed are "Strong recommendation against the intervention", "Conditional recommendation against the intervention", "Conditional recommendation for either the intervention or the comparator", "Conditional recommendation for the intervention", and "Strong recommendation for the intervention". The figure clarifies "GRADE = Grading of Recommendations Assessment, Development and Evaluation" and notes it was "Prepared with GRADEpro (https://gradepro.org) and modified by the authors".

---

### ESHRE guideline: recurrent pregnancy loss: an update in 2022 [^7f1014fc]. Human Reproduction Open (2023). High credibility.

Regarding classification and risk stratification for recurrent pregnancy loss, more specifically with respect to prognosis, ESHRE 2023 guidelines recommend to base prognosis on the patient's age number and her complete pregnancy history, including the number of previous pregnancy losses, live births, and their sequence.

---

### Optimizing natural fertility: a committee opinion [^63aa3a95]. Fertility and Sterility (2013). Medium credibility.

Clinical pregnancy probability — cycle day and age: The probability of clinical pregnancy increases from 3.2% on cycle day 8 to 9.4% on cycle day 12 and decreases to less than 2% on cycle day 21, and the likelihood of success decreases with increasing age.

---

### Global strategy for asthma management and prevention [^80beeef6]. GINA (2024). High credibility.

Factors contributing to poor adherence — patient beliefs and concerns: To understand the reasons behind patients' medication-taking behavior, it is important to elicit their beliefs and concerns about asthma and asthma medications. Both intentional and unintentional factors contribute to poor adherence, and issues of ethnicity, health literacy and numeracy are often overlooked. Patients may be concerned about known side-effects or about perceived harm.

---

### Standards of care in diabetes – 2025 [^5eb43e21]. Diabetes Care (2025). High credibility.

Regarding follow-up and surveillance for diabetes mellitus type 1, more specifically with respect to continuous glucose monitoring, ADA 2025 guidelines recommend to implement early continuous glucose monitoring in adult patients with T1DM to improve glycemic outcomes and QoL and minimize hypoglycemia.

---

### Donor human stool (Rebyota) [^80650d28]. FDA (2024). Medium credibility.

In the integrated efficacy analysis set, the demographic profile and baseline recurrent CDI characteristics of treated adults were similar in the REBYOTA and placebo groups. In Study 1, a total of 262 adults were randomized and treated, of which 177 adults received REBYOTA and 85 received placebo. Adults had a mean age of 60.1 years with 45.4% of adults 65 years of age or older, were mainly white (92.0%) and female (69.1%). In this study, 32.8% of adults received REBYOTA or placebo for their first recurrence of CDI. In Study 1, 87.4% of adults had received vancomycin alone prior to treatment. In Study 2, 39 adults received one dose of REBYOTA and one dose of placebo and 43 adults received two doses of placebo. Adults in these two groups had a mean age of 59.8 years with 42.7% of adults 65 years of age or older, were mainly white (97.6%) and female (63.4%). In this study, 89.0% of adults had received vancomycin prior to treatment.

Treatment success was defined as the absence of CDI diarrhea within 8 weeks of blinded treatment. CDI diarrhea was defined as the passage of ≥ 3 unformed/loose stools in ≤ 24 hours for at least 2 consecutive days and a positive stool test for the presence of C difficile toxin at the time of the diarrhea.

In the Bayesian analysis, the estimated rate of treatment success was significantly higher in the REBYOTA group (70.6%) than in the Placebo group (57.5%) through 8 weeks after completing blinded treatment, resulting in a difference of 13.1 percentage points (95% Credible Interval: 2.3, 24.0) which corresponds to a 99.1% posterior probability that REBYOTA is superior to Placebo (Table 2).

Study 1 evaluated sustained clinical response which was defined as treatment success at 8 weeks and no CDI event through 6 months after the last dose during the blinded period. The difference in sustained clinical response rate (9.1%; 95% CI: -3.6%, 21.7%) was not statistically significant between the REBYOTA (65.5%) and the placebo groups (56.5%).

---