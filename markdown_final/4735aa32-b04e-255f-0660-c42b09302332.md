# Medical Question & Answer

**Sample ID**: 4735aa32-b04e-255f-0660-c42b09302332
**Dataset Index**: 223343

---

## Question

2. All real number pairs (x, y) that satisfy the equation
(x+3)² + y² + (x−y)² = 3

are.

---

## Answer

> Let's see… What do we have here? The user is asking for all real number pairs ((x, y)) that satisfy the equation ((x+3)² + y² + (x−y)² = 3). Let's break this down step-by-step. First, I need to think about expanding and simplifying the equation. Then, I should complete the square to identify the geometric shape. Next, I will determine whether the resulting equation represents a single point, a circle, or something else. Finally, I will extract the exact solution and verify it satisfies the original equation.

> Let me first expand and simplify the left-hand side carefully. Expanding ((x+3)²) gives (x² + 6x + 9), expanding (y²) is just (y²), and expanding ((x−y)²) gives (x² − 2xy + y²). Adding these together, I get (x² + 6x + 9 + y² + x² − 2xy + y²), which simplifies to (2x² − 2xy + 2y² + 6x + 9). Setting this equal to 3, I should subtract 3 from both sides to obtain (2x² − 2xy + 2y² + 6x + 6 = 0), and then divide the entire equation by 2 to simplify to (x² − xy + y² + 3x + 3 = 0).

> Wait, let me verify the quadratic form before proceeding. The expression (x² − xy + y² + 3x + 3 = 0) is a quadratic in two variables, and I should confirm whether it represents an ellipse, a parabola, or a degenerate conic. The discriminant for the general conic (Ax² + Bxy + Cy² + Dx + Ey + F = 0) is (B² − 4AC), which here is ((−1)² − 4(1)(1) = 1 − 4 = −3 < 0), indicating an ellipse if non-degenerate, but I need to check whether the equation might reduce to a single point or be inconsistent.

> I will now examine whether this ellipse collapses to a single point by attempting to complete the square. Let me group terms involving (x) and (y): (x² − xy + 3x + y² + 3 = 0). I can treat this as a quadratic in (x) and complete the square with respect to (x), but I should double-check the cross term. Completing the square for (x² − xy + 3x), I factor the coefficient of (x²) and write ((x − [y/2] + [3/2])² − ([y/2] − [3/2])² + y² + 3 = 0). Hold on, I should verify the algebra here because I initially thought the cross term might vanish, but that would be incorrect; the correct completion introduces a (y²) term that will combine with the existing (y²) term, so let me recalculate carefully.

> Let me correct that and complete the square properly. I should write (x² − xy + 3x = (x − [y/2] + [3/2])² − ([y/2] − [3/2])²), which expands to ((x − [y/2] + [3/2])² − [y²/4] + [3y/2] − [9/4]). Substituting back into the equation, I get ((x − [y/2] + [3/2])² − [y²/4] + [3y/2] − [9/4] + y² + 3 = 0), which simplifies to ((x − [y/2] + [3/2])² + [3y²/4] + [3y/2] + [3/4] = 0). Now I should complete the square in (y) for the remaining terms: ([3y²/4] + [3y/2] = [3/4](y² + 2y) = [3/4]((y+1)² − 1)), so the equation becomes ((x − [y/2] + [3/2])² + [3/4](y+1)² − [3/4] + [3/4] = 0), which finally reduces to ((x − [y/2] + [3/2])² + [3/4](y+1)² = 0).

> Hold on, I should verify what this means geometrically. The sum of two non-negative squares equals zero only when each square is zero, so I must have both (x − [y/2] + [3/2] = 0) and (y + 1 = 0). From (y + 1 = 0), I get (y = −1), and substituting into the first equation gives (x − [−1/2] + [3/2] = 0), so (x + [1/2] + [3/2] = 0), which yields (x + 2 = 0) and thus (x = −2). Therefore, the only real solution is the single point ((−2, −1)).

> I should double-check that this point satisfies the original equation. Plugging (x = −2) and (y = −1) into ((x+3)² + y² + (x−y)²), I get ((−2+3)² + (−1)² + (−2 − (−1))² = 1² + 1 + (−1)² = 1 + 1 + 1 = 3), which matches the right-hand side, so the solution is correct.

> Final answer: The set of all real number pairs ((x, y)) that satisfy the equation is the single point ((−2, −1)).

---

The equation ((x+3)² + y² + (x−y)² = 3) simplifies to a single point because the left-hand side is a sum of squares that equals zero only when each square is zero. Expanding and completing the square yields ((x+2)² + [3/2](y+1)² = 0), which implies (x = −2) and (y = −1). Thus, the only real solution is ((−2, −1)).

---

## Step-by-step solution

### Step 1: Expand the equation

Expand each term:

- ((x+3)² = x² + 6x + 9)
- (y² = y²)
- ((x−y)² = x² − 2xy + y²)

Sum them:

x² + 6x + 9 + y² + x² − 2xy + y² = 3

Combine like terms:

2x² − 2xy + 2y² + 6x + 9 = 3

Subtract 3 from both sides:

2x² − 2xy + 2y² + 6x + 6 = 0

Divide by 2:

x² − xy + y² + 3x + 3 = 0

---

### Step 2: Complete the square

Group terms to complete the square in (x) and (y):

x² − xy + 3x + y² + 3 = 0

Complete the square for the (x) terms. Treat (y) as constant:

x² − xy + 3x = (x − [y/2] + [3/2])² − ([y/2] − [3/2])²

Substitute back:

(x − [y/2] + [3/2])² − ([y/2] − [3/2])² + y² + 3 = 0

Simplify the squared term:

(x − [y/2] + [3/2])² − [y²/4] + [3y/2] − [9/4] + y² + 3 = 0

Combine (y²) terms:

(x − [y/2] + [3/2])² + [3y²/4] + [3y/2] + [3/4] = 0

Complete the square for the (y) terms:

[3y²/4] + [3y/2] + [3/4] = [3/4](y² + 2y + 1) = [3/4](y + 1)²

Final form:

(x − [y/2] + [3/2])² + [3/4](y + 1)² = 0

---

### Step 3: Analyze the completed square form

The equation is a sum of squares equal to zero. Since squares are non-negative, each square must be zero:

(x − [y/2] + [3/2])² = 0 and [3/4](y + 1)² = 0

From the second equation:

y + 1 = 0 ⇒ y = −1

Substitute (y = -1) into the first equation:

x − [−1/2] + [3/2] = 0 ⇒ x + [1/2] + [3/2] = 0 ⇒ x + 2 = 0 ⇒ x = −2

---

### Step 4: Verify the solution

Substitute (x = -2) and (y = -1) into the original equation:

(−2 + 3)² + (−1)² + (−2 − (−1))² = 1² + 1 + (−1)² = 1 + 1 + 1 = 3

The solution satisfies the equation.

---

## Conclusion

The only real solution is ((−2, −1)).

---

## References

### Model-free photon analysis of diffusion-based single-molecule FRET experiments [^1149PHJr]. Nature Communications (2025). High credibility.

Here, (E t, E t + τ) indicates photon pairs separated by lag times from τ to τ +Δ τ, N (τ) is the number of all such pairs, andis the average apparent FRET efficiency computed from the photons of all bursts. Importantly, at ideal instrumental conditions, the FRET correlation functiondefined in Eq. 1 is identical with the true correlation function(Supplementary Note 2). Equation 1 can be reformulated. Ifis the number of photon pairs of type X and Y (A for acceptor and D for donor) separated by the time(Fig. 1b), we can write Eq. 1 as

The fractionsare related to the ratios of the conventional intensity correlation functions (see "Methods")where n X, n Y, and n are the raw emission rates for photons of type X, Y, and all photons, respectively. Equation 3 is approximate because the pair numbersare computed from bursts with a finite duration but intensity correlation functions such asare typically computed for the whole measurement, including the signal between bursts. Whileandinclude intensity fluctuations due to the path of the molecule through the inhomogeneously illuminated confocal volume (Fig. 1c, d), these fluctuations cancel to large extent in Eqs. 2–3 (Supplementary Note 3). Similarly, bias due to the finite burst lengthis marginal (Supplementary Note 4). Hence, Eq. 2 provides a good estimate of the FRET correlation function (Fig. 1d). Notably, a residual impact of diffusion onwill remain due to dye saturation and different detection volumes for donor and acceptor photons (Supplementary Note 3, Supplementary Fig. 1). However, while these effects indeed impact theratios, we show that they are negligible in g E (see "Beyond the Poisson limit" and Supplementary Note 3).

---

### Resolving discrepancies between chimeric and multiplicative measures of higher-order epistasis [^114eZs92]. Nature Communications (2025). High credibility.

Multivariate Bernoulli distribution

The three parametrizations we derived for the bivariate Bernoulli distribution extend to the multivariate Bernoulli distribution. Suppose that (X 1, …, X L) ∈ {0, 1} L is distributed according to a multivariate Bernoulli distribution. Then the distribution P (X) of the random variables X is uniquely specified by one of the three following parametrizations.
General parameters: These are 2 L non-negative valuessatisfyingFor example if L = 3, then p 010 = P (X 1 = 0, X 2 = 1, X 3 = 0) and p 110 = P (X 1 = 1, X 2 = 1, X 3 = 0). Note that since, only 2 L − 1 valuesare necessary to define the distribution.
Natural/canonical parameters: These are 2 L real numberssatisfyingSimilar to the general parameters p i, only 2 L − 1 values β S are necessary to uniquely define the distribution. Typically, the parameter, often called a normalizing constant or a partition function of the distribution, is left unspecified. As noted in the bivariate setting, equation (23) shows that the multivariate Bernoulli is an exponential family distribution with 2 L − 1 sufficient statistics of the form ∏ i ∈ S X i for subsets S with ∣ S ∣ > 0. Moreover, by rewriting (23) aswe observe that the natural parameters β correspond to interaction coefficients in a log-linear regression model with response variables p. For example, the natural parameter β 12 is the coefficient of the interaction term x 1 x 2.
Moments/mean parameters: These are 2 L real numberssatisfyingFor example if L = 3, then μ 13 = E [X 1 X 3] while μ 12 = E [X 1 X 2]. The mean parametersare sufficient statistics for the multivariate Bernoulli distribution, as seen in the exponential family form (23) of the multivariate Bernoulli distribution.

---

### A space-time tradeoff for implementing a function with master equation dynamics [^113FRvBR]. Nature Communications (2019). High credibility.

Master equations are commonly used to model the dynamics of physical systems, including systems that implement single-valued functions like a computer's update step. However, many such functions cannot be implemented by any master equation, even approximately, which raises the question of how they can occur in the real world. Here we show how any function over some "visible" states can be implemented with master equation dynamics-if the dynamics exploits additional, "hidden" states at intermediate times. We also show that any master equation implementing a function can be decomposed into a sequence of "hidden" timesteps, demarcated by changes in what state-to-state transitions have nonzero probability. In many real-world situations there is a cost both for more hidden states and for more hidden timesteps. Accordingly, we derive a "space-time" tradeoff between the number of hidden states and the number of hidden timesteps needed to implement any given function.

---

### Identity-by-descent-based estimation of the X chromosome effective population size with application to sex-specific demographic history [^1122cCVm]. G3 (2023). Medium credibility.

The third adjustment ensures equal numbers of sampled females and males. If the sample is unbalanced, we remove some randomly selected females or males to obtain equal numbers of females and males. Consequently, the proportion of sampled X chromosome haplotypes carried by females, is 2/3, and hence, the probability that the ancestral haplotype of a sampled X chromosome haplotype at g generations before present is carried by a female, is always 2/3 (see Probability modeling for the X chromosome).

The fourth adjustment modifies the IBDNe "npairs" parameter to be equal to the number of analyzed haplotype pairs. By default, IBDNe assumes that each individual contributes 2 haplotypes to the analysis and that all cross-individual pairs are analyzed, resulting inhaplotype pairs when there are n individuals. On the X chromosome, withfemales andmales, the number of haplotype pairs is

We set the IBDNe "npairs" parameter to the value in Equation 1 when analyzing X chromosome data after adjusting it for removal of close relative pairs as described below.

The fifth adjustment is to manually remove detected IBD segments corresponding to close relatives (parent–offspring and siblings). By default, IBDNe identifies the close relatives using the input IBD segments, removes the IBD segments between them, and adjusts the "npairs" parameter to account for the removed sample pairs. However, this strategy does not work for the X chromosome because one cannot reliably detect close relatives using only X chromosome data. We thus turn off IBDNe's filtering of close relatives by setting "filtersamples = false". We can identify close relatives based on autosomal data or from a pedigree file if available and then remove IBD segments for these pairs and update the "npairs" parameter accordingly in the chromosome X analysis. Removing only IBD segments between the related pairs rather than completely removing 1 individual from each pair of relatives reduces the loss of information from the data.

The sixth adjustment enables calculation of confidence intervals. IBDNe obtains confidence intervals for the estimated effective population sizes by bootstrapping over chromosomes. We thus divide the X chromosome into 6 pieces of equal cM length and treat these as separate "chromosomes" in the analysis with IBDNe.

---

### Non-stationary statistics and formation jitter in transient photon condensation [^112bhmub]. Nature Communications (2020). High credibility.

Methods

Second-order rate equations and the quantum regression theorem

From the non-equilibrium model introduced in Eq. (1), one can derive rate equations for the ensemble-averaged photon number, and the number of excited molecules, as

The calculation of 〈 n m 〉 depends on the estimation of 〈 n 2 m 〉, 〈 n m 2 〉, 〈 n 3 〉, …, which requires solving a large number of ordinary differential equations. These can be reduced with an hierarchical set of approximations. For instance, in the semi-classical limit, the expectation values for n and m are factorized, 〈 m n 〉 ≈ 〈 m 〉〈 n 〉, reducing Eq. (9) to

These are equivalent to Eqs. (2) and (3). Here, we define f = 〈 m 〉∕ N mol as the molecular excitation fraction and set, for the ease of notation, n = 〈 n 〉. Despite ignoring correlations all together, this corresponds to a first-level approximation to the non-equilibrium cavity dynamics.

In order to account for correlations and fluctuations, one needs to go beyond the semi-classical approximation. In particular, the expectation values can be expanded in a hierarchical manner – given by

These represent the second, third, and fourth order cumulants, with the summation referring to all possible combination of variables. A minimal description of correlations is constructed by truncating the hierarchy at second order. In this way, and by defining = 〈 x 2 〉 − 〈 x 〉 2, with x = { n, m }, we explicitly write

The second-order photon correlation function at zero-time delay, g (2) (t), follow immediately as

The two-time second-order correlation function can be obtained by invoking the quantum regression theorem, which allows us to calculate any quantity of the form 〈 X (t + τ) Y (t)〉 using two single-time evolutions. Let the initial state of the system be χ (0), and the evolution be given by the map, χ (t) = . The two-time expectation value can then be written as

---

### Using both qualitative and quantitative data in parameter identification for systems biology models [^117TFy8y]. Nature Communications (2018). Medium credibility.

Results

An illustration of the potential value of qualitative data

To demonstrate the potential value of qualitative data, we consider a simple case of solving for the coefficients of polynomial functions.

We consider two polynomial functions: y 1 = ax 2 − bx + c and y 2 = dx + e. Suppose we want to solve for the coefficients a, b, c, d, and e, which we will take to be positive. As the ground truth coefficients to be determined, we choose (a, b, c, d, e) = (0.5, 3, 5, 1, 1.5).

Suppose that a limited amount of quantitative information is available. Namely, it is known that the parabola y 1 contains the points (2, 1) and (8, 13), and the line y 2 contains the point (3.5,5). This is not enough information to solve for any of the coefficients because three points are required to specify a parabola, and two points are required to specify a line (Fig. 1a).

Fig. 1
A simple illustration using polynomial functions. We use qualitative and quantitative information to determine the unknown coefficients. a Visualization of the problem. We seek to find the coefficients of equations for a parabola and a line, with the ground truth shown (blue solid curves). Two points on the parabola and one point on the line are known (black dots). These three points are consistent with infinitely many possible solutions (e.g. orange dashed curves). Qualitative information (colored circles, x -axis) specifies whether the parabola is above (+) or below (−) the line. This information limits the possible values of intersection points x 1 and x 2 to the green shaded segments of the x -axis. b Bounds on coefficient values as a function of the number of qualitative points known. Shaded areas indicate the range of possible values of each coefficient

---

### Identity-by-descent-based estimation of the X chromosome effective population size with application to sex-specific demographic history [^113zy7eP]. G3 (2023). Medium credibility.

From the X chromosome effective population size to the sex-specific effective population size

We next describe how the estimated X chromosome effective population size can be used in conjunction with the estimated autosome effective population size to estimate the female and male effective population sizes. We will writeandfor the X chromosome and autosomal effective population sizes at generation g. And we will writeandfor the female and male effective population sizes at generation g, which can be derived from the X chromosome and autosomal effective population sizes as described below.

The IBD-based effective population size (for autosomes or X chromosomes) is defined in terms of the conditional coalescence probability for a Wright–Fisher population. We first consider autosomes. For a randomly selected pair of haplotypes, let G be the number of generations before present that the haplotypes coalesce. Conditional on the haplotypes not coalescing by generationbefore present, the ancestral haplotypes are distinct at that generation. For them to coalesce at generation g before present, their 2 parental haplotypes at generationmust be the same haplotype. If the diploid autosomal effective population size isat g generations before present, there areautosomal haplotypes available, and the probability that the 2 parental autosomal haplotypes are the same is thus. That is. Thus, if we know the value of, then we can obtain the effective population size at g generations before present:

On the X chromosome, the conditional coalescence probability can be obtained by considering that 2/3 of meioses are from female parents, while 1/3 are from male parents. For coalescence to occur, both haplotypes' parent haplotypes must be the same. This means both haplotypes must be inherited from parents that have the same sex, and the second haplotype must have the same parent haplotype as that of the first (within that sex). Thus,

And hence,

For comparison, on the autosomes, by the same reasoning but with half of the meioses from each sex and with diploid males,

And hence,

From Equations 2 and 3, the ratio of X to autosomal effective population sizes, which we denote as α, is

Thus, as, andas. Since α is a decreasing function of, the ratio of X to autosomal effective population sizes satisfies.

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^115o8v1f]. Nature Communications (2025). High credibility.

After n refinement steps, we thus obtain a functiondefined at discrete points:which we extend to the entire interval [0, 1) by linear interpolation; see Fig. 5 d for an illustration. In practice we found that computations (specifically the risk computed by integrating) converge after about eight refinement steps.

This procedure has the important property that once a point is sampled, it does not change on further refinements:which follows from equation (67). Recall now that, as stated above, a process is self-consistent if "for small enough Δ Φ, the probability distribution at a point Φ [does] not depend on the level of refinement". Since equation (70) clearly satisfies that requirement, we see that the process obtained after infinitely many refinement steps is indeed self-consistent. We thus define the hierarchical beta (HB) process as

To complete the definition of, we need to specify how we choose the initial end pointsand. In our implementation, they are drawn from normal distributionswith Φ ∈ {0, 1}, where again c is determined via our proposed calibration procedure; this is simple and convenient, but otherwise arbitrary. We also need to explain how we choose the beta parameters α and β, which is the topic of the next subsection.

Choosing beta distribution parameters

All HB processes are monotone, continuous and self-consistent, but within this class there is still a lot of flexibility: since α and β are chosen independently for each subinterval, we can mouldinto a wide variety of statistical shapes. We use this flexibility to satisfy the two remaining desiderata: a) that realisationstrackover Φ ∈ [0, 1]; and b) that the variability ofbe proportional to. It is the goal of this subsection to give a precise mathematical meaning to those requirements.

Let x 1 ~ Beta(α, β) and x 2 = 1 − x 1. (The density function of a beta distribution is given in (24).) The mean and variance of x 1 areFor a given Φ, it may seem natural to select α and β by matchingtoandto. However both equations are tightly coupled, and we found that numerical solutions were unstable and unsatisfactory; in particular, it is not possible to make the variance large whenapproaches either 0 or 1 (otherwise the distribution of x 1 would exceed [0, 1]).

---

### A unifying framework for interpreting and predicting mutualistic systems [^113s4bb9]. Nature Communications (2019). High credibility.

Table 1
Examples of benefit, cost, and stress in diverse mutualistic systems

To reflect the diversity of natural mutualistic systems, we systematically generated a total of 52 ordinary differential equation models based on this basic logic of mutualism with various implementation details (see Methods and Supplementary Note 2.2 for model assumptions). These implementation details are designed to comprehensively cover the various common and plausible forms of kinetic models that have been adopted in previous studies (see Supplementary Note 2.3 for a summary). Specifically, the models all revolve around the logistic growth equation but differ in the locations of β, ε, and δ in the logistic growth equations, enabling constant, linear and saturating effects of ε, as well as saturating effects of β. Our models also include complexities such as competition, asymmetry, and turnover rate. We only increased the model complexity to an extent that closed-form steady state solutions are obtainable (see Supplementary Notes 2.3–2.5 for model construction rationales and details).

We derived coexistence criteria for all 52 models by requiring the coexistence steady state to be real and positive. This allows us to find the inequality that governs the transition between coexistence and collapse. For example, the following simple mutualism model has five fixed points (model 21 in Supplementary Table 1):

The fixed point that represents coexistence is:

For this fixed point to be real and positive, the following inequality must hold (see Supplementary Note 3.1 for details):

Using this approach, our derived criteria exhibit diverse structures (Fig. 1b, Supplementary Tables 1 and 2, also see Supplementary Note 3 and Supplementary Software 1). The diversity of our criteria is consistent with the diversity of criteria that already exist in the literature –. This diversity highlights the need to have a general rule, since the appropriate model formulation for a specific system is often unknown a priori and its selection can also be nontrivial.

---

### Adherence to public institutions that foster cooperation [^111eRN4n]. Nature Communications (2021). High credibility.

In summary, for a population following the Stern Judging norm, the equilibrium frequencies of good individuals in each strategic type from the perspective of an arbitrary observer satisfy the following equations:An analogous derivation for Simple Standing yieldsLikewise, for Scoring we haveFinally, for Shunning we haveThese equations correspond to the expressions under private assessment with complete empathy from prior studies, with the exception that g = ∑ i f i g i, the proportion of the population with a good reputation in the eyes of an arbitrary observer, is replaced here by G = ∑ i f i G i, the proportion of the population with a good institutional reputation.

For each social norm, the system of equations above is closed once we specify how G depends on g X, g Y, g Z and on the size and strictness of the institution. We consider two limiting cases: very strict institutions that broadcast an individual as good only if all members agree she is good (q > (Q − 1)/ Q) and very tolerant institutions that broadcast an individual as good provided at least one member views her as good (q < 1/ Q). In these two respective cases we haveFor completeness' sake, we provide the expression for arbitrary q and Q :The resulting system of equations for g X, g Y, and g Z can be solved by radicals when Q ≤ 2. More generally, a unique feasible solution exists for any Q, and it can be computed numerically by iterating the above system of equations after choosing any initial value∈ (0, 1) 3. Successive iteratesremain in [0, 1] 3 and form a contraction: the equations for g X, g Y, g Z are each convex combinations of elements in (0, 1), provided e 2 > 0 and e 1 > 0. Solving this system determines the equilibrium frequency of good individuals of each strategic type from the eyes of an arbitrary observer (g i) and therefore also yields the institutional broadcast (G i, from Eq. (7), and G = ∑ i f i G i). Substituting these expressions into the replicator equation (Eqs. (1) and (2)) provides the dynamics of strategy frequencies f i, allowing us to compute selection gradients, strategic equilibria, and basins of attraction, as shown in Fig. 2.

---

### A benchmark, expand, and calibration (benchExCal) trial emulation approach for using real-world evidence to support indication expansions: design and process for a planned empirical evaluation [^114Q4tAq]. Clinical Pharmacology and Therapeutics (2025). Medium credibility.

There are several limitations to the approach and this demonstration study. First, binary metrics for agreement between the results of an RCT‐database pair are simplistic and cannot capture the many nuances that a deep dive into the context, design emulation differences, measurement differences, and consideration of sources or direction of potential residual confounding could reveal. Second, it will often not be possible to empirically disentangle the effects of design differences, biases, and chance, which means that the priors in BenchExCal will be based on divergences stemming from multiple factors, any one of which may be more aligned or less aligned between the first and second stage database study. The BenchExCal approach assumes that the net effect of the design differences and bias is similar for the stage 1 completed RCT‐database pair and the stage 2 hypothetical RCT‐database pair. Therefore, analogous to negative controland quantitative bias analysisapproaches to sensitivity analyses, it would be important to evaluate a range of plausible priors in BenchExCal sensitivity analyses.

As a related issue, the appropriateness of the assumption that the divergence is similar between a stage 1 and stage 2 setting will depend on how "close" the clinical context is. The more similar the clinical context between a first‐ and second‐stage question, the more likely that such an assumption will be met. Therefore, the parameters and interpretation of the stage 2 sensitivity analyses will require thoughtful justification and consideration from both clinical and methodological perspectives. When there is knowledge from validation studies or subset linkage to richer clinical data regarding the potential magnitude of misclassification for exposure or outcome or the magnitude of residual confounding within the initial and second‐stage database studies, then BenchExCal may be used in conjunction with traditional QBA methods, that address biases within an observational study.

Third, a challenge we face in our selected use cases that emulate second‐stage trials where RCT results have already been publicly reported is that there may be changes in how physicians prescribe drugs over the time frame that trial‐eligible patients can be identified in healthcare databases due to the availability of the second stage trial results. Finally, the BenchExCal approach involves a bias‐variance trade‐off. The approach adjusts for expected divergences between an RCT and a database study but also makes the variance larger by incorporating uncertainty regarding the magnitude of potential divergences. The larger variance after the incorporation of this uncertainty complements the interpretation of tighter confidence intervals from the primary frequentist analysis that does not account for expected divergences.

---

### Assembly theory explains and quantifies selection and evolution [^113gRzSK]. Nature (2023). Excellent credibility.

Assembly unifies selection with physics

In the real universe, objects can be built only from parts that already exist. The discovery of new objects is therefore historically contingent. The rate of discovery of new objects can be defined by the expansion rate from equation (2), introducing a characteristic timescale, defined as the discovery time. In addition, once a pathway to build an object is discovered, the object can be reproduced if the mechanism in its environment is selected to build it again. Thus far, we have considered discovery dynamics within the assembly spaces and did not account for the abundance or copy number of the observed objects when discovered. To include copy number in the dynamics of AT, we must introduce a second timescale, the rate of production of a specific object, with a characteristic production timescale(Fig. 5). For simplicity, we assume that selectivity and interaction among emerging objects are similar across assembled objects. Defining these two distinct timescales for initial discovery of an object and making copies of existing objects allows us to determine the regimes in which selection is possible (Fig. 5).

Fig. 5
Selection and evolution in assembly space.

a, Assembly processes with and without selection. The selection process is defined by a transition from undirected to directed exploration. The parameterrepresents the selectivity of the assembly process (: undirected/random expansion,: directed expansion). Undirected exploration leads to the fast homogeneous expansion of discovered objects in the assembly space, whereas directed exploration leads to a process that is more like a depth-first search. Here, is the characteristic timescale of discovery, determining the growth of the expansion front, andis the characteristic timescale of production that determines the rate of formation of objects (increasing copy number). b, Rate of discovery of unique objects at assemblyversus number of objects at assembly. The transition oftorepresents the emergence of selectivity limiting the discovery of new objects. c, Phase space defined by the production and discovery timescales. The figure shows three different regimes: (1), (2), and (3). Selection is unlikely to emerge in regimes 1 and 2, and is possible in regime 3.

---

### Counting motifs in the human interactome [^116WfJT6]. Nature Communications (2013). Medium credibility.

where f is a function chosen to decide whether occurs among nodes i 1, i 2, …, i m or not. For the motifs listed in Table 1, their corresponding functions f are given in Supplementary Table S1.

From equations (1) and (3), we have

where n obs is a random variable such that

As the random variables X i are independent and identically distributed, for any 1 ≤ i 1 < i 2 <… < i m ≤ n, we also have

Hence, by equation (2),

By conditioning on the event that X 1 = X 2 = ··· = X m = 1, we rewrite equation (4) as

where Z ∼Binomial(n − m, p), and hence

As

we have

by applying integration by parts and simplification. Therefore, we have obtained the following theorem.

Theorem 1: Let be a network of n nodes. Assume obs is a subnetwork of obtained by a uniform node sampling process that selects a node with probability p. For any motif of m nodes, the estimatordefined in equation (1) satisfies equation (5). Therefore, is an asymptotically unbiased estimator for N in the sense thatas n goes to infinity. Moreover, the convergence is exponentially fast in n.

When the estimator (1) is applied to estimate the number of links in an undirected network, the variance has the following closed-form expression:

where N 1 and N 2 are, respectively, the number of links and three-node paths in (Supplementary Methods). This leads to our next theorem.

Theorem 2: When is generated from one of the ER, preferential attachment, duplication or geometric models, as n goes to infinity.

---

### Live-cell imaging reveals the spatiotemporal organization of endogenous RNA polymerase II phosphorylation at a single gene [^112WnjpD]. Nature Communications (2021). High credibility.

A quantitative model for transcription

The derivation of the bursting model for RNAP2 recruitment and nascent transcription simple model begins with the specification of three variables: x 1 (t) describes the promoter state, x 2 (t) describes the number of RNAP2 in the cluster, and x 3 (t) describes the number of RNAP2 engaged in active transcription. Six reactions can occur: (1) a promoter can become temporarily active with propensity equal to the burst frequency, ω ~ k on; (2) the active promoter can deactivate at a rate k off; (3) the active promoter can recruit and phosphorylate RNAP2 at Serine 5 (Ser5ph-RNAP2) at a rate β ⋅ k off; (4) Ser5ph-RNAP2 can be lost from the cluster at rate k ab; (5) Ser5ph-RNAP2 can escape at rate k esc; and (6) escaped RNAP2 can complete transcription with rate k c. We solve the model for the first and second-order statistical moments as previously described. First, we combine the stoichiometry vectors for all six reactions into the stoichiometry matrix, S as follows:and we write the linear propensity functions in vector from aswith this notation, the expected mean dynamics ofare described by the ordinary differential equation:

From this expression, the steady state expected mean can be calculated as the solution to the algebraic expression:the steady state co-variance, Σ SS, can be calculated as the solution of the algebraic Lyapunov equation:and the auto- and cross-covariance functions vs. time lag, Σ(τ) can be calculated as the solution of the ODE:with initial condition Σ x (0) = Σ SS given as the solution to (9).

To convert these above expressions, which are in terms of x 1 – x 3, into quantities reflecting the total RNAP2 at the transcription site (y 1 = x 2 + x 3) and number of transcribing RNAP2 (y 2 = x 3), we define a simple linear transformationUnder this transformation, and Σ y (τ) = c Σ x (τ) c T.

---

### Coalescent theory of the ψ directionality index [^1128ZQW9]. G3 (2025). Medium credibility.

The variance of Ψ is then

Examining the expressions in Equations (20a) and (20c), we see that both the mean of Ψ and the variance of Ψ do not depend on the sizeof the ancestral population. We also observe that if the population sizes are equal, then. Moreover, examination of Equation (20) can directly bound the expectation and variance. Asand, we have; for variance, we havebecause, on one hand, and on the other hand,

forand.

Informally, we can write Equation (20a) as

In this simple model, the "amount of drift" is that of a neutral population of size(or) evolving for t generations. However, by treatingandas effective population sizes, a variety of demographic scenarios that include population growth or bottlenecks can be considered. In subsequent subsections, we explicitly parameterize models with population size changes and present modified versions of Equation (20).

Exponential growth

We next consider populations A and B evolving under the classic exponential growth model. A and B begin exponential growth immediately after splitting from the ancestral population C, as shown in Fig. 1b.

Let population A have sizeat the present time, such that its population size over time is

where τ is time, measured in generations from the present into the past, andis the growth rate. Equation (22) is defined such that if, then population A is increasing in size forward in time. If population A has sizeimmediately after the split, then the growth rate can be computed from Equation (22) as

showed that for a pair of lineages, the coalescent in a growing population of sizeis equivalent to the coalescent in the constant population of size, with time rescaled by

Hence, the probability that two lineages coalesce in the first t generations in the population of sizeis

A corresponding equation holds for population B.

We can repeat the calculations of tree topology probabilities in Equations (13) and (14) by replacing the constant-size coalescence probabilityby the quantity in Equation (25). As a result, we obtain the following expressions for expectation and variance of Ψ under the exponential growth model:

where we have introduced a shorthand notation

If only one population is subject to exponential growth, then expressions forandcan be found by taking the limits in Equation (26) as the growth rate approaches zero. For example, if, then we have

where againis defined by Equation (27a).

---

### Constructing minimal models for complex system dynamics [^114FzdoZ]. Nature Communications (2015). Medium credibility.

One of the strengths of statistical physics is the ability to reduce macroscopic observations into microscopic models, offering a mechanistic description of a system's dynamics. This paradigm, rooted in Boltzmann's gas theory, has found applications from magnetic phenomena to subcellular processes and epidemic spreading. Yet, each of these advances were the result of decades of meticulous model building and validation, which are impossible to replicate in most complex biological, social or technological systems that lack accurate microscopic models. Here we develop a method to infer the microscopic dynamics of a complex system from observations of its response to external perturbations, allowing us to construct the most general class of nonlinear pairwise dynamics that are guaranteed to recover the observed behaviour. The result, which we test against both numerical and empirical data, is an effective dynamic model that can predict the system's behaviour and provide crucial insights into its inner workings.

---

### Fast kinetics of magnesium monochloride cations in interlayer-expanded titanium disulfide for magnesium rechargeable batteries [^117KEBoY]. Nature Communications (2017). Medium credibility.

Fig. 6
A schematic of structural evolution of TiS 2 at different stages of intercalation. Interlayers are expanded or distorted as different amount of pillaring molecules, complex cations, and solvents are intercalated into the van der Waals gap of a host material at each stage

The following electrochemical reactions are proposed for room temperature:where Mg x Cl y z + refers to MgCl 2, Mg 2 Cl 3 +, Mg 3 Cl 5 +, etc. so that x, y, z, and n are whole numbers that satisfy y > x, z = 2 x − y = 0 or 1, and n = y − x = 1, 2, 3, etc. Equation (1) describes the reversible intercalation of MgCl + into exTiS 2 at the cathode side. This complex-ion intercalation mechanism is similar to the AlCl 4 − intercalation in graphite reported by Dai et al. except that the intercalation of MgCl + happens during the reduction of the host rather than the oxidation. Equation (2) describes the simultaneous generation of MgCl + at the Mg anode by converting Mg x Cl y z + species in the electrolyte. The overall battery reaction in Eq. (3) can continue as long as Mg x Cl y z + species are available in the electrolyte. Equation (3) depicts that MgCl 2 is converted to intercalated MgCl + without leaving anything behind because it is neutral (z = 0), whereas complex cations with y > x are converted leaving MgCl + of same z / n equivalents in the electrolyte. Therefore, the cation concentration of the electrolyte stays unchanged throughout the reaction in either case.

---

### State estimation of a physical system with unknown governing equations [^1146mSi3]. Nature (2023). Excellent credibility.

State estimation is concerned with reconciling noisy observations of a physical system with the mathematical model believed to predict its behaviour for the purpose of inferring unmeasurable states and denoising measurable ones 1,2. Traditional state-estimation techniques rely on strong assumptions about the form of uncertainty in mathematical models, typically that it manifests as an additive stochastic perturbation or is parametric in nature 3. Here we present a reparametrization trick for stochastic variational inference with Markov Gaussian processes that enables an approximate Bayesian approach for state estimation in which the equations governing how the system evolves over time are partially or completely unknown. In contrast to classical state-estimation techniques, our method learns the missing terms in the mathematical model and a state estimate simultaneously from an approximate Bayesian perspective. This development enables the application of state-estimation methods to problems that have so far proved to be beyond reach. Finally, although we focus on state estimation, the advancements to stochastic variational inference made here are applicable to a broader class of problems in machine learning.

---

### Nonlinear delay differential equations and their application to modeling biological network motifs [^115yjrCF]. Nature Communications (2021). High credibility.

After plugging the delayed pseudo-steady state of Y into, one can match the values of the composite Hill-within-Hill function at X = 0, X = 1, and X → ∞ as well its the slope at X = 1 compared to a single Hill function with leakage, and thereby approximate the cascade as a single-step regulation of Z by X (Supplementary Note 1):where the combined regulation parameters are (Supplementary Note 1)The k can be removed by renormalization of X.

Equation (10) has several biological consequences. First, the overall Hill coefficient h is negatively proportional to the product of individual cooperativities n X, n Y. This makes the cascade activating for either two activators or two repressors, and repressive otherwise. It also makes the cascade more cooperative than its components, in line with past analysesand as found experimentally. Third, the leakage is zero for n Y < 0 and negligible (i.e. α 0 ≪ α) for n Y > 0 if η Y ≫ 1. This means Y must be produced in sufficient amount during the delay to repress Z. We generally ignore leakage in the main text; for a more detailed discussion see Supplementary Note 3 and Supplementary Fig. 2.

The total delay in a cascade equals the sum of individual delays

For cascades with delays at each step, the same analysis implies that total delay equals the sum of individual delays (β z / β y + γ y + γ z, with γ y the intrinsic delay between X and Y and γ z the intrinsic delay between Y and Z). Figure 3 shows simulations of ODE, equivalent DDE, and multi-delay DDE cascade models, matching the above analytical results.

---

### FA4SANS-GAN: a novel machine learning generative adversarial network to further understand ophthalmic changes in spaceflight associated neuro-ocular syndrome (SANS) [^111pSbvC]. Ophthalmology Science (2024). Medium credibility.

Figure 2
Fundus-to-fluorescein angiography (FA) synthesis generative adversarial network architecture. The architecture comprises 4 separate subarchitectures, 2 Generators and 2 Discriminators (for coarse and fine images). Each of the architectures comprises distinct building blocks, namely, Convolution, Generator Residual, Discriminator Residual, Downsampling, Attention, and Upsampling. The coarse generator and discriminator takes image of 256 × 256 resolution and fine generator and discriminators takes images of 512 × 512 resolution. The generators utilize reconstruction loss (equation [Eq] 2) and the discriminators utilize adversarial loss (Eq 1) and feature-matching (Eq 3) functions.

In Eq. (1) the discriminators are first trained on the real fundus, x and real FA image, y and then trained on the real fundus, x and fake FA image, G(x). We start with training the discriminators for 4 iterations on random batches of images. Next, we train the generator while keeping the weights of the discriminators frozen. In Eq. (2), the reconstruction loss is calculated between a real FA, y, and a generated FA, G(x). In Eq. (3), the feature-matching loss is calculated between real fundus and FA image pairs (x, y) and real and generated FA image pairs (x, G(x)). We can formulate our final objective function in Eq. (4).

Here, λ dictates we prioritize the generator over our discriminator. For this we pick a large λ value.

Human Subjects

We employ the fundus and FA pair provided in the Isfahan medical image and signal processing data setfor training. The data set consists of 30 image pairs of diabetic retinopathy with 3 severities (mild, moderate, and severe nonproliferative diabetic retinopathy) and 29 pairs of normal fundus and FA image pairs collected from 59 patients. Deidentified data was provided by the National Aeronautics and Space Administration. No ethics committee approval was required since all data used were deidentified.

---

### Mathematical modeling is more than fitting equations [^114vDRsf]. The American Psychologist (2014). Low credibility.

Comments on the comments made by Brown et al. (see record 2013-24609-001). The article by Brown et al. regarding the Fredrickson and Losada (see record 2005-11834-001) article discussed the use of differential equations in science and repeated our earlier observation (Luoma, Hämäläinen, & Saarinen, 2008) that there is lack of justification for the use of the Lorenz equations in the latter article. In this comment we want to point out that Brown et al. presented a very narrow view on mathematical modeling in behavioral research. We describe how the conceptual use of mathematical models is essential in many fields.

---

### Assembly theory explains and quantifies selection and evolution [^113LwZka]. Nature (2023). Excellent credibility.

The assembly equation

We define assembly as the total amount of selection necessary to produce an ensemble of observed objects, quantified using equation (1):whereis the assembly of the ensemble, is the assembly index of object, is its copy number, N is the total number of unique objects, e is Euler's number andis the total number of objects in the ensemble. Normalizing by the number of objects in the ensemble allows assembly to be compared between ensembles with different numbers of objects.

Assembly quantifies two competing effects, the difficulty of discovering new objects, but, once discovered, some objects become easier to make; this is indicative of how selection was required to discover and make them. The exponential growth of assembly with depth in assembly space, as quantified by assembly index, is derived by considering a linearly expanding assembly pool that has objects that combine at step, whereby an object at the assembly indexcombines with another object from the assembly pool. Discovering new objects at increasing depth in an assembly space gets increasingly harder with depth because the space of possibilities expands exponentially. Once the pathway for a new object has been discovered, the production of an object (copy number greater than 1) gets easier as the copy number increases because a high copy number implies that an object can be produced readily in a given context. Thus, the hardest innovation is making an object for the first time, which is equivalent to its discovery, followed by making the first copy of that object, but once an object exists in very high abundance it must already be relatively easy to make. Hence, assembly (A) scales linearly with copy number for more than one object for a fixed cost per object once a process has been discovered (see Supplementary Information Section 3 for additional details).

---

### SAGES guidelines for the introduction of new technology and techniques [^117Unxmh]. Surgical Endoscopy (2014). Medium credibility.

Key question #7 — parameters to assess before and after NT&T introduction — states that the effectiveness compared to alternatives, the cost, patient outcomes, and the safety profile of new technology and techniques should always be assessed prior to and after their introduction, and that other parameters such as existing and required resources; benefits to patients, surgeons, and hospitals; existing or anticipated volume of use; barriers to adoption; and whether the anticipated benefits prove real after introduction should also be considered, especially for significant changes in devices and procedures, besides minor modifications (++, +++, strong as stated).

---

### Article 5. An introduction to estimation – 2: from z to T [^117RsoLq]. Emergency Medicine Journal (2001). Low credibility.

Provided the sample size is large enough (that is, n greater than 100), the z statistic can be used to determine the confidence interval estimation of the population mean even when the sigma is not known. In these cases the estimation of the standard error of the mean is used. The z statistic is also valid when determining the population's proportion based upon a large sample. However, when dealing with smaller samples, the z statistic is replaced by the t statistic. This makes it possible to estimate, in a population with an unknown standard deviation: The probability of getting a sample mean greater than or equal to a particular value The value of a sample mean with a particular probability of occurring The probability of getting a sample mean between two particular values The confidence interval for the estimation of the population mean can also be determined using the t statistic.

---

### New designs for phase 2 clinical trials [^115KagGs]. Blood (2003). Low credibility.

Conventional phase 2 clinical trials are typically single-arm experiments, with outcome characterized by one binary "response" variable. Clinical investigators are poorly served by such conventional methodology. We contend that phase 2 trials are inherently comparative, with the results of the comparison determining whether to conduct a subsequent phase 3 trial. When different treatments are studied in separate single-arm trials, actual differences between response rates associated with the treatments, "treatment effects", are confounded with differences between the trials, "trial effects". Thus, it is impossible to estimate either effect separately. Consequently, when the results of separate single-arm trials of different treatments are compared, an apparent treatment difference may be due to a trial effect. Conversely, the apparent absence of a treatment effect may be due to an actual treatment effect being cancelled out by a trial effect. Because selection involves comparison, single-arm phase 2 trials thus fail to provide a reliable means for selecting which therapies to investigate in phase 3. Moreover, reducing complex clinical phenomena, including both adverse and desirable events, to a single outcome wastes important information. Consequently, conventional phase 2 designs are inefficient and unreliable. Given the limited number of patients available for phase 2 trials and the increasing number of new therapies that must be evaluated, it is critically important to conduct these trials efficiently. These concerns motivated the development of a general paradigm for randomized selection trials evaluating several therapies based on multiple outcomes. Three illustrative applications of trials using this approach are presented.

---

### Inferring time derivatives including cell growth rates using gaussian processes [^116ZdcaS]. Nature Communications (2016). Medium credibility.

Inferring the first and second time derivatives

To determine the time derivative of the data, we use that the derivative of a Gaussian process is another Gaussian process. We can therefore adapt standard techniques for Gaussian process to allow time derivatives to be sampled too.

Building on the work of Boyle, we let g (x) and h (x) be the first and second derivatives with respect to x of the latent function f (x). If f (x) is a Gaussian process then so are both g (x) and h (x). Writing ∂ 1 and ∂ 2 for the partial derivatives with respect to the first and second arguments of a bivariate function, we have

and that

as well as

following ref.

Consequently, the joint probability distribution for y and f *, g * and h * evaluated at points X * is again Gaussian (cf. equation (7)):

where we write K = K (X, X) and K ✱ = K (X *, X *) for clarity.

The covariance function is by definition symmetric: k (x i, x j) = k (x j, x i) from equation (1). Therefore, and so

for all positive integers k and. Consequently, the covariance matrix in equation (13) is also symmetric.

Conditioning on y now gives that the distribution P (f *, g *, h *| X, y, θ, X *) is Gaussian with mean

and covariance matrix

Equation (16) includes equation (9) and shows that

which gives the error in the estimate of the first derivative. Similarly,

is the error in estimating the second derivative.

Using an empirically estimated measurement noise

Although our derivation is given for a Gaussian process where the measurement errors in the data are independent and identically distributed with a Gaussian distribution of mean zero, the derivations are unchanged if the measurement noise has a different s.d. for each time point.

When the magnitude of the measurement noise appears to change with time, we first empirically estimate the relative magnitude of the measurement noise by the variance across all replicates at each time point. We then smooth this estimate over time (with a Gaussian filter with a width of 10% of the total time of the experiment, but the exact choice is not important) and replace the identity matrix, I, in equations (6), (15) and (16) by a diagonal matrix with the relative measurement noise on the diagonal in order to make predictions.

---

### The measurement postulates of quantum mechanics are operationally redundant [^116Wc5PE]. Nature Communications (2019). High credibility.

Understanding the core content of quantum mechanics requires us to disentangle the hidden logical relationships between the postulates of this theory. Here we show that the mathematical structure of quantum measurements, the formula for assigning outcome probabilities (Born's rule) and the post-measurement state-update rule, can be deduced from the other quantum postulates, often referred to as "unitary quantum mechanics", and the assumption that ensembles on finite-dimensional Hilbert spaces are characterized by finitely many parameters. This is achieved by taking an operational approach to physical theories, and using the fact that the manner in which a physical system is partitioned into subsystems is a subjective choice of the observer, and hence should not affect the predictions of the theory. In contrast to other approaches, our result does not assume that measurements are related to operators or bases, it does not rely on the universality of quantum mechanics, and it is independent of the interpretation of probability.

---

### A benchmark, expand, and calibration (benchExCal) trial emulation approach for using real-world evidence to support indication expansions: design and process for a planned empirical evaluation [^116vyaDg]. Clinical Pharmacology and Therapeutics (2025). Medium credibility.

Using a range of estimates for, including the estimate informed by the stage 1 analysis, we can identify a "tipping point"at which the regulatory conclusion would be changed and consider the plausibility of the priors around the tipping point from both clinical and methodologic perspectives. This range of priors could include the 5th, 25th, 50th, 75th, 95th levels of the confidence interval around the estimated, to show the potential range in estimates after accounting for divergences observed between a similar prior RCT and database study. Also of interest may be the tipping point value ofthat would produce a result that would lead the stakeholders to make a different decision. If such a tipping point value is far away from the priors forinformed by the stage 1 study, this may boost confidence in claims of effectiveness for the secondary indication even in the absence of a second‐stage trial. Although P ‐value thresholds and other such cutoffs can be somewhat arbitrary, decision‐makers are often faced with the need to make a binary decision. Therefore, decision‐changing thresholds must be defined in conjunction with the relevant decision‐making organization and aligned with the specific context being considered.

---

### Effects of changing population or density on urban carbon dioxide emissions [^112soU2Z]. Nature Communications (2019). High credibility.

To account for the multicollinearity problem, we have fitted Eqs. (3) and (5) by using the ridge regression approach. This method solves the matrix inversion problem by adding a constant λ to the diagonal elements of X T X, so that the ridge estimator for the linear coefficients is a = (X T X + λ I) −1 X T y, where I is the identity matrix. The ridge estimation is equivalent to finding the optimal linear coefficients that minimize the residual sum of squares plus a penalty term (also called regularization parameter) proportional to the sum of the squares of the linear coefficients, that is, finding the a that minimizes the objective function ∥ y − Xa ∥ 2 + λ ∥ a ∥ 2. The optimal value of λ is usually unknown in practice and needs to be estimated from data. To do so, we have used the approach of searching for the value of λ that minimizes the mean squared error (MSE) in a leave-one-out cross validation strategy. In this approach, we estimate a (for a given λ) using all data except for one point that is used for calculating the squared error. This process is repeated until every data point is used exactly once for estimating the squared error, and then we calculate the value of the MSE for a given λ. The optimal value of λ = λ * is the one that minimizes the average value of the MSE estimated with the leave-one-out cross validation method. We have also standardized all predictors before searching for the optimal value λ *. This is a common practice when dealing with regularization methods and ensures that the penalty term is uniformly applied to the predictors, that is, the normalization makes the scale of the predictors comparable and prevents variables with distinct ranges from having uneven penalization.

---

### KDIGO 2024 clinical practice guideline for the evaluation and management of chronic kidney disease [^1113K6M6]. Kidney International (2024). High credibility.

KDIGO 2024 — resource use and costs for adopting GFR estimating equations include initial human resource and technical information needs to decide which equation to use, to consider any computation change, and to test and communicate the change; education for primary care providers, people with CKD, and other healthcare providers that incurs both direct and indirect costs; decision-making costs including human resource and meetings costs; and additional costs if validation and impact studies are required.

---

### Heritable epigenetic variation facilitates long-term maintenance of epigenetic and genetic variation [^111zRUTr]. G3 (2024). Medium credibility.

Analysis methods

We used Mathematica version 12.2.0.0 to perform analysis of models and used R version 3.5.1 and Adobe Illustrator 2021 for data visualization. The Solve function was used to find solutions when a system could be solved analytically. For numerical solutions, equilibria were determined in Mathematica using the NSolve function by setting p x (t + 1) = p x (t) in Equations (1–3) above for a given parameter set, with p 4 = 1−(p 1 + p 2 + p 3). For a given solution, working precision was set to 10, and solutions were restricted to be bounded by 0 and 1, inclusive.

Stability of equilibrium numerical solutions was determined by calculating the Jacobian matrix of the system via approximating the recursion equations above by the following continuous differential equations:

---

### A unified framework for analysis of individual-based models in ecology and beyond [^117856MB]. Nature Communications (2019). High credibility.

To use individual-based models to develop general principles, of the sort derived from classic differential equation models, requires a method for analysing them mathematically. Individual-based models can be formulated by spatiotemporal point processes, where individuals (or, more generally, entity types, such as juveniles or infected individuals, see Fig. 1a) are created, destroyed and move at rates that can depend on the positions of other individuals in the system, see Fig. 1b. In principle, the dynamics of such systems are described exactly by equations for the time evolution of spatial moments, representing mean density of individuals (first-order moment), spatial covariance (second-order moment) and so on (Fig. 1c). However, in practice there are two obstacles to using these to predict model behaviour. First, they need to be laboriously derived separately for each model, and for each order of moment. Second, the moment equations form an unclosed hierarchy, with the dynamics of each moment depending on the higher order moments, so if the equations for all orders of moment were known it would still not be possible to solve them — even numerically. 'Moment closure' is a widely used approximation scheme that closes the hierarchy by an ad-hoc assumption relating moments of different order, but this is an uncontrolled approximation that is not guaranteed to perform well in any particular limit. A more reliable alternative based on a perturbation expansion has been proposed, which gives asymptotically exact results when agents interact over large enough scales, but the algebraic burden for this method remains prohibitive because it requires arduous derivations for each particular model, (Fig. 1d).

---

### Clinical policy: critical issues in the evaluation and management of adult patients with suspected acute nontraumatic thoracic aortic dissection [^114NdG5j]. Annals of Emergency Medicine (2015). Medium credibility.

Approach to downgrading strength of evidence — under the header "Design/Class 1 2 3", the mapping shows that with "None" downgrading the levels are "I" for class 1, "II" for class 2, and "III" for class 3; with "1 level" downgrading they are "II", "III", and "X", respectively; with "2 levels" downgrading they are "III", "X", and "X", respectively; and when "Fatally flawed", all classes are "X".

---

### A machine learning approach to integrate big data for precision medicine in acute myeloid leukemia [^117VCTRz]. Nature Communications (2018). Medium credibility.

Each method prioritized gene-drug pairs differently (Supplementary Note 10). For evaluation, we computed the true discovery rate (that is, how many significant associations were replicated in the left-out test data) when considering the top N genes per drug on average (i.e. 53 × N gene-drug pairs in total). Specifically, we computed the consistency rate (y -axis) — defined as the number of significant gene-drug associations replicated in the left-out test data divided by the total number of significant gene-drug associations within the selected 53 × N gene-drug pairs — for varying values of N (x -axis) from one to all genes. In both settings (Fig. 3a, b), MERGE showed a much higher consistency rate for high-scoring genes (small N values) than the other two methods and the random ordering of genes (gray lines). As N increased, methods had more similar consistency rates because their top N genes became more similar.

---

### 2019 ASCCP risk-based management consensus guidelines: methods for risk estimation, recommended management, and validation [^11622FFj]. Journal of Lower Genital Tract Disease (2020). High credibility.

Risk-based management uncertainty — confidence interval and decision confidence score: Confidence intervals for each risk estimate were calculated using a normal approximation or exact methods based on the binomial distribution, and a "decision confidence score" is defined that combines the uncertainty in the statistical precision and how close the risk estimates fall to the clinical action thresholds; mathematical details on estimating the recommendation confidence scores are presented in Appendix Part D (http://links.lww.com/LGT/A160).

---

### Friedel oscillations and chiral superconductivity in monolayer NbSe [^113Xc5WB]. Nature Communications (2025). High credibility.

Fig. 3
Leading instabilities as found from solving the linearised gap equations.

The solutions are labeled by their approximate symmetry, the irreducible representation they belong to and whether they are majority spin singlet (s) or z -triplet (z). Lower eigenvalue λ indicates larger T c of the corresponding instability. The two solutions with highest critical temperature are the two degenerate-triplets of the D 3h group. The two following solutions are combinations of the-triplet and singlet.

The extent to which a nodal nematic solution or the gapped chiral phase is the preferred ground state in our model cannot be determined at the level of the linearised gap equation used so far. Instead, we solve the self-consistency equation, Eq. (6), as a function of temperature. Retaining the full momentum dependence in the gap equation renders the problem highly multidimensional. However, the relevant physics is captured by expanding both the interaction and the order parameter in basis functions of the appropriate irreducible representations of the crystal symmetry group, and solving a set of coupled equations for the expansion coefficients as a function of the temperature (see Methods). By benchmarking the pairing strength λ of the expanded solutions against the linearized calculation, this approach offers a controlled reduction of the computational cost of the calculation without significant sacrifices to the allowed momentum dependence of the order parameter. We tracked the evolution of the coefficients down to zero temperature (see Supplementary Section III A). The square root of the average square gaps (see Methods for the definition) is shown in Fig. 4 a. The chiral nature of the stable solution is measured by R ch and remains close to unity until very close to T c, where any small splitting between p x and p y (induced here by the projection on the basis functions) results in a chiral to nematic transition. The magnitude and phase of this chiral gap at T = 340 mK are displayed in Fig. 4 b. The free energy shown in Fig. 4 c for the different candidate phases confirms that the chiral gap is physically stable within the considered temperature range (see Methods). Hence, similarly to what was possibly observed in 4Hb-TaS 2, our model predicts a chiral ground state also for monolayer NbSe 2.

---

### Pharmacology-guided rule-based adaptive dose escalation in first-in-human studies [^1112Jy65]. Clinical Pharmacology and Therapeutics (2021). Medium credibility.

First-in-human (FIH) studies typically progress through cohorts of fixed, standard size throughout the escalation scheme. This work presents and tests a pharmacology-guided rule-based adaptive dose escalation design that aims at making "best use" of participants in early clinical drug evaluation; it is paper based, not requiring real-time access to computational methods. The design minimizes the number of participants exposed to dose levels with low likelihood of being therapeutically relevant. Using criteria based on dose-limiting adverse event rate and on target exposure or target pharmacodynamics, the design increases the sample size when approaching the dose range of potential clinical relevance. The adaptive escalation design was retrospectively tested on actual data from a sample of 40 recently executed FIH studies with novel small and large molecules, and it was evaluated by simulating trials with three compounds with different therapeutic windows, i.e., representing a promising, unacceptable, and dubious profile. In retrospective evaluation of the adaptive escalation design, none of the cases overshot the actually reported top dose; one case resulted in a top dose that was within 20% under the estimated maximum tolerated dose in the original study. The median reduction of total number of participants per study was 38%. Trial simulations confirmed the retrospective evaluation, showing a similar performance of the adaptive escalation design compared with the conventional 6 + 2 design, at a reduced study size for compounds with a presumed acceptable therapeutic window. The adaptive escalation design was shown to make "best use" of participants in FIH studies without compromising safety.

---

### State estimation of a physical system with unknown governing equations [^115CoBJD]. Nature (2023). Excellent credibility.

Discussion

We have presented a method for state estimation that enables the treatment of real-world problems in which there are notable modelling errors or the underlying governing equations are completely or partially unknown. This is an important class of problems that has thus far defied a computationally tractable, statistically rigorous solution. We have also provided results for governing-equation discovery for a variety of challenging problems. The results indicate that our approach can outperform state-of-the-art algorithms for governing-equation discovery often by more than an order of magnitude; see Methods for details. It is worth reiterating that many governing-equation learning algorithms only provide point parameter estimates. As demonstrated in Fig. 2f–h, our method enables Bayesian governing-equation identification, thereby allowing for probabilistic forecasts to be made.

Moreover, in cases in which specifying a dictionary of basis functions is not feasible (such as when the state space is high dimensional or it is not clear what basis functions are appropriate), we have shown that it is possible to infer the missing dynamics in the form of a neural network. It is worth noting here that our approach enables neural SDEs to be trained without using a computationally expensive forward solver and adjoint sensitivity calculations.

These results are made possible by a reparametrization trick for Markov Gaussian processes that we introduce here. We believe that there is substantial room for future work in applying this technique for performing variational inference over stochastic processes outside the state-estimation domain. Potential application areas include multiphysics ROM problems, state estimation with unknown observation models, stochastic closure modellingand inferring latent forcing models. Also, there is room for using more expressive approximations to the posterior over the parameters to better approximate uncertainty. Finally, there has been a recent resurgence of interest in using SDEs for building complex generative modelsand we expect that our approach could be used to gain deeper insights into the statistical underpinnings of such models.

---

### Fractional response analysis reveals logarithmic cytokine responses in cellular populations [^111LPMUV]. Nature Communications (2021). High credibility.

Calculation of typical fractions

The fractions of cells stimulated with dose i that have responses typical to dose j, v ij, can be easily calculated from data regardless of the number of doses and the type of experimental measurements. We have thatCalculation of typical fractions, v ij, with the above formula requires the possibility to examine the condition P (y | x j) > P (y | x k) for any experimentally observed response, y. The distributions P (y | x j) can be reconstructed from data using a variety of probability density estimators. The use of the available estimators, however, might be problematic for multivariate responses. We, therefore, propose a more convenient strategy. We replace the condition P (y | x j) > P (y | x k) with an equivalent condition that is computationally much simpler to evaluate. Precisely, we propose to use the Bayes formulaIf we set the equiprobable prior distribution, i.e. P (x j) = 1 /m, we have that P (y | x j) is proportional to P (x j | y) and the condition P (y | x j) > P (y | x k) is equivalent toThe above strategy allows avoiding estimation of the response distributions, P (y | x j), from data. For continuous and multivariate variable y the estimation of P (x j | y) is generally simpler than estimation of P (y | x j). Precisely, an estimatorof the distribution P (x j | y) can be built using a variety of Bayesian statistical learning methods. For simplicity and efficiency, here we propose to use logistic regression, which is known to work well in a range of applications. In principle, however, other classifiers could also be considered. The logistic regression estimators of P (x j | Y = y) arise from a simplifying assumption that log-ratio of probabilities, P (x j | Y = y) and P (x m | Y = y) is linear. Precisely, The above formulation allows fitting the logistic regression equations to experimental data, i.e. finding values of the parameters, α j and β j that best represent the data. The fitted logistic regression model allows assigning cellular responses to typical doses based on conditions given by Eq. 8. Formally, the fractions v ij defined by Eq. 6 are calculated aswhere n i is the number of cells measured for the dose x i, y l i denotes response of the l- th cell, andis equal 1 iffor anyand 0 otherwise.

---

### Pointwise error estimates in localization microscopy [^111MdUgE]. Nature Communications (2017). Medium credibility.

Covariance-based diffusion estimator

If x k (k = 0, 1,…) is the measured trajectory of a freely diffusing particle with diffusion constant D, the widely used model for camera-based tracking by Berglundpredicts that the measured step lengths Δ x k = x k +1 − x k are zero-mean Gaussian variables with covariances given by

and uncorrelated otherwise. Here, 0 ≤ R ≤ 1/4 is a blur coefficient that depends on how the images are acquired (for example, R = 1/6 for continuous illumination), Δ t is the measurement time-step, and ɛ 2 is the variance of the localization errors.

Substituting sample averages forandand solving for D yields a covariance-based estimator with good performance. If ɛ 2 is known or can be estimated independently, the first relation in equation (7) alone yields a further improved estimate of D. As we argue in Supplementary Note 9, these estimators apply also for variable localization errors if ɛ 2 is replaced by the average.

---

### Spin dephasing under nonlinear gradients: implications for imaging and field mapping [^1171KZF1]. Magnetic Resonance in Medicine (2012). Low credibility.

This work examines the prototypical MR echo that would be expected for a voxel of spins evolving in a strong nonlinear field, specifically focusing on the quadratic z(2) - ½(x(2) + y(2)) field. Dephasing under nonlinear gradients is increasingly relevant given the growing interest in nonlinear imaging, and here, we report several notable differences from the linear case. Most notably, in addition to signal loss, intravoxel dephasing under gradients creating a wide and asymmetric frequency distribution across the voxel can cause skewed and nonlinear phase evolution. After presenting the qualitative and analytical origins of this difference, we experimentally demonstrate that neglecting these dynamics can lead to significant errors in sequences that assume phase evolution is proportional to voxel frequency, such as those used for field mapping. Finally, simplifying approximations to the signal equations are presented, which not only provide more intuitive forms of the exact expression but also result in simple rules to predict key features of the nonlinear evolution.

---

### Principled approach to the selection of the embedding dimension of networks [^111Mzr7M]. Nature Communications (2021). High credibility.

There could be several ways of finding the solution of Eq. (1). Here, given the smoothness of the empirically observed loss functions, we opted for a parametric approach. Specifically, we found thatwhere s and α are fitting parameters, represents well the data in various combinations of embedding methods and embedded networks. Equation (2) is meant to describe the mathematical behavior of the loss function only in the range d ∈ [1, d r). In particular in the definition of Eq. (2), L ∞ appears as the value of the normalized loss function in the limit of infinitely large embedding dimensions. However, such a limit has no physical meaning and is used only to facilitate the mathematical description of the empirical loss function in the regime d ≪ d r. Best fits, obtained by minimizing the mean-squared error, of the function of Eq. (2) are shown in Figs. 1 a, b and 2 a. We performed a systematic analysis on a corpus of 135 real-world networks. We found that best estimatesof the asymptotic value L ∞ for the normalized embedding loss function are generally close to zero (Supplementary Table 1). Best estimatesof the factor s, regulating the rate of the power-law decay toward L ∞, seem very similar to each other, irrespective of the network that is actually embedded. Measured values ofindicate only a mild dependence on the underlying network (see Fig. 3). For uncorrelated clouds of points in d dimension, the central limit theorem allows us to predict that L (d) ~ 1/ d 1/2. Our best estimatesof the decay exponent α are generally greater than those expected for uncorrelated clouds of data points, indicating that the embedding algorithm correctly retains network structural information even in high-dimensional space. In systematic analyses performed on random networks constructed using either the ER and the BA models, we find that the size of the network is not an important factor in determining the plateau value L ∞. The decay exponent α and the rate s are positively correlated with density of the embedded network, but their values become constant in the limit of large network sizes (see Fig. 4).

---

### Complexity-aware simple modeling [^111VJYmW]. Current Opinion in Microbiology (2018). Low credibility.

Mathematical models continue to be essential for deepening our understanding of biology. On one extreme, simple or small-scale models help delineate general biological principles. However, the parsimony of detail in these models as well as their assumption of modularity and insulation make them inaccurate for describing quantitative features. On the other extreme, large-scale and detailed models can quantitatively recapitulate a phenotype of interest, but have to rely on many unknown parameters, making them often difficult to parse mechanistically and to use for extracting general principles. We discuss some examples of a new approach-complexity-aware simple modeling-that can bridge the gap between the small-scale and large-scale approaches.

---

### KDIGO 2022 clinical practice guideline for diabetes management in chronic kidney disease [^115uBKuu]. Kidney International (2022). High credibility.

Figure 5 — assessment of kidney function in glomerular disease — uses ml/min per 1.73 m² units and notes that the correction coefficient for race in GFR estimating equations is controversial and discussions are ongoing, referring readers to the KDIGO CKD guideline for more information.

---

### A benchmark, expand, and calibration (benchExCal) trial emulation approach for using real-world evidence to support indication expansions: design and process for a planned empirical evaluation [^111j22No]. Clinical Pharmacology and Therapeutics (2025). Medium credibility.

That said, to our knowledge, this project is novel in terms of the plan to systematically demonstrate and evaluate the BenchExCal approach to trial emulation through both simulation and empirical examples. The project will provide insights regarding how a first‐stage emulation of a completed trial for an initial indication can be used to increase confidence and inform the interpretation of a second‐stage emulation of a hypothetical trial designed to provide evidence for a supplemental indication. Although examples have been selected to provide a variety of learnings, five use cases will not be able to address all clinical and data scenarios that may be encountered when seeking a supplemental indication for a marketed drug. Similar follow‐on work will add to this library of learnings.

---

### The model muddle: in search of tumor growth laws [^114AxEcF]. Cancer Research (2013). Low credibility.

In this article, we will trace the historical development of tumor growth laws, which in a quantitative fashion describe the increase in tumor mass/volume over time. These models are usually formulated in terms of differential equations that relate the growth rate of the tumor to its current state and range from the simple one-parameter exponential growth model to more advanced models that contain a large number of parameters. Understanding the assumptions and consequences of such models is important, as they often underpin more complex models of tumor growth. The conclusion of this brief survey is that although much improvement has occurred over the last century, more effort and new models are required if we are to understand the intricacies of tumor growth.

---

### KDIGO 2024 clinical practice guideline for the evaluation and management of chronic kidney disease [^111eoLve]. Kidney International (2024). High credibility.

Values and preferences, resource use, and implementation — Using validated eGFR equations improves the accuracy of assessment of true GFR but remains imperfect, and no single equation performs consistently across all populations; the Work Group judged that people with CKD and their healthcare providers would want GFR estimated using the equation providing the greatest accuracy in the population of their geographical region, and there are several valid equations that can reasonably be used in local settings. Each region should have a mechanism for review and selection of equations for implementation by laboratories, and decisions at this level by continental or national organizations are likely to minimize the likelihood that decisions for equation use will be made within small geographical areas governed by local decisions, leading to greater variation in eGFR and uncertainty by people with CKD and healthcare providers. There are likely to be tradeoffs between optimal accuracy in local regions versus uniformity; equations optimized for a specific region can help to ensure that the GFR thresholds for disease definition, classification, and risk estimation have the same implications across regions; however, it would lead to barriers to implementation, as it will not be possible for all regions to conduct a sufficiently large and representative study to evaluate these equations and develop modifications. If not possible, or in the interim, we advise using equations that were developed in populations most similar to the available populations. Resource use includes initial costs such as human resource costs and education for providers, which incurs both direct and indirect costs.

---

### KDIGO 2024 clinical practice guideline for the evaluation and management of chronic kidney disease [^1132MXdu]. Kidney International (2024). High credibility.

KDIGO 2024 CKD — Recommendation 2.2.1 for CKD G3–G5 risk prediction states: In people with CKD G3–G5, we recommend using an externally validated risk equation to estimate the absolute risk of kidney failure (1A). The rationale places "a high value on the need and potential benefits for individual risk prediction to deliver personalized care for people with CKD" and encourages going "beyond broad categories of RR" to estimate absolute risk. Key information notes "a large body of evidence" supporting validated equations to estimate the risk of kidney failure requiring dialysis or transplant in CKD G3–G5, and that such equations have been "developed, externally validated, and implemented in labs, EMRs, and health systems".

---

### Optimal free descriptions of many-body theories [^117KvpbQ]. Nature Communications (2017). Medium credibility.

Methods

Optimization

The optimization to find σ andin (3) is performed by a Monte Carlo basin-hopping strategyusing the Nelder–Mead simplex algorithm for local minimization within basins of the cost function. This global strategy was selected to counteract an observed tendency for local methods to get trapped in local minima. The initial guess for this search is found as follows. The normalization constant E 0 is the lowest entanglement energy of the input entanglement spectrum { E }. We iteratively construct an approximate set of single-particle entanglement energies starting from an empty set. First, we take the lowest remaining level in the spectrum and subtract E 0 to produce a new single-particle level k. Then we remove the many-body levels, which are closest to the new combinatorial levels generated according to (2) by the additional single-particle level. This process is repeated until the input spectrum is exhausted. We can also introduce a truncation of the entanglement spectrum cutting off high entanglement energies, making the construction of the initial guess terminate faster. The minimization of D (σ, σ f) to identify the optimal free model for the Ising Hamiltonian (5) is calculated using a local Nelder–Mead method.

Finite-size scaling

We perform the finite-size scaling according to an ansatz (4). The parameters of the collapse were estimated using the method of ref. From the scaling ansatz (4) and for a trial set of scaling parameters g c, ν and ζ, the scaled values x L = (g − g c) L 1/ ν andare calculated from each unscaled data point. From this collection of scaled data points (x L, y L) across all L, we implicitly define a so-called master curve that best represents them. This curve y (x) is defined around a point x as the linear regression calculated by taking the scaled data points immediately left and right of x for each system size L. We characterize the deviation of the scaled points (x L, y L) from the master curve y (x L) using the χ 2 statistic. This measure is used as the cost function for an optimization problem over the scaling parameters g c, ν, ζ and θ, which can be solved using the same techniques as the previous problems.

---

### Estimands and complex innovative designs [^111UZwNj]. Clinical Pharmacology and Therapeutics (2022). Medium credibility.

Since the release of the ICH E9(R1) (International Council for Harmonisation of Technical Requirements for Pharmaceuticals for Human Use Addendum on Estimands and Sensitivity Analysis in Clinical Trials to the Guideline on Statistical Principles for Clinical Trials) document in 2019, the estimand framework has become a fundamental part of clinical trial protocols. In parallel, complex innovative designs have gained increased popularity in drug development, in particular in early development phases or in difficult experimental situations. While the estimand framework is relevant to any study in which a treatment effect is estimated, experience is lacking as regards its application to these designs. In a basket trial for example, should a different estimand be specified for each subpopulation of interest, defined, for example, by cancer site? Or can a single estimand focusing on the general population (defined, for example, by the positivity to a certain biomarker) be used? In the case of platform trials, should a different estimand be proposed for each drug investigated? In this work we discuss possible ways of implementing the estimand framework for different types of complex innovative designs. We consider trials that allow adding or selecting experimental treatment arms, modifying the control arm or the standard of care, and selecting or pooling populations. We also address the potentially data-driven, adaptive selection of estimands in an ongoing trial and disentangle certain statistical issues that pertain to estimation rather than to estimands, such as the borrowing of nonconcurrent information. We hope this discussion will facilitate the implementation of the estimand framework and its description in the study protocol when the objectives of the trial require complex innovative designs.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^116DSPos]. CDC (2011). Medium credibility.

Appendix G. Mathematical formula for sample size — sample size required to detect the difference of GM between two subpopulation groups defines a z test with null hypothesis GM1 = GM2 or GMmax/GMmin = 1 and alternative GM1 ≠ GM2 or GMmax / GMmin > 1, where GMmax = max(GM1, GM2) and GMmin = min(GM1, GM2). It considers power W (say 80% chance) to detect that one group's GM is at least k fold higher and gives the p-value as p = Prob(Z ≥ z) = 1 − φ−1(z) with Z ~ Normal (0,1). Given α (e.g., 0.05) and target ratio k, power is W = Prob (Z > φ−1(1 − α) − log10(k)/(S√(1/n1 + 1/n2))), where S is the expected standard deviation of log10(VL) in the population of interest; when n1 = n2 = n, W = Prob (Z > φ−1(1 − α) − log10(k)/(S√(2/n))). The required sample size is n = 2[(φ−1(1 − α) − φ−1(1 − W))S/log10(k)]^2, and tables are provided for W = 0.8 and W = 0.9 with α = 0.05.

---

### A general model for ontogenetic growth [^111RK5iT]. Nature (2001). Excellent credibility.

Several equations have been proposed to describe ontogenetic growth trajectories for organisms justified primarily on the goodness of fit rather than on any biological mechanism. Here, we derive a general quantitative model based on fundamental principles for the allocation of metabolic energy between maintenance of existing tissue and the production of new biomass. We thus predict the parameters governing growth curves from basic cellular properties and derive a single parameterless universal curve that describes the growth of many diverse species. The model provides the basis for deriving allometric relationships for growth rates and the timing of life history events.

---

### A comparison of phase II study strategies [^1172Tgi9]. Clinical Cancer Research (2009). Low credibility.

The traditional oncology drug development paradigm of single arm phase II studies followed by a randomized phase III study has limitations for modern oncology drug development. Interpretation of single arm phase II study results is difficult when a new drug is used in combination with other agents or when progression-free survival is used as the endpoint rather than tumor shrinkage. Randomized phase II studies are more informative for these objectives but increase both the number of patients and time required to determine the value of a new experimental agent. In this article, we compare different phase II study strategies to determine the most efficient drug development path in terms of number of patients and length of time to conclusion of drug efficacy on overall survival.

---

### Defining urge as an uncontrolled micturition explains pathogenesis, informs cure and helps solve the burgeoning OAB crisis [^114HyqwZ]. Neurourology and Urodynamics (2022). Medium credibility.

3.8 Challenging validity of the binary model by a Chaos feedback equation

As the binary feedback model, Figure 1, is the very basis for challenging existing OAB concepts, it, too, was challenged for validity by the nonlinear Chaos Theory feedback equation X next = Xc(1 − X), Figure 6, where "c" inversely represents reflex cortical plus peripheral suppression of afferent urothelial impulses "X" from urothelial stretch receptors "N". X = fraction of possible nerve impulses emanating from "N" in the micturition circuit of the model. Numeral "1" = maximum possible number of impulses in the circuit; "c" represents the closure reflex's inhibition of micturition, central and peripheral, in the Chaos equation.

How "c" and "X" were calculated.

X was given a mean value of 0.5, being a sufficient number for input to the micturition centre from the urothelial stretch receptors "N", Figure 1. "c" is inversely proportional to the quantum of inhibition. The clinical conditions were fitted as estimates into known "c" values from the Chaos Equation: Fowler's Syndrome (retention) with a very low "c" 0.1, normal (normally suppressed) 0.2, low compliance (activated but controlled) 2.0, urge incontinence (uncontrolled unstable) 3.6, example below.

Note: The calculation figures are given only as an example. At 3 decimal places, they are inherently inaccurate, as Lorenz's original discovery was based on inclusion of the 5th decimal number. We have only calculated on three.

---

### A note on genetic variance components in mixed models [^113WUN4m]. Genetic Epidemiology (2003). Low credibility.

Burton et al. ([1999] Genet. Epidemiol. 17:118–140) proposed a series of generalized linear mixed models for pedigree data that account for residual correlation between related individuals. These models may be fitted using Markov chain Monte Carlo methods, but the posterior mean for small variance components can exhibit marked positive bias. Burton et al. ([1999] Genet. Epidemiol. 17:118–140) suggested that this problem could be overcome by allowing the variance components to take negative values. We examine this idea in depth, and show that it can be interpreted as a computational device for locating the posterior mode without necessarily implying that the original random effects structure is incorrect. We illustrate the application of this technique to mixed models for familial data.

---

### Real-time estimation of immunological responses against emerging SARS-CoV-2 variants in the UK: a mathematical modelling study [^111svMSE]. The Lancet: Infectious Diseases (2025). High credibility.

Background

The emergence of SARS-CoV-2 variants and COVID-19 vaccination have resulted in complex exposure histories. Rapid assessment of the effects of these exposures on neutralising antibodies against SARS-CoV-2 infection is crucial for informing vaccine strategy and epidemic management. We aimed to investigate heterogeneity in individual-level and population-level antibody kinetics to emerging variants by previous SARS-CoV-2 exposure history, to examine implications for real-time estimation, and to examine the effects of vaccine-campaign timing.

Methods

Our Bayesian hierarchical model of antibody kinetics estimated neutralising-antibody trajectories against a panel of SARS-CoV-2 variants quantified with a live virus microneutralisation assay and informed by individual-level COVID-19 vaccination and SARS-CoV-2 infection histories. Antibody titre trajectories were modelled with a piecewise linear function that depended on the key biological quantities of an initial titre value, time the peak titre is reached, set-point time, and corresponding rates of increase and decrease for gradients between two timing parameters. All process parameters were estimated at both the individual level and the population level. We analysed data from participants in the University College London Hospitals-Francis Crick Institute Legacy study cohort (NCT04750356) who underwent surveillance for SARS-CoV-2 either through asymptomatic mandatory occupational health screening once per week between April 1, 2020, and May 31, 2022, or symptom-based testing between April 1, 2020, and Feb 1, 2023. People included in the Legacy study were either Crick employees or health-care workers at three London hospitals, older than 18 years, and gave written informed consent. Legacy excluded people who were unable or unwilling to give informed consent and those not employed by a qualifying institution. We segmented data to include vaccination events occurring up to 150 days before the emergence of three variants of concern: delta, BA.2, and XBB 1.5. We split the data for each wave into two categories: real-time and retrospective. The real-time dataset contained neutralising-antibody titres collected up to the date of emergence in each wave; the retrospective dataset contained all samples until the next SARS-CoV-2 exposure of each individual, whether vaccination or infection.

Findings

We included data from 335 participants in the delta wave analysis, 223 (67%) of whom were female and 112 (33%) of whom were male (median age 40 years, IQR 22–58); data from 385 participants in the BA.2 wave analysis, 271 (70%) of whom were female and 114 (30%) of whom were male (41 years, 22–60); and data from 248 participants in the XBB 1.5 wave analysis, 191 (77%) of whom were female, 56 (23%) of whom were male, and one (< 1%) of whom preferred not to say (40 years, 21–59). Overall, we included 968 exposures (vaccinations) across 1895 serum samples in the model. For the delta wave, we estimated peak titre values as 490.0 IC 50 (95% credible interval 224.3–1515.9) for people with no previous infection and as 702.4 IC 50 (300.8–2322.7) for people with a previous infection before omicron; the delta wave did not include people with a previous omicron infection. For the BA.2 wave, we estimated peak titre values as 858.1 IC 50 (689.8–1363.2) for people with no previous infection, 1020.7 IC 50 (725.9–1722.6) for people with a previous infection before omicron, and 1422.0 IC 50 (679.2–3027.3) for people with a previous omicron infection. For the XBB 1.5 wave, we estimated peak titre values as 703.2 IC 50 (415.0–3197.8) for people with no previous infection, 1215.9 IC 50 (511.6–7338.7) for people with a previous infection before omicron, and 1556.3 IC 50 (757.2–7907.9) for people with a previous omicron infection.

Interpretation

Our study shows the feasibility of real-time estimation of antibody kinetics before SARS-CoV-2 variant emergence. This estimation is valuable for understanding how specific combinations of SARS-CoV-2 exposures influence antibody kinetics and for examining how COVID-19 vaccination-campaign timing could affect population-level immunity to emerging variants.

Funding

Wellcome Trust, National Institute for Health Research University College London Hospitals Biomedical Research Centre, UK Research and Innovation, UK Medical Research Council, Francis Crick Institute, and Genotype-to-Phenotype National Virology Consortium.

---

### Reconstruction of human dispersal during aurignacian on pan-European scale [^1163kJxm]. Nature Communications (2024). High credibility.

Human mobility model

We define human dispersal as the propagation of population density. For the Palaeolithic, the anthropogenic impact on the environment is neglected. The population density (commonly in persons per 100 km −2, i.e. P 100 km −2) satisfieswhere t is time and p is net production rate (i.e. birth rate b minus death rate d). Equation (2) predicts an exponential population growth if p is linear to ρ. As both ρ = ρ (x, y, t) and p = p (x, y, t) are functions of space (x, y) and time, we rewrite Eq. (2) aswhere v is drift velocity and D is diffusion coefficient. In earlier studies, human dispersal is modelled as a diffusion/reproduction process, with no population drift. Here a drift term is included to better represent the macroscopic processes which systematically drive human dispersal. Equation (3) is capable of representing the various modes of human mobility known in demographic descriptions, e.g. the leapfrog mode, can be represented in Eq. (3) by the drift term. Also, the human dispersal routes may just be the consequence of strong diffusion in a preferred direction, e.g. in the direction of gradient Φ A v. Numerically, Eq. (3) is solved in two stepson a two-dimensional (longitude x and latitude y) grid, subject to initial and boundary conditions.

---

### KDIGO 2024 clinical practice guideline for the evaluation and management of chronic kidney disease [^115bx5dJ]. Kidney International (2024). High credibility.

Selection of GFR estimating equations — use of validated eGFR: We recommend using a validated GFR estimating equation to derive GFR from serum filtration markers (eGFR) rather than relying on the serum filtration markers alone (1D). Practice Point 1.2.4.1 advises to use the same equation within geographical regions (as defined locally [e.g., continent, country, and region] and as large as possible), noting that within such regions, equations may differ for adults and children. The recommendation places a high value on using an estimating equation validated in the population of interest, and the key points are to use an equation validated and most suited to the population of interest; there is potential for harm if people get different eGFR values when receiving care in different settings, and there is benefit to clinical care, research, and public health with the use of validated equations such that decisions, research findings, and public policy are informed by accurate estimates of CKD.

---

### SAGES guidelines for the introduction of new technology and techniques [^116yteh6]. Surgical Endoscopy (2014). Medium credibility.

Guideline 7 — The effectiveness compared to alternatives, the cost, patient outcomes, and the safety profile of new technology and techniques should always be assessed prior to and after their introduction. Other parameters such as existing and required resources; benefits to patients, surgeons, and hospitals; existing or anticipated volume of use; barriers to adoption; and whether the anticipated benefits prove real after introduction should also be considered, especially for significant changes in devices and procedures, besides minor modifications. (+++, strong).

---

### Is a one-size-fits-all'12-month rule' appropriate when it comes to the last search date in systematic reviews? [^115K1D5b]. BMJ Evidence-Based Medicine (2023). High credibility.

First, the increased breadth and depth of these reviews inevitably increases the work required. As figure 1 illustrates, broad effectiveness reviews cover more ground than simple effectiveness reviews. They may also require comparative work to assess the relative effectiveness of different intervention types. Thus, while the Cochrane Handbook recommends that searches are 'as up to date as possible', specifying within 12 months but ideally within 6 months of publication, it also acknowledges that reviews with a broad scope will 'require more resources'.

Complex effectiveness reviews often require significant conceptual development and interpretation, which can be time-consuming. The purpose of these reviews is to understand the critical components and mechanisms of multicomponent or multilevel interventions. Since methods to examine intervention complexity are relatively recent, this work also often requires methodological innovation, which further increases the time required.

Multicomponent approaches are often designed to understand intervention mechanisms and contextual complexity in addition to intervention effectiveness. As such, these approaches, such as mixed-method research synthesis or realist synthesis, often involve multiple synthesis stages.

Second, these types of review may also increase the extent of work required for effective patient and public involvement (PPI), which is vital to ensure that SRs produce useful findings.PPI can advance fields of study and help SRs address real-world questions from patient-based perspectives. However, the extent of PPI undertaken in any review will necessarily vary. As illustrated in figure 2, PPI is typically undertaken at the beginning of a review to identify relevant questions to answer. Further input during the synthesis stage and beyond can help to interpret emerging findings and disseminate findings to interested audiences. Input at the synthesis stage can assist with interpretation of the findings and is therefore particularly useful when broad questions are asked, or when conceptual development is a feature of the review. For example, in broad reviews examining multiple interventions, PPI can help interpret findings on effectiveness by providing insights about the likely acceptability of the various options reviewed. As such, the breadth and complexity of reviews and the extent of PPI required may be positively correlated.

---

### Observation and theory of X-ray mirages [^113keru3]. Nature Communications (2013). Medium credibility.

Numerical simulations

As already noted, the elaborated theory of the mirage and the mirage equation itself are written for an arbitrary-extended optically inhomogeneous amplifying medium, in particular, for any spatial distribution of n e (x, y, z) and G (x, y, z) of the plasma. For the solution of the whole problem, that is, for calculation of mirage radiation at the output of the plasma and for finding the position and shape of the virtual source in a particular physical situation, it is enough to numerically integrate equations (2 and 3) for given n e (x, y) and G (x, y). To do this, we rewrite the original equation (2) as:

whereand the complex amplitude of an incident field A inc (x, y, z) is considered to be given and the complex amplitude of X-ray mirage A M (x, y, z) = 0 at z = 0. In our opinion, the most effective numerical method for solving such an equation is the split-step method, which we applied for our calculations. For numerical solution of this parabolic equation, we used a rectangular grid of N x × N y elements with the spatial step sizes Δ x and Δ y in the x and y directions correspondingly. Decomposition of the total field into two fields, A M (x, y, z) and A inc (x, y, z), has allowed us to have a limited area where A M (x, y, z) is non-zero and also allowed to use the periodic boundary conditions of the split-step method.

In numerical calculations, we used a diffraction limited Gaussian beam A G (x, y, z) = as an incident radiation A inc (x, y, z). As equation (3) is a special case of equation (2), we used the same numerical procedure for its solution.

The calculation scheme is graphically shown in Supplementary Fig. S3. For the main calculations, we used the computational volume of 65 × 68 × 6,000 μm³ size with 640 × 670 elements in x − y computational grid. The number of steps along z axis was 670. For the calculations according to the simplified model, described in Supplementary Discussion, the numbers were 200 × 200 × 6,000, 1,024 × 1,024 and 1,024 correspondingly.

---

### Mixed models for repetitive measurements: the basics [^116RLqa2]. Fertility and Sterility (2024). Medium credibility.

Linear mixed models are regularly used within the field of reproductive medicine. This manuscript explains the basics of mixed models, when they could be used, and how they could be applied.

---

### General conditions for predictivity in learning theory [^116mv87F]. Nature (2004). Excellent credibility.

Developing theoretical foundations for learning is a key step towards understanding intelligence. 'Learning from examples' is a paradigm in which systems (natural or artificial) learn a functional relationship from a training set of examples. Within this paradigm, a learning algorithm is a map from the space of training sets to the hypothesis space of possible functional solutions. A central question for the theory is to determine conditions under which a learning algorithm will generalize from its finite training set to novel examples. A milestone in learning theory was a characterization of conditions on the hypothesis space that ensure generalization for the natural class of empirical risk minimization (ERM) learning algorithms that are based on minimizing the error on the training set. Here we provide conditions for generalization in terms of a precise stability property of the learning process: when the training set is perturbed by deleting one example, the learned hypothesis does not change much. This stability property stipulates conditions on the learning map rather than on the hypothesis space, subsumes the classical theory for ERM algorithms, and is applicable to more general algorithms. The surprising connection between stability and predictivity has implications for the foundations of learning theory and for the design of novel algorithms, and provides insights into problems as diverse as language learning and inverse problems in physics and engineering.

---

### Roflumilast (Zoryve) [^114frE3u]. FDA (2024). Medium credibility.

The dosage of roflumilast TOP for treatment of seborrheic dermatitis in both children (in patients ≥ 9 years) is 1 thin layer TOP daily (0.3% foam)

---

### KDIGO 2024 clinical practice guideline for the evaluation and management of chronic kidney disease [^115hgg6r]. Kidney International (2024). High credibility.

Selection of GFR estimating equations — We recommend using a validated GFR estimating equation to derive GFR from serum filtration markers (eGFR) rather than relying on the serum filtration markers alone (1D). Use the same equation within geographical regions (as defined locally [e.g., continent, country, and region] and as large as possible); within such regions, equations may differ for adults and children. This recommendation recognizes that there are now a number of validated GFR estimating equations available and places a high value on the use of an estimating equation for GFR that has been validated in the population of interest and which has been shown to be most accurate in comparison with mGFR. There is potential for harm if people get different eGFR values when receiving care in different settings, and using the same equation within the same geographical region can eliminate the source of variation that is related to the specific parameters of the GFR estimating equation. There is benefit to clinical care, research, and public health with the use of validated equations such that decisions, research findings, and public policy are informed by accurate estimates of CKD. Certainty of evidence: This recommendation is based on Work Group consensus regarding good clinical practice to use a GFR estimating equation validated in the population of interest.

---

### As-3 [^1132YBCb]. FDA (2025). Medium credibility.

Statement of idenity

ADDITIVE SOLUTION FORMULA 3 (AS-3), 250 mL

---

### Nonlinear delay differential equations and their application to modeling biological network motifs [^112rYiHh]. Nature Communications (2021). High credibility.

Chaotic behaviors are prevented in monotonic regulation with linear or cyclic network topology

Due to cyclic network topology (no more than one loop) and monotonic regulation, none of the motifs so far can yield chaotic dynamics. Either non-monotonic feedback (as in the Mackey-Glass equation) or non-cyclic topologyis required for chaos. Double feedback (Fig. 8 a) is a minimal motif fulfilling both requirements, although the non-monotonicity is sufficient (Supplementary Note 8).

The governing equation for such a double feedback motif is thus given by setting the output and both inputs of the logic equation (Eq. (19)) to X (and letting K = 1 for simplicity):

Chaotic behavior is possible for non-monotonic feedback with multiple feedback and disparate delays

Simulation of Eq. (37) demonstrates chaos for some parameters of positive/negative mixed feedback, as evidenced by sustained oscillations with variable maxima (Fig. 8 b), occupying an apparently fractal region in phase space (Fig. 8 c), and a large number of peaks in frequency space (Fig. 8 d). To confirm that the dynamics are in fact chaotic, we calculated the dominant Lyapunov exponent using the Wolf method(see "Methods"), finding a positive (i.e. chaotic) value of 0.0040 ± 0.00055 bits (mean ± standard deviation). We also calculated the box dimension of a reconstructed phase space with coordinates (X (T), X (T − 10), X (T − 20)) used by the Wolf method, yielding fractional dimension ~1.81, with 95% confidence interval (1.76, 1.87). These results indicate chaos (see Supplementary Fig. 4).

---

### The drug titration paradox updated and reinterpreted: with perfect titration, dose and Effect will Be uncorrelated [^11461cHw]. Clinical Pharmacology and Therapeutics (2025). Medium credibility.

RESULTS

Mathematical proof

The mathematical proof rests on the assumption that there is "gradual titration of dosing"so that the majority of the effect (E i), if not all, is bounded as follows:

In other words, if a patient is sensitive to a medication, the first dose will be supratherapeutic, and the subsequent dose will be less supratherapeutic until a desired therapeutic end‐point is obtained. While this is generally true for gradual and progressive titration, with a low tolerance for overshoot, this is more a reflection of the collective dosing regimen by humans titrating the medication.

Within the study of logic and mathematics, it is understood that a single counterexample suffices to disprove a proposition. In this case, using computer‐assisted dosing, one can identify alternative dosing schemes that would negate the above equations. One example of such a dosing scheme can be viewed as a modified PRC, which can be described using the following algorithm:
From the initial dose: If the dose is too big, pause drug administration. If the dose is too small, deliver the medication at a "high" dosing rate.
Repeat Step 1 until the desired clinical effect E T is observed clinically.
Estimate the Dose necessary to maintain E T.
If the absolute error is increasing (| E i – E T | > | E i‐1 – E T |), correct the dose to achieve the opposite effect.
If the rate of change in dose correction will lead to crossing E T, adjust the dose to prevent that from occurring (e.g. prevent oscillations).

Following this algorithm will allow for rapid observation of E T after the initial dose, overshoot while remaining bounded, and finally, a gradual convergence to E T with time.

This strategy prevents the bounds described previously and in Eq. 8 from being valid. As a result, it becomes possible that the dose trajectory may persist in the following state:for n > 0 and thus transforms Eq. 7, into either of the below:

Computational simulation proof

Figure 1 demonstrates the evolution of the correlation coefficient for the different dosing schemes described previously. While the original proposed dosing scheme consistently has a negative correlation coefficient, once outside the proposed dosing scheme, such as increasing the gain of the original manuscript or using tailored PI or PRC controllers, can result in positive correlation coefficients, which violates the stated paradox. Additionally, in the controller scheme C, the correlation coefficients converge toward zero.

---

### Inferring time derivatives including cell growth rates using gaussian processes [^115djhfb]. Nature Communications (2016). Medium credibility.

Methods

Using a Gaussian process to fit time-series data

In the following, we will denote a Gaussian distribution with mean μ and covariance matrix Σ as(μ, Σ) and use the notation of Rasmussen and Williamsas much as possible.

Prior probability

For n data points y i at inputs x i (each x i is a time for a growth curve), we denote the underlying latent function as f (x). We define a covariance matrix k (x, x ′), which has an explicit dependence on hyperparameters θ, and obeys

where the expectations are taken over the distribution of latent functions (samples of f (x)).

We interpret equation (1) as giving the prior probability distribution of the latent functions f (X), where were we use X to denote the inputs x i, such that

where K (X, X) is the n × n matrix with components k (x i, x j). With f denoting [f (x 1). f (x n)], this prior probability can be written as

noting the dependence of k (x, x ′; θ) on the hyperparameters θ.

---

### Quantitative MRI by nonlinear inversion of the Bloch equations [^116cV4Lr]. Magnetic Resonance in Medicine (2023). Medium credibility.

Purpose

Development of a generic model-based reconstruction framework for multiparametric quantitative MRI that can be used with data from different pulse sequences.

Methods

Generic nonlinear model-based reconstruction for quantitative MRI estimates parametric maps directly from the acquired k-space by numerical optimization. This requires numerically accurate and efficient methods to solve the Bloch equations and their partial derivatives. In this work, we combine direct sensitivity analysis and pre-computed state-transition matrices into a generic framework for calibrationless model-based reconstruction that can be applied to different pulse sequences. As a proof-of-concept, the method is implemented and validated for quantitative T₁ and T₂ mapping with single-shot inversion-recovery (IR) FLASH and IR bSSFP sequences in simulations, phantoms, and the human brain.

Results

The direct sensitivity analysis enables a highly accurate and numerically stable calculation of the derivatives. The state-transition matrices efficiently exploit repeating patterns in pulse sequences, speeding up the calculation by a factor of 10 for the examples considered in this work, while preserving the accuracy of native ordinary differential equations solvers. The generic model-based method reproduces quantitative results of previous model-based reconstructions based on the known analytical solutions for radial IR FLASH. For IR bSFFP it produces accurate T₁ and T₂ maps for the National Insitute of Standards and Technology (NIST) phantom in numerical simulations and experiments. Feasibility is also shown for human brain, although results are affected by magnetization transfer effects.

Conclusion

By developing efficient tools for numerical optimizations using the Bloch equations as forward model, this work enables generic model-based reconstruction for quantitative MRI.

---

### Cutting edge or blunt instrument: how to decide if a stepped wedge design is right for you [^112pSeFT]. BMJ Quality & Safety (2021). High credibility.

When might I consider doing a stepped wedge trial?

Research designs are shaped as much by practical constraints as by abstract schemes, and it is always a good idea to start with the constraints and work towards a design, rather than start with a design and try to fit it to constraints. These constraints will be unique to each research context, and box 1 lists some areas to think about. Still, there are some common features of settings where a stepped wedge trial might be considered as a possible design, and we now review these.

Box 1
Practical constraints on the design of a longitudinal cluster randomised trial

Are there limits on the time available to complete the evaluation, on the number of clusters, or on the number of participants (or the rate at which you can recruit participants) at each cluster? These constraints put limits on the overall scale of the evaluation, or force trade-offs between different design characteristics.
How will participants and their data be sampled in your study: as a series of cross-sectional surveys, as a continuous stream of incident cases, as a cohort followed over time, or some other way? Does the timescale divide into cycles, seasons or milestones that influence how you will sample participants and data?
Is there a limit on how many clusters can implement the intervention at the same time in the evaluation? If this is constrained by research resources (eg, if there are only enough trained research staff to implement the intervention one cluster at a time) then implementation must be staggered in some way.
If implementation is to be staggered, is there a minimum 'step length'? If the same team delivers the intervention in different clusters at different steps, then bear in mind it may take some time to get the intervention fully operational at a site, and the team will also need time to relocate from one cluster to the next.

---

### Variance component models for X-linked QTLs [^112HwTfP]. Genetic Epidemiology (2006). Low credibility.

This paper discusses the theory and implementation of a model for mapping X-linked quantitative trait loci (QTL). As a result of X inactivation, a female's body is subdivided into a number of patches. In each patch one of her two X chromosomes is randomly switched off. This smooths the allelic contributions in a heterozygote and implies that females should show less trait variation than males for an X-linked trait. The latest version of the genetic analysis program Mendel incorporates a simple variance component version of this model. An application to head circumference in autistic children illustrates Mendel in action.

---

### KDIGO 2024 clinical practice guideline for the evaluation and management of chronic kidney disease [^115r7k4W]. Kidney International (2024). High credibility.

KDIGO 2024 chronic kidney disease (CKD) guideline — research recommendations scope — states that "Research recommendations are presented in a separate section at the end of this document and are intended to guide the next set of important research questions to inform and improve outcomes of people living with CKD". They are "not exhaustive but are intended to help focus the clinical and research communities on unanswered questions including improving diagnostic tools and evaluation of kidney function, development and testing of risk prediction equations… evaluation of different therapies to delay progression… improved medication management, and optimal models of care", and the guideline emphasizes inclusion by stating, "We specifically require the community to be inclusive of people across the lifecycle and inclusive of sex and gender, and etiology of CKD, as important variables in all studies".

---

### Investigating suspected cancer clusters and responding to community concerns: guidelines from CDC and the council of state and territorial epidemiologists [^114nvBwd]. MMWR: Recommendations and Reports (2013). Medium credibility.

Cancer cluster investigation — Step 2 decision to close or proceed to Step 3 is based on multiple factors, and to interpret the standardized incidence ratio (SIR) the health agency must assess whether there are enough cases and population for statistical stability and, in general, that the population size of a typical census tract is the smallest denominator that will allow reliable results; if the numerator allows stability, agencies should consider whether the 95% CI exclude 1.0, along with environmental contaminants, population changes, and other information beyond the SIR to estimate whether cancers represent an excess and share a common etiology. A SIR of limited magnitude that is not statistically significant with lack of known contaminant association and no increasing trend justifies closing at Step 2, whereas a statistically significant SIR of great magnitude with an increasing trend and a known contaminant argues for continuing to Step 3; example thresholds indicate that an SIR of < 2.0 with CIs surrounding or overlapping 1.0 and < 10 cases might justify closing, while an SIR of > 4 with CIs not overlapping 1.0 and ≥ 10 etiologically linked cases should encourage advancing to Step 3.

---

### Elacestrant (Orserdu) [^111yBi3v]. FDA (2024). Medium credibility.

The dosage of elacestrant PO for treatment of breast cancer in postmenopausal female adults with ESR1 mutation (advanced or metastatic, ER-positive, HER2-negative, progression after ≥ 1 line of endocrine therapy) is 345 mg PO daily until disease progression or unacceptable toxicity

---

### Phase 0 / microdosing approaches: time for mainstream application in drug development? [^114uansU]. Nature Reviews: Drug Discovery (2020). High credibility.

Phase 0 approaches - which include microdosing - evaluate subtherapeutic exposures of new drugs in first-in-human studies known as exploratory clinical trials. Recent progress extends phase 0 benefits beyond assessment of pharmacokinetics to include understanding of mechanism of action and pharmacodynamics. Phase 0 approaches have the potential to improve preclinical candidate selection and enable safer, cheaper, quicker and more informed developmental decisions. Here, we discuss phase 0 methods and applications, highlight their advantages over traditional strategies and address concerns related to extrapolation and developmental timelines. Although challenges remain, we propose that phase 0 approaches be at least considered for application in most drug development scenarios.

---

### Appropriate systemic therapy dosing for obese adult patients with cancer: ASCO guideline update [^112B1Mnb]. Journal of Clinical Oncology (2021). High credibility.

Regarding specific circumstances for obesity, more specifically with respect to patients with cancer, ASCO 2021 guidelines recommend to calculate body surface area using any of the standard formulae. Insufficient evidence to support one formula for calculating body surface area over another.

---

### KDIGO 2024 clinical practice guideline for the evaluation and management of chronic kidney disease [^114ToLK7]. Kidney International (2024). High credibility.

CKD mineral–bone parameters by albuminuria category — figure context: The y axis represents the meta-analyzed absolute difference from the mean adjusted value at an eGFR of 80 ml/ min per 1.73 m^2 and albumin excretion < 30 mg/g (< 3 mg/mmol), with albuminuria categories defined as A1, albuminuria < 30 mg/g (< 3 mg/mmol); A2, albuminuria 30–300 mg/g (3–30 mg/mmol); and A3, > 300 mg/g (> 30 mg/mmol).

---

### KDIGO 2024 clinical practice guideline for the evaluation and management of chronic kidney disease [^117GjMLU]. Kidney International (2024). High credibility.

KDIGO 2024 — Selection of GFR estimating equations — We recommend using a validated GFR estimating equation to derive GFR from serum filtration markers (eGFR) rather than relying on the serum filtration markers alone (1D). Use the same equation within geographical regions (as defined locally [e.g., continent, country, region] and as large as possible). Within such regions, equations may differ for adults and children. Use of race in the computation of eGFR should be avoided, and estimate GFR in children using validated equations that have been developed or validated in comparable populations.

---

### Insights from a dynamical system approach into the history of atmospheric oxygenation [^1126fzHc]. Nature Communications (2024). High credibility.

Here µ denotes the number of moles in 1 atmosphere. The oxidation parameter, Ψ, is parameterized as log Ψ = p 1 x 4 + p 2 x 3 + p 3 x 2 + p 4 x 1 + p 5, where x = log(µ p O 2) and coefficients p 1 = 0.00286, p 2 = −0.1608, p 3 = 3.216, p 4 = −26.53, p 5 = 76.04 are resulted from fitting a curve to the previously published data (Figure S1). The details of the goodness of the fit, along with the range of values and their confidence intervals are shown in Figure S1.

While our parameterization scheme for the rate of atmospheric methane oxidation is similar to previous studies, some assumptions in this parameterization can potentially cause uncertainty in the rate of methane oxidation. For instance, this parameterization makes assumptions about the age of the Sun as well as about the surface temperature, both of which effects can potentially influence the photochemistry of CH 4 and O 2. The assumption about the age of the sun can impact the Lyman alpha UV flux, an important factor for kinetics of CH 4 photolysis CH 4 and the Earth's surface temperature can impact the water vapor content of the troposphere with impacts on the abundance of OH and the rate of methane oxidation. To account for the uncertainty associated with these assumptions, we varied the kinetics of atmospheric methane oxidation in our stochastic simulation.

Finally, considering that the methane mass balance equation depends on p O 2, we first solved the methane mass balance equation and obtained the fixed points (steady states) of the methane mass balance equation, and then used those fixed points to solve the oxygen mass balance equation.

---

### NIS-teen questionnaire-Q4… [^112gTTPi]. ftp.cdc.gov (2025). Medium credibility.

2 SECTION S Screener Instruction1 IF ANY S3_3M/D/Y_x = 77 OR 99 GO TO INSRUCTION2 ELSE IF AND. TIS_S1TERM TIS_AGE_CONFIRM So, you have a. Is that correct. YES. 1 GO TO CP_TISMULTIAGE. TIS_MULTIAG ELSEIF ALL TIS_S3AGE_x = 77 and/or 99 AND SUM > 0, GO TO INSTRUCTION1 ELSE GO TO TIS_SELECTION_INSTRUCTIONS1 TIS_MULTIAGE. S3_INTRO in NIS ELSEIF ANY YAGE_x > 12 and < 18, THEN RANDOMLY SELECT ONE OF THE CHILDREN BETWEEN 13 AND 17 TO BE THE SELECTED CHILD FOR THE TEEN SURVEY AND GO TO TIS_S3INTRO ELSE GO TO INSTRUCTION1. 7 TIS3CONF That would make this child years old; is that correct. YES. 1 IF, THEN GO.

TO TIS_S4 IF AND AND OTHER YAGE = , THEN GO TO TIS_S3INELG. IF AND AND OTHER YAGE < >, THEN GO TO TIS_SELECTION_INSTRUCTION.

---

### Hierarchical joint analysis of marginal summary statistics-part I: multipopulation fine mapping and credible set construction [^111G4nKX]. Genetic Epidemiology (2024). Medium credibility.

In multipopulation JAM, we assumeand adopt a fixed-effect approach. Then Equations (1) and (2) together form a two-stage model when individual-level data are available. The first stage is three separate linear phenotypic models whereas the second stage fits a fixed-effect meta-analysis model that combines all populations together. By replacing thein Equation (1) with Equation (2), Equation (1) then becomeswhererepresents the same source of error asbut with length, andby the fixed-effect model assumption. Equation (3) can also be written in a form akin to a mega-regression that incorporates the individual-level data of all populations:wheref andis the random error in the combined linear phenotypic model. With summary data in which only the marginal effect sizes and their standard errors are available, it is also possible to estimate the joint effect size, with an additional reference sample that estimates the correlation structures between the SNPs. Thus, Equation (4) can be written with only GWAS summary statistics to form a modified mJAM likelihood after linear transformation:wheredenotesandin Equation (4) respectively. By expanding each matrix, we haveandwhereandare populationspecific statistics and can be estimated by populationspecific GWAS summary statistics and a reference genotype matrix. Population-specific missing variants in the summary statistics or in the reference genotype can also be incorporated while calculatingand. Detailed derivation can be found in Supporting Information Methods.

---

### Estimation in medical imaging without a gold standard [^114M1qr5]. Academic Radiology (2002). Low credibility.

Rationale and Objectives

In medical imaging, physicians often estimate a parameter of interest (eg, cardiac ejection fraction) for a patient to assist in establishing a diagnosis. Many different estimation methods may exist, but rarely can one be considered a gold standard. Therefore, evaluation and comparison of different estimation methods are difficult. The purpose of this study was to examine a method of evaluating different estimation methods without use of a gold standard.

Materials and Methods

This method is equivalent to fitting regression lines without the x axis. To use this method, multiple estimates of the clinical parameter of interest for each patient of a given population were needed. The authors assumed the statistical distribution for the true values of the clinical parameter of interest was a member of a given family of parameterized distributions. Furthermore, they assumed a statistical model relating the clinical parameter to the estimates of its value. Using these assumptions and observed data, they estimated the model parameters and the parameters characterizing the distribution of the clinical parameter.

Results

The authors applied the method to simulated cardiac ejection fraction data with varying numbers of patients, numbers of modalities, and levels of noise. They also tested the method on both linear and nonlinear models and characterized the performance of this method compared to that of conventional regression analysis by using x-axis information. Results indicate that the method follows trends similar to that of conventional regression analysis as patients and noise vary, although conventional regression analysis outperforms the method presented because it uses the gold standard which the authors assume is unavailable.

Conclusion

The method accurately estimates model parameters. These estimates can be used to rank the systems for a given estimation task.

---

### Roflumilast (Zoryve) [^116oLut2]. FDA (2024). Medium credibility.

The dosage of roflumilast TOP for treatment of plaque psoriasis in adults is 1 application(s) TOP daily (0.3% cream or foam)

---

### KDIGO 2024 clinical practice guideline for the evaluation and management of chronic kidney disease [^114DbKke]. Kidney International (2024). High credibility.

KDIGO 2024 chronic kidney disease (CKD) G3–G5 — use of validated risk equations: In people with CKD G3–G5, we recommend using an externally validated risk equation to estimate the absolute risk of kidney failure (1A). The recommendation places a high value on individual risk prediction to deliver personalized care and cites a large body of evidence supporting validated equations that have been developed, externally validated, and implemented in labs, EMRs, and health systems.

---

### KDIGO 2024 clinical practice guideline for the evaluation and management of chronic kidney disease [^115qVZns]. Kidney International (2024). High credibility.

Risk prediction implementation and reporting — "The reporting of risk in the laboratory reports and EMRs should be standardized with appropriate guidance on risk thresholds, when available", and "Local validation studies can be performed to determine optimal calibration of the specific risk prediction equations before implementation"; "Risk prediction equations that are externally validated, and locally calibrated, when possible, can lead to improvement in the delivery of CKD care", and "The Work Group judged that the evidence and clinical utility of risk prediction equations was sufficiently strong to recommend widespread adoption", supporting that "Implementation of risk equations that are externally validated and use routinely collected data should be prioritized for health equity and global health considerations".

---

### Two-site HOphoto-oxidation on haematite photoanodes [^112jsyK7]. Nature Communications (2018). Medium credibility.

Model equations and numerical computations

The rates of the reaction steps of the water and H 2 O 2 photo-oxidation reactions, see eqs. (1)–(8), depend on the fractional surface coverage of species x, θ x, which are dimensionless by definition. The units of the electron and hole surface densities, σ e and σ h, respectively, are given by their number surface density [σ e] = [σ h] = mol m −2. The units of the rate constants are given according to the corresponding reaction orders: [k 1] = [k 2] = [k 3] = [k 4] = m 5 mol −2 s −1, [k 5] = m 3 mol −1 s −1, and [k −1] = [k 6] = m 2 mol −1 s −1, and thus the units of other parameters read as mol m −2 s −1 for p 1, mol m −2 for p 2 and mol m −3 for p 3 and p 4; p 1 — the flux of photo-generated holes from the surface to the electrolyte; p 2 — the electron density at the surface; p 3 — the HOO − concentration in the electrolyte (which is directly proportional to the concentration of H 2 O 2); and p 4 — the OH − concentration in the electrolyte (related to the pH).

Introducing rescaled variables and parameters, with, with, we obtain a combined set of dimensionless kinetic equations for reactions (1)–(8). For ease of reading the hat signs were removed in what follows and in Table 1 :We note that our model addresses the working conditions of haematite photoanodes at high pH levels –.

Table 1
Summary of dimensionless control parameters and the rate constants used for the numerical computation of the results presented in Fig. 4

For analysis, we first solve numerically the model equations under the charge and site conservation constraints described by Eqs. (9) and (10), respectively, and examine all the possible steady-state solutions (fix points) to Eqs. (12) through (16). The equations have been numerically solved by continuation method using the publically available package AUTO. To determine the temporal stability of each steady-state solution, we linearize Eqs (12)–(16) about the fix points and solve the standard eigenvalue problem, while correlating the negative eigenvalues with stable solutions.

---

### State estimation of a physical system with unknown governing equations [^116m1n97]. Nature (2023). Excellent credibility.

We introduce a method for handling model-structure uncertainty in a manner that recovers the interpretability of handcrafted models. We do so by learning the motion model in the form of a set of symbolic differential equations simultaneously to a state estimate. This modelling choice enables state estimation in situations in which there are substantial modelling errors or the underlying dynamics are partially or completely unknown. This advancement is made possible by a new reparametrization trick for Markov Gaussian processes that we introduce in this work.

Numerical studies are presented for a range of test cases to illustrate the performance of the proposed approach. In practical situations in which modelling errors are present (even small errors), our method outperforms standard state-estimation techniques. Furthermore, our approach for state estimation with unknown governing equations allows analysts to discover missing terms in (or the entirety of) the governing equations using indirect observations. In Methods, we show that our approach outperforms state-of-the-art algorithms for governing-equation discovery, particularly in the low-data and high-noise regimes, and can be used in situations in which the entire state is not measurable. Finally, in cases in which interpretable forward models are not required, we show that our approach can be used to infer neural stochastic differential equations (SDEs) without relying on a forward solver in training.

---

### Standards of care in diabetes – 2025 [^113RQNuE]. Diabetes Care (2025). High credibility.

Regarding follow-up and surveillance for diabetes mellitus type 1, more specifically with respect to continuous glucose monitoring, ADA 2025 guidelines recommend to implement early continuous glucose monitoring in adult patients with T1DM to improve glycemic outcomes and QoL and minimize hypoglycemia.

---

### An overview of phase 2 clinical trial designs [^115teSNB]. International Journal of Radiation Oncology, Biology, Physics (2022). Medium credibility.

Clinical trials are studies to test new treatments in humans. Typically, these treatments are evaluated over several phases to assess their safety and efficacy. Phase 1 trials are designed to evaluate the safety and tolerability of a new treatment, typically with a small number of patients (eg, 20–80), generally spread across several dose levels. Phase 2 trials are designed to determine whether the new treatment has sufficiently promising efficacy to warrant further investigation in a large-scale randomized phase 3 trial, as well as to further assess safety. These studies usually involve a few hundred patients. This article provides an overview of some of the most commonly used phase 2 designs for clinical trials and emphasizes their critical elements and considerations. Key references to some of the most commonly used phase 2 designs are given to allow the reader to explore in more detail the critical aspects when planning a phase 2 trial. A comparison of 3 potential designs in the context of the NRG-HN002 trial is presented to complement the discussion about phase 2 trials.

---

### Clinical practice guidelines for hemodialysis adequacy, update 2006 [^111GGPqC]. American Journal of Kidney Diseases (2006). Medium credibility.

Estimation of kidney function (CPG 1.2) — the section titled "Use of GFR-Estimating Equations and Clearances Rather Than Serum Creatinine to Guide Dialysis Initiation" states that "Variability in creatinine generation across the population makes serum creatinine level alone an inaccurate test for patients with kidney failure likely to benefit from dialysis treatment", and that "For most patients in CKD stages 4 and 5, estimating equations based on values of serum creatinine and other variables approximate GFR with adequate accuracy". Validated equations listed are, for "Age ≥ 18 Years", the "Cockcroft-Gault Equation", "MDRD 4 Variable Equation", and "MDRD 6 Variable Equation", and for "Age < 18 Years", the "Schwartz Formula", with "MDRD: Modification of diet in renal disease".

---

### KDIGO 2024 clinical practice guideline for the evaluation and management of chronic kidney disease [^115xnLHe]. Kidney International (2024). High credibility.

Chapter 1. Evaluation of CKD — equation validation across ages and kidney function: Research priorities include evaluating the accuracy of CKD U25 2021 eGFRcr and EKFC equations in diverse cohorts outside North America and Europe, specifically in children younger than 5 years, in children and adolescents with obesity, and in those with eGFR < 30 ml/min per 1.73 m²; determining validity of different estimating equations for GFR in children at different time points (2–5 years, 5–10 years, 10–14 years, and > 14 years); and evaluating which estimating equations should be used for eGFR in young adults and what criteria should be used to transition to adult eGFR equations if not using EKFC equations.

---

### KDIGO 2024 clinical practice guideline for the evaluation and management of chronic kidney disease [^113aJFS7]. Kidney International (2024). High credibility.

KDIGO 2024 — application of new CKD progression models notes that "they should be used to predict disease progression in people with CKD G1–G2 and may supplement established risk equations among people with CKD G3", and that people identified as intermediate risk "(e.g., > 1% per year) with these tools may benefit from the earlier initiation of therapy and closer follow-up", while those identified as high risk "(e.g., > 5% per year) may have the largest benefit from multidrug therapy to slow progression".

---

### Standards of care in diabetes – 2025 [^11566QPS]. Diabetes Care (2025). High credibility.

Regarding follow-up and surveillance for diabetes mellitus type 1, more specifically with respect to continuous glucose monitoring, ADA 2025 guidelines recommend to assess skin reactions, either due to irritation or allergy, and address to aid in successful use of devices.

---

### Standards of care in diabetes – 2025 [^1177SxqY]. Diabetes Care (2025). High credibility.

Regarding specific circumstances for diabetic nephropathy, more specifically with respect to pediatric patients, ADA 2025 guidelines recommend to determine the eGFR at the time of diagnosis and annually thereafter.

---

### KDIGO 2024 clinical practice guideline for the evaluation and management of chronic kidney disease [^112MDRKR]. Kidney International (2024). High credibility.

Early-stage chronic kidney disease risk prediction applicability and progression example — Practice Point 2.2.4 notes that risk prediction equations developed for use in people with CKD G3–G5, may not be valid for use in those with CKD G1–G2. The Work Group recognizes that in earlier stages (G1–G3), large declines in eGFR can occur in 2- to 5-year time frames without reaching kidney failure. Figure 16 illustrates this with a CKD G1–G2 profile in which kidney failure risk is 0.07% over 2 years and 0.32% over 5 years versus CKD progression risk of 10.4% over 3 years, for a 50-year-old male with diabetes with eGFR 80 ml/min per 1.73 m² and urine ACR 1 g/g.