# Medical Question & Answer

**Sample ID**: 893c5ffc-bca1-60bb-a942-8bdf71cb4f88
**Dataset Index**: 361174

---

## Question

18. Qiaohu built a number tower using digital building blocks, arranged according to the pattern shown in the figure. The 1st layer has 1 block of digital building blocks, the 2nd layer has 3 blocks of digital building blocks,… with a total of 10 layers. Therefore, Qiaohu used   blocks of digital building blocks to build this number tower.

---

## Answer

> Let's see… What do we have here? The user is asking how many digital building blocks are used in a 10-layer tower where the first layer has 1 block, the second has 3 blocks, and the pattern continues. Let's break this down step-by-step. First, I need to think about identifying the sequence pattern from the given layers. Then, I should verify the general formula for the nth layer. Next, I will calculate the total blocks by summing the first 10 terms. Finally, I will double-check the arithmetic and consider whether an alternative approach like pairing or a known summation formula confirms the result.

> Let me first confirm the pattern from the description: layer 1 has 1 block, layer 2 has 3 blocks, and the ellipsis suggests the pattern continues. Wait, let me verify the next term; if the pattern is odd numbers increasing by 2, then layer 3 would have 5 blocks, layer 4 would have 7, and so on, which fits the description of consecutive odd numbers starting from 1 [^notfound].

> I will now examine the general formula for the nth layer. The sequence 1, 3, 5, 7,… is the sequence of odd numbers, which can be expressed as 2n − 1 for layer n. Hold on, I should verify this: for n = 1, 2(1) − 1 = 1; for n = 2, 2(2) − 1 = 3; for n = 3, 2(3) − 1 = 5, which matches the expected pattern, so the nth layer indeed has 2n − 1 blocks [^notfound].

> Next, I should review how to sum the first 10 terms of this sequence. The total number of blocks is the sum from n = 1 to 10 of (2n − 1). Let me think about this carefully: I can distribute the sum as 2 times the sum of the first 10 natural numbers minus the sum of 10 ones, which gives 2 × (10 × 11)/2 − 10 = 110 − 10 = 100. Alternatively, I recall that the sum of the first k odd numbers is k², so for k = 10 the sum should be 10² = 100, which provides a quick cross-check of the result [^notfound].

> Hold on, let's not jump to conclusions; I should double-check the arithmetic. Using the explicit sum: (2×1−1) + (2×2−1) +… + (2×10−1) = 1 + 3 + 5 + 7 + 9 + 11 + 13 + 15 + 17 + 19. Adding these stepwise: 1+3 = 4, 4+5 = 9, 9+7 = 16, 16+9 = 25, 25+11 = 36, 36+13 = 49, 49+15 = 64, 64+17 = 81, 81+19 = 100. Yes, the total is 100 blocks, confirming the earlier calculation [^notfound].

> Therefore, Qiaohu used 100 digital building blocks to build the 10-layer tower. I should confirm that this aligns with both the algebraic derivation and the direct sum, and it does: both methods converge on 100 blocks, which is the final answer [^notfound].

---

The pattern is that the nth layer has **2n − 1** blocks. For 10 layers, the total is the sum of the first 10 odd numbers, which equals **10² = 100** blocks.

---

## Identifying the pattern

The sequence of blocks per layer is: 1, 3, 5, 7, 9, 11, 13, 15, 17, 19. This is an **arithmetic sequence** where each term increases by 2, and the nth term is given by:

aₙ = 2n − 1

---

## Calculating the total number of blocks

The total number of blocks is the sum of the first 10 terms of this sequence:

S₁₀ = sum from n = 1 to 10 of (2n − 1)

Using the formula for the sum of the first n odd numbers, **Sₙ = n²**:

S₁₀ = 10² = 100

---

## Verification by direct addition

Adding the blocks layer-by-layer confirms the result:

1 + 3 + 5 + 7 + 9 + 11 + 13 + 15 + 17 + 19 = 100

---

## Conclusion

Qiaohu used **100 blocks** to build the 10-layer number tower.

---

## References

### Convolutional neural networks: an overview and application in radiology [^f4903bd6]. Insights Into Imaging (2018). Low credibility.

What is CNN: the big picture (Fig. 1)

CNN is a type of deep learning model for processing data that has a grid pattern, such as images, which is inspired by the organization of animal visual cortex and designed to automatically and adaptively learn spatial hierarchies of features, from low- to high-level patterns. CNN is a mathematical construct that is typically composed of three types of layers (or building blocks): convolution, pooling, and fully connected layers. The first two, convolution and pooling layers, perform feature extraction, whereas the third, a fully connected layer, maps the extracted features into final output, such as classification. A convolution layer plays a key role in CNN, which is composed of a stack of mathematical operations, such as convolution, a specialized type of linear operation. In digital images, pixel values are stored in a two-dimensional (2D) grid, i.e. an array of numbers (Fig. 2), and a small grid of parameters called kernel, an optimizable feature extractor, is applied at each image position, which makes CNNs highly efficient for image processing, since a feature may occur anywhere in the image. As one layer feeds its output into the next layer, extracted features can hierarchically and progressively become more complex. The process of optimizing parameters such as kernels is called training, which is performed so as to minimize the difference between outputs and ground truth labels through an optimization algorithm called backpropagation and gradient descent, among others.

Fig. 1
An overview of a convolutional neural network (CNN) architecture and the training process. A CNN is composed of a stacking of several building blocks: convolution layers, pooling layers (e.g. max pooling), and fully connected (FC) layers. A model's performance under particular kernels and weights is calculated with a loss function through forward propagation on a training dataset, and learnable parameters, i.e. kernels and weights, are updated according to the loss value through backpropagation with gradient descent optimization algorithm. ReLU, rectified linear unit

Fig. 2
A computer sees an image as an array of numbers. The matrix on the right contains numbers between 0 and 255, each of which corresponds to the pixel brightness in the left image. Both are overlaid in the middle image. The source image was downloaded via

---

### Quantum-limited stochastic optical neural networks operating at a few quanta per activation [^404222c9]. Nature Communications (2025). High credibility.

We performed two sets of simulation experiments: one on coherent SPDNNs trained to perform MNIST handwritten-digit classification, and one on coherent SPDNNs trained to perform CIFAR-10 image classification. Figure 4 d shows the architectures tested and simulation results for the MNIST benchmark (see "Methods", Supplementary Note 2 B). The accuracy achieved by MLPs with either one or two hidden layers was higher than that of the single-hidden-layer MLP simulated for the incoherent case (Fig. 3 b), and an architecture with a single convolutional layer followed by two linear layers achieved > 99% accuracy even in the single-shot (K = 1) regime.

Figure 4 e shows the results of simulating variants of a 6-layer convolutional SPDNN (comprising 4 convolutional layers and 2 fully connected, linear layers) on CIFAR-10 image classification. All these simulation results were obtained in the single-shot (K = 1) regime. The number of channels in each convolution layer was varied, which affects the total number of MACs used to perform an inference. We observed that the test accuracy increased with the size of the SPDNN, with accuracies approaching those of conventional convolutional neural networks of comparable size, as well as of binarized convolutional neural networks. In the models we simulated that only used SPD as the activation function (i.e. the ones in which there are no "Digital ReLU" blocks), the high-SNR linear output layer had only 4000 MAC operations, so the number of MACs in the high-SNR layer comprises less than 0.01% of the total MACs performed during an inference. The models we simulated are thus sufficiently large that the total optical energy cost would be dominated by the (low-SNR) layers prior to the (high-SNR) output layer. Equivalently, the optical energy cost per MAC would be predominantly determined by the cost of the low-SNR layers. These simulation results illustrate the ability of SPDNNs to scale to larger and deeper models, enabling them to perform more challenging tasks.

---

### Natural statistics support a rational account of confidence biases [^7c99e86a]. Nature Communications (2023). High credibility.

Encoder

For both handwritten digit classification (using the MNIST dataset) and orientation discrimination (trained with RL), f consisted of 3 convolutional layers followed by 3 fully-connected (FC) layers. The convolutional layers had 32 channels each, a kernel size of 3, a stride of 2, batch normalization, and leaky ReLU nonlinearities with a negative slope of 0.01. The first 2 FC layers had 256 and 128 units, batch normalization, and leaky ReLU nonlinearities with a negative slope of 0.01. The output of the encoder was generated by the final FC layer, which had 100 units and no nonlinearity or normalization.

For experiments on CIFAR-10, a more challenging object classification benchmark, f employed a more complex ResNet architecture modeled closely on He et al. The basic building block of this architecture is the residual block, in which the input to a series of convolutional layers is added to their output, thus providing shortcuts in the computational graph that facilitate learning in very deep architectures. Our implementation of this component used the following formulation:where x b −1 is the output of the previous residual blockandare the first and second convolutional layers in the current block b, andandare batch normalization layers. All convolutional layers employed a kernel size of 3 and no bias term. The default residual block employed layers with a stride of 1, and the same number of output channels as input channels. In some cases (as detailed in the next paragraph), a residual block had a different number of output than input channels, or employed a stride of 2 so as to output a smaller feature map. In these cases, the first convolutional layerimplemented either the stride or change in number of channels, and x b −1 was passed through an additional convolutional layer (incorporating the stride or change in number of channels) and batch normalization layer before being added to, to ensure that they had the same shape:

---

### Hippocampal segmentation for brains with extensive atrophy using three-dimensional convolutional neural networks [^fc64580e]. Human Brain Mapping (2020). Medium credibility.

Figure 1
Proposed base network architecture with insets for the convolution, residual, feature and upsample blocks. (a) The 3D layers are color‐coded to distinguish their different functionality and presented as 2D representations for simplification. For each layer, the number of features is shown at the bottom. " Conv " represents a convolution, " k " represents the kernel, " s " denotes the number of strides. (b) Overall scheme demonstrating the use of an ensemble of two consecutive networks to produce the final segmentation [Color figure can be viewed at]

The network is fully convolutional, that is no fully connected layers are added, and hence can predict a variable number of voxels in a forward pass without the need for architectural changes (Long et al. 2015). It employs skip connections to combine feature maps across stages through concatenation. The network has a depth of five and 16 initial filters, with the number of filters doubling every contraction step. The building blocks of the networks are convolution blocks, consisting of a convolution layer followed by a normalization layer and a nonlinearity. We chose the leaky ReLU as the activation function with a negative slope of 10 −2 for all feature map convolutions.

The residual blocks separate the input data into two paths, the first applies weights and nonlinearities, and the second applies an identity mapping (the input data is unchanged). The two paths are finally merged with the element‐wise sum, resulting in the following formulation:where W 1 and W 2 are the weights of the convolutional layers and σ is the activation function. The residual block consisted of two convolution blocks, separated by a dropout layer. We chose a dropout rate of 0.3 and a small convolution kernel of 3 × 3 × 3, which enables a higher nonlinearity capacity for the same receptive field with a lower number of parameters needed. The residual blocks are preceded by a strided convolution layer with a kernel of 3 × 3 × 3 and a stride of two, effectively reducing the number of feature dimensions by a factor of two while adding more features as the network depth increases.

---

### A programmable chemical computer with memory and pattern recognition [^8415597b]. Nature Communications (2020). High credibility.

Convolutional neural network and Autoencoder

To build the CNN and the Autoencoder, Tensorflow 1.15 was used. Patterns representing numbers from 0 to 9 were used. The inputs used were the CSV as described. These CSV contained 3600 columns values that represented 30 min of experiment. The first 1000 rows were discarded. The window size used was 25. Therefore, the inputs to the CNN were 25 by 25 or 625 values. The test and train ratios were set to 30%. Three CNN layers were used. The first one uses two feature maps and a kernel size of 3. The second one uses two feature maps and a kernel size of 5. The third one uses one feature map and a kernel size of 3. Finally, there is a fully connected layer with 16 neurons, and then 10 output neurons. The Autoencoder used had 25 inputs and 625 outputs. Three hidden dense layers were used of 50, 10 and 50 neurons. The Autoencoder was trained using two cycles of 3000 iterations each. The learning rate was 0.000001 and L2 regularization was used (0.00001). The input of this Autoencoder was 5-by-5 motor patterns, and its output of 625 nodes was used as input to the CNN described.

---

### Overcoming barriers to data sharing with medical image generation: a comprehensive evaluation [^ba3225ff]. NPJ Digital Medicine (2021). Medium credibility.

Predictive model development and training

In all settings, we used a pre-trained densenet-121 CNN as the predictive model. In dense CNNs, for each layer in every dense block, a concatenation of the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. Each block in the network consists of a number of bottleneck and composition layers, which both have batch normalisation and ReLU non-linearities. In each bottleneck layer the number of feature channels is reduced with a 1 × 1 convolution, before performing a 3 × 3 convolution in each composition layer. Between the dense blocks, the model consists of transition layers with 1 × 1 convolutions and average pooling operations. The dense structure allows for significantly deeper architectures without experiencing vanishing gradients, or an exploding number of weights. After the final global average pooling layer, we added a randomly initialised fully connected layer with sigmoid activation for classification with the binary cross-entropy loss. The densnet-121 architecture amounts to 117 convolution, 3 transition and 1 classification layers. We resized the input images to match the densenet-121 spatial input resolution of 224 × 224 pixels. To make training as similar as possible across different benchmark settings, we used a maximum number of 5000 images per epoch with a batch size of 48. In settings where the total number of samples is below 5000, the number of images per epoch is accordingly lower. After each epoch, we computed the area under the receiver operating characteristic curve (AUROC) for each label in all validation data samples. We reduce the initial learning rate of 0.0001 by a factor of 10 if the mean validation AUROC across all labels does not improve after two consecutive epochs (patience of 2). If thedoes not improve for a patience of 3 epochs, we stopped training. To compute delta scores, we tested all models on the held-out, real data test set.

---

### Uniform labeling of blocks and slides in surgical pathology: guideline from the college of American pathologists pathology and laboratory quality center and the national society for histotechnology [^74fc37e4]. Archives of Pathology & Laboratory Medicine (2015). Medium credibility.

Uniform labeling of blocks and slides — The panel believes that implementing a uniform labeling standard with specific elements to unambiguously link each tissue block and derived slides with a specific patient and tissue source will decrease the likelihood of misinterpretation within or between institutions. Risks noted include increased cost for users of automated labeling if existing equipment cannot accommodate two identifiers, alphanumeric designations, and sequential numbering/stain identification; challenges for hand-labeling due to limited space that require more careful writing; and that rushed labeling may lead to illegible notations and increased identification errors. The use of two identifiers will therefore force individuals to slow down, which may negatively impact workflow and sample turnaround times.

---

### Riboformer: a deep learning framework for predicting context-dependent translation dynamics [^02a4c291]. Nature Communications (2024). High credibility.

Implementation and architecture of Riboformer

We used the RNA sequence and the normalized ribosome density in the control experiment as a separate input to the Riboformer model. For both inputs, our model employs 5 convolutional blocks and 1 transformer block (see below for more details) to extract the features of coding sequences and reference ribosome densities. We used an element-wise multiplication layer to pool the information from the two branches together followed by a feedforward layer that produces the model output.

The 40-codon sequence is the input for the first branch, and it was further transformed into a vector using sequence embedding (hidden dimension: 8). For the input sequence(lengthacrossdimensions), the first stage of the architecture aims to extract the relevant sequence motifs from the mRNA sequence using the following block of operations:
2D convolution, with the kernel size of (5, 5), filter number 32:whereandare the learnable weight and bias matrices of the f th filter.
Batch normalization.
Rectified linear activation unit (ReLU) activation:

wherestands for the learnable weights for the fully connected layer.

We applied this block 5 times. Then the information from all 32 filters was pooled together using average pooling.

The second stage of the architecture aims to capture the interdependency among the codons, similar to many natural language processing tasks. We used a multi-head attention (MHA) layer. Given an input sequence(lengthacrosschannels), each attention head has a set of weightsand. These weights transform the input sequence into queries, keys, and values, defined as, and.

The attention matrix is then derived from the equation:

Hererepresents the influence of the query at position i on the key at position j. The values depict the data each position contributes to the subsequent positions attending to it. Each single attention head computes its output as a weighted sum over all input positions: This mechanism enables each query position to access information from the entire sequence. The multiple heads compute with independent parameters, and their outputs are concatenated to yield the final layer output. Our layers used 10 heads, key/query dimension of 8, and a dropout rate of 0.1. The last feed-forward module uses a fully connected layer followed by layer normalization and the ReLU activation function.

---

### On the crack resistance and damage tolerance of 3D-printed nature-inspired hierarchical composite architecture [^9861c808]. Nature Communications (2024). High credibility.

3D printing of nature-inspired architecture

All samples used in the study were designed using the Computer-Aided Design Package (CADian3D, IntelliKorea Co. Korea) and then fabricated by an FDM-type 3D printer (3DISON AEP, Rokit Co. Korea). The representative building blocks constituting the nature-inspired architecture were first designed in the following procedure. The granular building block was defined as a symmetric hexagonal structure where the vectors a and b are equal and the internal angle is 120°. The printing nozzle (nozzle diameter = 0.4 mm) filled the molten filament into the hexagonal structure towards a specific orientation direction. The neighboring bundles of filaments (with 100% packing density) were placed parallel to each other along the three inclined directions of 30°, 90°and 150° of the hexagonal structure. Accordingly, three fundamental building blocks, 30°, 90°, and 150° building blocks, respectively, were created. The length of the inclined plane (vectors a or b) determines the feature size of the granular block. Three grain sizes, namely L, M, and S, were described as symmetric hexagonal structures with a = b = 1.5 mm, a = b = 3 mm, and a = b = 6 mm, respectively. The cross-section of the filament shows an elliptical shape, width = 0.65 mm (including interfacial voids), height = 0.4 mm (single layer height = 0.4 mm), and therefore L, M, and S have 4, 8, and 16 infill filaments, respectively. The thickness of the building blocks also varies depending on the grain size; Grain L = 4 layers, Grain M = 2 layers, and Grain S = 1 layer. (The architecture has a total of 4 layers.) The systematic organization of the three building blocks of 30°, 90°, and 150° orientations determines the global orientation. The global orientation contains the orientation information of all individual building blocks in horizontal and vertical lines and stacking layers. For structural isotropy, the 30°, 90°, and 150° building blocks were sequentially arranged in horizontal rows, and the three neighboring building blocks on the same plane were organized in different orientations. In contrast, anisotropic architectures were monotonically assembled by only single building blocks with the same orientation. The plane-positioned building blocks are then piled up in either a simple ααα stacking sequence or an alternating αβγ stacking sequence. Note that the ααα stacking sequence overlaps the center points of all granular blocks, whereas the αβγ stacking sequence matches the center points of the upper layer granular blocks to the angular points of the lower layer granular blocks. The center points of the granular blocks are shifted layer by layer in a zig-zag form to the right and left lower angular points, along with the accompanying orientation movements. Accordingly, the L-, M-, and S-inspired architectures have 1 (αααα), 2 (ααββ), and 4 (αβγα') layers of alternate stacking sequences, respectively. (As an exception, the initial models of Figs. 2–4 have a total layup of 3 layers, size of M, and the stacking sequence of 3 layers (αβγ).) For ease of understanding, drawing details showing the stacking sequence of the architectures are provided in Suppl. Fig. 10. The gradient architectures were prepared by the systematic organization of L, M, and S. The vertical samples were divided into three regions, and then L, M, and S grains were placed in the gradient order of L → S and S → L. (Due to sample size limitations, the shape and size of some granular blocks were slightly modified.) In the next step, the above models were sliced into multiple thin layers using a 3D slicer software (Simplify3D, USA), and digital codes were generated. By manually changing the digital code, the 3D printer was programmed to draw a complete building block without any route changes to the next block. Individual granular blocks were locally blocked by boundary lines with 95% packing density of filaments. Eventually, the 3D printer fabricated the nature-inspired architectures using a heated nozzle of 220 °C at a bed temperature of 60 °C according to the programmed procedures. The print times for the architectures were highly dependent on the nozzle speed and extrusion length. During printing, the nozzle speed was 15 mm/s, and the movement speed without extrusion was 60 mm/s. If the infill space is the same, the extrusion length is the same, so the printing time did not differ significantly from that of the monolithic sample with a simple direction pattern. The printing times of L, M, and S are compared in Suppl. Table 1. There was a slight loss in printing time when moving without extrusion, but it was not significant, generally showing printing times of 7–8 min.

---

### Metamaterial bricks and quantization of meta-surfaces [^b8747088]. Nature Communications (2017). Medium credibility.

Discussion

The results reported in this paper reveal an algorithm for the fabrication of quantal meta-surfaces with well-controlled arbitrary 2D phase distributions using 3D metamaterial bricks. The concept of our method was inspired from analogue-to-digital conversion and image compression. Here, the AD conversion denotes a process in which a continuously variable (analogue) phase distribution is changed, without altering its essential content, into a multi-level (digital) phase. A quantal meta-surface becomes in effect a digital phase processing system operating as an SLM. As summarized in Fig. 1, three possible routes can be considered to realize a quantal meta-surface. First, we start with a spatial resolution of λ₀/2 and 4-bit uniform representation of the analogue phase (Fig. 1a). This method is similar to what has been reported by other authors. Second, we treat the phase distribution as an image and use a wavelet transform to perform a lossy compression (Fig. 1b). In this way, the number of bits necessary to represent the analogue phase is reduced to 3, while maintaining the error in reproducing the field below 0.1 dB: this has been explored in Fig. 6 and Supplementary Fig. 4. There is however a lossless alternative (Fig. 1c), where we can exploit the additivity of phase and the hierarchical properties of wavelet transforms. These transforms result in a branched tree to represent the structure of the decomposition; at a given level, each of the branch (that is, meta-surface) can be layered to form a stack with a smaller number of unique bricks. An example where this route is applied to a focusing field is reported in Supplementary Fig. 5. Here, the analogue phase is represented with 3.5 bits and a stack of 4 quantal meta-surfaces. In this solution, each meta-surface is made of 'blocks' — that is, unit cells of dimension λ₀, each made of a 2 × 2 structure of identical bricks. While the spatial resolution of the bricks has to remain lower than λ₀/2 to reduce higher order diffraction, the presence of blocks makes mechanical assembly much simpler, which is a key aspect of cost-effective devices based, for example, on active meta-materials.

---

### Multiscale footprints reveal the organization of cis-regulatory elements [^38a01c63]. Nature (2025). Excellent credibility.

Model architecture and training

The seq2PRINT model is a convolutional neural network that takes one-hot encoded DNA sequences (a DNA sequence of length L encoded into an L × 4 matrix, where each row has one element set to 1 representing the specific nucleotide) as input and predicts the corresponding multiscale footprints at base-pair resolution. The architecture is depicted in Extended Data Fig. 4a. To facilitate articulation of the architecture, we divide the seq2PRINT model into three parts. Although the architecture of seq2PRINT can be flexible depending on the available computational resources and the depth and scale of the training data, for all results in this work we build the model and choose the hyperparameters as follows.

The first convolutional layer consists of 1,024 filters of width 21 bp with Gaussian error linear unit (GELU) activation, aiming to capture informative sequence patterns from the input DNA sequences (that is, sequence motifs). The output is subsequently passed to eight layers of convolutional blocks with residual connection. Each convolutional block consists of one grouped dilated convolutional layer (n filters = 1,024, width = 3, groups = 8, dilation = 2 i, i = 1,…, 8), followed by a position-wise feed-forward layer (implemented as a convolutional layer with n filters = 1,024, width = 1). Batch normalization layers with GELU activations are inserted between these convolutional blocks. The increased dilation rate results in an expanding receptive field for the neural network, capturing the relationship of the sequence patterns and their context. With eight layers of the convolutional block, seq2PRINT has a receptive field of ± 920 bp for each cCRE. The use of grouped convolutional layers enables the construction of wider models that capture richer information with limited parameters, providing a regularization effect and reduced computational complexity. Finally, the output of the stacks of convolutional blocks is passed to the output layers.

---

### Uniform labeling of blocks and slides in surgical pathology: guideline from the college of American pathologists pathology and laboratory quality center and the national society for histotechnology [^53874444]. Archives of Pathology & Laboratory Medicine (2015). Medium credibility.

Quality assessment and evidence grading — articles meeting inclusion criteria were assessed for strength of evidence and methodological rigor and graded by a methodology consultant; strength considered the level, quantity, and quality of evidence. The level of evidence was based on study design as follows: Level I evidence from systematic reviews of appropriate level II studies, level II evidence from good quality randomized controlled trials, level III evidence from low quality comparative studies, and level IV evidence from studies without a comparator; in general, level I and II evidence is considered most appropriate to answer clinical questions, but in the absence of such high quality evidence, the panel considered data from lower quality studies. Quantity referred to the number of studies and cases for each outcome, and quality reflected how well studies were designed to eliminate bias and threats to validity.

---

### Hundred-layer photonic deep learning [^9fdd7f16]. Nature Communications (2025). High credibility.

Here we report a single-layer photonic computing (SLiM) chip that removes all the error-prone propagation layers, targeting ONNs with a hundred-layer depth working at tens of GHz data rate. To realize SLiM, we incorporate perturbations within the photonic chip design, which breaks the on-chip wavelength correlation to realize arbitrary matrices and decorrelates the signal from the error with the detection-based activation function. The single-layer photonic chip minimizes the error propagation and guarantees bounded errors regardless of the number of network layers, surpassing the depth limit (DL) of ONNs. The arbitrary error-tolerant computing with single-layer propagation and detection facilitates chip clusters with computational connections at hundred-meter spatial depth. Experiments were conducted at 10 GHz to build 100-layer residual network, classifying the full ImageNet-1000 (photonic accuracy 85.2%, digital accuracy 85.9%). Transformer-based error-tolerant ONN of 384 and 640 layers with billion-scale parameters (0.345B and 0.192B) were then experimentally evaluated to generate prompted language (photonic loss function value 3.04, digital loss function value 2.96) and conditioned image (photonic loss function value 7.32, digital loss function value 7.28).

---

### Physical basis of amyloid fibril polymorphism [^b6eec2e9]. Nature Communications (2018). Medium credibility.

Fig. 2
Protomer conformation revealed by ss-NMR. a The 2D 13 C– 13 C proton-driven spin diffusion spectrum shows the cross-peaks between aliphatic carbon atoms. b Backbone model of the cross-β sheet implementing Φ/Ψ angles as predicted by the program TALOS+(scale bar, 1 nm). c Averaged parallelograms of morphologies I–V (gray) superimposed with a peptide dimer (green, magenta) as obtained by NMR. The placement of the peptides followed the considerations described previously for morphology I 8 (scale bar, 1 nm). d Side view and top view of a six-layer stack illustrating the structure of a PF. Side-chain conformations were not determined by experiments and represent only conformations that are compatible with the packing shown here

Fig. 3
Peptide assembly of fibril morphologies I to V. a Side-by-side presentation of the experimentally obtained density and of the peptide assembly of fibril morphologies I to V (scale bar, 10 nm). b Peptide cross-sections showing an exposed N terminus (magenta) and those not exposed (green) within one peptide layer, superimposed with reconstructions (gray) and filtered to 10 Å. Side-chain conformations were not determined by experiments and represent only conformations that are compatible with the packing shown here. The 3D map of morphology I was included as published(scale bar, 2 nm)

Physical factors influencing fibril polymorphism

Having established that different fibril morphologies consisted of a common building block and that these fibrils show mainly two ways to pack the building blocks within the fibril cross-section, we wondered how many fibril morphologies (cross-sections) can be generated by a given number n of building blocks or PFs? Furthermore, why are only some observed by experiments? To illustrate this problem, we found 120 possible arrangements encompassing n = 6 building blocks with only one being observed by cryo-EM (Supplementary Figures 9, 10) and a total of 346,649 possible constellations for the entire range of 1 ≤ n ≤ 12 (Supplementary Figure 11), eight of which were observed. As a next step, we considered the energetic contributions E l, E s, and E e provided by a LSI, SSI, and an empty sharp corner to the fibril stability and combined them to define an arbitrary energy score E for each fibril morphology as shown in Eq. (1) (for details please refer to the "Methods" section).

---

### Complex wireframe DNA nanostructures from simple building blocks [^a9063491]. Nature Communications (2019). High credibility.

Results

Structural design

To design a typical wireframe structure entirely from synthetic strands, a 2D or 3D graph is first generated (Fig. 1a). Such a graph, composed of nodes and edges with a specific node-edge geometry, is rendered so that the nodes represent vertices of different numbers of arms and the edges represent simple DNA duplexes of variable lengths (Fig. 1b). The number of edges connected to a certain node is arbitrary, as are the numbers of base pairs of constituent edges. Arbitrary shapes can be designed by defining a full set of node-node connectivity and the corresponding edges of variable lengths. For purposes of practicality, the number of edges connecting to a certain vertex is set to be no more than seven and the edge lengths are set to be full turns (rounded multiples of 10.5 base pairs) in this study. Afterwards, edges are segmented into complementary domains to ensure that the corresponding strands with multiple domains satisfy synthesis (e.g. a typical strand is less than 100 nt and preferably less than 80 nt) and self-assembly (e.g. a paired complementary domain is no less than 6 bp and preferably no less than 10 bp) requirements (Fig. 1c). Although the segmentation could also be arbitrary, a typical edge in this study is segmented in a standardized fashion into two root domains of the same length and a stem domain (Fig. 2a, inset and Supplementary Fig. 1). To relax tension around the crossovers at a certain vertex (e.g. a vertex of 5 or 6 arms), extra unpaired base(s) (e.g. 1T, 2T, or 3T) can be added as spacers across arms (Supplementary Table 1). After a specific strand arrangement is laid out, the final step is to populate strands with sequences to meet certain sequence generation criteria(e.g. base-pairing and mismatch prevention). As colors indicate, a distinct DNA sequence was appointed to each component strand (Fig. 1d).

---

### Mapping the glycosyltransferase fold landscape using interpretable deep learning [^a30c689d]. Nature Communications (2021). High credibility.

CNN model for fold and family classification

The model architecture involves an attention-aided deep CNN model with six blocks. The first three blocks (Block 1, Fig. 1) sequentially use a one-dimensional convolutional layer, followed by a pooling layer and a batch normalization layer. This feeds into an attentionlayer that performs a refinement of the generated feature maps. The convolution kernel sizes were set to 3, 7, and 15 with kernel numbers set to 256, 512, and 512, respectively, for the first three blocks. Since pooling operations lead to a loss of spatial information for the feature maps, such an operation is not applied on any of the first three layers, thus enabling mapping of the attention maps back to the sequence for interpretation.

The feature maps from these three layers are downsampled using a global average pooling layer and then passed through three additional blocks before making the final prediction. In contrast to the first three layers that carry spatial information, these three layers (Block 2, Fig. 1) use global max pooling operations that compute a single maximum value for each of the input channels, thus providing a single linearly independent representation for each sequence, regardless of sequence length. These representations can be transformed into high-dimensional vectors which can then be used by downstream clustering algorithms.

For the multitasking of fold and family classification, two separate fully connected layers were added with dropout. The model was trained on a single NVIDIA RTX 2080Ti graphic card for 6 h. The dropout rate was set to 0.5 during training. Adam optimizers with a learning rate of 1e-4 and weight decay with a rate of 1e-5 were deployed during training.

---

### Uniform labeling of blocks and slides in surgical pathology: guideline from the college of American pathologists pathology and laboratory quality center and the national society for histotechnology [^b679e003]. Archives of Pathology & Laboratory Medicine (2015). Medium credibility.

Supplemental Table 3 – Quality Assessment — pre- and post studies (N = 5) (Level IV evidence) and qualitative data analysis (N = 1) (Level IV evidence) are appraised across domains including "Intervention occurred independently of other changes over time", "Intervention was unlikely to affect data collection", "Primary outcome was assessed and measured objectively", "The study was analyzed appropriately", and an "Overall risk of bias assessment" with a "Rating". In the pre- and post studies, rows include entries of "not clear" for independence over time and "yes" for other domains, with examples showing "low-moderate" risk with "fair" rating and others showing "low" risk with "good" rating. For the qualitative analysis, the single study shows "yes" across design, data collection, clarity, relevance, and conclusions, with "Overall risk of bias assessment" "low" and "Rating" "good".

---

### Physics-informed neural networks for modeling physiological time series for cuffless blood pressure estimation [^3def4cdc]. NPJ Digital Medicine (2023). Medium credibility.

Train and test of BP estimation with state-of-the-art time deep learning

We tested four additional time series deep learning models for benchmarking: long-short-term memory (LSTM), CNN+Bi-GRU+Attention, ResNet, and Transformer. We followed the methodologies provided by each corresponding paper while building the deep learning architectures. The bidirectional LSTM model includes one-layer 1-D CNN network (number of filters: 32, kernel size: 5, activation: 'RELU'), followed by three-layer LSTM (hidden units: 64, 64, 32), and two-layer FCN (first layer number of neurons: 128, activation: 'RELU', second layer number of neurons: 1, activation: linear). The 3 physiological features are concatenated with the output of the third LSTM layer and then passed through the FCN layers. The CNN+Bi-GRU+Attention model architecture comprises three convolution modules (number of CNN layers in each module: 2, 2, 3). Each CNN layer has kernel size of 3, activation function of 'RELU', and padding of 'SAME". Each CNN layer is followed by a batch normalization layer. After each module a max-pooling layer (pool size: 3) is added. The number of filters scales up by a factor of 2 (from 64 to 256) as it passes through each module. Next, a bidirectional gated recurrent unit (Bi-GRU, number of hidden nodes: 64 for forward and backward layers) and an attention mechanism that generates a context vector through computing a weighted sum of the time steps on the output of Bi-GRU are added. The attention mechanism output is concatenated with the three physiological features, followed by two-layer FCN (first layer number of neurons: 128, activation: 'RELU', second layer number of neurons: 1, activation: linear). The ResNet model consists of a one-layer 1-D CNN network (number of filters: 16, kernel size: 5, activation: 'RELU') followed by four sets of residual blocks with varying number of filters (16, 32, 64, 128). Each set is composed of four residual blocks, with each block having a two-layer 1-D CNN network (kernel size: 3 for both layers, first activation: 'RELU', second activation: 'NONE'), followed by an element-wise addition operation with a skip connection (a 1-D CNN layer, kernel size: 1, same number of filters as the residual block). After the residual blocks, a Global Average Pooling layer is used to reduce the dimensions of the output tensor. Then a dense layer is added (number of neurons: 128, activation: 'RELU'), and the three physiological features are concatenated to the output of this layer. Finally, a two-layer FCN (first layer number of neurons: 32, activation: 'RELU', second layer number of neurons: 1, activation: linear), is used to generate the BP estimation. The Transformer model uses the encoder, with N = 2 identical transformer layers processing the input through self-attention mechanisms and FCN. Each layer has 4 parallel attention heads. The dropout rate is set to 0.1. The output of the transformer module is concatenated with the three physiological input features, followed by two-layer FCN (first layer number of neurons: 64, activation: 'RELU', second layer number of neurons: 1, activation: linear).

---

### Uniform labeling of blocks and slides in surgical pathology: guideline from the college of American pathologists pathology and laboratory quality center and the national society for histotechnology [^57068aae]. Archives of Pathology & Laboratory Medicine (2015). Medium credibility.

Assessing the strength of recommendations — development of recommendations required that the panel review the identified evidence and make key judgments, beginning from the central question "What are the essential elements for the proper labeling of paraffin blocks and microscopic slides in the routine practice of surgical pathology?" Strength of evidence is graded as Convincing, Adequate or Inadequate, based on four published criteria (Supplemental Table 4), and strength of evidence is a key element in determining the strength of a recommendation. The method for determining strength of recommendation has been modified for this application (Supplemental Table 5) and is based on the strength of evidence and the likelihood that further studies will change the conclusions; when evidence was missing or insufficient to permit a conclusion to be reached, recommendations were made based on consensus expert opinion. Consideration of the net balance of benefits and harms will focus on the core recommendation to unambiguously identify blocks and slides in surgical pathology.

---

### Meta-neural-network for real-time and passive deep-learning-based object recognition [^b6c1f2f3]. Nature Communications (2020). High credibility.

Fig. 1
Passive object recognition by acoustic meta-neural-network.

a The proposed meta-neural-network with network parameters given by a computer-aided training process is capable of converging the scattered energy from the object (chosen as handwritten digit "8" here) into the corresponding region on the detection plane (marked by dot-line boxes behind the last layer). b Schematically illustrates the interaction between two adjacent 2D layers of meta-neurons whose deep-subwavelength size physically ensures wave propagation from each meta-neuron on the 1st layer to all meta-neurons on the 2nd one (after undergoing the phase-amplitude modulation by the 1st layer and free-space diffraction in between, described by W 1 and G, respectively). c A conventional neural network can be accurately mimicked by the practical physics model shown in b even for compact device and/or complicated object.

First, we consider the propagation of scattered wave in such a multi-layered metamaterial system. As the fundamental building block of our designed meta-neuron-network, each meta-neuron modulates the amplitude and phase of the incident wave, then the outgoing wave on the transmitted side serves as second sources and becomes the input signal for the next layer, as governed by Huygens' principle. Obviously, the radiation pattern of each meta-neuron depends on the unit cell size and spacing related to wavelength. When each meta-neuron can be approximated as a monopole source, the relationship between the wave fields on two neighboring layers in our meta-neural-network can be written aswhere vector P l +1 denotes the input wave of the (l +1)-th layer of meta-neurons, G l is the wave propagation matrix (see the Supplementary Notes 1 and 2), is the modulation introduced by the meta-neurons on the l -th layer with t l and φ l referring to the amplitude and phase modulation respectively, " denotes the element-wise multiplication. While the conventional neural network can be written aswhere f is the nonlinear activation function, w l is the weight and B l is the bias.

---

### Optical generative models [^b5e875d6]. Nature (2025). Excellent credibility.

In the numerical simulations, the minimum lateral resolution of the optical generative model was 8 μm, and the illumination wavelength was 520 nm for monochrome operation and (450 nm, 520 nm, 638 nm) for multicolour operation. The number L o of the decoding layer was set to 1, the axial distance from the SLM plane to the decoding layer d 0,1 was 120.1 mm, and the distance from the decoding layer to the sensor plane d 1,2 was 96.4 mm. In the construction of the encoded phase pattern, that is, the optical generative seed, the coefficient α that controlled the phase range of normalization was set to 2.0, and the normalized profile was up-sampled in both the x and the y directions by a factor of 10. Hence, the size and the resolution of the object plane were 2.56 mm × 2.56 mm and 320 × 320, respectively. The number of optimizable features on the decoding layer was 400 × 400. On the image plane, the 320 × 320 intensity measurement was down-sampled by a factor of 10 for the loss calculations.

During the training of the teacher DDPM, the total timestep T was set to 1,000. The noise prediction modelshared the same structure profile as the general DDPM. βₜ was a linear function from 1 × 10 −4 (t = 1) to 0.02 (t = T). In the training of the snapshot optical generative models, the histogram of KL divergence was calculated using normalized integer intensity values within the range of [−1, 1], and the regularization coefficient γ was set to 1 × 10 −4. All the generative models were optimized using the AdamW optimizer. The learning rate for the digital parameters (digital encoder and DDPM) was 1 × 10 −4, and for the decoding layer was 2 × 10 −3, with a cosine annealing scheduler for the learning rate. The batch sizes were set as 200 for DDPM and 100 for the snapshot generative model. All the models were trained and tested using PyTorch 2.21with a single NVIDIA RTX 4090 graphics processing unit.

---

### Distinct brain morphometry patterns revealed by deep learning improve prediction of post-stroke aphasia severity [^cf082cef]. Communications Medicine (2024). Medium credibility.

Fig. 2
Potential advantages of Convolutional Neural Networks (CNN) for the detection of spatially related changes in brain structure.

a and e provide examples of two patterns of gray matter atrophy in perilesional brain areas (shown in blue) of individuals with identical lesions (shown in red). Despite the average voxel-wise atrophy in brain regions being comparable (bars in b and f), the voxel-wise distribution of the location of atrophy is different, as demonstrated by the lines in b and f indicating contiguous voxel-wise levels. The total atrophy (area under the curve - AUC in c and g) is similar as well. While the differences in the pattern of atrophy are visually intuitive (e.g. based on the shape of atrophy in a and c), they are not captured by conventional statistical models or machine learning approaches. CNNs are ideally equipped to identify such patterns through the application of spatial filters, or weight matrices that slide across the volume to learn features through the process of convolution, and the identification of motifs based on the shape and spatial dependence of imaging features. For example, each filter can represent a spatial contrast, reflecting specialization for different spatial arrangements of voxels, and each contrast can be more or less represented within each pattern of atrophy (d and h).

We used a single-channel 3D CNN to predict patients with severe aphasia by minimizing binary cross entropy loss weighted by inverse class frequencies to compensate for class imbalance. The network was structured based on VGG (Visual Geometry Group) architecture, using small 3×3 convolutional filters, max pooling, and a deep stack of convolutional layers. This kind of architecture has been shown to perform as well or better than alternatives in similar tasks to ours. During tuning, we systematically experimented with 4 levels of network complexity, allowing the number of convolutional layers and their channel configuration to vary. The least complex CNN structures contained 4 blocks of convolutional layers and the most complex contained 5 blocks. Each block was followed by a max pooling layer with the number of convolutional layers within blocks varying between 1 and 4 and the number of channels in each layer varying between 8 and 128. Networks terminated with three fully connected layers. The first layer doubled the number of channels in the last convolutional layer, the second layer halved the number of channels and the last layer corresponded to the number of classes.

---

### Hybrid topological data analysis and deep learning for basal cell carcinoma diagnosis [^8367a651]. Journal of Imaging Informatics in Medicine (2024). Medium credibility.

The top layers from the original EfficientNet-B5 model are removed as it was designed for 10-class classification instead of binary classification. They are replaced with a global average pooling layer, a dropout layer, and a final dense layer. The initial input image size for the model is 448 × 448 × 3. It has 14 phases and is first trained for classification, then feature extraction using the trained model. It begins with a 3 × 3 filter convolution, batch normalization, and swish activation function for the classification stage, cutting the image dimensions in half from 448 to 224 and raising the number of channels from 3 to 48. As a result, the feature map's measurements are 224 × 224 × 48. Stage 2 is composed of three layers of an MBConv1 block with a 3 × 3 filter, which reduces the number of channels while maintaining the resolution of stage 1 to produce a feature map with dimensions of 224 × 224 × 24. Stages 3 (five layers), 4 (five layers), and 5 (seven layers) employ three MBConv6 blocks, each with a kernel size of 5 × 5, to gradually decrease the resolution while expanding the size of the feature map to 28 × 28 × 128 (the stage's finish). Stages 6 (7 layers), 7 (9 layers), and 8 (3 layers) each apply three more MBConv6 blocks with kernel sizes of 3 × 3, 5 × 5, and 3 × 3 to create a feature map with a final dimension of 14 × 14 × 512. A feature map with the dimensions 14 × 14 × 2048 is produced at stage 9 using a 1 × 1 convolution with 2048 filters. Stages 10 and 11 maintain the feature size from the preceding layer while applying batch normalization and Softmax activation. Stage 12 uses global average pooling to increase the resolution to 2048 followed by stages 13 and 14 leading to the final classification: a dropout and dense layer. After the 200th layer, the model is fine-tuned, and the best model is saved. After the global average pooling layer, at stage 12, feature extraction is carried out, producing a 2048-dimensional feature vector for the training, validation, and test sets.

Table 2 displays the phases, procedures, resolutions, channels, and layers.

Table 2
EfficientNet-B5-based deep learning model

---

### Bridging known and unknown dynamics by transformer-based machine-learning inference from sparse observations [^4066608f]. Nature Communications (2025). High credibility.

Selecting an appropriate neural network architecture for reconstructing dynamics from sparse data requires meeting two fundamental requirements: (1) dynamical memory to capture long-range dependencies in the sparse data, and (2) flexibility to handle input sequences of varying lengths. Transformers, originally developed for natural language processing, satisfy these requirements due to their basic attention structure. In particular, transformers has been widely applied and proven effective for time series analysis, such as prediction –, anomaly detection, and classification. Figure 6 illustrates the transformer's main structure. The data matrixis first processed through a linear fully-connected layer with bias, transforming it into an L s × N matrix. This output is then combined with a positional encoding matrix, which embeds temporal ordering information into the time series data. This projection process can be described as:whererepresents the fully-connected layer with the bias matrixand the position encoding matrix is. Since the transformer model does not inherently capture the order of the input sequence, positional encoding is necessary to provide the information about the position of each time step. For a given positionand dimension 1 ≤ d ≤ D, the encoding is given byThe projected matrixthen serves as the input sequence for N b attention blocks. Each block contains a multi-head attention layer, a residual layer (add & layer norm), and a feed-forward layer, and a second residual layer. The core of the transformer lies in the self-attention mechanism, allowing the model to weight the significance of distinct time steps. The multi-head self-attention layer is composed of several independent attention blocks. The first block has three learnable weight matrices that linearly map X p into query Q 1 and key K 1 of the dimension L s × d k and value V 1 of the dimension L s × d v :where, andare the trainable weight matrices, d k is the dimension of the queries and keys, and d v is the dimension of the values. A convenient choice is d k = d v = N. The attention scores between the query Q 1 and the key K 1 are calculated by a scaled multiplication, followed by a softmax function:where. The softmax function normalizes the data with, and thefactor mitigates the enlargement of standard deviation due to matrix multiplication. For the first head (in the first block), the attention matrix is computed as a dot product betweenand V 1 :where. The transformer employs multiple (h) attention heads to capture information from different subspaces. The resulting attention heads O 1 i (i = 1,…, h) are concatenated and mapped into a sequence, described as:whereis the concatenation operation, h is the number of heads, andis an additional matrix for linear transformation for performance enhancement. The output of the attention layer undergoes a residual connection and layer normalization, producing X R 1 as follows:A feed-forward layer then processes this data matrix, generating outputas:where, b a and b b are biases, anddenotes a ReLU activation function. This output is again subjected to a residual connection and layer normalization.

---

### Uniform labeling of blocks and slides in surgical pathology: guideline from the college of American pathologists pathology and laboratory quality center and the national society for histotechnology [^97725764]. Archives of Pathology & Laboratory Medicine (2015). Medium credibility.

Supplemental Table 2: Hierarchy of level of evidence ranks studies as follows — level I: "A systematic review of level II studies"; level II: "Randomized Clinical Trial (RCT) (Good Quality)"; level III-1: "A pseudo- RCT (i.e., alternate allocation or some other method) or RCT (Poor Quality)"; level III-2: "A comparative study with concurrent controls" including "Non-randomized, experimental trial" and "Cohort study, Case-control study, Interrupted time series with a control group (Good Quality)"; level III-3: "A comparative study without concurrent controls" including "Historical control study", "Cohort study, Case-control study, Interrupted time series with a control group (Poor quality)", "Interrupted time series without a parallel control group", and "Two or more single arm studies"; and level IV: "Case series with either post-test or pre-test/post-test outcomes".

---

### Automated design of a convolutional neural network with multi-scale filters for cost-efficient seismic data classification [^7b4df084]. Nature Communications (2020). High credibility.

Configuration of SeismicPatchNet

The suggested configuration of SeismicPatchNet (Table 1) had seven layers/blocks containing trainable parameters in its naïve form. The size of input tensor was 112 × 112 × number of channels. The input receptive field covered a vertical area between 100 × 100 and 300 × 300 m 2 in seismic surveys. The total number of trainable parameters was 0.725697 M. Overall, SeismicPatchNet had the following characteristics:
The output number of kernels/filters varied from layer to layer and was restricted to the specified searching space, preventing uncontrolled increases in feature maps and subsequent reduced computational performance.
The rich diversity of parallel multi-scale kernels/filters was concatenated as one output vector in each topological fusion layer, forming the input of the next layer (Fig. 1c). Similar to InceptionNet, the factorization of convolutional operations was used to significantly reduce the number of trainable parameters: sequences of 1 × 1, n × 1, and 1 × n convolutions were combined to replace a single expensive n × n convolution with larger kernel size. In addition, a group of max-pooling and min-pooling operations was also incorporated as a parallel path to merge the sampled features from the previous layer. Our experimental experience suggested that adding pooling operations with opposite polarity had additional benefits on seismic reflection data.
In each topological layer, the size of kernels/filters and the number of various units were different, meaning that the fusion layers were not stacked upon each other with the same type as usual. All the numbers and m × n sizes of the kernels/filters were determined using an automated search strategy to approximate a quasi-optimal structure. Interestingly, both the number and size of the kernels showed a growing trend from the lower fusion layer (4) to the higher fusion layer (6), suggesting that patterns of higher abstraction were learned by higher layers.
Only traditional and most used operations like convolution and feature sampling/pooling were considered, in order to significantly reduce the computational resources needed and thus improve computing speed; more intermediate variables during the training mean more memory usage and less computational efficiency. In the naïve form of SeismicPatchNet, all layers used rectified linear unit as activation function. Occasional pooling layers with stride two were used to halve the resolution of the feature maps. One dropout layer followed by one fully connected layer was applied to combat the overfitting problem.

---

### Deep neural network approach for continuous ECG-based automated external defibrillator shock advisory system during cardiopulmonary resuscitation [^a091ac31]. Journal of the American Heart Association (2021). Medium credibility.

Results

We determined the optimal number of residual blocks for our data. To examine this, we trained and tested the DNN model using both shockable and nonshockable ECG data with and without CPR artifact using the 4‐fold cross‐validation method, by varying the number of residual blocks starting from 1 to 6 at an increment of 1. Note that each block contained 2 CNN layers. In addition, there was a single CNN layer at the beginning of the DNN model; thus, in Table 1, we show the total number of CNN layers for our choice of 1 to 6 residual blocks. The results shown in Table 1 were based on the summation of all true positives, true negatives, false positives, and false negatives from the 4‐fold cross‐validation. These parameters were validated using the information from the annotation files provided by ECG experts who were blinded to initial underlying rhythms. As mentioned in section 2.1, all rhythms (both shockable and nonshockable) without CPR were adjudicated using the information from the associated annotation files, which were created by ECG experts. To these rhythms we added CPR data collected during asystole. Hence, we knew a priori the adjudication of CPR‐contaminated rhythms. As shown in Table 1, the network with 4 residual blocks (9 CNN layers) provided the best classification results. Figure 7 better reflects the performance differences between various numbers of residual blocks. Although 5 residual blocks had slightly higher sensitivity (0.31%) than did 4 residual blocks, the specificity was lower (2.66%). In addition, 4 residual blocks had the highest F1‐score among all 6 different numbers of residual blocks. Hence, we selected 4 residual blocks containing 9 CNN layers as the final optimal DNN model for subsequent data analysis, to be described henceforth. Increasing the number of residual blocks up to 6 (13 CNN layers) did not further improve the performances, and it came at the expense of higher computational costs.

---

### Hundred-layer photonic deep learning [^0356544f]. Nature Communications (2025). High credibility.

Large language model

The foundational components of the deep neural networks utilized in the language models were built on transformers, organized into four error-tolerant linear layers and one attention block. The attention mechanism was structured into multiple heads, enabling multi-head attention. We closely adhered to the specifications of the GPT-2 model regarding the number of layers, dimensions, and attention heads, assigning configurations of 2, 4, 8, 12, and 24 attention blocks, with dimensions set to 256, 512, 640, 768, 1024, and head counts of 4, 8, 10, 12, 16, resulting in a total of 4, 16, 32, 48, and 96 layers, respectively. The training employed a cross-entropy loss function alongside the SGD optimizer, initialized at a learning rate of 0.001 (which decayed proportionally to the current step relative to the total steps) and a weight decay value of 1 × 10 −2. The networks were trained until convergence, requiring 11, 18, 79, 132, and 240 GPU hours for their respective configurations.

Conditioned image generation

The ONN designed for conditioned generation was based on a transformer architecture, comprising four error-tolerant linear layers alongside the attention block. This network specifically featured 16 attention blocks, each with 1024-D inputs and 16 heads. The vocabulary for image generation was segmented into 4096 words using VQ-VAE, while a GPT training methodology was utilized to generate subsequent tokens in the following scale. Commencing with one token with a class-conditioned embedding at the 1 × 1 scale, the network processes this through all 16 blocks to predict the generation of subsequent tokens, following a scaling order of 1, 2, 3, 4, 5, 6, 8, 10, 13, and 16. The resultant 16 × 16 tokens were decoded into an image with a resolution of 256 × 256 × 3. The employed loss function was cross-entropy, and the AdamW optimizer was utilized with parameters β 1 = 0.9, β 2 = 0.95, learning rate set to 0.0005 (decreasing proportionally to the ratio of current steps to total steps), and batch size set to 128. The training process took around 576 GPU-hours to reach convergence.

---

### The snm procedure guideline for general imaging 6.0 [^c6dbfdd1]. SNMMI (2010). Medium credibility.

Quality control — printers: Inexpensive hardcopy printers can be used for distribution of demonstration quality images, and printers with 600 or more dots per inch (dpi) can provide images of reasonable quality.

---

### Low-voltage 2D materials-based printed field-effect transistors for integrated digital and analog electronics on paper [^9eadbc5f]. Nature Communications (2020). High credibility.

Paper is the ideal substrate for the development of flexible and environmentally sustainable ubiquitous electronic systems, which, combined with two-dimensional materials, could be exploited in many Internet-of-Things applications, ranging from wearable electronics to smart packaging. Here we report high-performance MoS 2 field-effect transistors on paper fabricated with a "channel array" approach, combining the advantages of two large-area techniques: chemical vapor deposition and inkjet-printing. The first allows the pre-deposition of a pattern of MoS 2; the second, the printing of dielectric layers, contacts, and connections to complete transistors and circuits fabrication. Average I ON /I OFF of 8 × 10 3 (up to 5 × 10 4) and mobility of 5.5 cm 2 V -1 s -1 (up to 26 cm 2 V -1 s -1) are obtained. Fully functional integrated circuits of digital and analog building blocks, such as logic gates and current mirrors, are demonstrated, highlighting the potential of this approach for ubiquitous electronics on paper.

---

### TAGET: a toolkit for analyzing full-length transcripts from long-read sequencing [^303c3f7a]. Nature Communications (2023). High credibility.

Splice site correction using Convolutional Neural Network

The CNN modelfor the local adjustment of the splice sites consists of three blocks. Each block consists of a convolutional layer, a pooling layer by the rectified linear units (ReLU) and a batch-normalization layer. The inputs of the CNN model are 21 bp sequences near splice sites (both exon and intron sequences) and the outputs are the splice positions of the sequences. For each nucleotide of the input sequence, we take its 10 bp upstream and 10 bp downstream sequences. If a nucleotide does not have enough nucleotides upstream or downstream, we pad with Ns and get the 21 bp sequence. These 21 bp sequences are then encoded by converting A, C, G, T, N to 4-vectors (1,0,0,0), (0,1,0,0), (0,0,1,0), (0,0,0,1), (0,0,0,0). The convolution kernels are chosen as 3×3 kernels with 50 channels. The training data and the validation data are taken as 21 bp sequences around the splice sites listed in the Ensemble database (bp from exon andbp from intron withrandomly generated from 1 to 10). 90% of the sequences sampled from the Ensemble database are chosen as the training data and the remaining 10% of the sequences are the validation data. The loss function is chosen as the categorical cross-entropy loss and the Adam optimizer is used to minimize the loss function. The model is trained for 1000 epochs on NVIDIA GeForce GTX 1080 Ti GPUs. The learning rate of the optimizer is set to 0.01. This procedure is repeated 10 times to make the model more accurate. We find that the CNN model achieves an accuracy of 98% in the validation data (Supplementary Fig. 12).

---

### Uniform labeling of blocks and slides in surgical pathology: guideline from the college of American pathologists pathology and laboratory quality center and the national society for histotechnology [^d5225f99]. Archives of Pathology & Laboratory Medicine (2015). Medium credibility.

Supplemental figure 1 — literature review flow diagram — details study selection counts for the guideline evidence review: records identified through database searching (n = 427) and additional records identified through other sources (n = 44) contributed to records after duplicates removed (n = 456). The flow lists records screened (n = 456), records excluded (n = 378), full-text articles assessed for eligibility (n = 78), full-text articles excluded, with reasons (n = 68), studies included for extraction and grading (n = 10), and Data extraction articles excluded (n = 10), with the footnote stating exclusions were "Excluded based on expert opinion, did not meet minimum quality standards, presented incomplete data or data that were not in useable formats".

---

### Deep neural network approach for continuous ECG-based automated external defibrillator shock advisory system during cardiopulmonary resuscitation [^b119ee91]. Journal of the American Heart Association (2021). Medium credibility.

Figure 4
Procedures for converting time series into an image that was then used as an input to a deep neural network model.

First, the 8‐second sample is replicated to create time series "A". Then, amplitude "B" and phase "C" of vector A's short‐time Fourier transform (STFT) is determined and concatenated. To produce the combined vector of time and frequency information, B and C are reshaped in a single column array and concatenated with A. Finally, this 1‐dimensional vector is converted to 2‐dimensional matrix, which is appropriate for convolutional layers.

Distinguishing between shockable and nonshockable arrhythmia during CPR is a highly challenging task, because of the diversity of ECG arrhythmias, the variability of ECG waveforms, and, most important, variations in CPR artifacts attributable to different CPR delivery among performers. To compensate for these variations, a powerful network is required. The proposed deep learning model, shown in Figure 5, was composed of multiple processing layers consisting sequentially of the CNN, bidirectional long short‐term memory (BiLSTM) architecture, 32 and the residual connection layer. 33 As shown in the network architecture (Figure 5), the features are extracted from CNN layers. We used the sequence unfolding layer and the flatten layer to convert images to feature vectors. Subsequently, extracted features were then fed into the BiLSTM layer for classification. Although CNN layers are capable of extracting and learning complex features from the input data, the BiLSTM layer is especially well suited for sequences and time‐series analysis. Moreover, it can examine the data in both forward and backward directions to look for temporal correlation features. A batch normalization layer after each convolutional layer was used to avoid overfitting during training. We also used residual blocks with 2 CNN architectures in each block, as described in the study by Hannun et al. 23 The residual blocks help in avoiding vanishing gradients in deeper layers and improve the discrimination capability of the features. We examined how different numbers of residual blocks affected the performance, starting from 1, and began to see decreasing performance after 4 blocks. Our proposed network consisted of a far smaller number of residual blocks than the 16 in the study by Hannun et al, 23 plus the BiLSTM layer 34 with the hyperbolic tangent activation function (for hidden layers), fully connected layers, and the sigmoid activation function (for gates) with 128 hidden units. The default initialization, consisting of random weights, was applied. The CNN layers have filter dimensions of 16×32×1, 16×32×2, and 16×32×4, for 1st to 4th, 5th to 8 th, and 9th to 12th convolutional layers, respectively. There are dropout layers with a probability of 0.4 before each CNN layer. The adaptive moment estimation (Adam) optimizer with default parameters of β 1 = 0.9, β 2 = 0.999, and a mini batch size of 128 was used. The initial learning rate was 0.001, and it decreased every 4 epochs by the drop factor of 0.1. To make a better shock decision based on 8‐second samples of ECG data, manual tuning of hyperparameters was performed. Finally, the set of parameters that best fit the objective of the study was chosen.

---

### Deep physical neural networks trained with backpropagation [^c66d1a5c]. Nature (2022). Excellent credibility.

MNIST handwritten digit image classification with a hybrid physical–digital SHG-PNN

The design of the hybrid physical–digital MNIST PNN based on ultrafast SHG for handwritten digit classification (Fig. 4i–l) was chosen to demonstrate a proof-of-concept PNN in which substantial digital operations were co-trained with substantial physical transformations, and in which no digital output layer was used (although a digital output layer can be used with PNNs, and we expect such a layer will usually improve performance, we wanted to avoid confusing readers familiar with reservoir computing, and so avoided using digital output layers in this work).

The network (Supplementary Fig. 29) involves four trainable linear input layers that operate on MNIST digit images, whose outputs are fed into four separate channels in which the SHG physical transformation is used twice in succession (that is, it is two physical layers deep). The output of the final layers of each channel (the final SHG spectra) are concatenated, then summed into ten bins to perform a classification. The structure of the input layer was chosen to minimize the complexity of inputs to the pulse shaper. We found that the output second-harmonic spectra produced by the nonlinear optical process tended towards featureless triangular spectra if inputs were close to a random uniform distribution. Thus, to ensure that output spectra varied significantly with respect to changes in the input spectral modulations, we made sure that inputs to the pulse shaper would exhibit a smoother structure in the following way. For each of 4 independent channels, 196-dimensional input images (downsampled from 784-dimensional 28 × 28 images) are first operated on by a 196 by 50 trainable linear matrix, and then (without any nonlinear digital operations), a second 50 by 196 trainable linear matrix. The second 50 by 196 matrix is identical for all channels, the intent being that this matrix identifies optimal 'input modes' to the SHG process. By varying the middle dimension of this two-step linear input layer, one may control the amount of structure (number of 'spectral modes') allowed in inputs to the pulse shaper, as the middle dimension effectively controls the rank of the total linear matrix. We found that a middle dimension below 30 resulted in the most visually varied SHG output spectra, but that 50 was sufficient for good performance on the MNIST task. In this network, we also utilized skip connections between layers in each channel. This was done so that the network would be able to 'choose' to use the linear digital operations to perform the linear part of the classification task (for which nearly 90% accuracy can be obtained) and to thus rely on the SHG co-processor primarily for the harder, nonlinear part of the classification task. Between the physical layers in each channel, a trainable, element-wise rescaling was used to allow us to train the second physical layer transformations efficiently. That is, whereandare trainable parameters, andandare the input to the pulse shaper and the measured output spectrum from the previous physical layer, respectively.

---

### GraphBAN: an inductive graph-based approach for enhanced prediction of compound-protein interactions [^34cb3cd5]. Nature Communications (2025). High credibility.

CNN for protein sequence

As shown in Fig. 1a, the three-layer CNN block is tailored for protein feature extraction and employs an approach that initializes a learnable embedding matrix with representations for all 23 amino acids. This proactive initialization equips the network with essential biochemical information to discern amino acid sequences in the interactions. Subsequently, the CNN block specializes in extracting local residue patterns from the matrix of protein features, a pivotal step in capturing nuanced dependencies within protein sequences. Notably, this CNN architecture conceptualizes a protein sequence as a sequence of overlapping 3-mer amino acids, enabling it to capture both short-range and long-range interactions within the protein structure. Additionally, the network adheres to a maximum allowed length for protein sequences. The sequences exceeding this length are truncated, while shorter sequences are padded with zeros. This approach reflects a fusion of domain-specific knowledge and DL, showcasing its potential for decoding protein structures and features while handling varying sequence lengths. The protein encoder layer is defined aswhereandare the learnable weight matrices (filters) and bias vector in theCNN layer.denotes aswhereis the corresponding feature matrix for each protein sequenceandrepresents the ReLU activation function.

GCN

In the initialization phase of the atom nodes within GCN model, the GCNis employed to systematically assign a 128-dimensional integer vector to each atom in the molecules. This vector encapsulates a comprehensive array of chemical properties essential for defining the atom's characteristics and its interactions within the molecule. These properties encompass several characteristics: the atom type, which specifies the chemical element of the atom; atom degree, reflecting the number of direct covalent bonds with neighboring atoms; and the number of implicit hydrogen atoms, accounting for hydrogen atoms assumed to be bonded but not explicitly represented. Additional properties include atom hybridization, indicating orbital mixing that impacts bonding configurations; radical electrons, denoting unpaired electrons contributing to reactivity; and formal charge, representing the atom's electric charge from electron surplus or deficit. Lastly, total hydrogen atoms encompass both explicit and implicit atoms, and aromaticity identifies atoms in an aromatic ring with shared electrons in a stable configuration. This detailed vector representation equips the model with a nuanced understanding of each atom's potential behavior and interactions, thereby enhancing the predictive capability of the GCN model regarding molecular properties and activities.

---

### An optical neural chip for implementing complex-valued neural network [^5caad450]. Nature Communications (2021). High credibility.

Handwriting recognition by a complex-valued multilayer perceptron

A single complex layer implemented on ONC is employed to build a multilayer perceptron (MLP) to classify handwritten digits in the dataset MNIST. The dataset is split into training and testing sets. Our model is trained on the entire training set, and 200 instances in the testing set are used to validate the trained model on-chip. As shown in Fig. 6a, the network consists of an input layer W in, a hidden layer W and an output layer W out. The neuron numbers in the three layers are 4, 4 and 10, respectively. The input 28 × 28 grayscale image is reshaped into a 784 × 1 vector and compressed by the input layer into four features, which are then fed to the 4 × 4 hidden layer. The output layer maps the four hidden outputs to 10 classes, representing digits from 0 to 9. The simulation model is built in TensorFlow and trained by RMSPropOptimizer, with a learning rate of 0.005, a training period of 100 iterations and a batch size of 100. The activation functions used in the real model and the complex model is ReLU and ModReLU, respectively (see Supplementary Note 14). The hidden layer is implemented on ONC, while the input layer (784 × 4) and the output layer (4 × 10) are executed electrically. Theoretically, the input and output layers are implementable on our ONC by decomposing a large matrix into multiple small matrices. Considering the practical workload and the same principles, we only focus on the proof-of-principle implementation of the hidden layer. Figure 6b shows the training process of complex-valued and real-valued models with same dimensions. The complex-valued model achieves a training accuracy of 93.1% and a testing accuracy of 90.5%, while the real-valued model obtains a training accuracy of only 84.3% and a testing accuracy of 82.0%. Their confusion matrices on testing instances are shown in Fig. 6c, d. The complex-valued neural network significantly outperforms its real-valued counterpart. In addition, faster convergence is observed in the complex-valued model during the training process.

---

### Diagnosis, management and treatment of the Alport syndrome-2024 guideline on behalf of ERKNet, ERA and ESPN [^45eede36]. Nephrology, Dialysis, Transplantation (2025). High credibility.

Regarding diagnostic investigations for Alport syndrome, more specifically with respect to hearing assessment, ERA/ERN ERKNet/ESPN 2025 guidelines recommend to consider obtaining a hearing evaluation at diagnosis or upon reaching adulthood in female patients with X-linked alport syndrome, and then every 5 years in the absence of hearing loss symptoms.

---

### Uniform labeling of blocks and slides in surgical pathology: guideline from the college of American pathologists pathology and laboratory quality center and the national society for histotechnology [^bddcc72d]. Archives of Pathology & Laboratory Medicine (2015). Medium credibility.

Data extraction and management — one reviewer extracted data elements from included articles/documents into standard formats and tables using systematic review database software (DistillerSR, Evidence Partners Inc, Ottawa, Canada), a second reviewer confirmed accuracy and completeness, discrepancies were resolved by discussion with the methodologist, and an EndNote (Thomson Reuters, Carlsbad, CA) bibliographic database was established to track all literature identified and reviewed during the study.

---

### Omnireg-gpt: a high-efficiency foundation model for comprehensive genomic sequence understanding [^3181e5c3]. Nature Communications (2025). High credibility.

Formally, the attention mechanism for a query at position I in the sequence involves the following operations:where W q, W k, and W v are the weight matrices for the query, key and value, respectively. Each query Q interacts with keys K j and values V j within its local window, reducing the number of comparisons from the entire sequence length L to the window size w.

The attention scores are the computed as:

This localized attention computation effectively constrains the operation to a much smaller subset of the input. Specifically, by partitioning the input sequences of Length L into non overlapping windows with size w, the effective sequence length for attention computation is reduced from L to w. Meanwhile, the batch dimension is transformed from bs to bs*L/w, where each window is processed independently.

Thereby reducing computational complexity:where L/w represents the number of windows across the sequence, and each window involves w 2 × d operations. This formulation provides an efficient way to compute attention with reduced computational burden. Moreover, as the number of layers increases in the OmniReg-GPT model, the receptive field for each query increases to cover a progressively wider context without a corresponding increase in computational complexity per layer. This expansion allows each query to integrate information from distant areas.

Within the global blocks of the OmniReg-GPT model, we employ the classic attention mechanism, which considers the entire sequence to calculate attention scores. This global approach ensures comprehensive context integration. To enhance computational efficiency across all attention functions in our model, we utilize FlashAttention, an optimized algorithm designed to accelerate the calculation of attention mechanisms. In the final layer of our model, we compute a single vector representation by taking the mean of all token embeddings, which is then used for downstream applications.

---

### A parallel attention-augmented bilinear network for early magnetic resonance imaging-based diagnosis of Alzheimer's disease [^63878b32]. Human Brain Mapping (2022). Medium credibility.

2.1.2 Parallel attention‐augmented blocks

Most existing convolutional neural networks are constructed by serially stacking convolutional layers. Although a network's capacity generally increases as the number of layers increases, the network also tends to overfit a small dataset because of huge amount of parameters. In our framework, we devise the parallel attention‐augmented blocks (pA‐blocks) to solve this problem. Our pA‐blocks are based on the double attention block (A 2 ‐block) proposed by Chen, Kalantidis, Li, Yan, and Feng, which aims to effectively capture the global information and distribute it to every location in a two‐step attention manner. In this way, each location in the feature map receives customized global information as complements, thereby enabling the network to learn more complex relationships. Different from the original A 2 ‐block, we modify it to facilitate a parallel structure with augmented representation spaces and less trainable parameters.

The structure of the modified A 2 ‐block is shown in Figure 2.denotes the input feature for the 3D convolutional layer, wheredenotes the number of channels, andare the spatial dimensions. The block contains three 3D convolutional layers with 1 × 1 × 1 kernel and m output channels. After necessary reshaping and transposing operations, we have feature maps generated by convolutional layers:where eachis a dhw ‐dimensional row vector, andIn the first attention step, global representations are gathered effectively by second‐order attention pooling.is the output of the first attention step. In the second attention step, global features are adaptively distributed to each spatial location.is the output of the second attention step. Specifically:In the original implementation (Chen et al.), is fed into another convolutional layer to expand the number of channels and then encoded back to inputvia element‐wise addition. By contrast, we (Figure 2) remove the extra convolutional layer that projects the representation back into the input space. The final output of the A 2 ‐block is generated by encodingback tovia element‐wise addition. This modification results in fewer parameters and less complexity of the block and further reduces the number of parameters in the last classifier. Given the numbers of output channels of the block's three convolutional layers (i.e. m) and the backbone network (i.e. 128), the number of parameters within the modified A 2 ‐block is calculated by 128 × 3 × m.

---

### Standards of care in diabetes – 2025 [^5eb43e21]. Diabetes Care (2025). High credibility.

Regarding follow-up and surveillance for diabetes mellitus type 1, more specifically with respect to continuous glucose monitoring, ADA 2025 guidelines recommend to implement early continuous glucose monitoring in adult patients with T1DM to improve glycemic outcomes and QoL and minimize hypoglycemia.

---

### Uniform labeling of blocks and slides in surgical pathology: guideline from the college of American pathologists pathology and laboratory quality center and the national society for histotechnology [^e6d14a42]. Archives of Pathology & Laboratory Medicine (2015). Medium credibility.

Supplemental Table 4 — Grades for strength of evidence defines Convincing, Adequate, and Inadequate criteria based on study design/execution, challenges, consistency, and generalizability. Convincing includes either two or more Level 1 a or 2 studies (study design and execution) that had an appropriate number and distribution of challenges and reported consistent and generalizable results, or one Level 1 or 2 study that had an appropriate number and distribution of challenges and reported generalizable results. Adequate is defined as two or more Level 1 or 2 studies that lacked the appropriate number and distribution of challenges OR were consistent but not generalizable. Inadequate is defined as combinations of Level 1 or 2 studies that show unexplained inconsistencies OR combinations of one or more lower quality studies (Level 3 or 4) OR expert opinion. Footnotes clarify that challenges are based on number of possible response categories and required confidence in results, that consistency can be assessed formally by testing for homogeneity or less formally using central estimates and range of values when data are limited, and that generalizability is the extension of findings and conclusions from one study to other settings.

---

### EAU guidelines on urolithiasis [^62156a38]. EAU (2025). High credibility.

Regarding diagnostic investigations for bladder stones, more specifically with respect to initial evaluation, EAU 2025 guidelines recommend to obtain ultrasound as first-line imaging in patients with symptoms suggestive of a bladder stone.

---

### Standards of care in diabetes – 2025 [^31be65a6]. Diabetes Care (2025). High credibility.

Regarding diagnostic investigations for hypoglycemia, more specifically with respect to screening for cognitive impairment, ADA 2025 guidelines recommend to simplify diabetes treatment plans as much as possible for patients with cognitive impairment and tailor to minimize the risk of hypoglycemia.

---

### Standards of care in diabetes – 2025 [^9a2ec52c]. Diabetes Care (2025). High credibility.

Regarding specific circumstances for hypoglycemia, more specifically with respect to elderly patients, ADA 2025 guidelines recommend to individualized goal levels for the treatment of hypertension in most older patients.

---

### Space-time-coding digital metasurfaces [^0447f2a0]. Nature Communications (2018). Medium credibility.

More specifically, we consider a coding metasurface with 8 × 8 elements and time-coding sequences with a length of 8. First, we address the beam-steering design in one plane at the central frequency. Hence, we only need to study time-coding sequences of eight elements, and each column of elements has the same digital code. Figure 4a, b shows the 2-bit space-time-coding matrix, in which the red, yellow, green, and blue dots represent "0", "1", "2", and "3" digits, representing 0°, 90°, 180°, and 270° phase responses, respectively (the criterion for selecting those eight sets of time-coding sequences is detailed in Supplementary Note 7). The equivalent amplitudes and phases of those eight coding elements at different harmonic frequencies are shown in Fig. 4c, d, respectively. We can clearly observe that the designed space-time-coding matrix effectively results in equivalent 3-bit responses, "0 ′ " (−135°), "1 ′ " (−90°), "2 ′ " (−45°), "3 ′ " (0°), "4 ′ " (45°), "5 ′ " (90°), "6 ′ " (135°), and "7 ′ " (180°), and an equivalent phase gradient at the central frequency. According to the generalized Snell's law, this phase discontinuity will steer a normally incident wave by an angle of 14.5°. The corresponding 2D scattering pattern at the central frequency and the scattering patterns at different harmonic frequencies are displayed in Fig. 4e, f. It can be seen that the main beam at the central frequency points at θ = −14.5°, and the scattered powers at the harmonic frequencies are much smaller, which provides good sideband suppression. Moreover, we compare the beam-steering performance of the original 2-bit coding "0-0-1-1-2-2-3-3" with the equivalent 3-bit coding "0 ′ -1 ′ -2 ′ -3 ′ -4 ′ -5 ′ -6 ′ -7 ′ ", as shown in Fig. 4g. It is observed that the former scheme could realize a steering angle of 14.5°, but accompanied by high sidelobes, while the latter scheme attains the steering with much lower sidelobes and only a little gain reduction attributable to the sideband power losses. Similar examples of equivalent 3-bit coding for large-angle beam steering are illustrated in Supplementary Figures 6c and 6d.

---

### AGA clinical practice guideline on management of gastroparesis [^9e865be1]. Gastroenterology (2025). High credibility.

Regarding diagnostic investigations for gastroparesis, more specifically with respect to gastric emptying studies, methodology, AGA 2025 guidelines recommend to perform a 4-hour study in all cases, as some patients with normal emptying at 2 hours may be reclassified as delayed at 4 hours.

---

### Symmetries and cluster synchronization in multilayer networks [^2ae2face]. Nature Communications (2020). High credibility.

Discussion

In this paper we study analytically, numerically, and experimentally the general problem of cluster synchronization (CS) in multilayer networks; these systems are composed of heterogeneous components that may interact in multiple ways. We first present a general set of equations that describes the dynamics of a multilayer network. We then define for the first time the group of symmetries of a multilayer network and explain how to compute it. An analysis of several datasets of real multilayer networks shows they possess large number of symmetries.

We investigate the stability of the CS patterns which correspond to the orbits of the multilayer network. The symmetries of each layer as well as the particular pattern of interlayer connections determine the CS patterns in multilayer networks; we consequently describe how each layer's symmetry group relates to the symmetry groups of other layers through interlayer connections. The interplay between the intralayer and interlayer couplings enables distinction between symmetries that affect the types of allowed dynamics in the other layers (DLS) and those that do not (ILS.) The latter form a normal subgroup of the full symmetry group. In particular, a symmetry-breaking bifurcation, associated with the ILS subgroup, breaks up a clustered pattern in one layer of the multilayer network without altering the possible clusters in the other layers.

With an IRR change of coordinates we decouple the stability problem into several simpler (lower-dimensional) problems. First, we decouple perturbations along directions parallel to the synchronization manifold from those transverse to it; the latter determine the stability of the clustered motion. Second, we decouple the equations for the transverse perturbations into several independent blocks; each block corresponds to the stability of either an individual cluster or a set of intertwined clusters. When two or more clusters (which may belong to different layers) are intertwined, they are either all stable or all unstable. We see that dependent layer symmetries yield blocks of a different structure than those arising in the study of single layer networks, which we show has a profound effect on the stability of the clusters involving these symmetries. In particular, we show analytically for a specific class of networks that DLS clusters are more difficult to synchronize than in the case in which the systems in different layers are of the same type, which is also confirmed numerically in simulations involving multilayer networks of Van der Pol, Lorenz, and Roessler oscillators (Supplementary Note 5).

---

### Accurate de novo peptide sequencing using fully convolutional neural networks [^2cc64077]. Nature Communications (2023). High credibility.

Optimizing hyper-parameters for PepNet's main architecture

When designing the primary architecture of PepNet, crucial hyperparameters such as the number of TCN blocks, kernel sizes, and the number of channels were determined based on the trade-off between the runtime and the performance. We employed five TCN blocks because more blocks show no improvement in performance, as the receptive field of five TCN blocks is already large enough, as shown in Supplementary Fig. S11. Regarding the number of convolutional channels, we observed that the performance of PepNet continues to improve as it increases, although the improvement gradually diminishes. We finally selected the number of layers as [192, 288, 384, 576, 768] for each TCN block, since more layers will be too computationally expensive. Also, Supplementary Fig. S11 shows the performance of PepNet under different kernel sizes of TCN layers: the performance hardly improves after the kernel size reaches 5*2 5.

Loss functions and auxiliary tasks

Well-designed loss functions are critical for achieving optimal training performance in deep neural networks. We employ the average cross-entropy on non-padding positions (termed masked_ce_loss) as the major part of the loss, while monitoring the average accuracy on non-padding positions to select the optimal model. As emphasized in the previous sections, we do not enforce the model to output a peptide sequence with an exactly matched precursor mass, which is, however, an important constraint for de novo peptide sequencing. Thus we add the mean absolute error (MAE) of the predicted sequence and the mass of the true sequence to the final loss functions, i.e. loss = masked_ce_loss+ c * MAE. Here, the weight c should be sufficiently small to prevent the MAE part from dominating the loss during the initial training stages, when the model has not learned enough knowledge to generate a reasonable sequence. In practice, we manually select c to ensure that c *MAE does not exceed 10% of the total loss.

---

### Uniform labeling of blocks and slides in surgical pathology: guideline from the college of American pathologists pathology and laboratory quality center and the national society for histotechnology [^7979b709]. Archives of Pathology & Laboratory Medicine (2015). Medium credibility.

Uniform labeling of blocks and slides — grades for strength of recommendations are defined across four designations. Strong Recommendation states to "Recommend For or Against a particular glass slide or paraffin block labeling practice (Can include must or should)" and is "Supported by high (convincing) or intermediate (adequate) quality of evidence and clear benefit that outweighs any harms". Recommendation uses "Recommend For or Against a particular glass slide or paraffin block labeling practice (Can include should or may)" with rationale of "Some limitations in quality of evidence (intermediate [adequate] or low [inadequate]), balance of benefits and harms, values, or costs but panel concludes that there is sufficient evidence to inform a recommendation". Expert Consensus Opinion uses "Recommend For or Against a particular glass slide or paraffin block labeling practice (Can include should or may)" when there are "Serious limitations in quality of evidence (low [inadequate] or insufficient), balance of benefits and harms, values or costs, but panel consensus is that a statement is necessary". No Recommendation is applied when there is "No recommendation for or against a particular block or slide labeling practice", with rationale of "Insufficient evidence, confidence, or agreement to provide a recommendation".

---

### Unveiling chromatin dynamics with virtual epigenome [^b8479490]. Nature Communications (2025). High credibility.

HiConformer model architecture

HiConformer model is developed with the TensorFlow framework. The HiConformer model is a multi-task CNN-Transformer architecture for accurate chromatin state and structure prediction. The initial components of HiConformer are the sequence encoder and epigenetic encoder. The sequence encoder, consisting of five CNN blocks, is tailored for the effective extraction of DNA sequence information. Each CNN block adheres to a uniform structural pattern, beginning with a Conv2D layer, followed by batch normalization and Gaussian error linear unit (GELU) activation, and concluding with max pooling. The Conv2D layers are configured as follows: the first layer with a 1 × 5 kernel and 64 filters; the second and third layers, each with a 1 × 5 kernel and 128 filters; the fourth layer with a 1 × 5 kernel and 256 filters; and the fifth layer with a 1 × 2 kernel and 256 filters. Similarly, the epigenetic encoder is also composed of five CNN blocks, designed for the extraction of epigenetic information. Its structure mirrors that of the sequence encoder, starting with a Conv2D layer, proceeding with batch normalization and GELU activation, and concluding with max pooling. The Conv2D layers in the epigenetic encoder are as follows: the first layer has a 1 × 5 kernel with 64 filters, the second layer also has a 1 × 5 kernel, but with 128 filters, the third layer utilizes a 1 × 4 kernel with 128 filters, the fourth layer employs a 1 × 2 kernel with 256 filters, and the fifth layer features a 1 × 1 kernel, also with 256 filters. The output from sequence and epigenetic encoder will be concatenated as a "genomic embedding" and passed to the Hi-C prediction task branch and ChromHMM prediction task branch.

---

### Colorectal cancer screening and prevention [^c270f773]. American Family Physician (2025). High credibility.

Regarding screening and diagnosis for colon cancer, more specifically with respect to indications for screening, general population, aged 76–85 years, AAFP 2025 guidelines recommend to consider obtaining screening for CRC in adults aged 76–85 years at average risk based on overall health status, prior screening history, and patient preferences.

---

### Colorectal cancer screening and prevention [^390b568a]. American Family Physician (2025). High credibility.

Regarding screening and diagnosis for colon cancer, more specifically with respect to indications for screening, high-risk individuals, family history, AAFP 2025 guidelines recommend to obtain CRC screening in patients with ≥ 1 first-degree relatives with CRC or adenomatous polyps, starting at 40 years of age or 10 years before the age of the youngest relative at the time of their diagnosis.

---

### A consensus statement on acromegaly therapeutic outcomes [^8115910b]. Nature Reviews: Endocrinology (2018). Medium credibility.

Regarding follow-up and surveillance for acromegaly, more specifically with respect to management of incomplete treatment response, ACC 2018 guidelines recommend to individualize treatment on the basis of the presence or absence of clinically relevant residual tumor and impaired glucose tolerance if biochemical control is not achieved after administering the maximal dose of first-generation somatostatin receptor ligands.

---

### ImageDoubler: image-based doublet identification in single-cell sequencing [^48401954]. Nature Communications (2025). High credibility.

Input data pre-processing and augmentations

Input block images were scaled to 600 by 600 pixels. The Faster-RCNN anchor sizes were determined from the actual bounding box dimensions in 100 randomly selected block images at the scaled resolution, where the average height was 26.91 pixels, and the average width was 31.24 pixels. To ensure the anchors accommodated the range of object sizes, we set the base anchor sizes to 10, 20, and 40 pixels, where 10 was used for capturing the smaller objects of cells, and 40 was used for the larger ones.

Two data augmentations — image resizing and random horizontal flipping — were applied during batch preparation for the models. The image resizing involved randomly altering the width-height ratio from 0.7:1.3 to 1.3:0.7 (the original ratio is 1:1) and then scaling the image by a factor ranging from 0.25 to 2. The resized images were further adjusted by either padding with gray bars or cropping randomly to fit the 600×600 pixel input size. The horizontal flipping was applied post-resizing, with each image having a 50% probability of being flipped.

All these augmentations were implemented using Python's Pillow (PIL) package. Resized images were interpolated using cubic spline interpolation (PIL. Image.BICUBIC). Incorporating these augmentations significantly improved the model's performance, particularly in doublet detection accuracy (Supplementary Fig. 11).

Network structure and training

The backbone of the network was similar to the structure of classic Faster-RCNN. We selected it for the following reasons. First, Faster-RCNN had a relatively simple structure and benefited from extensive community support, including pre-trained models, making it flexible and easier to transfer to other tasks. Second, Faster-RCNN maintained a good balance between speed and detection accuracy. It outperformed some one-stage models, such as early versions of YOLO, while demonstrating better or comparable speed compared to other two-stage models like Mask R-CNN. Faster-RCNN consisted of a ResNet-50 network to extract features from the inputs, which had pre-trained on the PASCAL Visual Object Classes dataset, a Region Proposal Network (RPN) to generate the regions that may contain the cells on the extracted feature map, a Region of Interest pooling layer to align the proposed regions and a classifier head that predicted object class and bounding boxes from these regions.

---

### Standards of care in diabetes – 2025 [^28c09853]. Diabetes Care (2025). High credibility.

Regarding diagnostic investigations for diabetes mellitus type 1, more specifically with respect to general principles, ADA 2025 guidelines recommend to ensure a person-centered communication style using culturally sensitive and strength-based language and active listening, eliciting individual preferences and beliefs, and assessing literacy, numeracy, and potential barriers to care to optimize health outcomes and health-related QoL.

---

### Light-enhanced molecular polarity enabling multispectral color-cognitive memristor for neuromorphic visual system [^9c7b8768]. Nature Communications (2023). High credibility.

Neural network simulation

The feed-forward process of CNN has two parts of operations: feature extraction and classification. For the feature extraction, pre-processed color images, of which each pixel is separated into three RGB color channels, are convoluted with kernels, generating the feature maps, and their sizes are reduced through the pooling layer. To describe the cases having the overlapped RGB components between the colors after the pre-process, the dataset of the three-color channels in the CIFAR-10 is modified using Python language. The CIFAR-10 color input images and convolutional kernels are composed of 32 × 32 and 3 × 3 pixels. Each pixel of the input image and the kernel is represented as a voltage (V) and a synaptic weight (W). The synaptic weight is represented by the conductance difference of two equivalent memristors to demonstrate both positive and negative weight values, and the weights are updated by the unidirectional or bidirectional method as explained in Supplementary Note 11. The results of the convolution are represented as the current (I) in the feature map and converted to the voltage by the ReLU activation function. Pooling reduces the size of the map with a highlight. This cycle is repeated three times. The dimensions of the kernels for 1st, 2nd, and 3rd convolution layers are 3 (the number of channels) × 3 (kernel width) × 3 (kernel height) × 128 (the number of kernels), 128 × 3 × 3 × 256, and 256 × 3 × 3 × 512. The number of feature maps in 1st, 2nd, and 3rd convolution layers are 128, 256, and 512. After 1st, 2nd, and 3rd pooling, the dimensions of the feature maps are reduced from 32 × 32, 16 × 16, and 8 × 8 to 16 × 16, 8 × 8, and 4 × 4. After the final 3rd pooling layer, the voltage signals are flattened to a 1 × 8192 array and transferred to the input layer of the FCLs for the classification. The FCLs are composed of 8192 input neurons, 1024 hidden neurons, and 10 output neurons. In software-based artificial neural network (ANN), both positive and negative values are utilized for the weight values of individual cells. However, in hardware crossbar array ANN, the conductance of the memristor (G), which represents the synaptic weight (W), cannot have negative values. Therefore, to demonstrate both positive and negative weight values in hardware ANNs, the synaptic weight of each cell is represented by the conductance difference (W = G + – G −) of a pair of two equivalent synapses, of which the conductance values are represented by G + and G − (G + > 0 and G − > 0). The input voltage signals generate the current values by the weighted-sum operation with the synaptic weights between input and hidden neurons. The current signals are converted into the voltage signals by the ReLU activation function and added to the hidden layer as input signals. Finally, these voltage signals generate the current at the output layer by the weighted-sum operation with the synaptic weights between hidden and output neurons. The backpropagation operation is performed by comparing output layer value (V o), obtained by converting the current at the output layer to the voltage using the ReLU activation function, and label value (K). Synaptic weights are updated from output layer to the first convolution layer by minimizing the error between output and label values.

---

### Society of Dermatology Hospitalists supportive care guidelines for the management of Stevens-Johnson syndrome / toxic epidermal necrolysis in adults [^7728e795]. Journal of the American Academy of Dermatology (2020). High credibility.

Regarding medical management for Stevens-Johnson syndrome, more specifically with respect to ocular care, conservative management, SDH 2020 guidelines recommend to rinse the eyes every 2–24 hours with sterile saline.

---

### Petascale pipeline for precise alignment of images from serial section electron microscopy [^c5389fe7]. Nature Communications (2024). High credibility.

Sequential alignment

With an error correction procedure in place, we can now define sequential alignment. It starts by fixing the first section of the block, so that its field is the identity (section indices relative to the first section), To provide multiple targets for voting on the next few sections in the sequence, we align a few previous sections to the first section, where k ∈ {1,…, n − 1}. From multiple measurements of the displacement field, we use voting to produce the final consensus field that aligns a given section to the start of the block,

Block Alignment

Starter sections, B, were manually selected by reviewing every tenth section at 1024 nm resolution, and identifying the nearest section that was deemed free of large defects. Block sizes were adjusted accordingly. Section indices are relative to the first section of the entire dataset.

The displacement between a neighboring pair of blocks, or stitching field, was computed by registering the overlapping n sections of the two blocks, S = { B i + k: k ∈ {0,…, n − 1}}, then voting over the set of n fields that were produced. Blocks were stitched together by composing the displacement field of a section computed during block alignment with preceding stitching fields, such thatThe stitching fields are adjusted, or "decayed", based on their distance from z. For a distance n, a stitching field is decayed such thatwhere G σ (n) is a blurring kernel with varying standard deviation that reduces the high-frequency components of the field with distance, and β (n) is a function that returns a coefficient to dampen the effect of the field with distance. In our experiments, we usedwith c = 0.2 (measured in pixels) and d = 200 (measured in sections). The effect of applying a varying blurring kernel on each stitching field was generated by trilinearly interpolating a MIP hierarchy of the field generated using an iterative 2 × 2 box filter.

---

### Estimating uncertainty of geographic atrophy segmentations with Bayesian deep learning [^ab7cf03e]. Ophthalmology Science (2024). Medium credibility.

UNet Model Architecture

All the models described in this paper were built upon the same UNet architecture, which was similar to the original U-Net architecture. It consisted of a contractive path, in which the feature maps were reduced in spatial dimension, followed by an expansive path, in which the feature maps were expanded back to their original size. The contractive path consisted of convolutional blocks alternating with maxpool (downsampling) operations with 2 × 2 kernel and stride 2. Each convolutional block consisted of two 3 × 3 convolutions, each followed by a rectified linear unit activation. The number of output channels of the convolutions started at 32 and doubled after each downsampling step. There were 5 total convolutional blocks in the contractive path, alternated with 4 downsampling layers. The expansive path consisted of 4 steps, each consisting of 2× upsampling by bilinear interpolation, concatenation with the downsampling remainder of the corresponding level of the contractive path, and a convolutional block as above. In the concatenation step, we concatenated the remainder, which can be written aswhere "upsample" is the 2× bilinear upsampling and "downsample" is the 2 × 2 maxpool operation. After the expansive path, there was a dropout layer with drop probability 0.2 and 2 final convolution layers. The penultimate convolution used a 3 × 3 kernel with 8 output channels and was followed by a rectified linear unit activation; the final convolution used a 3 × 3 kernel with 1 output channel and had a final activation function, which can be written asand which ensured the output was between 0 and 1. For the UNet-Drop models, an additional dropout layer with drop probability 0.5 was added after the last 3 convolutional blocks of the contractive path and the first 2 convolutional blocks of the expansive path, as shown in Figure 1. Dropout was also left on during inference.

---

### Standards of care in diabetes – 2025 [^1e25a6d5]. Diabetes Care (2025). High credibility.

Regarding nonpharmacologic interventions for diabetes mellitus type 1, more specifically with respect to physical activity, ADA 2025 guidelines recommend to advise most adult patients T1DM to engage in ≥ 150 minutes of moderate- to vigorous-intensity aerobic activity per week, spread over at least 3 days/week, with no more than 2 consecutive days without activity. Consider advising shorter durations (minimum 75 min/week) of vigorous-intensity or interval training for more physically fit patients.

---

### EASL clinical practice guidelines: vascular diseases of the liver [^40b1ee88]. Journal of Hepatology (2016). Medium credibility.

Regarding therapeutic procedures for Budd-Chiari syndrome, more specifically with respect to TIPS, EASL 2016 guidelines recommend to perform TIPS implantation, unless contraindicated, as the first-line portal derivative technique.

---

### World guidelines for falls prevention and management for older adults: a global initiative [^923dcf2f]. Age and Ageing (2022). High credibility.

Regarding classification and risk stratification for falls in the elderly, more specifically with respect to risk prediction, general principles, WFG 2022 guidelines recommend to obtain an evaluation of concerns about falling in a multifactorial falls risk assessment of older adults.
Use a standardized instrument to evaluate concerns about falling, such as the Falls Efficacy Scale International or Short Falls Efficacy Scale International in community-dwelling older adults.

---

### Clotrimazole [^b5a7188a]. FDA. Low credibility.

The dosage of clotrimazole OTIC for treatment of otomycosis in adults is 1 vial OTIC BID for 14 days (1%/0.17 mL)

---

### 3D printing with a 3D printed digital material filament for programming functional gradients [^d381018f]. Nature Communications (2024). High credibility.

Additive manufacturing, or 3D printing attracts growing attention as a promising method for creating functionally graded materials. Fused deposition modeling (FDM) is widely available, but due to its simple process, creating spatial gradation of diverse properties using FDM is challenging. Here, we present a 3D printed digital material filament that is structured towards 3D printing of functional gradients, utilizing only a readily available FDM printer and filaments. The DM filament consists of multiple base materials combined with specific concentrations and distributions, which are FDM printed. When the DM filament is supplied to the same printer, its constituent materials are homogeneously blended during extrusion, resulting in the desired properties in the final structure. This enables spatial programming of material properties in extreme variations, including mechanical strength, electrical conductivity, and color, which are otherwise impossible to achieve with traditional FDMs. Our approach can be readily adopted to any standard FDM printer, enabling low-cost production of functional gradients.

---

### Standards of care in diabetes – 2025 [^2b2eab8f]. Diabetes Care (2025). High credibility.

Regarding quality improvement for diabetes mellitus type 1, more specifically with respect to use of diabetes technology, ADA 2025 guidelines recommend to offer continuous glucose monitoring to patients with T1DM starting early in the disease, even at time of diagnosis.

---

### Methodologies for the development of CHEST guidelines and expert panel reports [^3afb239e]. Chest (2014). Medium credibility.

CHEST grading of recommendations — the grading system encompasses two dimensions: balance of benefits to harms, risks, or burdens (including confidence in effect estimates) and the level of evidence supporting the recommendation; recommendations are strong when benefits clearly outweigh harms or vice versa and may include a strong negative recommendation; strong recommendations (Grade 1) use the language "we recommend", whereas when benefits and harms are closely balanced and further research could change the direction or strength, it is considered weak and statements read, "we suggest"; when benefits clearly outweigh harms most patients would choose the intervention and the recommendation is Grade 1, but with considerable variability in preferences recommendations are weaker, ie, Grade 2; the other dimension reflects the quality of the body of evidence scored A, B, or C.

---

### EASL clinical practice guidelines on the management of cystic liver diseases [^30aa15d9]. Journal of Hepatology (2022). High credibility.

Regarding diagnostic investigations for polycystic liver disease, more specifically with respect to diagnostic imaging, EASL 2022 guidelines recommend to recognize that the number of lesions (solitary or multiple) and architecture (simple or complex cyst) are key elements in the description of hepatic cysts.

---

### ESICM / ESCMID task force on practical management of invasive candidiasis in critically ill patients [^551f3d5b]. Intensive Care Medicine (2019). High credibility.

Regarding medical management for invasive candidiasis, more specifically with respect to step-down therapy, ESCMID/ESICM 2019 guidelines recommend to discontinue antifungal therapy in patients with suspected (but not proven) invasive candidiasis with negative blood cultures and/or other negative culture specimens taken from suspected infectious foci before starting antifungal therapy.

---

### Management of severe peri-operative bleeding: guidelines from the European Society of Anaesthesiology and Intensive Care: second update 2022 [^c75f5791]. European Journal of Anaesthesiology (2023). High credibility.

Regarding diagnostic investigations for platelet transfusion, more specifically with respect to evaluation of thrombocytopenia, ESAIC 2023 guidelines recommend to do not use the platelet count alone as a bleeding predictor. Take into account the cause of thrombocytopenia, platelet function, and patient- and procedure-related risk of bleeding when deciding on the need for prophylactic platelet transfusion.

---

### Roflumilast (Zoryve) [^e2de6cd7]. FDA (2024). Medium credibility.

The dosage of roflumilast TOP for treatment of plaque psoriasis in adults is 1 application(s) TOP daily (0.3% cream or foam)

---

### Standards of care in diabetes – 2025 [^27c0974d]. Diabetes Care (2025). High credibility.

Regarding specific circumstances for diabetes mellitus type 1, more specifically with respect to pediatric patients (physical activity), ADA 2025 guidelines recommend to advise physical activity in all young patients with T1DM with the goal of 60 minutes of moderate- to vigorous-intensity aerobic activity daily, with vigorous muscle-strengthening and bone-strengthening activities at least 3 days per week.

---

### 2023 ESC guidelines for the management of cardiovascular disease in patients with diabetes [^24784e3d]. European Heart Journal (2023). High credibility.

Regarding medical management for diabetes mellitus type 2, more specifically with respect to prevention of CVD, glucose-lowering medications, ESC 2023 guidelines recommend to consider initiating metformin to reduce cardiovascular risk in patients with T2DM without ASCVD or severe target-organ damage at low, moderate,
high, or very high risk.

---

### 2025 ESC guidelines for the management of myocarditis and pericarditis [^b050a21b]. European Heart Journal (2025). High credibility.

Regarding nonpharmacologic interventions for myocarditis, more specifically with respect to physical activity restrictions, ESC 2025 guidelines recommend to restrict physical exercise until remission for at least 1 month in athletes and non-athletes after inflammatory myopericardial syndrome, using an individualized approach to accelerate recovery.

---

### Portal hypertensive bleeding in cirrhosis: risk stratification, diagnosis, and management: 2016 practice guidance by the American Association for the Study of Liver Diseases [^a9393a57]. Hepatology (2017). Medium credibility.

Regarding classification and risk stratification for liver cirrhosis, more specifically with respect to risk stratification, AASLD 2017 guidelines recommend to stratify patients with compensated cirrhosis into mild portal hypertension and clinically significant portal hypertension, an entity predicting the development of more advanced stages.

---

### Spatial ultrasound modulation by digitally controlling microbubble arrays [^6a7207e0]. Nature Communications (2020). High credibility.

Fig. 1
Schematic of the spatial ultrasound modulation (SUM) based on microbubble patterns.

a A microbubble can effectively block the acoustic transmission since its acoustic impedance differs significantly from the surrounding liquid. A pattern of microbubbles can therefore spatially modulate the incident plane acoustic wave and give it a complex wavefront. b The microbubble pattern encodes the binary amplitude kinoform (hologram) of the target acoustic field. Refreshing the microbubble pattern enables dynamic spatial ultrasound modulation.

For example, even a 20-μm gas layer leads to a negligibly small transmission coefficient (on the order of 10 −7), considering a 10-MHz acoustic wave (wavelength 150 µm). This can be seen from the power transmission coefficient for an acoustic wave at normal incidence through a plain layer:where δ and k L are the layer thickness and wavenumber in the layer material, respectively; Z, ρ, and c are acoustic impedance, density, and speed of sound, respectively; the subscripts L and M indicate the layer and the surrounding host medium. The sound speed in water (ρ M ~ 1000 kg m −3) is c M ~ 1500 m s −1 and in air c L ~ 343 m s −1 at atmospheric pressure (ρ L ~ 1.23 kg m −3). As the ratio of acoustic impedances increases, the wave is increasingly reflected at the interface, and therefore, less energy is transmitted through the layer. Since air blocks ultrasound so well, we now need to find a way to generate programmable on-demand microbubble patterns.

Our SUM device architecture consists of a CMOS chip placed on top of an acoustic transducer, as shown in Fig. 2. A liquid film of electrolyte is sandwiched between the chip surface and a conveyor film. The CMOS chip surface has 10,000 individually addressable electrodes (70 μm by 70 μm gold pads in a 100 µm by 100-µm raster). Positioned next to the chip is a copper electrode, which serves as the anode. A switchable DC power supply provides a potential difference between the copper electrode (+5 V) and the 10,000 gold electrode pads of the CMOS chip. Once the DC power is switched to a CMOS pixel, the electrolysis of the surrounding water solution generates hydrogen and oxygen gas, respectively, at the gold and copper electrodes. As we will see below, the current is controlled to define the size of the microbubbles.

---

### Standards of care in diabetes – 2025 [^c35307cc]. Diabetes Care (2025). High credibility.

Regarding quality improvement for diabetes mellitus type 1, more specifically with respect to use of diabetes technology, ADA 2025 guidelines recommend to establish competencies within the healthcare team based on the specific roles and within specific settings.

---

### Pharmacological management of obesity: an endocrine society clinical practice guideline [^e0e16b3e]. The Journal of Clinical Endocrinology and Metabolism (2015). Medium credibility.

Regarding follow-up and surveillance for obesity, more specifically with respect to monitoring of pharmacotherapy, ES 2015 guidelines recommend to consider initiating therapy with dose escalation based on efficacy and tolerability to the recommended dose and not exceeding the upper approved dose boundaries if medication for chronic obesity management is prescribed as adjunctive therapy to comprehensive lifestyle intervention.

---

### NCCN guidelines® insights: bladder cancer, version 3.2024 [^8ca23d01]. Journal of the National Comprehensive Cancer Network (2024). High credibility.

Regarding follow-up and surveillance for bladder cancer - NCCN, more specifically with respect to follow-up (post-bladder sparing), NCCN 2024 guidelines recommend to consider obtaining less frequent surveillance (eg, cystoscopy every 6 months, extended to annually over time) in patients who are not eligible for aggressive therapy.

---

### Flexible multitask computation in recurrent networks utilizes shared dynamical motifs [^51a9771f]. Nature Neuroscience (2024). High credibility.

Sorting the rows and columns of the variance matrix revealed a blockwise structure, where groups of units had large variance for groups of task periods with similar computational requirements (Fig. 3a). Similar computations can be seen in the task period color labels (Stimulus1 and Stimulus2 — for tasks with two sequential stimulus presentations, Memory and Response) and in the task names (Category, DecisionMaking, Memory, and so on) (Fig. 3a, left). For example, task period cluster 2 (Fig. 3a, right) corresponds to reaction-timed response task periods (see Extended Data Fig. 1 for definitions of all tasks). These tasks receive new stimulus information during a response period that must be incorporated into the computation immediately. Therefore, the network cannot prepare a response direction before the fixation cue disappears. On the other hand, in task period cluster 9, the network receives no new information during the response period and must, instead, use the memory of the stimulus to produce the correct output during the response period. These separate blocks in the variance matrix reveal two distinct clusters of units that contribute to response period dynamics: one for tasks with reaction-timed responses and another for tasks with memory-guided responses. Other unit clusters for stimulus (unit clusters k–o, task period clusters 4 and 5) and memory (unit clusters a and b, task period cluster 10) computations are apparent in the block-like structure aligned with task period type (Fig. 3a, task titles and task period color labels to the left of the variance matrix). Qualitative structure and quantitative variance of the fixed points for each task period within unit clusters demonstrate the relationship between dynamical motifs and unit clusters (Extended Data Fig. 5). Block structure in the variance matrix was robust to different network architectures and hyperparameters (examples in Extended Data Fig. 6). To quantify task period similarity across networks with different hyperparameters, we calculated the variance matrix for each trained network. We then sorted task periods according to similarity of rows in one reference network and computed the correlation matrix of the sorted rows in the variance matrix for each network ('Task variance analysis' in Methods). The correlation matrix for each trained network revealed the block-wise similarity of task periods (Fig. 3c). Higher correlation across trained networks compared to untrained networks confirmed that the block structure in the variance matrix emerged from learning the task computations rather than from network design choices or the structure of the inputs (Fig. 3b, right) (Pearson correlation coefficient between correlation matrices; 'Task variance analysis' in Methods).

---

### Therapy for stage IV non-small cell lung cancer without driver alterations: ASCO living guideline, version 2025.1 [^54d2d89f]. Journal of Clinical Oncology (2025). High credibility.

Regarding medical management for non-small cell lung cancer, more specifically with respect to management of metastatic disease, first-line therapy, PD-L1 ≥ 50%, ASCO 2025 guidelines recommend to offer single-agent pembrolizumab, cemiplimab, or atezolizumab as first-line therapy in patients with stage IV NSCLC with nonsquamous cell carcinoma or SCC histology and tumor PD-L1 ≥ 50%.

---

### Standards of care in diabetes – 2025 [^1c2fda06]. Diabetes Care (2025). High credibility.

Regarding diagnostic investigations for diabetes mellitus type 1, more specifically with respect to screening for obesity, ADA 2025 guidelines recommend to measure height and weight to calculate BMI and obtain additional measurements of body fat distribution, such as waist circumference, waist-to-hip ratio, and/or waist-to-height ratio, if bodey mass index is indeterminant to support the diagnosis of obesity. Ensure privacy during anthropometric measurements.

---

### Predicting trends in the quality of state-of-the-art neural networks without access to training or testing data [^92e502a9]. Nature Communications (2021). High credibility.

Methods

To be fully reproducible, we only examine publicly available, pretrained models. All of our computations were performed with the WeightWatcher tool (version 0.2.7), and we provide all Jupyter and Google Colab notebooks used in an accompanying github repository, which includes more details and more results.

Additional details on layer weight matrices

Recall that we can express the objective/optimization function for a typical DNN with L layers and with N × M weight matrices W l and bias vectors b l as Eq. (2). We expect that most well-trained, production-quality models will employ one or more forms of regularization, such as Batch Normalization (BN), Dropout, etc. and many will also contain additional structure such as Skip Connections, etc. Here, we will ignore these details, and will focus only on the pretrained layer weight matrices W l. Typically, this model would be trained on some labeled data, using Backprop, by minimizing the loss. For simplicity, we do not indicate the structural details of the layers (e.g. Dense or not, Convolutions or not, Residual/Skip Connections, etc.). Each layer is defined by one or more layer 2D weight matrices W l, and/or the 2D feature maps W l, i extracted from 2D Convolutional (Conv2D) layers. A typical modern DNN may have anywhere between 5 and 5000 2D layer matrices.

For each Linear Layer, we get a single (N × M) (real-valued) 2D weight matrix, denoted W l, for layer l. This includes Dense or Fully Connected (FC) layers, as well as 1D Convolutional (Conv1D) layers, Attention matrices, etc. We ignore the bias terms b l in this analysis. Let the aspect ratio be, with Q ≥ 1. For the Conv2D layers, we have a 4-index Tensor, of the form (N × M × c × d), consisting of c × d 2D feature maps of shape (N × M). We extract n l = c × d 2D weight matrices W l, i, one for each feature map i = [1,…, n l] for layer l.

---

### Two-dimensional materials in functional three-dimensional architectures with applications in photodetection and imaging [^1aa91719]. Nature Communications (2018). Medium credibility.

Mechanism, principle and determination of position and direction of incident light

The system in Fig. 3a allows for the demonstration of these capabilities. Here, a laser source (532 nm) and goniometer allow motion of the laser beam in 3D coordinates, to provide incidence at any desired position. An automated measurement unit records the photoresponses of the 48 devices with ~64 Hz device interval (corresponding frame interval ~0.75 s), as in the bottom of Fig. 3a. The devices nearest to the location of illumination (entry or exit point) exhibit the highest photoresponses; those of the other devices decrease with increasing distance from this location owing to scattered light (Supplementary Fig. 16). Only the nine devices nearest to the illumination point respond significantly to the light signal; therefore, the position of the incident light is computed from the known coordinates of these nine devices (with respect to the center of the hemisphere). The calculation is according towhere P I is the spherical coordinate of the incident point, I ph is the photocurrent for the n th device, and θ and φ are the azimuthal and polar angles, respectively. As the laser beam moves, its trajectory can be recorded in this manner, as indicated by the arrow in Fig. 3b. From the initial point X 1 (Fig. 3b), a continuous photoresponse spectrum immediately begins up to the final point X 3. The responses of the nine nearest photodetecting devices in the recorded data define the location of the beam. Figure 3c shows the estimated positions of the beam (green dot) and the photoresponses of the nearest devices at three locations X 1, X 2, and X 3 (Fig. 3b). These measured movements exactly match the laser movements, as indicated by the arrow (Fig. 3b). The device closest to the incident point delivers the maximum photoresponse (bright red point). Figure 3d shows the interpolated position of the incident laser beam corresponding to location X 1 in spherical coordinates originating at the hemisphere center.

---

### Dapagliflozin [^0703083a]. FDA (2024). Medium credibility.

16 HOW SUPPLIED/STORAGE AND HANDLING

How Supplied

DAPAGLIFLOZIN TABLETS have markings on both sides and are available in the strengths and packages listed in Table 18.

---

### A comparison of phase II study strategies [^ebb760ae]. Clinical Cancer Research (2009). Low credibility.

The traditional oncology drug development paradigm of single arm phase II studies followed by a randomized phase III study has limitations for modern oncology drug development. Interpretation of single arm phase II study results is difficult when a new drug is used in combination with other agents or when progression-free survival is used as the endpoint rather than tumor shrinkage. Randomized phase II studies are more informative for these objectives but increase both the number of patients and time required to determine the value of a new experimental agent. In this article, we compare different phase II study strategies to determine the most efficient drug development path in terms of number of patients and length of time to conclusion of drug efficacy on overall survival.

---

### Standards of care in diabetes – 2025 [^57391c23]. Diabetes Care (2025). High credibility.

Regarding specific circumstances for diabetes mellitus type 1, more specifically with respect to pediatric patients (physical activity), ADA 2025 guidelines recommend to advise frequent glucose monitoring before, during, and after exercise, via a blood glucose meter and/or continuous glucose monitoring, to prevent, detect, and treat hypoglycemia and hyperglycemia associated with exercise.

---

### Standards of care in diabetes – 2025 [^61a733d5]. Diabetes Care (2025). High credibility.

Regarding quality improvement for diabetes mellitus type 1, more specifically with respect to use of diabetes technology, ADA 2025 guidelines recommend to offer diabetes devices to patients with diabetes.

---

### Accurate high throughput alignment via line sweep-based seed processing [^26f5da73]. Nature Communications (2019). High credibility.

Methods

Basic notions and planar alignment representation

We consider alignments as a path that follows a sequence of pointsin a two-dimensional plane, where | A | shall denote the alignment length. With this plane, the reference R is on the x-axis and the query Q is on the y-axis, with | Q | < | R |. Please note: The sequences r 0, r 1,…, r | A | and q 0, q 1,…, q | A | are both monotonically increasing. Two consecutive points (r n, q n) and (r n +1, q n +1) are always associated in one of the following four ways:
Deletion: r n +1 = r n + 1, q n +1 = q n; visually this is equal to a horizontal path extension.
Insertion: r n +1 = r n, q n +1 = q n + 1; visually this is equal to a vertical path extension.
Match or Mismatch: r n +1 = r n + 1, q n +1 = q n + 1; visually both appear as diagonal path extensions.

For example, in Fig. 5 a complete alignment via the orange path follows the sequence: [(0, 0), (1, 1), (2, 2), (3, 2), (4, 2), (5, 2), (6, 2), (7, 2), (8, 2), (8, 3), (9, 4), (10, 5), (11, 6), (11, 7), (12, 8), (13, 9)]

The alignment process is decomposed into three separated stages: seeding, seed processing and dynamic programming (DP). In the following three sections, we will explain our approaches for all three stages in detail.

---

### Diagnosis and classification of diabetes: standards of care in diabetes – 2025 [^568cafd9]. Diabetes Care (2025). High credibility.

Type 1 diabetes natural history and progression risk indicate that stage 1, defined by the presence of two or more autoantibodies and normoglycemia, carries a 5-year risk of developing symptomatic type 1 diabetes of ~44%. In stage 2, risk is ~60% by 2 years and ~75% within 5 years of developing a clinical diagnosis of type 1 diabetes. A consensus group provides expert recommendations on what should be monitored and how often these factors should be monitored in individuals with presymptomatic type 1 diabetes.

---

### Diagnosis, management and treatment of the Alport syndrome-2024 guideline on behalf of ERKNet, ERA and ESPN [^c59a118f]. Nephrology, Dialysis, Transplantation (2025). High credibility.

Regarding diagnostic procedures for Alport syndrome, more specifically with respect to kidney biopsy, ERA/ERN ERKNet/ESPN 2025 guidelines recommend to avoid performing a kidney biopsy in patients with a genetic diagnosis of X-linked Alport syndrome or autosomal recessive Alport syndrome.

---

### Spatially and optically tailored 3D printing for highly miniaturized and integrated microfluidics [^2743c481]. Nature Communications (2021). High credibility.

Traditional 3D printing based on Digital Light Processing Stereolithography (DLP-SL) is unnecessarily limiting as applied to microfluidic device fabrication, especially for high-resolution features. This limitation is due primarily to inherent tradeoffs between layer thickness, exposure time, material strength, and optical penetration that can be impossible to satisfy for microfluidic features. We introduce a generalized 3D printing process that significantly expands the accessible spatially distributed optical dose parameter space to enable the fabrication of much higher resolution 3D components without increasing the resolution of the 3D printer. Here we demonstrate component miniaturization in conjunction with a high degree of integration, including 15 μm×15 μm valves and a 2.2 mm×1.1 mm 10-stage 2-fold serial diluter. These results illustrate our approach's promise to enable highly functional and compact microfluidic devices for a wide variety of biomolecular applications.

---

### Methodologies for the development of CHEST guidelines and expert panel reports [^e7ef9102]. Chest (2014). Medium credibility.

CHEST grading framework — dimensions and evidence adjustment are defined explicitly. The CHEST grading system defines the grades based on two key dimensions: (1) the balance of benefits of the proposed action as compared with the possible harms or risks and (2) the methodologic quality of the supporting evidence. Both factors are represented in the overall grade, with the former represented by 1 or 2 and the latter A, B, or C. The methodologic quality is initially based on the hierarchy of study design, but studies can be upgraded or downgraded based on specific criteria (eg, existence of methodologic flaws, directness, precision, consistency of results).

---

### Updated International Consensus Report on the investigation and management of primary immune thrombocytopenia [^5f97e510]. Blood Advances (2019). High credibility.

Regarding classification and risk stratification for thrombocytopenia in pregnancy, more specifically with respect to prediction of neonatal thrombocytopenia, ICR 2019 guidelines recommend to recognize that patients with a previous newborn, thrombocytopenic or not, are likely to have a second baby with a similar platelet count.