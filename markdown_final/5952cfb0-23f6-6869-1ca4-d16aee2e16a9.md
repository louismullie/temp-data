# Medical Question & Answer

**Sample ID**: 5952cfb0-23f6-6869-1ca4-d16aee2e16a9
**Dataset Index**: 4152

---

## Question

Find all pairs of integers $(x, y)$ such that $7 x-3 y = 2$

---

## Answer

> Let's seeâ€¦ What do we have here? The user is asking for all integer pairs (x, y) that satisfy the linear Diophantine equation 7x âˆ’ 3y = 2. Let's break this down step-by-step. First, I need to think about whether integer solutions exist and why. Then, I should find a particular solution. Next, I will derive the general solution from the homogeneous equation. After that, I should verify the solution algebraically. Finally, I will consider any constraints or alternative representations and summarize the answer clearly.

> Let me first confirm the existence of integer solutions. A linear Diophantine equation ax + by = c has integer solutions if and only if gcd(a, b) divides c. Here, a = 7, b = âˆ’3, and c = 2, and gcd(7, 3) = 1, which divides 2, so integer solutions exist. I should double-check that I'm using the absolute value for gcd, which I am, so the condition is satisfied [^notfound].

> Next, I should find a particular solution. By inspection, I can try small integer values. If I let x = 2, then 7(2) = 14, so I need âˆ’3y = 2 âˆ’ 14 = âˆ’12, which gives y = 4. Thus, (x0, y0) = (2, 4) is a particular solution. Wait, let me verify: 7(2) âˆ’ 3(4) = 14 âˆ’ 12 = 2, which checks out [^notfound].

> Now, I will derive the general solution. The homogeneous equation is 7x âˆ’ 3y = 0, which implies 7x = 3y. Since 7 and 3 are coprime, x must be a multiple of 3 and y must be a multiple of 7, so the homogeneous solutions are (x, y) = (3k, 7k) for any integer k. Adding the particular solution, the general solution is (x, y) = (2 + 3k, 4 + 7k) for k âˆˆ â„¤. Hold on, I should verify the signs: because the original equation is 7x âˆ’ 3y = 2, the homogeneous increment should indeed be (3k, 7k) so that 7(3k) âˆ’ 3(7k) = 0, which is correct [^notfound].

> I will now examine the solution algebraically to ensure correctness. Substitute x = 2 + 3k and y = 4 + 7k into 7x âˆ’ 3y: 7(2 + 3k) âˆ’ 3(4 + 7k) = 14 + 21k âˆ’ 12 âˆ’ 21k = 2, which matches the right-hand side. This confirms the general solution is valid for all integers k [^notfound].

> Let me consider alternative representations and constraints. The solution can also be written as x â‰¡ 2 (mod 3) and y â‰¡ 4 (mod 7), which is equivalent to the parametric form. I should confirm that no additional constraints are implied by the problem; since the question asks for all integer pairs, the parameter k can be any integer, positive, negative, or zero, yielding infinitely many solutions [^notfound].

> Final answer: All integer solutions are given by (x, y) = (2 + 3k, 4 + 7k) for any integer k. For example, k = 0 gives (2, 4), k = 1 gives (5, 11), and k = âˆ’1 gives (âˆ’1, âˆ’3), each of which satisfies 7x âˆ’ 3y = 2 [^notfound].

---

The general solution to the equation (7x - 3y = 2) is given by:

[
x = 3k + 2, \\quad y = 7k + 4
]

where (k) is any integer. This means there are infinitely many integer solutions, one for each integer value of (k). For example, when (k = 0), the solution is ((2, 4)); when (k = 1), the solution is ((5, 11)); and when (k = -1), the solution is ((-1, -3)).

---

## General solution

To find the general solution, we first find a particular solution and then extend it to all integers using the homogeneous solution.

---

### Particular solution

By inspection, a particular solution is:

[
x_0 = 2, \\quad y_0 = 4
]

Substituting these values into the equation:

[
7(2) - 3(4) = 14â€“12 = 2
]

This confirms that ((2, 4)) is indeed a solution.

---

### Homogeneous solution

Next, we consider the homogeneous equation:

[
7x - 3y = 0
]

Rearranging, we get:

[
7x = 3y \\implies \\frac{x}{3} = \\frac{y}{7}
]

Since (x) and (y) must be integers, the smallest non-trivial solution is (x = 3) and (y = 7). Therefore, the general solution to the homogeneous equation is:

[
x_h = 3k, \\quad y_h = 7k
]

where (k) is any integer.

---

### General solution

Combining the particular solution with the homogeneous solution, the general solution is:

[
x = x_0 + x_h = 2 + 3k, \\quad y = y_0 + y_h = 4 + 7k
]

where (k) is any integer.

---

## Verification

To verify the general solution, substitute (x = 2 + 3k) and (y = 4 + 7k) into the original equation:

[
7(2 + 3k) - 3(4 + 7k) = 14 + 21k - 12â€“21k = 2
]

This confirms that the general solution satisfies the equation for any integer (k).

---

## Examples of solutions

Here are some specific solutions for different values of (k):

| **K** | **X** | **Y** |
|-|-|-|
| 0 | 2 | 4 |
| 1 | 5 | 11 |
| -1 | -1 | -3 |
| 2 | 8 | 18 |
| -2 | -4 | -10 |

---

## Conclusion

The general solution to the equation (7x - 3y = 2) is:

[
x = 2 + 3k, \\quad y = 4 + 7k
]

where (k) is any integer. This represents infinitely many integer solutions, one for each integer value of (k).

---

## References

### Solving olympiad geometry without human demonstrations [^47a1942c]. Nature (2024). Excellent credibility.

Traceback for geometric-rule deduction

To do this, we record the equality transitivity graph. For example, if ' a = b ', ' b = c ', ' c = d ' and ' a = d ' are deduced, which results in nodes a, b, c and d being connected to the same 'equality node' e, we maintain a graph within e that has edges [(a, b), (b, c), (c, d), (a, d)]. This allows the traceback algorithm to perform a breadth-first search to find the shortest path of transitivity of equality between any pair of variables among a, b, c and d. For collinearity and concyclicity, however, the representation is more complex. In these cases, hypergraphs G (V, E) with 3-edges or 4-edges are used as the equality transitivity graph. The traceback is now equivalent to finding a minimum spanning tree (denoted MST in the following equation) for the target set S of nodes (three collinear nodes or four concyclic nodes) whose weight is the cardinality of the union of its hyperedges e â€²:

Such optimization is NP-hard, as it is a reduction from the decision version of vertex cover. We simply use a greedy algorithm in this case to find a best-effort minimum spanning tree.

Traceback for algebraic deduction

Traceback through Gaussian elimination can be done by recognizing that it is equivalent to a mixed integer linear programming problem. Given the coefficient matrix of input equations A constructed as described in the previous sections and a target equation with coefficients vector b âˆˆ R N, we determine the minimal set of premises for b by defining non-negative integer decision vectors x, y âˆˆ Z M and solve the following mixed-integer linear programming problem:

The minimum set of immediate parent nodes for the equality represented by b will be the i th equations (i th rows in A) whose corresponding decision value (x i âˆ’ y i) is non-zero.

---

### Inferring time derivatives including cell growth rates using gaussian processes [^d99eaf3f]. Nature Communications (2016). Medium credibility.

Inferring the first and second time derivatives

To determine the time derivative of the data, we use that the derivative of a Gaussian process is another Gaussian process. We can therefore adapt standard techniques for Gaussian process to allow time derivatives to be sampled too.

Building on the work of Boyle, we let g (x) and h (x) be the first and second derivatives with respect to x of the latent function f (x). If f (x) is a Gaussian process then so are both g (x) and h (x). Writing âˆ‚ 1 and âˆ‚ 2 for the partial derivatives with respect to the first and second arguments of a bivariate function, we have

and that

as well as

following ref.

Consequently, the joint probability distribution for y and f *, g * and h * evaluated at points X * is again Gaussian (cf. equation (7)):

where we write K = K (X, X) and K âœ± = K (X *, X *) for clarity.

The covariance function is by definition symmetric: k (x i, x j) = k (x j, x i) from equation (1). Therefore, and so

for all positive integers k and. Consequently, the covariance matrix in equation (13) is also symmetric.

Conditioning on y now gives that the distribution P (f *, g *, h *| X, y, Î¸, X *) is Gaussian with mean

and covariance matrix

Equation (16) includes equation (9) and shows that

which gives the error in the estimate of the first derivative. Similarly,

is the error in estimating the second derivative.

Using an empirically estimated measurement noise

Although our derivation is given for a Gaussian process where the measurement errors in the data are independent and identically distributed with a Gaussian distribution of mean zero, the derivations are unchanged if the measurement noise has a different s.d. for each time point.

When the magnitude of the measurement noise appears to change with time, we first empirically estimate the relative magnitude of the measurement noise by the variance across all replicates at each time point. We then smooth this estimate over time (with a Gaussian filter with a width of 10% of the total time of the experiment, but the exact choice is not important) and replace the identity matrix, I, in equations (6), (15) and (16) by a diagonal matrix with the relative measurement noise on the diagonal in order to make predictions.

---

### Topological zero-dimensional defect and flux States in three-dimensional insulators [^378d6f37]. Nature Communications (2022). High credibility.

As previously for integer dislocations, the interface between the top and bottom surfaces contains an effective fourfold Dirac cone from the two twofold surface Dirac cones contributed by each bulk band inversion at k D, a (one twofold Dirac cone from each of the top and bottom surfaces). However, unlike for integer dislocations, the-symmetric mass of the fourfold Dirac cone carries a relative phase of e i Ï• between the regions inside and outside of the pair of flux tubes. Hence crucially, and unlike in the previous case of edge and screw dislocations, the relative sign of the fourfold Dirac mass for Ï€ -flux tubes is independent of k D, a. This implies that when Ï• = Ï€, the two flux tubes bind an odd (anomalous) number of helical pairs of modes if the bulk contains an odd total number of band inversions between Kramers pairs of states at TRIM points such that â€” through the Fuâ€“Kane parity criterion â€” the bulk is a 3D TI. Alternatively, this result may be summarized through the statement that Ï€ -flux tubes bind anomalous helical modes in an- and-symmetric 3D insulator if the 2D momentum-space Hamiltonian in only one of theBZ planes is equivalent to a 2D TI, because a 3D TI can be expressed as a helical pump of a 2D TI. As shown in SN 2A3 and 2B2, we find more generally that a parallel pair of x âˆ¥2 -directed Ï€ -flux tubes separated by a distance along x âˆ¥1 sums the 2D topology of all of the momentum-space Hamiltonians in the-indexed BZ planes of the pristine insulating crystal [see Eq. (10), and note that x âˆ¥1,2 span the plane perpendicular to x âŠ¥]. The summed 2D momentum-space topology is then projected onto the real-space surface spanning the flux tubes.

---

### The supercurrent diode effect and nonreciprocal paraconductivity due to the chiral structure of nanotubes [^86e2d1c4]. Nature Communications (2023). High credibility.

Supercurrent diode effect

When a supercurrent passes through the nanotube, the Cooper pairs acquire a momentum p and a kinetic energy Î¾ (p). The order parameter is determined by the Ginzburg-Landau equation asand the supercurrent isNote that Î± < 0 since T < T c. The critical currents J c Â± are the absolute values of the maximum and minimum, respectively, of J x (n, q) as n and q are varied.

For general parameters, J c Â± can be determined numerically and the resulting diode efficiency, is shown in Fig. 2 as functions of the magnetic flux Ï•, the angle Î¸ and the temperature, respectively. Figure 2 a shows a periodicity in Ï•, similar to the Little-Parks oscillation. Different curves are for various values of the ratio r = R / l 0, with R being the radius of the nanotube and. When r is small and Ï• / Ï• 0 is close to a half-integer, the transverse momentum, p y = (n âˆ’ Ï• / Ï• 0) â„ / R â‰ˆ â„ /(2 r l 0), costs so high a kinetic energy Î¾ (p) that it kills the superconductivity (i.e. Ïˆ n â†’ 0), leading to vanishing J c Â±. We define Î· in this case to be zero, resulting in the curve with r = 1 in Fig. 2 (a). As r increases, J c Â± becomes nonzero for arbitrary magnetic flux and discontinuities occur as Ï• / Ï• 0 changes across half-integers, which originates from the quantization of the transverse index n in Eq. (7). When r â‰« 1, discontinuities disappear while non-smooth kinks remain and âˆ£ Î· âˆ£ decreases. From Fig. 2 b, one finds that Î· vanishes whenever Î¸ becomes a multiple of Ï€ /2. This is expected because the nanotubes in these cases are not chiral and the inversion symmetry is preserved, forbidding the SDE. As Î¸ / Ï€ deviate from half-integers, âˆ£ Î· âˆ£ increases sharply and extreme values of Î· are reached quickly. Note that the positions of the extreme points depend on the ratio m 0 / m 1, which measures the strength (and the sign) of inversion symmetry breaking. The temperature dependence has the usual feature, as shown in Fig. 2 (c).

---

### Using both qualitative and quantitative data in parameter identification for systems biology models [^fc71653e]. Nature Communications (2018). Medium credibility.

Results

An illustration of the potential value of qualitative data

To demonstrate the potential value of qualitative data, we consider a simple case of solving for the coefficients of polynomial functions.

We consider two polynomial functions: y 1 = ax 2 âˆ’ bx + c and y 2 = dx + e. Suppose we want to solve for the coefficients a, b, c, d, and e, which we will take to be positive. As the ground truth coefficients to be determined, we choose (a, b, c, d, e) = (0.5, 3, 5, 1, 1.5).

Suppose that a limited amount of quantitative information is available. Namely, it is known that the parabola y 1 contains the points (2, 1) and (8, 13), and the line y 2 contains the point (3.5,5). This is not enough information to solve for any of the coefficients because three points are required to specify a parabola, and two points are required to specify a line (Fig. 1a).

Fig. 1
A simple illustration using polynomial functions. We use qualitative and quantitative information to determine the unknown coefficients. a Visualization of the problem. We seek to find the coefficients of equations for a parabola and a line, with the ground truth shown (blue solid curves). Two points on the parabola and one point on the line are known (black dots). These three points are consistent with infinitely many possible solutions (e.g. orange dashed curves). Qualitative information (colored circles, x -axis) specifies whether the parabola is above (+) or below (âˆ’) the line. This information limits the possible values of intersection points x 1 and x 2 to the green shaded segments of the x -axis. b Bounds on coefficient values as a function of the number of qualitative points known. Shaded areas indicate the range of possible values of each coefficient

---

### Nonlinear delay differential equations and their application to modeling biological network motifs [^832976d6]. Nature Communications (2021). High credibility.

Results

Our results are divided up into 7 sections corresponding to 7 different regulatory networks of increasing complexity. An overall reference table (Table 2) is included in the discussion.

Motif 0: direct Hill regulation

We first describe how we use DDEs to model simple regulation of a gene Y by a gene X as the most basic motif, and then provide a simple yet unified mathematical framework for both activation and inhibition with delay.

Activation and inhibition can both be modeled with a single unified function

We consider a transcription factor x regulating the production of a protein y (Fig. 2). If x activates y, the production rate of y increases with increasing x, generally saturating at a maximum rate Î±. Often there is an additional cooperativity parameter n which determines how steeply y increases with x in the vicinity of the half-maximal x input value k. In this framework, n = 0 is constitutive production, n = 1 is a Michaelis-Menten regulation, 1 < n < 5 is a typical biological range of cooperativity, and n â†’ âˆž is a step-function regulation. If x represses y, the same conditions hold except that the production rate of y then decreases with increasing x. A standard quantitative model for this behavior is called the Hill function, and serves as a good approximation for many regulatory phenotypes in biology, including transcription and translation rates, phosphorylation, enzymatic activity, neuronal firing, and so forth. In general there may also be a leakage rate Î± 0 that yields constant y production in the absence of x. The concentration of y is also generally removed at a rate Î² proportional to its concentration. This removal term can represent many biophysical processes, such as degradation, dilution, compartmentalization, or sequestration; for simplicity we mainly use the term "degradation."

---

### A transcriptomic axis predicts state modulation of cortical interneurons [^6a37ddfc]. Nature (2022). Excellent credibility.

Initial registration

The initial registration step finds offsets between all image tiles using the anchor images taken on round 8 (which we refer to as 'reference images'). We use this to define a global coordinate system for the entire tissue sample.

Because we use a square tiling strategy, each tile may have up to four 'neighbours': other tiles with which it has a region of substantial overlap. We denote the set of neighbouring tile pairs as ð”‘.

Spots first are detected in each tile's reference images, as local maxima of the filtered image exceeding a fixed detection threshold. To align the reference images, we loop over all pairs of neighbouring tiles, and compute an offset to register the overlapping regions of the filtered reference images of these two tiles. The offset between two tiles T 1 and T 2 is found by exhaustive search over all 2D shifts in a range around to the shift expected from the microscope's position sensor. For each shift, we find for each spot s on T 1 the pixel distance D s to the nearest spot on T 2 after the shift has been applied. A score is computed as, and the final shift vectoris taken as the one that maximizes this score; that is, the one with the most near neighbours.

We define a single global coordinate system by finding the coordinate origin X T for each tile T. Note that this problem is overdetermined as there are more neighbour pairs than there are tiles. We therefore compute the offsets by minimizing the loss function

Differentiating this loss function with respect to X T yields a set of simultaneous linear equations, the solution of which yields the origins of each tile on the reference round. The results of this step suffice to define a global coordinate system, but do not provide pixel-level alignment of images from multiple colour channels on multiple rounds, owing to the occurrence of chromatic aberration and small rotational or non-rigid shifts. The latter will be dealt with in the next step, through point cloud registration.

---

### Recovery after stroke: not so proportional after all? [^d3259e7a]. Brain (2019). Medium credibility.

Spurious r(X,Î”) are likely when Ïƒ Y /Ïƒ X is small

For any X and Y, it can be shown that:

A formal proof of Equation 1 is provided in the Supplementary material, Appendix A [proposition 4 and theorem 1; also see]; its consequence is that r(X,Î”) is a function of r(X, Y) and Ïƒ Y /Ïƒ X. To illustrate that function, we performed a series of simulations (Supplementary material, Appendix B) in which r(X, Y) and Ïƒ Y /Ïƒ X were varied independently. Figure 2 illustrates the results: a surface relating r(X,Î”) to r(X, Y) and Ïƒ Y /Ïƒ X. Figure 3 shows example recovery data at six points of interest on that surface.

Figure 2
The relationship between r(X, Y), r(X,Î”) and Ïƒ Y /Ïƒ X. Note that the x -axis is log-transformed to ensure symmetry around 1; when X and Y are equally variable, log(Ïƒ Y /Ïƒ X) = 0. Supplementary material, proposition 7 in Appendix A, provides a justification for unambiguously using a ratio of standard deviations in this figure, rather than Ïƒ Y and Ïƒ X as separate axes. The two major regimes of Equation 1 are also marked in red. In Regime 1, Y is more variable than X, so contributes more variance to Î”, and r(X,Î”) â‰ˆ r(X, Y). In Regime 2, X is more variable than Y, so X contributes more variance to Î”, and r(X,Î”) â‰ˆ r(X,âˆ’X) (i.e. âˆ’1). The transition between the two regimes, when the variability ratio is not dramatically skewed either way, also allows for spurious r(X,Î”). For the purposes of illustration, the figure also highlights six points of interest on the surface, marked Aâ€“F; examples of simulated recovery data corresponding to these points are provided in Fig. 3.

---

### Simultaneous polydirectional transport of colloidal bipeds [^ef97c8bb]. Nature Communications (2020). High credibility.

Compacted parallel polydirectional loop

To find the loopfor given target displacements w n 1,â€¦, w n l for a set of target lengths b n 1,â€¦, b n l, we start by calculating all fence points for each target length. Then, we transcribe the fence points back to control spaceby rescaling. All fence points inlie on the equator (see Fig. 1 b yellow and pink circles) and hence they differ only by the azimuthal angle of the external field. For every neighboring fence azimuthal angles we choose one intermediate azimuthal angle between them. These intermediate points are then transcribed to each of the unidirectional transcription spaces of the target set of bipeds and we calculate their minimum distance Î” to the fences. The intermediate points having transcriptions with Î” smaller than a fixed threshold are discarded. Every pair formed by two of the non-discarded points defines a fundamental parallel polydirectional loop. Every fundamental parallel polydirectional loop causes a unidirectional displacement for every target length and hence can be represented as a displacement in a 2 l -dimensional vector space of the l target lengths. As the target displacements can be represented in this vector space, the parallel polydirectional loopcan be found as an integer linear combination of fundamental parallel polydirectional loops, i.e. by solving the resulting system of linear equations. Note that only an integer number of every fundamental loop is valid as a solution since only integer winding numbers are possible. If the selected threshold for the robustness is too large, it may be impossible to find an integer solution, but there is always a solution if we sufficiently reduce the minimum robustness. There exist an infinite number of parallel polydirectional loops decoding the same displacements and the one found by solving the linear system of equations is just one of them, not necessarily the most compact. Therefore, we obtain a compaction of this parallel polydirectional loop by searching for pairs (and triplets) of fundamental loops in the parallel polydirectional loop that can be replaced by a single fundamental loop having the same net displacement. This is repeated in an iterative scheme until no further compaction of the loop is found. See Supplementary Movie 9 for a detailed visual explanation of the algorithm.

---

### LinkedSV for detection of mosaic structural variants from linked-read exome and genome sequencing data [^f423756f]. Nature Communications (2019). High credibility.

If we regard (R (F i 1), L (F i 2)) as a point in a two-dimensional plane, according to Eq. (2), for almost all (98.01%) of the fragment pairs (F i 1, F i 2), ((R (F i 1), L (F i 2)) is restricted in a G Ã— G square region with the point (b 1, b 2) being a vertex (Supplementary Fig. 14b).

We used a graph-based method to fast group the points into clusters and find square regions where the numbers of points were more than expected. First, every possible pair of endpoints (R (F 1), L (F 2)) meeting B (F 1) = B (F 2) formed a point in the two-dimensional plane. Each point indicated a pair of fragments that share the same barcode. For example, if ten fragments share the same barcode, pairs of endpoints will be generated. A point/pair of endpoints may or may not support an SV because there are two possible reasons for observing two fragments sharing the same barcode: (1) the two fragments originated from two different HMW DNA molecules but were dispersed into the same droplet partition and received the same barcode and (2) the two fragments originated from the same HMW DNA molecule but the reads were reconstructed into two fragments due to an SV. The points are sparsely distributed in the two-dimensional plane and it is highly unlikely to observe multiple points in a specific region. Next, a k â€“ d tree (k = 2) was constructed, of which each node stores the (X, Y) coordinates of one point. A k â€“ d tree is a binary tree that enable fast query of nearby nodes. Therefore, we could quickly find all pairs of points within a certain distance. Any two points (x 1, y 1) and (x 2, y 2) were grouped into one cluster if | x 1 âˆ’ x 2 | < G and | y 1 âˆ’ y 2 | < G. For each cluster, if the number of points in the cluster was more than a user-defined threshold (default: 5), it was considered as a potential region of enriched fragment endpoints. If the points in the cluster were not within a G Ã— G square region, we used a G Ã— G moving square to find a square region where the points are best enriched. Theoretically, the best enriched square region should contain 98.01% (0.99 Ã— 0.99) of the points, according to Eq. (2). The predicted breakpoints were the X and Y coordinates of the right-bottom vertex of the square. The points in the square region were subjected to a statistical test describe below.

---

### A stochastic vs deterministic perspective on the timing of cellular events [^c3a6938f]. Nature Communications (2024). High credibility.

Case 1: the simple birthâ€“death process

When r = 1, the model given in reaction scheme (8) coincides with a simple birthâ€“death process. We are interested in the mean waiting time for the birthâ€“death process to reach a fixed protein number N, given that the system is started from n initial proteins. Simulations of the simple birthâ€“death process suggest the MFPT is smaller than the deterministic FPT (see Fig. 2 B, but note that only a small sample of trajectories are shown here for visualisation purposes). We now prove this analytically. To begin, we find an analytical solution for the MFPT of the stochastic description of the system, as given by the CME (see Equation (3) of the Supplementary Information).

Starting from Eq. (7), it can be shown (Supplementary Note 2), that the expected waiting time to reach N proteins given the system is started from n â‰¤ N, is given by, where for real number x and positive integer m, the notationabbreviates x (x âˆ’ 1)â€¦ (x âˆ’ (m âˆ’ 1)), the falling factorial of x. In what follows for simplicity we shall set n = 0. The deterministic description of the system is given by the reaction rate equation d Ï„ X = K âˆ’ X, with X (0) = 0, and solving for X (Ï„) yields X (Ï„) = K (1 âˆ’ e âˆ’ Ï„). Solving for Ï„ then gives, Consider some proportion 0 < Ï < 1, and consider the expected waiting time to reach Ï K, that is, some proportion Ï of the steady-state mean. Then Eq. (10) simplifies towhich we note is independent of K, and where the requirement of Ï < 1 enables this to be well defined. Now let us consider Eq. (9) for N = âŒŠ Ï K âŒ‹. For notational simplicity only, we assume that Ï K is an integer, and again observe that the numeratoris bounded above by (Ï K) i +1. Replacing this in Eq. (9) givesRecognising the Taylor expansion ofaround 0 (for Ï < 1), we find that Eq. (12) is the truncated Taylor expansion of Eq. (11). Thus, as each term of the series is strictly positive, it follows that for any proportion Ï of the steady-state mean, the deterministic waiting time is a strict upper bound for the mean waiting times. An example is given in Fig. 2 C for Ï = 0.8. Here we show how the ratio of the stochastic mean waiting time to the deterministic waiting time, Î·, of the simple birthâ€“death process scales with increasing K. Note that since Î´ = r = 1, K is equal to the steady-state mean number of molecules.

---

### Sensitive dependence of pairing symmetry on Ni-ecrystal field splitting in the nickelate superconductor LaNiO [^45a473cd]. Nature Communications (2025). High credibility.

The discovery of high-temperature superconductivity in La 3 Ni 2 O 7 under pressure has drawn great attention. However, consensus has not been reached on its pairing symmetry in theory. By combining density-functional-theory (DFT), maximally-localized-Wannier-function, and linearized gap equation with random-phase-approximation, we find that the pairing symmetry of La 3 Ni 2 O 7 is d x y, if its DFT band structure is accurately reproduced by a downfolded bilayer two-orbital model. More importantly, we reveal that the pairing symmetry of La 3 Ni 2 O 7 sensitively depends on the crystal field splitting between two Ni- e g orbitals. A slight increase in Ni- e g crystal field splitting alters the pairing symmetry from d x y to s Â±. Such a transition is associated with the change in inverse Fermi velocity and susceptibility, while the shape of Fermi surface remains almost unchanged. Our work highlights the sensitive dependence of pairing symmetry on low-energy electronic structures in multi-orbital superconductors, which calls for care in the downfolding procedure when one calculates their pairing symmetry.

---

### The XZZX surface code [^8f6ac9c8]. Nature Communications (2021). High credibility.

This structured noise model thus leads to two distinct regimes, depending on which failure process is dominant. In the first regime where, we expect that the logical failure rate will decay like. We find this behaviour with systems of a finite size and at high bias where error rates are near to threshold. We evaluate logical failure rates using numerical simulations to demonstrate the behavior that characterises this regime; see Fig. 6 (a). Our data show good agreement with the scaling ansatz. In contrast, our data are not well described by a scaling.

Fig. 6
Sub-threshold scaling of the logical failure rate with the XZZX code.

a Logical failure rateat high bias near to threshold plotted as a function of code distance d. We use a lattice with coprime dimensions d Ã— (d + 1) for d âˆˆ {7, 9, 11, 13, 15} at bias Î· = 300, assuming ideal measurements. The data were collected usingiterations of Monte-Carlo (MC) samples for each physical rate sampled and for each lattice dimension used. The physical error rates used are, from the bottom to the top curves in the main plot, p = 0.19, 0.20, 0.21, 0.22 and 0.23. Error bars represent one standard deviation for the Monte-Carlo simulations. The solid lines are a fit of the data to, consistent with Eq. (2), and the dashed lines a fit to, consistent with Eq. (3) where we would expect, see Methods. The data fit the former very well; for the latter, the gradients of the best fit dashed lines, as shown on the inset plot as a function of, give a linear slope of 0.61(3). Because this slope exceeds the value of 0.5, we conclude that the sub-threshold scaling is not consistent with. b Logical failure ratesat modest bias far below threshold plotted as a function of the physical error rate p. The data (markers) were collected at bias Î· = 3 and coprime d Ã— (d + 1) code dimensions of d âˆˆ {5, 7, 9, 11, 13, 15} assuming ideal measurements. Data is collected using the Metropolis algorithm and splitting method presented in refs. The solid lines represent the prediction of Eq. (3). The data show very good agreement with the single parameter fitting for all system sizes as p tends to zero.

---

### Theory and rationale of interpretable all-in-one pattern discovery and disentanglement system [^64945457]. NPJ Digital Medicine (2023). Medium credibility.

Definitions, equations and algorithmic steps

The input dataset, R, is an MÃ—N matrix, where M is the number of entities (i.e. records), A m = 1,â€¦ M, each of which is assigned with a unique Entity ID (EID) i, and N is the number of attributes, A 1,â€¦, A N. As for a mixed-mode dataset containing numerical or categorical values, we discretize the numerical values into quantized intervals with equal frequency such that each attribute would assume a finite set of values treated as categorical values denoted as, where AV j is the j th value in A i. For each record, the categorical and discretized values are represented as A i = AV j and referred to as discrete attribute values (AVs).

The first step is constructing association matrix with adjusted standardized residuals. Treating an AV pair as a compound event co-occurring on the same entities in the dataset, we intend to find out whether an AV-association is statistically significant. We adopt a well-established statistical measure, known as adjusted statistical residual (SR)to represent the statistical weight of the event as defined in Definition 1 for an AV pair, and generalize it for high order AV-association referred to as a pattern if the association is statistically significant.

Definition 1. statistical residual (SR)

The statistical residual is a statistical measure, referred to as the adjusted standardized residual, to account for the deviation of the observed frequency of occurrences of a compound event (such as an AV pair) from the expected frequency if the co-occurrence of the event is random (i.e. with statistically independent AVs, Definition 2). For an AV pair (AV 1 â†” AV 2), it is written as SR (AV 1 â†” AV 2) in Eq. (1)where Occ (AV 1) and Occ (AV 2) are the frequency of occurrences of the AV pair where Occ (AV 1 â†” AV 2) is the total number of co-occurrences of the AV pairs (AV 1 and AV 2) in an entity; Exp (AV 1 â†” AV 2) is the expected frequency if the AVs in the pair are independent; and T is the total number of records. By converting the frequency of all the distinct AV pairs in the frequency association table obtained from data into SR though the Address Table, we obtain a square SR-Matrix (Definition 3).

---

### Approximations to the expectations and variances of ratios of tree properties under the coalescent [^8355e038]. G3 (2022). Medium credibility.

Properties of gene genealogies such as tree height (H), total branch length (L), total lengths of external (E) and internal (I) branches, mean length of basal branches (B), and the underlying coalescence times (T) can be used to study population-genetic processes and to develop statistical tests of population-genetic models. Uses of tree features in statistical tests often rely on predictions that depend on pairwise relationships among such features. For genealogies under the coalescent, we provide exact expressions for Taylor approximations to expected values and variances of ratios Xn/Yn, for all 15 pairs among the variables {Hn, Ln, En, In, Bn, Tk}, considering n leaves and 2 â‰¤ k â‰¤ n. For expected values of the ratios, the approximations match closely with empirical simulation-based values. The approximations to the variances are not as accurate, but they generally match simulations in their trends as n increases. Although En has expectation 2 and Hn has expectation 2 in the limit as nâ†’âˆž, the approximation to the limiting expectation for En/Hn is not 1, instead equaling Ï€2/3â€“2â‰ˆ1.28987. The new approximations augment fundamental results in coalescent theory on the shapes of genealogical trees.

---

### Nonlinear delay differential equations and their application to modeling biological network motifs [^22bd51a1]. Nature Communications (2021). High credibility.

Motif III: logic

A general class of functions used to describe natural,â€“ and synthetic, biological networks are logic gates, which have two inputs regulating a single output (Fig. 5). Gates exhibit either high ("on") or low ("off") output depending on whether inputs are on or off. For example, the AND gate specifies high output only if both inputs are on. In this section we provide a specific DDE-based framework that covers 14 out of 16 possible 2-input logic operations, and show that these operations form a continuous 2D parameter space.

Fig. 5
A simple approximation for digital logic using a sum of Hill terms recapitulates all monotonic logic functions in a single parameter space.

a A prototypical regulatory network involving logic where X and Y both regulate Z, which must integrate the two signals using some logic before it can in turn activate a downstream reporter R. b Parameter space showing regions where regulation approximately follows 14 of the 16 possible 2-input logic functions depending on the strength of two single-variable Hill regulation terms (Î· Z 1: regulation of Z by X, Î· Z 2: regulation of Z by Y). Network logic can be smoothly altered by varying the parameters (Î· Z 1, Î· Z 2), with a change of sign in (n 1, n 2) required to switch quadrants. The bottom-left quadrant shows that very weak regulation in both terms leads to an always-off (FALSE) function, weak regulation in one arm only leads to single-input (X, Y) functions, strong regulation in both arms leads to an OR function, and regulation too weak in either arm alone to activate an output but strong enough in sum leads to an AND function. The other three quadrants are related by applying NOT to one or both inputs, with function names related by de Morgan's lawNOT(X OR Y) = NOT X AND NOT Y. In particular, X IMPLY Y = NOT(X) OR Y, X NIMPLY Y = X AND NOT(Y), X NOR Y = NOT X AND NOT Y, and X NAND Y = NOT X OR NOT Y. Truth tables for all 16 logic gates are provided in Supplementary Table 1 for reference. The two non-monotonic logic functions, X XOR Y and X XNOR Y, are those 2 of 16 not reproduced directly using this summing approximation. They can be produced by layering, e.g. NAND gates. c Representative time traces for AND (Î· Z 1 = Î· Z 2 = 0.9) and OR (Î· Z 1 = Î· Z 2 = 1.8) gates with n 1 = n 2 = âˆ’2, n 3 = âˆ’20, Î· R = Î· Z 1 + Î· Z 2. The functionwhen n > 0, when n < 0.

---

### Sensitive dependence of pairing symmetry on Ni-ecrystal field splitting in the nickelate superconductor LaNiO [^2d74740b]. Nature Communications (2025). High credibility.

In this work, we combine density-functional-theory (DFT), maximally-localized-Wannier-function (MLWF), and linearized gap equation calculations with random-phase-approximation (RPA) â€“ to study the superconducting pairing symmetry of tetragonal La 3 Ni 2 O 7 by downfolding the DFT band structure to a bilayer two-orbital model. We find that the leading superconducting instability of La 3 Ni 2 O 7 has d x y symmetry, if its DFT band structure is accurately reproduced by the downfolded model and the interaction strengths are in a physically reasonable range. More importantly, we show that a slight increase in the crystal field splitting between two Ni- e g orbitals (< 0.2 eV) changes the pairing symmetry from d x y to s Â±. We elucidate that such a transition is associated with the change in inverse Fermi velocity and susceptibility, while the shape of Fermi surface remains almost unchanged. Our work highlights the sensitive dependence of La 3 Ni 2 O 7 pairing symmetry on Ni- e g crystal field splitting, which provides one source of discrepancy in the pairing symmetry reported in the literature. Such sensitivity calls for care in the procedure of downfolding DFT band structure to low-energy effective models when one calculates the pairing symmetry of multi-orbital superconductors.

---

### Evolutionary dynamics of any multiplayer game on regular graphs [^916cb3ba]. Nature Communications (2024). High credibility.

General replicator equations

The evolution of frequencies x 1, x 2,â€¦, x n can be deduced through the microscopic strategy update process. Specifically, in an infinite population, i.e. N â†’ âˆž, a single unit of time comprises N elementary steps, ensuring that each individual has an opportunity to update their strategy. During each elementary step, the frequency of i -players increases by 1/ N when a focal j -player (where j â‰  i) is chosen to update its strategy and is replaced by an i -player. Similarly, the frequency of i -players decreases by 1/ N when a focal i -player is selected to update its strategy and the player who takes the position is not an i -player. Based on this perception, we derive a simple form of the replicator equations for i = 1, 2,â€¦, n in the weak selection limit (Supplementary Note 2) :We find that Eq. (4) offers an intuitive understanding, if we introduce the following two concepts: (1), the expected accumulated payoff of the i -player (zero steps away on the graph), and (2), the expected accumulated payoff of the i -player's neighbors (one step away on the graph). These concepts suggest that. Under pairwise comparison, the reproduction rate of i -players is dependent on how their accumulated payoff exceeds that of their neighbors. In essence, the evolution of x i is the competition between an individual and its first-order neighbors, which aligns with the results obtained by a different theoretical framework in two-strategy systems. We further extend it to n -strategy systems in the framework of pair approximation. We also verify that the death-birth rule is essentially the competition between an individual and its second-order neighbors for n -strategy systems (Supplementary Information).

---

### Resolving discrepancies between chimeric and multiplicative measures of higher-order epistasis [^7747af71]. Nature Communications (2025). High credibility.

Equation (21) demonstrates that X = (X 1, X 2) follows an exponential family distribution, a wide class of distributions that includes many common distributions including normal distributions or Poisson distributions. In particular, using the terminology of exponential families, equation (21) shows that the sufficient statistics of X are X 1, X 2, and X 1 X 2, with corresponding canonical parameters Î² 1, Î² 2, and Î² 12. As a result, the distribution P (X) is uniquely defined by the expected values E [X 1], E [X 2], E [X 1 X 2] of the sufficient statistics, sometimes called the moments or the mean parameters of the distribution. Thus, we obtain a third parametrization of the distribution P (X) using the moments Î¼ 0 = 1, Î¼ 1 = E [X 1], Î¼ 2 = E [X 2], Î¼ 12 = E [X 1 X 2]. The elements of the vector Î¼ = (1, Î¼ 1, Î¼ 2, Î¼ 12) of moments are sometimes called the mean parameters of the distribution.

---

### Identity-by-descent-based estimation of the X chromosome effective population size with application to sex-specific demographic history [^b06b3b19]. G3 (2023). Medium credibility.

Results

Simulation study

We checked that the distribution of X chromosome IBD segments in femaleâ€“female haplotype pairs, in femaleâ€“male haplotype pairs, and in maleâ€“male haplotype pairs is consistent in all simulations (Supplementary Fig. 2). We compared the estimated X chromosomeobtained by splitting the X chromosome into 6 pieces to enable bootstrapping with theestimated on the undivided X chromosome, and the results are consistent (Supplementary Fig. 3).

We comparedestimated from the simulated autosome and X chromosome data to the actualfor the UK-like demographic model. The actual autosome or X chromosomecan be obtained from the sex-specific effective population sizes using Equations 2 and 3. The estimatedgenerally matches the trueclosely (Fig. 1 and Supplementary Fig. 4). However, some discrepancies exist between the estimated and actualbecause IBDNe cannot localize sharp changes in the population size to the exact generation and it tends to oversmooth corners of the trajectory of the effective population size over time.

Fig. 1.
Estimates of the autosome, X chromosome, and sex-specificin a UK-like simulation study. Autosomalis shown in the left column, X chromosomein the middle column, and sex-specificin the right column. The top row is with an equal sex ratio, while the second row is with 20% females. Results for other choices of sex ratios can be found in Supplementary Fig. 4. Y -axes showplotted on a log scale. In cases where the estimated sex-specificis negative (see Methods), it is not shown.

We next used the estimated X chromosomeand autosometo estimate the sex-specific effective population sizes. We find that the formulas for the sex-specificas functions of the autosome and X chromosome(Equations 5, 6) are sensitive to errors inestimation. The estimated sex-specificis similar to the actual values most of the time, but at around 15 generations ago, where there was a large change in the population growth rate, we observe greater inaccuracy in the estimated, and the estimated sex-specificdiffers from the actual sex-specific(Fig. 1 and Supplementary Fig. 4).

---

### Mapping two-dimensional polar active fluids to two-dimensional soap and one-dimensional sandblasting [^ad46a7b7]. Nature Communications (2016). Medium credibility.

Calculation of the nonlinear lengths

The nonlinear lengths Î¾ x, y can be calculated most conveniently from the equilibrium 2D smectic model (51). By definition, Î¾ x and Î¾ y are the lengths along x and y beyond which the anharmonic terms in equation (51) become important. To determine these lengths, we treat the anharmonic terms perturbatively, and calculate the lowest order correction to the harmonic terms. In a finite system of linear dimensions L x, y, this 'perturbative' correction will indeed be perturbative (that is, small) compared with the 'bare' values of the harmonic terms. However, they grow without bound with increasing L x, y, and, hence, eventually cease to be small; that is, the perturbation theory breaks down at large L x, y. The values of L x, y above which the perturbation theory breaks down are the nonlinear lengths Î¾ x, y.

Calculating the lowest order correction to the compression modulus B (that is, the coefficient of (âˆ‚ y h) 2 in the smectic Hamiltonian (51)) can be graphically represented by the Feynman diagram in Fig. 3. This leads to a correction to compression modulus:

In this calculation, we have taken L y, the system size along y, to be infinite. By the definition of Î¾ x, | Î´B | = B for L x = Î¾ x, which gives

where in the second equality we have used the relations B = 2 Î±v 0 2, and k B T = D between the parameters of the smectic and those of the original incompressible active fluid.

Likewise, doing the same calculation for L y = Î¾ y, L x = âˆž, we find

---

### Nonlinear delay differential equations and their application to modeling biological network motifs [^c2c24300]. Nature Communications (2021). High credibility.

After plugging the delayed pseudo-steady state of Y into, one can match the values of the composite Hill-within-Hill function at X = 0, X = 1, and X â†’ âˆž as well its the slope at X = 1 compared to a single Hill function with leakage, and thereby approximate the cascade as a single-step regulation of Z by X (Supplementary Note 1):where the combined regulation parameters are (Supplementary Note 1)The k can be removed by renormalization of X.

Equation (10) has several biological consequences. First, the overall Hill coefficient h is negatively proportional to the product of individual cooperativities n X, n Y. This makes the cascade activating for either two activators or two repressors, and repressive otherwise. It also makes the cascade more cooperative than its components, in line with past analysesand as found experimentally. Third, the leakage is zero for n Y < 0 and negligible (i.e. Î± 0 â‰ª Î±) for n Y > 0 if Î· Y â‰« 1. This means Y must be produced in sufficient amount during the delay to repress Z. We generally ignore leakage in the main text; for a more detailed discussion see Supplementary Note 3 and Supplementary Fig. 2.

The total delay in a cascade equals the sum of individual delays

For cascades with delays at each step, the same analysis implies that total delay equals the sum of individual delays (Î² z / Î² y + Î³ y + Î³ z, with Î³ y the intrinsic delay between X and Y and Î³ z the intrinsic delay between Y and Z). Figure 3 shows simulations of ODE, equivalent DDE, and multi-delay DDE cascade models, matching the above analytical results.

---

### Predominant regional biophysical cooling from recent land cover changes in Europe [^0e10c03c]. Nature Communications (2020). High credibility.

The size of the local window is 5-by-5 grids, approximately 60-by-60 km, in which the local climate is assumed to be uniform. Climatic gradients that result from topographical differences are masked according to refs. To unmix the temperature signals resulting from the mix of the possible land cover transitions among the various classes, a linear regression approach is applied separately to the N windows, where for window i, X i is the explanatory variable, i.e. a matrix containing the fractions of the transitions of all the land cover classes for the 25 grids of each window (with the first column made of ones to capture the intercept), y i is a vector that contains the 25 values of changes in temperature, É› i is the error term referring to the model residuals, and Î² i is the vector of the regression coefficients. The residuals, É› i, i = 1, 2,â€¦, N, are assumed to be independently and identically distributed with mean 0 and variance. Equation (2) can be explicitly written for window i as a system of equations:where the dependence on i is dropped to simplify the notation. x nm is the fraction of change in land class m in grid n, n is the number of grids of the local window (n = 25) and m is the number of land cover classes (m = 14). The aim is to find the difference in regression coefficients between each pair of land classes to inform about the effect of the land transition on temperature by solving the system of Eq. (3). Once the system is solved, we can use the coefficients Î² i for window i to understand the local temperature effects of a given transition of land cover classes from j to k by setting x j = âˆ’1 and x k = 1, and all the other x to 0.

---

### Optimal free descriptions of many-body theories [^f77fa88f]. Nature Communications (2017). Medium credibility.

Methods

Optimization

The optimization to find Ïƒ andin (3) is performed by a Monte Carlo basin-hopping strategyusing the Nelderâ€“Mead simplex algorithm for local minimization within basins of the cost function. This global strategy was selected to counteract an observed tendency for local methods to get trapped in local minima. The initial guess for this search is found as follows. The normalization constant E 0 is the lowest entanglement energy of the input entanglement spectrum { E }. We iteratively construct an approximate set of single-particle entanglement energies starting from an empty set. First, we take the lowest remaining level in the spectrum and subtract E 0 to produce a new single-particle level k. Then we remove the many-body levels, which are closest to the new combinatorial levels generated according to (2) by the additional single-particle level. This process is repeated until the input spectrum is exhausted. We can also introduce a truncation of the entanglement spectrum cutting off high entanglement energies, making the construction of the initial guess terminate faster. The minimization of D (Ïƒ, Ïƒ f) to identify the optimal free model for the Ising Hamiltonian (5) is calculated using a local Nelderâ€“Mead method.

Finite-size scaling

We perform the finite-size scaling according to an ansatz (4). The parameters of the collapse were estimated using the method of ref. From the scaling ansatz (4) and for a trial set of scaling parameters g c, Î½ and Î¶, the scaled values x L = (g âˆ’ g c) L 1/ Î½ andare calculated from each unscaled data point. From this collection of scaled data points (x L, y L) across all L, we implicitly define a so-called master curve that best represents them. This curve y (x) is defined around a point x as the linear regression calculated by taking the scaled data points immediately left and right of x for each system size L. We characterize the deviation of the scaled points (x L, y L) from the master curve y (x L) using the Ï‡ 2 statistic. This measure is used as the cost function for an optimization problem over the scaling parameters g c, Î½, Î¶ and Î¸, which can be solved using the same techniques as the previous problems.

---

### Guidelines for validation of next-generation sequencing-based oncology panels: a joint consensus recommendation of the Association for Molecular Pathology and college of American pathologists [^01148c3a]. The Journal of Molecular Diagnostics (2017). Medium credibility.

Nonparametric tolerance intervals for non-normal distributions: The above estimate of the tolerance interval would only be applicable to a population that is normally distributed, but when the underlying population is often not normal (eg, when there is a natural boundary that the data cannot exceed (ie, 0% or 100%)) it is helpful to define tolerance intervals using nonparametric methods; the one-sided nonparametric tolerance interval can be determined by finding the value for k that satisfies the cumulative binomial equation, where CL is the confidence level (eg, 0.95), and by setting k = 0 (ie, 0 failures) the formula can be simplified.

---

### Fractional response analysis reveals logarithmic cytokine responses in cellular populations [^0d1364aa]. Nature Communications (2021). High credibility.

Calculation of typical fractions

The fractions of cells stimulated with dose i that have responses typical to dose j, v ij, can be easily calculated from data regardless of the number of doses and the type of experimental measurements. We have thatCalculation of typical fractions, v ij, with the above formula requires the possibility to examine the condition P (y | x j) > P (y | x k) for any experimentally observed response, y. The distributions P (y | x j) can be reconstructed from data using a variety of probability density estimators. The use of the available estimators, however, might be problematic for multivariate responses. We, therefore, propose a more convenient strategy. We replace the condition P (y | x j) > P (y | x k) with an equivalent condition that is computationally much simpler to evaluate. Precisely, we propose to use the Bayes formulaIf we set the equiprobable prior distribution, i.e. P (x j) = 1 /m, we have that P (y | x j) is proportional to P (x j | y) and the condition P (y | x j) > P (y | x k) is equivalent toThe above strategy allows avoiding estimation of the response distributions, P (y | x j), from data. For continuous and multivariate variable y the estimation of P (x j | y) is generally simpler than estimation of P (y | x j). Precisely, an estimatorof the distribution P (x j | y) can be built using a variety of Bayesian statistical learning methods. For simplicity and efficiency, here we propose to use logistic regression, which is known to work well in a range of applications. In principle, however, other classifiers could also be considered. The logistic regression estimators of P (x j | Y = y) arise from a simplifying assumption that log-ratio of probabilities, P (x j | Y = y) and P (x m | Y = y) is linear. Precisely, The above formulation allows fitting the logistic regression equations to experimental data, i.e. finding values of the parameters, Î± j and Î² j that best represent the data. The fitted logistic regression model allows assigning cellular responses to typical doses based on conditions given by Eq. 8. Formally, the fractions v ij defined by Eq. 6 are calculated aswhere n i is the number of cells measured for the dose x i, y l i denotes response of the l- th cell, andis equal 1 iffor anyand 0 otherwise.

---

### The impact of structural heterogeneity on excitation-inhibition balance in cortical networks [^cc9071d8]. Neuron (2016). Low credibility.

As in the balanced network ofwe assume that the synapses are strong, such that action potentials in a small fraction of presynaptic neurons are sufficient to evoke an action potential in the postsynaptic neuron. Therefore, the excitatory and inhibitory synaptic currents in Equation 1 are each large relative to threshold, and for the net current to be within range of the threshold, the excitatory and inhibitory contributions need to approximately cancel each other, yielding a set of linear equations that we refer to as the balance conditions:For a network with small CV k, allare approximately 1. Thus, the near-threshold condition reduces to the two linear equations of the balanced state of homogeneous networks, two equations for the two unknowns r E and r I. In such a network it has been shown that the mean population rates will dynamically adjust their value and arrive rapidly at a steady state that satisfies these equations. This balancing of the net excitatory and inhibitory currents yields a state where most of the neurons are near threshold and driven by the fluctuations in synaptic current.

However, in the case where CV k is non-negligible, each relative in-degreemay be substantially different, and therefore there is no pair of excitatory and inhibitory population rates that can combine to satisfy Equation 2 for more than a small fraction of the population. Due to the substantial difference between in-degrees within the network, any given population rates will only balance synaptic current of a small fraction of neurons. The remaining neurons will either have a larger ratio of inhibitory to excitatory in-degrees, and therefore be completely suppressed by strong total inhibitory current, or they will have a smaller ratio, in which case they will be driven to high firing rates with regular inter-spike intervals (ISIs). Thus, we expect the dynamic balance between excitation and inhibition to fail in heterogeneous networks (see Experimental Procedures).

---

### Reconstructing the evolution history of networked complex systems [^58993ddf]. Nature Communications (2024). High credibility.

The theoretical relationship betweenand x

A brief mathematical derivation of the theoretical result about the relationship between the overall errorof the restored sequence and the accuracy x of the ensemble model is provided here. Without loss of generality, assume that the ground truth sequence is(normalized by the number of edges E). For the u i j in Eq. (4), its expectation and variance are (in subsequent derivation, the counts are normalized by E for convenience)Then for the Borda count u i, its expectation isThe approximation is obtained since (E + 1)/ E â‰ˆ 1 for large E. Treating E and x as constants, E (u i) is a linear function of i with two boundaries E (u 1) = 1 âˆ’ x and E (u E) = x. In other words, the normalized Borda counts of the E edges are evenly distributed over the interval [1 âˆ’ x, x]. According to the mean field theory, the positionof edge i in the restored sequenceshould be the length from 1 âˆ’ x to u i divided by the total length of the interval 2 x âˆ’ 1, i.e. Then the expectation and variance ofare (plugging in Eqs. (5) and (6))Therefore, is an unbiased estimate of Î± i so that the mean-squared error is just the variance and the root-mean-squared error is the standard deviation, as stated by Eq. (2).

Pseudo code for the simulations in Fig. 2

To better explain the simulations involved in Fig. 2, we provide the pseudo code to implement the simulations. The essential idea is that in the simulations, we only need to specify the pairwise accuracy x to obtain the restored sequence for a fine-grained ground-truth sequence of edges, i.e. no need to actually pass through an ensemble model to obtain the generation order of each edge pair.

---

### Optimal flip angles for in vivo liver 3D tmapping and bmapping at 3T [^f3413961]. Magnetic Resonance in Medicine (2023). Medium credibility.

2 THEORY

2.1 mapping

mapping using the SPGR sequence is based on acquiring data at different FAs to reconstruct the SPGR curve and estimate thethat best fits the data points. The SPGR signal assuming steadyâ€state is:whereis the (noiseless) signal acquired using an excitation FA of, is a numerical constant including the proton density, signal decay due torelaxation, and thereceive sensitivity; TR is the repetition time, andis the longitudinal relaxation time.

2.2 mapping

inhomogeneities are the main source of inaccuracy in determiningthrough Equation (1)., Formapping, the ratio (was taken between two fully relaxed signals acquired at FA and FA. The true FA exciting the spins is estimated through Equation (2) for:where r = (x, y, z). Thecorrection factor is the ratio between the true FA and the nominal FA prescribed at the scanner.

However, a nonâ€uniform slice profile invalidates Equation (2). To correct for slice profile effects, the complex transverse signal needs to be simulated and integrated across the slice before taking the ratio of the absolute values of the signals. A lookâ€upâ€table based on this procedure can be used to interpolate the acquired signal ratio and find the true FA exciting the spins.

2.3 map variance

The optimal pair of FAs that minimize thefactor variance is found through error propagation of the noise from FAsandaccording to Equations (3) and (4)whererepresents the signal noise at FA. Onceis determined, Equation (4) gives an estimate of the variance associated with themap.

---

### A solution to the collective action problem in between-group conflict with within-group inequality [^1e238285]. Nature Communications (2014). Medium credibility.

Equation (19) shows that individual effortsincrease with valuation v i. In contrast, individual reproductive success at equilibrium is

and thus decreases with valuation. That is, the higher costs paid by high-rank individuals negate their larger shares of the reward.

If not all group members make a positive effort, then the derivations become more complex. Let there be n e contributors and v be their total valuation (v = âˆ‘ v i where the sum is over n e contributors). Then, summing up equation (16) over all contributors, at equilibrium their total effort X satisfies to a quadratic equation

Of the two solutions of this quadratic, only the smallest one, X *, is relevant biologically (the other solution leads to negative fertilities). Note that cX * â‰¤ n e + bn Î£ v i < n (1+ b). (The first inequality follows from the requirement that all f i values are non-negative: which one then sums up over all contributing individuals).

The individual effortscan be found from equation (16) by substituting X * into

This equilibrium is biologically feasible (that is, all), if

Equation (23) shows that individual effort increases with valuation. However, within-group reproductive success

and is decreasing with valuation v i because cX * â‰¤ n (b +1).

By differentiating the expression for X *, one can show that X * always increases with v and generally decreases with n e. (The latter requires that v exceeds a certain threshold that depends on b and n but is relatively small). That is, decreasing the number of contributors and/or increasing their total share of the reward results in increasing group effort. Moreover, analytical derivation and numerical simulations strongly suggest that the derivative of v crit with respect to X is positive, so that the threshold valuation increases with decreasing the number of contributors. This leads to a recursive procedure for finding the equilibrium: (i) setfor all ranks for which inequality (20) is not satisfied; (ii) using appropriate values of n e and v, find X * from equation (22); (iii) if there are ranks for which inequality (24) is not satisfied, set their contributionsto zero, recompute n e and v, and return to step (ii).

---

### Nonlinear delay differential equations and their application to modeling biological network motifs [^b91959df]. Nature Communications (2021). High credibility.

A two-parameter summing function reproduces all 2-input monotonic logic gates

We first write out nondimensionalized equations corresponding to the logic gate motif as depicted in Fig. 5 a. We assume that the degradation constants (Î²) for Z and R are equal for simplicity, and that there is no leakage.

We describe the regulation of Z by X and Y using a sum of Hill terms. Logic gate behavior can be captured with other approaches such as a product of Hill terms, or summation within a single Hill term, each representing subtly different biology. We choose the separate Hill term approach as it describes many logic functions simply by tuning regulatory strengths, and can be extended to include multiplicative terms (Supplementary Note 5). Caveats include multiple states for Z, requiring additional binarization via R (Fig. 5 c), as well as poor response to ratiometric inputs, discussed in the next section on feedforward motifs.

Using Eq. (19), we can characterize the motif logic based on the idea of dynamic range matching. Every regulator in Eq. (19) is effectively compared against unity in the denominator of the Hill function for its corresponding output. For instance, Z provides an "on" or "off" signal to R if Z > 1 or Z < 1 respectively. Z can take on values below 1 as long asand, otherwise Z will always activate R, as indicated by the areas marked TRUE in Fig. 5.

Let us say that X and Y settle on steady-state values X * â‰¡ Î· X, Y * â‰¡ Î· Y, as inputs to our logic gate. A value of Î· X or Î· Y significantly greater than 1 is then "high" (true, 1), and "low" (false, 0) if much less than 1. We then want to determine whether Z * is greater than (true) or less than (false) 1. For example, if n 1,2 > 0 (two repressors), Table 1 gives the possible steady states of Z. If Î· Z 1 and Î· Z 2 are greater than 1, these steady states approximate a NAND gate. If they are less than 1, but sum to greater than 1, the steady states instead approximate a NOR gate.

---

### A space-time tradeoff for implementing a function with master equation dynamics [^47aa6146]. Nature Communications (2019). High credibility.

The following definition is standard:

Definition 1. A stochastic matrix P is called embeddable if P = T (0, 1) for some CTMC T with finite rates.

As it turns out, many stochastic matrices cannot be implemented by any master equation. (The general problem of finding a master equation that implements some given stochastic matrix P is known as the embedding problem in the mathematics literature â€“). One necessary (but not sufficient) condition for a stochastic matrix P to be implementable with a master equation is,

When P represents a single-valued function f which is not the identity, and the conditions of Eq. (1) are not satisfied. Therefore, no non-trivial function can be exactly implemented with a master equation. However, as we show constructively in Supplementary Note 2, all non-invertible functions (e.g. bit erasure, which corresponds tocan be approximated arbitrarily closely using master equation dynamics. Intuitively, since the determinant of such functions equals 0, they can satisfy Eq. (1) arbitrarily closely.

To account for such cases, we introduce the following definition:

Definition 2. A stochastic matrix P is limit-embeddable if there is a sequence of CTMCs with finite rates, { T (n) (t, t â€²): n = 1, 2,â€¦ }, such that

Note that while each T (n) has finite rates, in the limit these rates may go to infinity (this is sometimes called the "quasistatic limit" in physics). This is precisely what happens in the example of (perfect) bit erasure, as shown explicitly in Supplementary Note 1.

We use the term master equation to broadly refer to a CTMC with finite rates, or the limit of a sequence of such CTMCs.

---

### Restricting mutualistic partners to enforce trade reliance [^e7008610]. Nature Communications (2016). Medium credibility.

and

We use the expressions forandin equations (6a) and (6b) in the equation for fungal fitness equation (5b) and calculate the maxima with respect to allocation to trade and phosphorus restriction, âˆ‚w f / âˆ‚q f = âˆ‚w f / âˆ‚r = 0. Again, at an ESS, any individual receives population mean fitness to first order, so we take the focal fungus' strategy to be the population average. Substituting, we find that at a fixed point

and

Using the expressions forandfrom equations (7a) and (7b) in equations (6a) and (6b), we find plant strategy as a function of model parameters only

Therefore, equations (7a)â€“(7d), specify a co-evolutionary ESS when each of, andare between 0 and 1, as all values outside this range are not feasible strategies. The values ofandspecified by equations (7a) and (7d) are always between 0 and 1. The value ofspecified by equation (7b) is between 0 and 1 if e > n f / b and P p > C p (n p âˆ’1)(be âˆ’ n f) P f / n p ((1âˆ’ b) e + n f), while the value ofspecified by equation (7c) is between 0 and 1 if. Hence, when these three inequalities are satisfied, equations (7a)â€“(7d), are a co-evolutionary ESS.

We look for other strategies that are constrained maxima. That is, strategies where any alternative with higher fitness is not feasible (allocations < 0 or > 1). We eliminateandas this would leave plants without carbon, andandas this would leave mycorrhizal fungi with either zero fitness or no phosphorus. We first setand work out the evolved best response values for the other variables if all other individuals use the same strategy:

We verify whether fitness at q f = 0 is greater than at q f > 0, given the corresponding values of, and:

Hence, the strategies defined by expression (8) are a co-evolutionary ESS if the inequalities in expression (9) are satisfied. We then setand find the condition for a co-evolutionary ESS by the same method with:

---

### Generating conjectures on fundamental constants with the ramanujan machine [^75e8911d]. Nature (2021). Excellent credibility.

Fundamental mathematical constants such as e and Ï€ are ubiquitous in diverse fields of science, from abstract mathematics and geometry to physics, biology and chemistry 1,2. Nevertheless, for centuries new mathematical formulas relating fundamental constants have been scarce and usually discovered sporadically 3â€“6. Such discoveries are often considered an act of mathematical ingenuity or profound intuition by great mathematicians such as Gauss and Ramanujan 7. Here we propose a systematic approach that leverages algorithms to discover mathematical formulas for fundamental constants and helps to reveal the underlying structure of the constants. We call this approach 'the Ramanujan Machine'. Our algorithms find dozens of well known formulas as well as previously unknown ones, such as continued fraction representations of Ï€, e, Catalan's constant, and values of the Riemann zeta function. Several conjectures found by our algorithms were (in retrospect) simple to prove, whereas others remain as yet unproved. We present two algorithms that proved useful in finding conjectures: a variant of the meet-in-the-middle algorithm and a gradient descent optimization algorithm tailored to the recurrent structure of continued fractions. Both algorithms are based on matching numerical values; consequently, they conjecture formulas without providing proofs or requiring prior knowledge of the underlying mathematical structure, making this methodology complementary to automated theorem proving 8â€“13. Our approach is especially attractive when applied to discover formulas for fundamental constants for which no mathematical structure is known, because it reverses the conventional usage of sequential logic in formal proofs. Instead, our work supports a different conceptual framework for research: computer algorithms use numerical data to unveil mathematical structures, thus trying to replace the mathematical intuition of great mathematicians and providing leads to further mathematical research.

---

### Data-driven discovery of dimensionless numbers and governing laws from scarce measurements [^310b7972]. Nature Communications (2022). High credibility.

The optimization of dimensionless learning is different from general regression optimization approaches because only dimensionless numbers with small rational powers are preferred, such as âˆ’1, 0.5, 1, or 2, etc. Therefore, instead of searching for the best basis coefficients with a lot of decimals like other neural network-based methods, such as DimensionNet, zero-order optimization methods are used in this work. It includes grid search or pattern search-based two-level optimization and can be more efficient in finding the best basis coefficients. No gradient information and learning rate are required and the choice of grid interval is more flexible. Even though these zero-order optimization approaches can get stuck in local minima, increasing the number of initial points can easily eliminate this issue. More detailed pros and cons of different optimization methods are described in Section 4.5 of the SI.

The proposed method divides the identification process of differential equations into two steps to identify consistent parameterized governing equations efficiently. The first step is to identify a temporary governing equation in which the regression coefficients can be a constant or variable depending on how the simulation or experiment parameters are set. In the next step, dimensionless learning aims to recover the expression of the varying coefficients by leveraging the dimension of these coefficients. By combining these two steps, the proposed method can efficiently obtain a consistent dimensionally homogeneous governing equation with a small amount of data. In contrast, the standard SINDy falls short of achieving a consistent parameterized differential equation for the same system with different parameters. For example, the governing equation for the springâ€“massâ€“damper system is. If we use different parameters (damping coefficient c, spring constant k, or mass m) in this system, SINDy can only provide scalar coefficients for x andrather than the expressionsand, respectively. Other advanced SINDy approaches deal with this issue by multiplying the candidate terms by a set of predetermined parameters. Although these approaches can address this inconsistent governing equation problem, it couples the optimization of identifying candidate terms and parameterized coefficients, making the optimization more difficult. If there are many combinations of parametric derivative or non-derivative terms, this problem can become more difficult and unmanageable.

---

### Restricting mutualistic partners to enforce trade reliance [^ad00c90a]. Nature Communications (2016). Medium credibility.

We verify whether fitness at r = 0 is greater than at r > 0, given the corresponding values of, and:

In the parameter region from expression (11), the best response values in equation (10) are feasible (and equation (10) is thus a co-evolutionary ESS) if

Finally, we set. In this case, the phenotype of plants may or may not respond to a small change in fungal strategy, depending on parameter values. We first suppose that it does not, so

Now, we verify whether fitness at x < 1 is indeed greater than at x = 1, given the corresponding values of, and:

Hence, plant phenotype does not deviate fromwhen fungal strategy is at its optimum and equation (13) is thus a co-evolutionary ESS whenever parameter values satisfy inequality (14).

If instead the phenotype of plants responds to fungal strategy when, fungal strategy is set so thatin equation (7d) is exactly 1:

We verify whether fitness atis a maximum, given the corresponding values of, and:

So, equation (15) is a co-evolutionary ESS whenever parameter values satisfy inequality (16), but not inequality (14). If inequality (14) is satisfied, plant phenotype would not deviate fromwith a change in fungal strategy.

---

### An analytical theory of balanced cellular growth [^ee17f303]. Nature Communications (2020). High credibility.

Proof: As A is an active matrix, all fluxes v j = p j k j (a) (constraint (12)) are non-zero. We can thus express the individual protein concentrations as p j = v j / k j (a). Inserting v j from the investment equation (Theorem 5) directly leads to the above equation. â–¡

We now insert the equations for the individual proteins into the total protein constraint (13) to obtain an explicit expression for the growth rate.

Theorem 7 (Growth equation): Letbe an active stoichiometric matrix with full column rank n, and let y = [P, a] T be a concentration vector. Let v be a corresponding BGS. Let B and D be the basis and dependency matrices, respectively, as defined in Theorem 5. Then the growth rate isif for all reactions, and no balanced growth is possible otherwise.

Proof: According to Theorem 6, the individual protein concentrations are. The flux v j catalyzed by protein j must be active, and thus p j has to be positive for all j. Substituting the expressions for p j into the proteome constraint (13), we obtainThe sum on the r.h.s. is positive, and dividing by it results in the growth equation. â–¡

Thus, if the active matrix A of a BGS is full rank, there are unique and explicit mathematical solutions for p, v, and Î¼. In particular, this is the case for optimal growth states (Theorem 9), as well as for all other EGSs. In this section, we did not impose any density constraints (such as constraint (14)), and thus Theorems 1â€“7 remain valid under arbitrary density constraints as long as these are respected by the concentration vector y = [P, a] T.

---

### An analytical theory of balanced cellular growth [^98f39440]. Nature Communications (2020). High credibility.

Without dependent reactants, A is a square matrix with a unique inverse A âˆ’1, and x â‰¡ [P, a] T is the corresponding vector of independent concentrations. Multiplying both sides of the mass balance constraint (1) by A âˆ’1, we obtain (Theorem 5)The right-hand side of the mass balance constraint (1) quantifies how much of each component x i needs to be produced to offset the dilution that would otherwise occur through the exponential volume increase at rate Î¼.quantifies the proportion of flux v j invested into offsetting the dilution of component i, and we thus name A âˆ’1 the investment (or dilution) matrix; see Supplementary Fig. 1 for examples. In contrast to the mass-normalized stoichiometric matrix A, which describes local mass balances, A âˆ’1 describes the structural allocation of reaction fluxes into offsetting the dilution of all downstream cellular components, carrying global, systems-level information.

From the kinetic equation (Eq. (3)), p j = v j / k j (a), and inserting v j from the investment equation (Eq. (4)) giveswhere âˆ‘ i sums over the total protein and individual reactant concentrations (Theorem 6). Substituting these expressions into the total protein sum (Eq. (2)) and solving for Î¼ results in the growth equation (Theorem 7)

As detailed in "Methods" (Theorems 5â€“7), a corresponding result also holds for BGSs with dependent reactants. Thus, for any active matrix A with full column rank (in particular for all active matrices of EGSs) and for any corresponding concentration vector x, there are unique and explicit mathematical solutions for the fluxes v, individual protein concentrations p, and growth rate Î¼. If Î¼ (Eq. (6)) and all individual protein concentrations p j (Eq. (5)) are positive, the cellular state is a BGS; otherwise, no balanced growth is possible at these concentrations.

---

### Robust electron pairing in the integer quantum hall effect regime [^f4aabc01]. Nature Communications (2015). Medium credibility.

Electron pairing is a rare phenomenon appearing only in a few unique physical systems; for example, superconductors and Kondo-correlated quantum dots. Here, we report on an unexpected electron pairing in the integer quantum Hall effect regime. The pairing takes place within an interfering edge channel in an electronic Fabry-Perot interferometer at a wide range of bulk filling factors, between 2 and 5. We report on three main observations: high-visibility Aharonov-Bohm conductance oscillations with magnetic flux periodicity equal to half the magnetic flux quantum; an interfering quasiparticle charge equal to twice the elementary electron charge as revealed by quantum shot noise measurements, and full dephasing of the pairs' interference by induced dephasing of the adjacent inner edge channel-a manifestation of inter-channel entanglement. Although this pairing phenomenon clearly results from inter-channel interaction, the exact mechanism that leads to electron-electron attraction within a single edge channel is not clear. We believe that substantial efforts are needed in order to clarify these intriguing and unexpected findings.

---

### The eighty five percent rule for optimal learning [^5399633a]. Nature Communications (2019). High credibility.

Methods

Optimal error rate for learning

In order to compute the optimal difficulty for training, we need to find the value of Î” that maximizes the learning gradient, âˆ‚ ER/ âˆ‚Î². From Eq. (3) we haveFrom here the optimal difficulty, Î” *, can be found by computing the derivative of the gradient with respect to Î”, i.e. Setting this derivative equal to zero gives us the following expression for the optimal difficulty, Î” *, and error rate, ER * where p â€²(x) denotes the derivative of p (x) with respect to x. Because Î² and Î” * only ever appear together in these expressions, Eq. (26) implies that Î² Î” * is a constant. Thus, while the optimal difficulty, Î” *, changes as a function of precision (Fig. 1c), the optimal training error rate, ER * does not (Fig. 1d). That is, training with the error rate clamped at ER * is guaranteed to maximize the rate of learning.

The exact value of ER * depends on the distribution of noise, n, in Eq. (2). In the case of Gaussian noise, we havewhich implies thatand that the optimal difficulty isConsequently the optimal error rate for Gaussian noise isSimilarly for Laplacian noise and Cauchy noise (p (x) = (Ï€ (1 + x 2)) âˆ’1) we have optimal error rates of

---

### An analytic approximation of the feasible space of metabolic networks [^e63ca097]. Nature Communications (2017). Medium credibility.

Assuming a steady-state condition within a cell, metabolic fluxes satisfy an underdetermined linear system of stoichiometric equations. Characterizing the space of fluxes that satisfy such equations along with given bounds (and possibly additional relevant constraints) is considered of utmost importance for the understanding of cellular metabolism. Extreme values for each individual flux can be computed with linear programming (as flux balance analysis), and their marginal distributions can be approximately computed with Monte Carlo sampling. Here we present an approximate analytic method for the latter task based on expectation propagation equations that does not involve sampling and can achieve much better predictions than other existing analytic methods. The method is iterative, and its computation time is dominated by one matrix inversion per iteration. With respect to sampling, we show through extensive simulation that it has some advantages including computation time, and the ability to efficiently fix empirically estimated distributions of fluxes.

---

### Holimap: an accurate and efficient method for solving stochastic gene network dynamics [^ccf3e72b]. Nature Communications (2024). High credibility.

Methods

Determining the effective parameter for the LMA

For the linear network in Fig. 2 b, the evolution of moments is governed byInserting Eq. (2) into Eq. (5) gives a closed set of moment equations, from which the values of g 0, Î¼ 1,1, and Î¼ 1,0 can be computed approximately. Finally, using these values, the effective parametercan be obtained from Eq. (2). The remaining steps for the LMA are the same as for the 2-HM.

Determining the effective parameters for the 4-HM

For the autoregulatory circuit, it follows from Eq. (1) thatFor the linear network in Fig. 2 d, the evolution of moments is governed byFrom these equations, it is easy to show thatMatching Eqs. (6) and (8), we find thatandshould satisfy the following system of linear equations:Matching the first and third identities in Eqs. (1) and (7), it is clear thatandshould satisfy the following system of linear equations:wherehas been determined by solving Eq. (9). Compared with Eq. (4), Eq. (10) has an additional term. This is because Ï b remains unchanged for the 2-HM but is changed for the 4-HM.

Finally, inserting Eqs. (9) and (10) into Eq. (7) gives a system of closed moment equations and hence the values of all zeroth, first, and second-order moments can be approximately calculated. Substituting these moments into Eqs. (9) and (10), one can finally solve for the four effective parameters, andof the linear network. The 4-HM predicts the protein distribution of the nonlinear network by solving the CME of the linear one in Fig. 2 d with the values of the effective parameters found above.

---

### Discretization of the total magnetic field by the nuclear spin bath in fluorine-doped ZnSe [^a5f59872]. Nature Communications (2018). Medium credibility.

Fig. 2
Magnetic field dependence of time-resolved traces. Contour plots of TRKR signal as function of applied magnetic field B x and delay time, using pump excitation powers of 0.3 mW (a) and 10 mW (b). Note the different intensity scales in the two panels. White lines are the calculated magnetic field dependencies of the electron Larmor frequency without nuclear contribution. c Waterfall of the TRKR spectra in the magnetic field range of 22â€“30 mT (black lines). Red dashed lines, plotted over the black lines, are the fits by A KR. d Extracted fit functions from c are shown with phase Ï† set to zero to highlight the stability of the frequencies around a whole integer of T R

From analyzing the plots with the fit function A KR, one can extract the precession frequency, as shown in Fig. 3 in units of the laser repetition frequency Î© 0 = 2 Ï€ / T R (left axes), vs. the applied field B x in absolute units (bottom) and in units of B 0 (top), confirming the surprising finding of strong deviations from the linear dependence (the solid black line) on B x: the precession frequency shows a non-monotonous behavior with distinct steps. The steps are most prominent for the low power case (blue points in Fig. 3a), where pronounced plateaus are seen, separated from each other by Î© 0, so that the precession frequencies are given by Ï‰ = k Î© 0 with integer k. The plateaus are centered around B x = kB 0 with the same k and the jumps from a plateau to the next higher one, k + 1, occur at the field strengths B x = (2 k + 1) B 0 /2. The equation Ï‰ = k Î© 0 is identical to the previously observed mode-locking condition in a QD ensemble, where the large inhomogeneity in the ensemble prevented the observation of precession frequency discretization. Turning to the high pump power case in Fig. 3b, one still finds indications of the discretization even though the steps are somewhat tilted and smoothed. Furthermore, the staircase is shifted now: the plateaus have an average frequency of Ï‰ = (2 k + 1) Î© 0 /2, and the jumps between them occur at B x = kB 0. Taking the B x -linear frequency as a baseline, the measured precession frequency shows symmetric, quasi-oscillatory behavior around it, with sections with comparatively enhanced frequency alternating with sections of reduced frequency.

---

### Thinking outside the curve, part I: modeling birthweight distribution [^b60d1510]. BMC Pregnancy and Childbirth (2010). Low credibility.

b. Simulation study on calibrating confidence intervals

For our second simulation study we generated 25 overlapping data sets of size 50,000 from design C in Table 3, the degree of overlap consistent with a population of 200,000. For each of various C between 2.0 and 5.0, we used Equation (7) to form confidence intervals for the mixture parameters p 1, p 2, p 3, p 4, Î¼ 1, Î¼ 2, Î¼ 3, Î¼ 4, Ïƒ 1, Ïƒ 2, Ïƒ 3, Ïƒ 4. We recorded how many of the mixture parameters were contained in their respective confidence intervals. This was repeated nine more times, and we tabulated how many of the 120 = 12 Ã— 10 confidence intervals contained their targets. Confidence intervals were also formed using Equation (6) for comparative purposes. The above steps were repeated with overlapping data sets consistent with a population of 1,000,000 and with nonoverlapping data sets consistent with an effectively infinite population.

The results are summarized in Table 5. With an effectively infinite population, only 81.7% of the confidence intervals formed using Equation (6) contained their targets at C = 5.0. The confidence intervals formed using Equation (7) contained their targets 95.0% of the time at C = 2.5. The adjustment suggested by Equation (8) appears reasonable: Ï† = .05 = 50,000/1,000,000 and N rep = 25 yield C Ï† = 1.315 C 0, which accords with the 95.8% capture of mixture parameters at C = 3.5 â‰ˆ 1.315 Ã— 2.5 with a population of 1,000,000.

Table 5
Confidence Interval Coverage Probabilities in Simulation Studies

The row with " C " = 2 and "Population size" = 200,000 identifies the numbers and percentages of confidence intervals containing their targets of mixture parameters, based on 10 repetitions in each of which 25 samples of size 50000 were simulated from a 4-component normal mixture with 12 parameters; results under the heading of "Bias adjustment included" are based on Equation (7) with C = 2, results under the heading of "Bias adjustment omitted" are based on Equation (6) with C = 2, and the 25 samples of size 50000 had overlap consistent with a population size of 200,000. Other rows correspond to different choices of C and/or population sizes.

---

### Imaging high-speed friction at the nanometer scale [^adfb53e4]. Nature Communications (2016). Medium credibility.

Intermodulation measurement and scanning feedback

The cantilever is excited with a split-piezo actuator at two frequencies f 1, f 2 centered on torsional resonance f 0, t and separated by Î” f = f 2 âˆ’ f 1 < < f 0, t. The drive frequencies f 1 and f 2 are chosen such that they are both integer multiples of Î” f. The drive is synthesized, and the response is measured with a synchronous multifrequency lockin amplifier (Intermodulation Products AB;)which also calculates the feedback error signal used by the host AFM. A proportional-integral feedback loop adjusts the probe height to keep the f 1 response amplitude at the set-point value. The exact type of feedback used is not critical to the method, only that it is responsive enough to track the surface topography at the desired scan speed. We also desire that the feedback error is small enough, such that we can approximate the probe height as being constant during the time T = (Î” f) âˆ’1 needed to measure the response. This time defines one pixel of the 42 amplitude and phase image-pairs acquired at each frequency, during a single scan.

Model and equations of motion

A schematic representation of the model can be seen in Fig. 1. Performing force balance on both masses results in two coupled one-dimensional equations of motion in the lateral position of the tip apex x t, and tip base x b.

The coupling forceis linear in the deformation of the tip, d = x t âˆ’ x b, and damping linear in. F noise (t) is a random noise force with a Gaussian distribution. The strength of the noise is characterized by the standard deviation Ïƒ noise, given in Table 1. The nonlinear frictional forceis derived from damped motion in a periodic potential U (x t) = U 0 cos(2 Ï€x t / a 0). The drive force F d = K [A 1 cos(2 Ï€f 1 t)+ A 2 cos(2 Ï€f 2 t)] is applied at two frequencies as described above.

---

### An analytical theory of balanced cellular growth [^e01d64fe]. Nature Communications (2020). High credibility.

The marginal net benefits can now be expressed as differences between benefits and costs. To calculate the direct marginal net benefits, we must use the growth equation derived in Theorem 7, where we defined proteome fractions Ï• j â‰¡ p j / P. The first form given for Î¼ (P, a) here quantifies growth as a function of the state variables x i, and it would be straight forward to calculatefrom this expression. However, to establish a formal link between marginal net benefits and protein investment, we will instead go via the second form, which arises from Theorem 6 and was used to derive the growth equation, and the third form, which expresses this relationship in terms of the proteome fractions Ï• j. When we take the partial derivatives with respect to the state variables x i in the second and third forms, we must make sure that we keep the right terms constant: when expressed in terms of the x i, the expression p j / Î¼ is in fact independent of Î¼ (Theorem 6), and we hence need to take the derivatives while keeping Î¼ constant. We thus get for the direct marginal net benefits:where we used the fact that the proteome fractions must add to 1, âˆ‘ j Ï• j = 1. Thus, the direct marginal net benefit of the cellular concentration x i is identical to the total associated changes in proteome fractions caused by this change.

---

### A continuous-time maxSAT solver with high analog performance [^87d3d16d]. Nature Communications (2018). Medium credibility.

Algorithm description

Here, we give a simple, nonoptimized variant of the algorithm (see flowchart in Supplementary Fig. 2). Better implementations can be devised, for example with better fitting routines, however the description below is easier to follow and works well. Given a SAT problem, we first determine the b parameter as described previously. Step 1: initially we set, Î“ min, = and t max. Unless specified otherwise, in our simulations we used Î“ min = 100, Î“ max = 2 Ã— 10 6, t max = 50. Step 2: to initialize our statistics, we run Î“ min trajectories up to t max, each from a random initial condition. For every such trajectory Ï‰ we update the p (E, t) distributions as function of the energies of the orthants visited by Ï‰. We record the lowest energy value found. Step 3: starting from Î“ = Î“ min + 1 and up to Î“ max, we continue running trajectories in the same way and for each one of them check: (a) If, set, update p (E, t) and go to Step 4. (b) If Î“ just reached, go to Step 4. (c) If Î“ = Î“ max, output "Maximum number of steps reached, increase Î“ max ", output the lowest energy value found, the predictedand the quality of fit for, then halt. Step 4: using the p (E, t) distributions, estimate the escape rates Îº (E) as described in the corresponding Methods section. Step 5: the Îº (E) curve is extrapolated to the E âˆ’ 1 value obtaining Îº (E âˆ’ 1) and then using this we predict(as described in another Methods section). Further extrapolating the Îº (E) curve to Îº = 0 we obtain(see the corresponding Methods section). Step 6: we check the consistency of the prediction defined here as saturation of the predicted values. We call it consistent, ifhas not changed during the last 5 predictions. If it is not consistent yet, we continue running new trajectories (Step 4). If the prediction is consistent, we check for the following halting conditions: (i) Ifthen we decide the global optimum has been found:and skip to Step 7. (ii) If the fitting is consistently predicting(usually it is very close,) we check the number of trajectories that has attained states with, i.e. = . If it is large enough (e.g. > 100), we decide to stop running new trajectories and setand go to Step 7. (iii) Ifthen we most probably have not found the global optimum yet and we go to Step 4. We added additional stopping conditions that can shorten the algorithm in case of easy problems, see Methods corresponding section, but these are not so relevant. Step 7: the algorithm ends and outputs, values, the Boolean variables corresponding to the optimal state found, along with the quality of fit.

---

### Multistability and anomalies in oscillator models of lossy power grids [^49da9f59]. Nature Communications (2022). High credibility.

Theorem 5

Given a dissipative flow network problem, define the odd weighted Laplacian L f and the even weighted degree matrix D g. If, for all i âˆˆ {1,â€¦, n }, then there exists a sufficiently small step size Ïµ > 0 such that the flow mismatch iteration S Ïµ [equation (37)] is contracting.

The left-hand side of equation (45) quantifies the amount of dissipation that is "seen" at each node of the network, which vanishes for lossless couplings. The right-hand side accounts both for the strength of the coupling between each pair of oscillators, through the weights, and for the connectedness of the graph, Î» 2 being the algebraic connectivity. Under our assumptions [G u is connected, couplings are strictly increasing on R (Î³)], the right-hand side of equation (45) is necessarily positive. For couplings with sufficiently low dissipation, equation (45) is then satisfied, which, combined with Theorem 4, yields the following corollary.

Corollary 6

If equation (45) is satisfied, then there is at most a unique synchronous state of equation (2) in each winding cell. The number of synchronous states is then bounded from above by the number of winding cells.

Theorem 5 and Corollary 6 give a rigorous, even though conservative, sufficient condition for at most uniqueness of synchronous states in each winding cell. However, computing the eigenvalues of the odd weighted Laplacian can be time-consuming. We, therefore, propose some lower bounds on Î» 2 (L f) in the Supplementary Information (Prop. S2) that are state-independent and may ease the verification of equation (45). The bounds are adapted from standard results of algebraic graph theory.

---

### High-brightness scalable continuous-wave single-mode photonic-crystal laser [^372de56d]. Nature (2023). Excellent credibility.

Methods

Derivation of frequencies and radiation constants of modes A and C

The frequency and radiation constant (Î´ A, Î± A) of mode A, which has the lowest radiation constant among all four modes, and those (Î´ C, Î± C) of its counterpart mode C can be obtained by solving the following equation based on three-dimensional coupled-wave theory,:

The first and second terms on the right-hand side of equation (2) represent the Hermitian and non-Hermitian coupling processes described in Fig. 1c, d, respectively. The third term on the right-hand side of equation (2) represents the deviation of the wave number from the Î“ point in an arbitrary direction represented by wave numbersand, which induces a change in the eigenfrequency of each mode.

Here, we consider the eigenfrequencies of the modes in the Î“â€“M direction, which is parallel to the axis of symmetry of the double-lattice photonic crystal (y = x). Based on the symmetry along y = x, the coupled-wave matrices on the right-hand side of equation (2) can be block diagonalized using the basis-transformation matrix P:

as

Then, the coupled-wave equation (2) can be divided into the following two forms:andwhere equation (5) gives the coupling between a pair of electric-field vectors R x + R y and S x + S y, which leads to the formation of modes A and C (Supplementary Fig. 1).

The frequencies and radiation constants of modes A and C can be then derived from equation (5) as follows:where Î´ A + i Î± A /2 of mode A and Î´ C + i Î± C /2 of mode C correspond to the negative and positive square-root terms, respectively, and R and I are defined asand, respectively.

Note on the threshold gain margin Î” Î± v

It is difficult to specify a general value of Î” Î± v sufficient for single-mode oscillation in PCSELs. However, we have found that increasing Î” Î± v by simultaneously reducing R and Î¼ contributes to the preservation of single-mode oscillation even in the presence of a non-uniform in-plane refractive index distribution borne by various physical phenomena. These findings will be reported separately.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^dee6cacc]. CDC (2011). Medium credibility.

Community viral load sample size calculations â€” sample size to detect differences in geometric mean (GM) viral load depends on power and standard deviation (S). The table specifies "Î± = 0.05 and W = 0.8," with S columns 1 to 1.5; for k = 3 the minimum sample sizes by S are 54, 66, 78, 92, 106, and 122, and "1.2 is the standard deviation of national VL data," so at S = 1.2 the needed sample size is 78.

---

### Quantum mechanics as classical statistical mechanics with an ontic extension and an epistemic restriction [^493d213c]. Nature Communications (2017). Medium credibility.

How does the phase space distribution evolve over time? It is clear from the above construction that it is determined by the time evolution of S C and Ï, since this pair yields the phase space distribution. The time evolution of Ï depends, via the continuity equation, on the velocity field, which in turn is determined by S C via Eq. (1). Hence, to obtain the time evolution of the phase space distribution, it suffices to know the dynamical equation for S C (q; t). Of course, we need constraints, characterizing the dynamics of the ensemble trajectories in phase space, to single out the dynamics of S C (q; t). The following theorem, proved in "Methods" subsection "Proof of Theorem 1", then applies:

Theorem 1. Consider an ensemble of trajectories satisfying Eq. (1) or equivalently Eq. (3). For a classical Hamiltonian H (p, q) up to second order in momentum, the constraint that the ensemble of trajectories conserves the probability current and average energy implies a unique dynamics for S C (q, t), given by the Hamiltonâ€“Jacobi equation, Eq. (2):The Hamiltonâ€“Jacobi equation immediately implies the usual Liouville equation describing the time evolution of the phaseâ€“space distribution.

In the course of the paper, we shall not explicitly use Theorem 1. However, as mentioned earlier, it provides physical insight into, and thus anticipates, Theorem 3 below, which states that the SchrÃ¶dinger equation can likewise be derived by imposing conservation of average energy and probability current. The only difference is that rather than constraining the ensemble of trajectories to satisfy Eq. (3), which is necessary for the derivation of the Hamiltonâ€“Jacobi equation, for deriving the SchrÃ¶dinger equation we need a different class of trajectories, satisfying certain constraints to be discussed in the next subsection (cf. Eq. 6). On the one hand, this line of reasoning will provide a smooth formal and conceptual quantum-classical correspondence. On the other hand, we expect it to also tell us what really distinguishes quantum mechanics from classical statistical mechanics.

---

### Input-output maps are strongly biased towards simple outputs [^50fca0dd]. Nature Communications (2018). Medium credibility.

Many systems in nature can be described using discrete input-output maps. Without knowing details about a map, there may seem to be no a priori reason to expect that a randomly chosen input would be more likely to generate one output over another. Here, by extending fundamental results from algorithmic information theory, we show instead that for many real-world maps, the a priori probability P(x) that randomly sampled inputs generate a particular output x decays exponentially with the approximate Kolmogorov complexity [Formula: see text] of that output. These input-output maps are biased towards simplicity. We derive an upper bound P(x)â‰²[Formula: see text], which is tight for most inputs. The constants a and b, as well as many properties of P(x), can be predicted with minimal knowledge of the map. We explore this strong bias towards simple outputs in systems ranging from the folding of RNA secondary structures to systems of coupled ordinary differential equations to a stochastic financial trading model.

---

### Choosing phase II endpoints and designs: evaluating the possibilities [^a7f48ef2]. Clinical Cancer Research (2012). Low credibility.

Selecting the phase II design and endpoint to achieve the best possible chance of success for a confirmatory phase III study in a particular disease and treatment setting is challenging but critical. Simulating from existing clinical trial data sets and from mathematical models can be useful tools for evaluating statistical properties.

---

### Prominent josephson tunneling between twisted single copper oxide planes of BiSrLaCuO [^bf9007ac]. Nature Communications (2023). High credibility.

Josephson tunneling in twisted cuprate junctions provides a litmus test for the pairing symmetry, which is fundamental for understanding the microscopic mechanism of high temperature superconductivity. This issue is rekindled by experimental advances in van der Waals stacking and the proposal of an emergent d+id-wave. So far, all experiments have been carried out on Bi 2 Sr 2 CaCu 2 O 8+x (Bi-2212) with double CuO 2 planes but show controversial results. Here, we investigate junctions made of Bi 2 Sr 2-x La x CuO 6+y (Bi-2201) with single CuO 2 planes. Our on-site cold stacking technique ensures uncompromised crystalline quality and stoichiometry at the interface. Junctions with carefully calibrated twist angles around 45Â° show strong Josephson tunneling and conventional temperature dependence. Furthermore, we observe standard Fraunhofer diffraction patterns and integer Fiske steps in a junction with a twist angle of 45.0 Â± 0.2Â°. Together, these results pose strong constraints on the d or d+id-wave pairing and suggest an indispensable isotropic pairing component.

---

### Finite temperature quantum annealing solving exponentially small gap problem with non-monotonic success probability [^066f3172]. Nature Communications (2018). Medium credibility.

The rates Î³ (Ï‰) satisfy the quantum detailed balance condition, Î³ (âˆ’ Ï‰) = e âˆ’ Î²Ï‰ Î³ (Ï‰), where Ï‰ â‰¥ 0. In our model each qubit is coupled to an independent pure-dephasing bath with an Ohmic power spectrum:with UV cutoff Ï‰ c = 8 Ï€ GHz and the dimensionless coupling constant Î·g 2 = 1.2 Ã— 10 âˆ’4. The choice for the UV cutoff satisfies the assumptions made in the derivation of the master equation in the Lindblad form. Note that we do not adjust any of the master equation parameter values, which are taken from Ref. Details about the numerical solution procedure are given in Section Methods, and in Supplementary Note 5 we also confirm that the validity conditions for the derivation of the master equation are satisfied for a relevant range of n values given the parameters of our empirical tests.

Numerically solving the master equation while accounting for all thermally populatedstates is computationally prohibitive, but we can partly verify our interpretation by restricting the evolution of the system described in Eq. (8) to the vacuum and single-fermion states. This is justified in Supplementary Note 3, where we show that transitions between states differing by more than a single fermion are negligible. In other words, the dominant thermal transitions occur from the vacuum to the single-fermion states, from the single-fermion states to the two-fermion states, etc. The restriction to the vacuum and single-fermion states further simplifies the master Eq. (8) to:whereare the single-particle fermion energy populations and {Î» b } their energies found by solving Eq. (5), and M ab [Eq. (9)]. For a better approximation that accounts for more states, we can also perform a two-fermion calculation where we keep the vacuum, the first k * one-fermion states and the next k * (k * âˆ’ 1)/2 two-fermion states. For two-fermion simulations the master equation becomes Eqs. (11) and (12) along withwhere all summations run from 1 to k *, and p ij denotes the population in the two-particle fermion energy state.

---

### Model selection: finding the right fit [^6940b900]. The Journal of Foot and Ankle Surgery (2019). Medium credibility.

There are numerous possible goals for building statistical models. Those statistical goals, the associated model types, and each statistical tool involved in model building come with its own assumptions and requirements. In turn, these requirements must be met if we are to ensure that our models produce meaningful, interpretable results. However, beyond these technical details is the intuition, and the additional set of tools and algorithms, used by the statistician, to build the contextually appropriate model: not only must we build an interpretable model, we must build a model that answers the particular question at hand and addresses the particular goal we have in mind. In this column we discuss the methods by which statisticians build models for description, risk factor identification, and prediction.

---

### Modeling macroscopic brain dynamics with brain-inspired computing architecture [^408b57b0]. Nature Communications (2025). High credibility.

Prior studies, employed a uniform time step across the entire model. However, the above results and analyses reveal that distinct variables necessitate different time step sizes in low-precision simulation. Therefore, we propose a multi-timescale simulation approach for the low-precision brain dynamics model, allowing each variable to update at different time intervals. The term "multi-timescale simulation" is used because the method stems from the different evolution rates among variables in the dynamic system. However, it is important to clarify that this is a computational strategy and is distinct from the neuroscientific concept of temporal scales. Figure 2 c illustrates an example of this method. We first set the smallest time step sizefor the "fastest" variable (e.g. Î” t S in Fig. 2 c), and then set the time steps of other variables to be multiples of. Compared to normal simulation, multi-timescale simulation needs to address the interdependencies among variables with different time steps. We establish two rules to solve the problem. First, if a variable x with a time step of t x is updated to a value x [t] at time t, its value remains x [t] during the interval from t to t + t x. This rule ensures that the value of each variable is defined at every moment during the simulation. Second, when a variable x with a time step of t x is updated at time t and depends on a variable y with a different time step of t y, it needs to use the historical value of y at time t âˆ’ t x (i.e. the time of the last update of x) rather than at time t âˆ’ t y (i.e. the time of the last update of y). In this way, when the update of a "fast" variable depends on "slow" variables or vice versa, we can select reasonable values for differential equations during the simulation process. Based on the observations from Supplementary Fig. S2 and empirical tuning, we determine the specific time steps for each variable in our simulation. Our experimental results demonstrate that the proposed discretization method is effective, at least within the context of resting-state whole-brain model simulation. However, we acknowledge that this approach has inherent limitations: when a dynamical model cannot achieve effective low-precision simulation with uniform time steps and exhibits high sensitivity to temporal resolution, higher data precision may be the only viable solution to maintain adequate numerical accuracy.

---

### Causality matters in medical imaging [^06b31f0c]. Nature Communications (2020). High credibility.

To facilitate this task, we offer in Table 3 a step-by-step summary of our recommendations, and Fig. 5 presents a generic 'scaffold' diagram from which most typical workflows can be adapted. Readers may then refer to the other tables for help in identifying the components of their own diagram for the problem at hand. We believe that this exercise of building the full causal story of a dataset will encourage analysts to consider potential underlying biases more thoroughly, and that it may, like the TRIPOD checklist, lead to 'more comprehensive understanding, conduct, and analysis of prediction model studies'.

Table 3
Step-by-step recommendations.

Fig. 5
A 'scaffold' causal diagram summarising typical medical imaging workflows.

We believe most practical cases can be adapted from this generic structure by removing or adding elements. Here are represented a variety of possible prediction targets (marked Y 1 â€“ Y 4): some anticausal (Y 1, Y 2) and others, causal (Y 3, Y 4). 'Annotation' here refers to any image-derived data, such as lesion descriptions, regions of interest, spatial landmark coordinates, or segmentation maps. Note that annotators will often be aware of the patients' records and diagnoses, in which cases there could be additional arrows from Y 1 or Y 2 towards Y 4.

---

### Theory and rationale of interpretable all-in-one pattern discovery and disentanglement system [^7a7dd585]. NPJ Digital Medicine (2023). Medium credibility.

Methods

Introduction

Before we present the notations, definitions, and the equations of PDD, we first give a high-level conceptual view of the methodology. The following items (labeled with the header M) represent the methodologies we developed in support of the PDD paradigm. We link them to the definitions, algorithmic steps, and experimental evidence. Specifically, we, via M1, use statistical residual (SR) (Definition 1), SR-Matrix and AV-Vector (Step 1 and Definition 2), Principal Component Analysis (Step 2) to obtain a small set of Statistically Significant Association Disentangled Spaces DS (Step 2, Step 3, and Definition 6).

M2, obtain from each DS AV clusters (Step 3 Definition 8) and statistically significant attribute association patterns (Step 5, Definition 10) and pattern clusters (Step 5) without class labels given, or the need for feature engineering and machine training (Step 6 and Step 7) â€” achieving AV clustering, pattern discovery, pattern clustering and entity clustering in an All-in-One System.

M3, use the intersection of AV Entity ID lists (Definition 3, Fig. 1c, d) of AVs in an AV Group (Definition 7) from the Entity ID Address Table (created at the outset in Step 1, Definition 3) to obtain the frequency count of AV Groups for Frequency-Matrix construction (Step 1) and pattern discovery (Step 5, Fig. 1d) â€” shifting the search of association or discriminative information or patterns from entities employed in the existing ML models to a direct simple table-lookup and intersecting computation process for pattern discovery and class association â€” a great complexity reduction of the search process in ML.

M4, use dataset with class labels to exploit the ground truth, and dataset without class label to minimize label bias to obtain auto-error-correcting class associations (Step5, Step 6 Step 7 and Definition 10) and entity clusters (Step 5, Step 6, Step 7 and Definition 11) â€” an unique way of combining the strength of supervised and unsupervised methods in auto-correcting class association and entity clustering;

---

### Modeling the effect of hyperoxia on the spin-lattice relaxation rate R1 of tissues [^7c4335b6]. Magnetic Resonance in Medicine (2022). Medium credibility.

Estimating Oxygen Consumption Ratefromand R t

Using the Kroghâ€Erlang solution (Equation 9) also requires knowledge of the maximum tissue oxygen consumption rate, M 0. Once R t is found, we can derive M 0 from the OEF using the following logic and assumptions. It is important to note that in order for the assumptions of the following equations to be valid, M 0 must be calculated in the normoxic state (i.e. the patient is breathing air). Under this assumption, the SO 2 along the capillary can be calculated using the linear equation:Using Equation (6), which relates mean SO 2 to the OEF, we can create a linear equation of SO 2 along z, the mean SO 2 in the capillary (SO 2 cap, mean) will be equal to the linear Equation (14) at 0.5 L. Therefore, M 0 can be related to OEF using the following steps:which can be rearranged to calculate M 0 :Finally, with M 0 and R t estimated from the OEF and blood volume, the tissue PO 2 (intoâ€tissue axis) can be calculated. Since the PO 2 along the capillary (P cap) on air and oxygen breathing is known from Step 1, the PO 2 throughout the Krogh cylinder can be calculated for both normoxic and hyperoxic situations (shown in Supporting Information Figure S4).

2.4 Step 3. Calculatingin the three compartments

---

### The making of the standard model [^0fc9a4ce]. Nature (2007). Excellent credibility.

A seemingly temporary solution to almost a century of questions has become one of physics' greatest successes.

---

### Estimating with confidence the risk of rare adverse events, including those with observed rates of zero [^766792a7]. Regional Anesthesia and Pain Medicine (2002). Low credibility.

Omission of a confidence interval (CI) associated with the risk of a serious complication can lead to inaccurate interpretation of risk data. The calculation of a CI for a risk or a single proportion typically uses the familiar Gaussian (normal) approximation. However, when the risk is small, "exact" methods or other special techniques should be used to avoid overshooting (risks that include values outside of [0,1]) and zero width interval degeneration. Computer programs and simple equations are available to construct CIs reasonably accurately. In the special case in which the complication has not occurred, the risk estimated with 95% confidence is no worse than 3/n, where n is the number of trials.

---

### Practical guide to understanding multivariable analyses: part aâ€¦ [^021c7d3a]. Otolaryngology â€” Head and Neck Surgery (2013). Low credibility.

Multivariable analyses are complex statistical methods to evaluate the impact of multiple variables on outcomes of interest. Books have been written on each of these methods detailing the mathematical and statistical objectives and processes. However, we have found very little in the way of brief reports that help the nonstatistically trained physician obtain a basic understanding of multivariable analyses in order to have some understanding of the increasing literature using these methods. This work is organized in 2 parts. This article, Part A, addresses the "big 4" algebraic methods of multivariable analysis. The primary focus of Part A is to present a brief "primer" to help the reader understand the methods and uses; it expressly avoids the many details of statistical assumptions, calculations, and myriad branching alternatives. Part B will concentrate on conjunctive consolidation and will focus on enough information to allow the interested reader to actually perform the analysis. For the statistical scholar, we have included references to several voluminous serious works.

---

### Developing clinical prediction models: a step-by-step guide [^136c5a2c]. BMJ (2024). Excellent credibility.

Step 7: Deal with missing data

General considerations on missing data

After removing predictors or outcomes with many missing values, as outlined in step 5, we might still need to address missing values in the retained data. Relying only on complete cases for model development â€” that is, participants with data for all variables â€” can dramatically reduce the sample size. To mitigate the loss of valuable information during model development and evaluation, researchers should consider imputing missing data.

---

### Minimum electric-field gradient coil design: theoretical limits and practical guidelines [^13a4a404]. Magnetic Resonance in Medicine (2021). Medium credibility.

FIGURE 4
A, A region contours of constant Bz for Y gradient coil, with (red) and without (black) E max optimization, showing that gradient distortion is virtually identical despite the peak Eâ€field being reduced by a factor of 2.8 for the Y coil. B, Difference in the B y components of the gradient field with versus without Eâ€field optimization. Values are normalized to the gradient strength yielding units of length (mm) and show a highly uniform concomitant yâ€directed field added to the optimized (asymmetric) solution. C, Wire pattern representing the difference between optimized (asymmetric) and nonoptimized (symmetric) solutions with same current scaling as Figure 3. Similar results are obtained for the X gradient coil

The key difference between the Eâ€field minimized and nonminimized solutions is the addition of a uniform concomitant B 0 field component in the x or y direction. Figure 4B shows a difference plot of the concomitant B y field, normalized to gradient strength (units of millimeters), which is responsible for the reduction in Eâ€field. Figure 4C shows the winding that would need to be added to the Y symmetric coil (Figure 3B) to obtain the winding pattern of the Y asymmetric coil (Figure 3D), and shows the fundamental difference between the two solutions. The winding in Figure 4C produces a uniform field transverse to the main B 0 field, resulting in a concomitant component with additional magnetic stored energy. Conceptually, this can be thought of as a second coil, which if activated, converts the symmetric coil into an asymmetric coil with reduced Eâ€field but otherwise identical distortion. The added concomitant field in Figure 4B is highly uniform with a 0.85% variation over the 26â€cm imaging region, resulting in minimal distortion of the desired gradient field; this concomitant field can be compensated with a phase/frequency offset correction by the scanner. Figure 4B shows that the average concomitant field of 88.5 mm within the imaging region can be directly compared with the z 0x defined by Meier et al, 29 in which a value of 127 mm was reported for an asymmetric head gradient. The z 0x for both the ESP and HG2 gradients are approximately 120 mm 30; these are both fully asymmetric designs (single eye per half coil with all return currents flowing above the head), similar to the original concept defined by Roemer. 12 The X coil of Figure 3 requires a smaller concomitant field (z 0y = 57.8 mm) than the Y coil, reflecting a decreased need to pull Eâ€field off the torso region due to the smaller extent of the body in the anteriorâ€“posterior direction.

---

### Causal diagrams and multivariate analysis II: precision work [^8dbed18b]. The Journal of Foot and Ankle Surgery (2014). Low credibility.

In this Investigators' Corner, I continue my discussion of when and why we researchers should include variables in multivariate regression. My examination focuses on studies comparing treatment groups and situations for which we can either exclude variables from multivariate analyses or include them for reasons of precision.

---

### Dimensionless learning based on information [^90f574e3]. Nature Communications (2025). High credibility.

The applications of dimensional analysis extend beyond the construction of dimensionally consistent physical laws. It provides the conditions under which two systems share identical behavior (dynamic similarity), allowing predictions from laboratory experiments to be extended to real-world applications. Dimensional analysis also facilitates dimensionality reduction, i.e. simplifying physical problems to their most fundamental forms, thus decreasing the amount of data required for analysis. Another application is the discovery of self-similar variables, for which systems exhibit invariant solutions under appropriate scaling. Finally, dimensional analysis can reveal the physical regimes in which different phenomena dominate (e.g. incompressible versus compressible flow), enabling researchers to identify the most influential physical mechanisms in a given context.

A key landmark of dimensional analysis is the Buckingham- Ï€ theorem, which offers a systematic framework for deriving dimensionless variables. However, the solution is not unique, as there are infinitely many possible ways to construct these variables. To address this limitation, recent studies have developed data-driven tools to identify unique dimensionless variables that minimize the error for a given model structure, particularly in the data-rich context enabled by modern simulations and experiments. These methods combine dimensional analysis with machine learning techniques to identify dimensionless variables using multivariate linear regression, polynomial regression â€“, ridge regression, hypothesis testing, Gaussian process regression, neural networks, sparse identification of nonlinear dynamics, clustering, symbolic regression, and entropic principles. Table 1 offers a non-exhaustive comparative overview of data-driven methods for discovering dimensionless variables, highlighting their respective capabilities. These include applicability to ordinary and partial differential equations (ODEs/PDEs), the ability to rank dimensionless variables by predictability (and thus relative importance), identification of distinct physical regimes, detection of self-similar behavior, and extraction of characteristic scales (e.g. length and time scales of the system). Another key capability is whether the method can determine a bound on the minimum possible error across all modelsâ€“which in turn enables the definition of a model efficiency. The comparison in Table 1 indicates that, although many approaches incorporate several of these properties, no single method currently supports all of these capabilities simultaneously. One notable shortcoming of previous data-driven methods is that they are not model-free; i.e. the discovery of dimensionless variables relies on a predefined model structure (e.g. linear regressions, neural networks,.). This can lead to potentially biased results, as the dimensionless variables identified may not be optimal for other models. A detailed account of the challenges in dimensionless learning can be found in the Supplementary Materials.

---

### Evidence that homologous X-chromosome pairing requires transcription and ctcf protein [^1748407b]. Nature Genetics (2007). Medium credibility.

X-chromosome inactivation (XCI) ensures the equality of X-chromosome dosages in male and female mammals by silencing one X in the female. To achieve the mutually exclusive designation of active X (Xa) and inactive X (Xi), the process necessitates that two Xs communicate in trans through homologous pairing. Pairing depends on a 15-kb region within the genes Tsix and Xite. Here, we dissect molecular requirements and find that pairing can be recapitulated by 1- to 2-kb subfragments of Tsix or Xite with little sequence similarity. However, a common denominator among them is the presence of the protein Ctcf, a chromatin insulator that we find to be essential for pairing. By contrast, the Ctcf-interacting partner, Yy1 (ref. 8), is not required. Pairing also depends on transcription. Transcriptional inhibition prevents new pair formation but does not perturb existing pairs. The kinetics suggest a pairing half-life of < 1 h. We propose that pairing requires Ctcf binding and co-transcriptional activity of Tsix and Xite.

---

### Long-term memory induced correction to arrhenius law [^495130a6]. Nature Communications (2024). High credibility.

Fig. 4
Check of the approximations of the theory.

a Check of the stationary covariance approximation (i.e.): comparison between Ïˆ Ï€ (t) = Var(x Ï€ (t)) measured in numerical simulations (symbols) and Ïˆ (t) (dashed line: H = 3/8, full line H = 1/4). b Check of Eq. (9): comparison between the value m Ï€ (t) in simulations (symbols) and x 0 Ï• (t) (full line: x 0 = l /2 for H = 3/8; dashed line x 0 = l for H = 1/4). Note that m Ï€ (t) â‰ƒ x 0 Ï• (t) is expected at large times only. c Check of the short-time scaling regime for H = 3/8. d Check of the long-time scaling regime (11) for H = 3/8. In a, c, d, the initial position is drawn from an equilibrium distribution, corresponding to our predictions for x 0 = 0. When present, error bars represent 68% confidence intervals.

General results

This approach first shows unambiguously that the mean FPT is finite. Indeed, we show in SI (Section B) that the solution to Eq. (8) satisfies at long timeswhich can be checked directly in numerical simulations, see figure 4b. This scaling, together with Eq. (6), shows that the mean FPT is finite. This contradicts the results obtained with the generalized Fokkerâ€“Planck equationor with the Wilemski-Fixman approximation. The latter amounts to assuming that the process is at all times in an equilibrium state, and would thus yield m Ï€ (t) â‰ƒ L Ï• (t), leading to an infinite mean FPT when Î± < 1 (as noted earlier in a similar, but out of equilibrium, situation). Beyond this proof of finiteness, our approach yields a quantitative determination of âŒ© T âŒª by solving numerically the integral equation (8) for m Ï€ (t) and next using Eq. (6); this shows quantitative agreement with numerical simulations in Fig. 3.

---

### Developing clinical prediction models: a step-by-step guide [^ae794539]. BMJ (2024). Excellent credibility.

Step 12: Assess the predictive ability of individual predictors (optional step)

In prediction modelling, the primary focus is typically not on evaluating the importance of individual predictors; rather, the goal is to optimise the model's overall predictive performance. Nevertheless, identifying influential predictors might be of interest, for example, when evaluating the potential inclusion of a new biomarker as a routine measurement. Also, some predictors might be modifiable, raising the possibility that they could play a part in prevention if their association with the outcome is causal. Therefore, as an additional, optional step, researchers might want to assess the predictive capacity of the included predictors.

Looking at estimated coefficients in (generalised) linear regression models is a simple way to assess the importance of different predictors. However, when the assumptions of linear regression are not met, for example, when there is collinearity, these estimates might be unreliable. However, note that multicollinearity does not threaten a model's predictive performance, just at the interpretation of the coefficients. Another method to assess the importance of a predictor, also applicable to machine learning models, is to fit the model with and without this predictor and note the reduction in model performance; omitting more important predictors will lead to a larger reduction in performance. More advanced methods include the permutation importance algorithmand SHAP (Shapley additive explanations); we do not discuss these here.

Regardless of the method we choose to assess predictor importance, we should be careful in our interpretations; associations seen in data might not reflect causal relationships (eg, see the "Table 2 fallacy"). A thorough causal inference analysis is needed to establish causal associations between predictors and outcomes.

---

### Developing clinical prediction models: a step-by-step guide [^854a7b21]. BMJ (2024). Excellent credibility.

Step 8: Fit the prediction models

Modelling strategies

The strategies for model development should be specified in the protocol (step 5). Linear regression for continuous outcomes, logistic regression for binary outcomes, and Cox or simple parametric models for survival outcomes are the usual starting points in modelling. If the sample size is large enough (see step 6), models can include nonlinear terms for continuous predictors or interactions between predictors. More advanced modelling strategies, such as machine learning models (eg, random forests, support vector machines, boosting methods, neural networks, etc), can also be used. These strategies might add value if there are strong nonlinearities and interactions between predictors, although they are not immune to biases. As discussed under step 10, a final strategy needs to be selected if several modelling strategies are explored.

Dealing with competing events

When predicting binary or time-to-event outcomes, we should consider whether there are relevant competing events. This situation occurs when several possible outcomes exist, but a person can only experience one event. For example, when predicting death from breast cancer, death from another cause is a competing event. In this case, and especially whenever competing events are common, we should use a competing risks model for the analysis, such as a cause specific Cox regression model. A simpler approach would be to analyse a composite outcome.

Data driven variable selection methods

We advise against univariable selection methods â€” that is, methods that test each predictor separately and retain only statistically significant predictors. These methods do not consider the association between predictors and could lead to loss of valuable information. Stepwise methods for variable selection (eg, forward, backwards, or bidirectional variable selection) are commonly used. Again, they are not recommended because they might lead to bias in estimation and worse predictive performance. If variable selection is desirable â€” for instance, to simplify the implementation of the model by further reducing the number of predetermined predictors â€” more suitable methods can be used as described below.

---

### Thinking outside the curve, part II: modeling fetal-infant mortality [^27ea1e69]. BMC Pregnancy and Childbirth (2010). Low credibility.

a. Simulation study to calibrate confidence intervals

We simulated 25 overlapping data sets of size 50,000, the degree of overlap consistent with a population of 200,000, based on the specifications in Table 2. The mixture density and the risk functions were chosen to mimic the patterns actually observed for white singletons born to heavily-smoking mothers; see panel b of Figure One from the previous paper and Figure 1d of the present paper. For each of various C between 2.0 and 5.0, we used Equation (6) to form confidence intervals for mortality risks at selected birthweights, namely r 1 (Î¼ 1 - Ïƒ 1), r 1 (Î¼ 1), r 1 (Î¼ 1 + Ïƒ 1), r 2 (Î¼ 2 - Ïƒ 2), r 2 (Î¼ 2), r 2 (Î¼ 2 + Ïƒ 2), r 3 (Î¼ 3 - Ïƒ 3), r 3 (Î¼ 3), r 3 (Î¼ 3 + Ïƒ 3), r 4 (Î¼ 4 - Ïƒ 4), r 4 (Î¼ 4), and r 4 (Î¼ 4 + Ïƒ 4). Above, Î¼ j and Ïƒ j denote the mean and standard deviation of the birthweights in component j (1 â‰¤ j â‰¤ 4). This was repeated nine more times, and we tabulated how many of the 120 = 12 Ã— 10 confidence intervals contained their targets. Confidence intervals were also formed using Equation (5) for comparative purposes. The above steps were repeated with overlapping data sets consistent with a population of 1,000,000 and with nonoverlapping data sets consistent with an effectively infinite population.

Table 2
Mixture Model and Mortality Functions for Simulation Study

The probability density for the mixture model used in our simulation study is specified, as are the mortality risk functions associated with the mixture model components. Above, z is defined as (x - 3000)/1000, where x is birthweight in grams.

The results are summarized in Table 3. With an effectively infinite population, only 75.0% of the confidence intervals formed using Equation (5) contained their targets at C = 5.0. On the other hand, the confidence intervals formed using Equation (6) contained their targets 95.0% of the time at C = 4.0. The latter finding provided the rationale for taking C 0 = 4.0 when constructing confidence intervals for mortality risks in our examples with real data.

---

### Cross-orientation suppression in visual area V2 [^676e7153]. Nature Communications (2017). Medium credibility.

Quadratic convolutional model

The model seeks to predict a neuron's response Y t (measured as a number of action potentials/spikes) given the stimulus X t presented to the animal. The first step of the model is to convolve the identical subunits with the stimulus, which is equivalent to extracting patches x i, t from the stimulus from different positions and times. The patches are passed through logistic subunits with identical quadratic filter J, linear filter v (1), and bias a (1) to produce the first layer's response

where Ïƒ(x) is the logistic function

To take into account the possibility of the overall position invariance, the responses of the quadratic logistic subunits are pooled using the weights v (2) and rectified

where d is a scaling factor, a (2) is another scalar bias, and R + (x) is the soft-plus rectifier

All of the parameters of the model (a (1), v (1), J, v (2), a (2), d) were fit by minimizing the Poisson negative log-likelihood

using stochastic gradient descent. For our analysis of V2 data, stimuli were 20 Ã— 20-pixels by 10 frames in time, binned at 16 ms. The patches x were 16 Ã— 16-pixels by one frame, making v (1) a 256-dimensional vector, and J a 256 Ã— 256 matrix. The offsets in pixel-space were 1-pixel, which produced a 5 Ã— 5 spatial grid for pooling responses of the quadratic subunits defined by equations (1) and (2). We also used 10 latencies to account for the neural response dynamics, which in total results in 5 Ã— 5 Ã— 10 = 250 dimensional vector v (2).

We divided the training data into fourths and used three-fourths to calculate the gradient and one-fourth as a validation set to determine when to stop training. For each neuron, we calculated four models each with a different fourth of the data as its validation set (Supplementary Fig. 7). The four models for each neuron were averaged, while variability between these can be used to gauge the impact of noise variability and stochasticity in the optimization process on model parameters.

The data set also included a separate set of responses to different repeated movies. Those data were used to evaluate model performance and are separate from the data used for fitting and early stopping. The model performance was evaluated in terms of correlation coefficients between measured and predicted responses after compensating for finite-size effects in the data.

---

### Abstract representations of events arise from mental errors in learning and memory [^748e9b84]. Nature Communications (2020). High credibility.

Estimating parameters and making quantitative predictions

Given an observed sequence of nodes x 1,â€¦, x t âˆ’1, and given an inverse temperature Î², our model predicts the anticipation, or expectation, of the subsequent node x t to be. In order to quantitatively describe the reactions of an individual subject, we must relate the expectations a (t) to predictions about a person's reaction timesand then calculate the model parameters that best fit the reactions of an individual subject. The simplest possible prediction is given by the linear relation, where the intercept r 0 represents a person's reaction time with zero anticipation and the slope r 1 quantifies the strength with which a person's reaction times depend on their internal expectations.

In total, our predictionscontain three parameters (Î², r 0, and r 1), which must be estimated from the reaction time data for each subject. Before estimating these parameters, however, we first regress out the dependencies of each subject's reaction times on the button combinations, trial number, and recency using a mixed effects model of the form 'RT~log(Trial)*Stage+Target+Recency+(1+log(Trial)*Stage+Recency|ID)', where all variables were defined in the previous section. Then, to estimate the model parameters that best describe an individual's reactions, we minimize the RMS prediction error with respect to each subject's observed reaction times, where T is the number of trials. We note that, given a choice for the inverse temperature Î², the linear parameters r 0 and r 1 can be calculated analytically using standard linear regression techniques. Thus, the problem of estimating the model parameters can be restated as a one-dimensional minimization problem; that is, minimizing RMSE with respect to the inverse temperature Î². To find the global minimum, we began by calculating RMSE along 100 logarithmically spaced values for Î² between 10 âˆ’4 and 10. Then, starting at the minimum value of this search, we performed gradient descent until the gradient fell below an absolute value of 10 âˆ’6. For a derivation of the gradient of the RMSE with respect to the inverse temperature Î², we point the reader to the Supplementary Discussion. Finally, in addition to the gradient descent procedure described above, for each subject we also manually checked the RMSE associated with the two limits Î² â†’ 0 and Î² â†’ âˆž. The resulting model parameters are shown in Fig. 4 a, b for random walk sequences and Fig. 4 g, h for Hamiltonian walk sequences.

---

### A space-time tradeoff for implementing a function with master equation dynamics [^57fa10b0]. Nature Communications (2019). High credibility.

Master equations are commonly used to model the dynamics of physical systems, including systems that implement single-valued functions like a computer's update step. However, many such functions cannot be implemented by any master equation, even approximately, which raises the question of how they can occur in the real world. Here we show how any function over some "visible" states can be implemented with master equation dynamics-if the dynamics exploits additional, "hidden" states at intermediate times. We also show that any master equation implementing a function can be decomposed into a sequence of "hidden" timesteps, demarcated by changes in what state-to-state transitions have nonzero probability. In many real-world situations there is a cost both for more hidden states and for more hidden timesteps. Accordingly, we derive a "space-time" tradeoff between the number of hidden states and the number of hidden timesteps needed to implement any given function.

---

### As-3 [^4f47e930]. FDA (2025). Medium credibility.

Statement of idenity

ADDITIVE SOLUTION FORMULA 3 (AS-3), 250 mL

---

### A multi-step model of Parkinson's disease pathogenesis [^e2cef97a]. Movement Disorders (2021). Medium credibility.

Box 1
Armitage and Doll Multistep Model â€“ Heuristic Argument*

If disease development depends on one step, incidence in a given year will be proportional to the chance of undergoing that step:If twoâ€steps are required, then incidence is the product of the chance of undergoing the first step by age t and the rate of undergoing the second step:And for nâ€steps:Taking log of both sides returns the equation for a straight line:The slope is one less than the number of steps required to develop the disease, and the intercept represents the combined probability of undergoing these steps.

*see Webster 2019 for full derivation.

A multistep model of pathogenesis could account for many of the epidemiological observations made in PD. These include the variability in the expression of disease and age of onset in carriers of diseaseâ€causing mutations, and the multiple environmental and genetic associations that confer a risk of developing PD. A multistep model could also explain the phenotypic variability seen in PD, if it is assumed that at least some steps apply to specific neuronal populations rather than the nervous system as a whole. Furthermore, the predictions arising from such a model can be used to test specific hypotheses about basic observations in PD, such as whether the higher incidence and prevalence of PD in males observed in most parts of the world relates to differential environmental exposures by sex.

---

### Thinking outside the curve, part II: modeling fetal-infant mortality [^63e34d85]. BMC Pregnancy and Childbirth (2010). Low credibility.

Table 4
Mixture Modeling Results for White Singleton Infants

Parameters in a 4-component normal mixture model for birthweight distribution are estimated, based on 25 samples of size 50,000 from the population of white singletons in general. Confidence intervals are constructed using Equations (6) and (7) with C 0 = 2.5 and Ï† = .0055.

Table 5
Mortality for White Singleton Infants

Mortality risks and odds ratios are estimated at selected birthweights, based on 25 samples of size 50,000 from the population of white singletons in general. Confidence intervals are constructed using Equations (6) and (7) with C 0 = 4.0 and Ï† = .0055.

The confidence intervals for p 2, p 3, Î¼ 2, Ïƒ 4 are considerably narrower than they were for white singletons born to heavily-smoking mothers, as are the confidence bounds for r 1 (x) in the VLBW range, r 2 (x) in the MLBW range, and r 3 (x) in much of the NBW range. The confidence bounds for r 2 (x) do not overlap those for r 3 (x) or r 4 (x) anywhere in the NBW range, indicating heterogeneity in mortality risk. In particular, the odds of mortality at 3000 g are an estimated 9.77 times as large in component 2 as in component 3 (95% confidence interval, 2.35 to 40.6) and an estimated 4.15 times as large in component 2 as in component 4 (95% confidence interval, 2.04 to 8.43).

The reason that mixture parameters and mortality risks are estimated more precisely for white singletons in general than for white singletons born to heavily-smoking mothers is that N rep = 25 samples of size 50,000 from a population of 9,162,303 contain approximately 1,171,467 distinct records, far more than the approximately 202,677 distinct records contained in N rep = 25 samples of size 50,000 from a population of 202,849. (Section II of [Additional file 1] from our previous paper provides a formula from which one may approximate the number of distinct records in multiple samples from the same population.) Even more precise estimation is possible for white singletons in general if N rep is taken larger.

---

### Microbes can help explain the evolution of host altruism [^5bd0ac37]. Nature Communications (2017). Medium credibility.

Equal horizontal transmission probabilities

If both microbes have the same horizontal transmission probability (T Î± = T Î² = T), and there is no baseline altruism among the hosts (b g, c g = 0), the condition for the spread of altruism, derived from equation (6), becomes:

Or (under the constraint that VT > 0 and T â‰ 0):

No horizontal transmission

When there is no horizontal transmission (T Î±, T Î² = 0), no genetic background of altruistic behaviour in the population (b g, c g = 0), and perfect vertical transmission (VT = 1), equation (4) becomes:

It is straightforward to see that foras defined in equation (12) the conditioncan never be satisfied (under the constraint c > 0), that is, altruism cannot evolve.

Altruism determined by host genotype only

We now analyse the classical dynamics of a population in which microbes do not affect altruistic behaviour, and the latter is fully determined by a locus with two alleles. Individuals that possess allele A behave altruistically and pay a fitness cost 0 < c g < 1, whereas the recipient gets a fitness benefit, b g > c g. Individuals that possess allele E do not pay a fitness cost or help their partner. We denote the proportion of individuals with allele A by p, and those with allele E by q = 1âˆ’ p. When deriving, the proportion of allele A in the next generation we get:

which is analogous to equation (12). As was the case for equation (12), condition (13) is never satisfied (under the constraint c g > 0) and therefore altruism cannot evolve if encoded in the host genes.

---

### Efficient estimation of marker effects in plant breeding [^e570d89a]. G3 (2019). Medium credibility.

Iterative single-stage method

The previous section presented how the algorithm for FLM works in the case where each individual has a single phenotypic value. Now consider the scenario of replicated trials, where genotyped individuals are replicated across multiple environments. This approach is here referred to as fast Laplace model in single-stage (FLM-SS). The term "single-stage" has been used to define the joint modeling of replicated observations at the phenotypic level or with genomic information, which is not to be confused with the "single-step" that elsewhere defines models that combine pedigree and genomic information.

Algorithm 1 Fast Laplace model

Setup initial values:

1. Computefor each marker
2. Compute
3. Setas initial value for all

Repeat until convergence:

1. Update intercept
2. Loop formarker in
3. Update residual variance

Most iterative methods provide an alternative way of fitting mixed model without changing its statistical properties. Efficient algorithms to solve complex models with marker information use conditional expectations, going back and forth with the estimation of fixed and random effect coefficients, without altering the outcome. The following model can illustrate the single-stage procedure:where y is the vector of phenotypes, X and b represent the design matrix and fixed effect coefficients used to capture nuisance parameters, such as environmental sources of variation. The random terms Z and a correspond to the incidence matrix of individuals and additive genetic effects, hereby estimated from the WGR. Residuals are assumed to be normally distributed as, such that residuals can be autocorrelated or not.

Fixed effect coefficients are solved via least squares, conditioning the response variable to all terms but the fixed effect. This conditioning works by reshaping the linear model into:Providing the following solution of coefficients:To avoid building large and dense design matrix of marker effects, the random effect coefficients are updated using a link function in two steps. First, estimate the least-squared genetic values as follows:The coefficients are solved as:Note the values ofare not the final random coefficients, but intermediary outcomes serving as input to WGR. The shrinkage of genetic values occurs whenis updated into a.

---

### Developing clinical prediction models: a step-by-step guide [^52143837]. BMJ (2024). Excellent credibility.

Step 9: Assess the performance of prediction models

General concepts in assessing model performance

We assess the predictive performance of the modelling strategies explored in step 8. Specifically, we contrast predictions with observed outcomes for people in a dataset to calculate performance measures. For continuous outcomes like blood pressure this is straightforward: observed outcomes can be directly compared with predictions because they are on the same scale. When dealing with binary or survival outcomes, the situation becomes more complex. In these cases, prediction models might give the probability of an event occurring for each individual while observed outcomes are binary (event or no event) or involve time-to-event data with censoring. Consequently, more advanced methods are required.

Dimensions of prediction performance

Prediction performance has two dimensions, and it is essential to assess them both, particularly for binary and survival outcomes (see glossary in table 1).

Discrimination â€” for continuous outcomes, discrimination refers to the model's ability to distinguish between patients with different outcomes: good discrimination means that patients with higher predicted values also had higher observed outcome values. For binary outcomes, good discrimination means that the model separates people at high risk from those at low risk. For time-to-event outcomes, discrimination refers to the ability of the model to rank patients according to their survival; that is, patients predicted to survive longer survived longer.
Calibration relates to the agreement between observed and predicted outcome values. For continuous outcomes, good calibration means that predicted values do not systematically overestimate or underestimate observed values. For binary and survival outcomes, good calibration means the model does not overestimate or underestimate risks.

Discrimination and calibration are essential when evaluating prediction models. A model can have good discrimination by accurately distinguishing between risk levels, but still have poor calibration owing to a mismatch between predicted and observed probabilities. Moreover, a well calibrated model might have poor discrimination. Thus, a robust prediction model should have good discrimination and calibration. Box 1 outlines measures for assessing model performance.

Box 1
Measures of performance of prediction models for different types of outcomes

---

### A probabilistic model for the MRMC method, part 1: theoretical development [^4e751f00]. Academic Radiology (2006). Low credibility.

Rationale and Objectives

Current approaches to receiver operating characteristic (ROC) analysis use the MRMC (multiple-reader, multiple-case) paradigm in which several readers read each case and their ratings (or scores) are used to construct an estimate of the area under the ROC curve or some other ROC-related parameter. Standard practice is to decompose the parameter of interest according to a linear model into terms that depend in various ways on the readers, cases, and modalities. Though the methodologic aspects of MRMC analysis have been studied in detail, the literature on the probabilistic basis of the individual terms is sparse. In particular, few articles state what probability law applies to each term and what underlying assumptions are needed for the assumed independence. When probability distributions are specified for these terms, these distributions are assumed to be Gaussians.

Materials and Methods

This article approaches the MRMC problem from a mechanistic perspective. For a single modality, three sources of randomness are included: the images, the reader skill, and the reader uncertainty. The probability law on the reader scores is written in terms of three nested conditional probabilities, and random variables associated with this probability are referred to as triply stochastic.

Results

In this article, we present the probabilistic MRMC model and apply this model to the Wilcoxon statistic. The result is a seven-term expansion for the variance of the figure of merit.

Conclusion

We relate the terms in this expansion to those in the standard, linear MRMC model. Finally, we use the probabilistic model to derive constraints on the coefficients in the seven-term expansion.

---

### Evaluation of the evenness score in next-generation sequencing [^1f5f1936]. Journal of Human Genetics (2016). Low credibility.

The evenness score (E) in next-generation sequencing (NGS) quantifies the homogeneity in coverage of the NGS targets. Here I clarify the mathematical description of E, which is 1 minus the integral from 0 to 1 over the cumulative distribution function F(x) of the normalized coverage x, where normalization means division by the mean, and derive a computationally more efficient formula; that is, 1 minus the integral from 0 to 1 over the probability density distribution f(x) times 1-x. An analogous formula for empirical coverage data is provided as well as fast R command line scripts. This new formula allows for a general comparison of E with the coefficient of variation (= standard deviation Ïƒ of normalized data) which is the conventional measure of the relative width of a distribution. For symmetrical distributions, including the Gaussian, E can be predicted closely as 1-Ïƒ(2)/2â©¾Eâ©¾1-Ïƒ/2 with Ïƒ â‰¤ 1 owing to normalization and symmetry. In case of the log-normal distribution as a typical representative of positively skewed biological data, the analysis yields Eâ‰ˆexp(-Ïƒ*/2) with Ïƒ*(2) = ln(Ïƒ(2)+1) up to large Ïƒ (â‰¤ 3), and Eâ‰ˆ1-F(exp(-1)) for very large Ïƒ (â©¾2.5). In the latter kind of rather uneven coverage, E can provide direct information on the fraction of well-covered targets that is not immediately delivered by the normalized Ïƒ. Otherwise, E does not appear to have major advantages over Ïƒ or over a simple score exp(-Ïƒ) based on it. Actually, exp(-Ïƒ) exploits a much larger part of its range for the evaluation of realistic NGS outputs.

---

### What quantile regression does and doesn' T Do: a commentary on petscher and logan (2014) [^220ddc10]. Child Development (2019). Medium credibility.

Petscher and Logan's (2014) description of quantile regression (QR) might mislead readers to believe it would estimate the relation between an outcome, y, and one or more predictors, x, at different quantiles of the unconditional distribution of y. However, QR models the conditional quantile function of y given x just as linear regression models the conditional mean function. This article's contribution is twofold: First, it discusses potential consequences of methodological misconceptions and formulations of Petscher and Logan's (2014) presentation by contrasting features of QR and linear regression. Second, it reinforces the importance of correct understanding of QR in empirical research by illustrating similarities and differences in various QR estimators and linear regression using simulated data.

---

### Thinking outside the curve, part I: modeling birthweight distribution [^49eb3a50]. BMC Pregnancy and Childbirth (2010). Low credibility.

Results

1. Pragmatics for mixture modeling

a. Finite normal mixture models

Many phenomena cannot be accurately described via a normal distribution. When no other commonly used probability distribution seems appropriate, a finite normal mixture model is often reasonable. We now briefly describe the model. Readers interested in theoretical developments may consult references and works cited therein.

Let f (x;Î¼,Ïƒ) denote the probability density for the normal distribution with mean Î¼ and standard deviation Ïƒ. A finite normal mixture model with k components has probability density

A common way to interpret Equation (1) is to imagine that the full population consists of k subpopulations. The proportion of individuals in the full population belonging to subpopulation j is p j. In subpopulation j, measurements are normally distributed with mean Î¼ j and standard deviation Ïƒ j.

The mixture components may or may not represent subpopulations with obvious biological definitions outside the statistical model. For example, in a 2-component normal mixture describing birthweights for white singletons in the United States, there is not an obvious biological characterization for the two components: we may say that the component with the smaller mean reflects compromised pregnancies, but we cannot immediately attribute the compromised pregnancies to a specific biological mechanism.

Ideally, modeling with finite normal mixtures may lead to discoveries of subpopulations with biological definitions that were not immediately obvious, although the mixture components themselves may still only be approximations to such subpopulations.

---

### Testing for Hardy-Weinberg proportions: have we lost the plot? [^61bccd14]. The Journal of Heredity (2014). Low credibility.

Testing for Hardy-Weinberg proportions (HWP) is routine in almost all genetic studies of natural populations, but many researchers do not demonstrate a full understanding of the purposes of these tests or how to interpret the results. Common problems include a lack of understanding of statistical power and the difference between statistical significance and biological significance, how to interpret results of multiple tests, and how to distinguish between various factors that can cause statistically significant departures. In this perspective, which focuses on analysis of genetic data for nonmodel species, I 1) review factors that can cause departures from HWP at individual loci and linkage disequilibrium (LD) at pairs of loci; 2) discuss commonly used tests for HWP and LD, with an emphasis on multiple-testing issues; 3) show how to distinguish among possible causes of departures from HWP; and 4) outline some simple steps to follow when significant test results are found. Finally, I 5) identify some issues that merit particular attention as we move into an era in which analysis of genomics-scale datasets for nonmodel species is commonplace.

---

### A machine learning approach to integrate big data for precision medicine in acute myeloid leukemia [^fdc04a89]. Nature Communications (2018). Medium credibility.

Each method prioritized gene-drug pairs differently (Supplementary Note 10). For evaluation, we computed the true discovery rate (that is, how many significant associations were replicated in the left-out test data) when considering the top N genes per drug on average (i.e. 53 Ã— N gene-drug pairs in total). Specifically, we computed the consistency rate (y -axis) â€” defined as the number of significant gene-drug associations replicated in the left-out test data divided by the total number of significant gene-drug associations within the selected 53 Ã— N gene-drug pairs â€” for varying values of N (x -axis) from one to all genes. In both settings (Fig. 3a, b), MERGE showed a much higher consistency rate for high-scoring genes (small N values) than the other two methods and the random ordering of genes (gray lines). As N increased, methods had more similar consistency rates because their top N genes became more similar.

---

### Modeling uncertainty in single-copy assays for HIV [^c02085f8]. Journal of Clinical Microbiology (2012). Low credibility.

We present a simple computational model of measurement accuracy for single-copy sensitivity assays (SCA) of HIV RNA that was developed from first principles. The model shows that the SCA is significantly right-skewed. Measured virus concentrations of 1 and 10 virions/ml had overlapping 95% confidence intervals and were statistically indistinguishable.

---

### Introduction to statistical modelling 2: categorical variables and interactions in linear regression [^5a3f2495]. Rheumatology (2015). Low credibility.

In the first article in this series we explored the use of linear regression to predict an outcome variable from a number of predictive factors. It assumed that the predictive factors were measured on an interval scale. However, this article shows how categorical variables can also be included in a linear regression model, enabling predictions to be made separately for different groups and allowing for testing the hypothesis that the outcome differs between groups. The use of interaction terms to measure whether the effect of a particular predictor variable differs between groups is also explained. An alternative approach to testing the difference between groups of the effect of a given predictor, which consists of measuring the effect in each group separately and seeing whether the statistical significance differs between the groups, is shown to be misleading.

---

### A general model for ontogenetic growth [^10660e44]. Nature (2001). Excellent credibility.

Several equations have been proposed to describe ontogenetic growth trajectories for organisms justified primarily on the goodness of fit rather than on any biological mechanism. Here, we derive a general quantitative model based on fundamental principles for the allocation of metabolic energy between maintenance of existing tissue and the production of new biomass. We thus predict the parameters governing growth curves from basic cellular properties and derive a single parameterless universal curve that describes the growth of many diverse species. The model provides the basis for deriving allometric relationships for growth rates and the timing of life history events.

---

### How dieting might make some fatter: modeling weight cycling toward obesity from a perspective of body composition autoregulation [^32b99e3d]. International Journal of Obesity (2020). Medium credibility.

Two exponential fits are shown: a generalized linear model (GLM) with 95% confidence intervals (solid line), and a linear model (LM), with R 2 values (dotted line). The four figures correspond to the four methods presented in the main text for computing the value FAT END at the final time END at which the subjects would have completely recovered their initial FFM. The two red squares correspond to the US Army Ranger data points presented in the section on 'Applications of the Model'.

Their exact values depend on the method used for computing the values FAT END when the FFM has been completely recovered and on the type of statistical regression used. As explained previously, we consider four methods for computing FAT END. In Fig. 4, we show the fits obtained from a generalized linear model (GLM) with 95% confidence intervals (solid line) and from a linear model (LM), with R 2 values (dotted line). Following the performance of diagnostics tests to analyze the residuals, it is found that the GLM satisfies the major assumptions of regression analysis better than the LM, especially concerning the normality assumption of the error terms. Nevertheless, as shown in Fig. 4, both models give curves that are close to each other. In simple terms, the main difference between the above LM and GLM approaches to estimate the best parameter values for the constants a and b is the way the error term is handled. In the LM method, we write y = a Ã— e bx Ã— Îµ, where Îµ is a random error variable that follows a log-normal distribution with parameters Î¼ = 0 and Ïƒ 2. Taking the logarithm on both sides of the equality gives, which corresponds to the linear model, where, x â€² = x, Î² 1 = b andis a random error variable that follows a normal distribution with mean Î¼ = 0 and variance Ïƒ 2. Note that in the LM method, the error term Îµ is multiplied with the exponential function y = a Ã— e bx. In the GLM method, the error term is instead added to the exponential function. Formally, we write y = a Ã—e bx + Îµ, where Îµ is a random error variable that follows a normal distribution with mean 0 and variance Ïƒ 2. As a consequence, the GLM admits the possibility of the value y = 0 (which is excluded in LM) and assumes that the variance V (y) is constant, while the LM method assumes that V (y) varies with x. Finally, note that the LM method admits exact analytical solutions for the model parameters a and b, while the GLM method requires numerical optimization algorithms to find the best values (the maximum likelihood estimates) for a and b.

---

### Quantum trajectory framework for general time-local master equations [^ae7b6015]. Nature Communications (2022). High credibility.

Fig. 3
Illustration of the influence martingale for the random unitary model.

Monte Carlo averages versus master equation predictions for the energy level populations of the random unitary model (18). The Lindblad weights are, and. The initial condition of the qubit iswhere g, e are the ground and excited state of Ïƒ 3, respectively. The black full line gives the prediction by solving the master equation (18). The crosses show the stochastic average after 10 3 realizations and the (light) green shaded area displays fluctuation-related errors estimated as in Fig. 2. Similarly, the diamonds show the ensemble average after 10 4 realizations and the (dark) brown shaded area the estimated error.

As a further example of how the influence martingale associates a quantum trajectory picture even to master equations with non positive definite solutions, we consider a Redfield equation model. We call Redfield a master equation of the form (1) obtained from an exact system-environment dynamics by implementing the Bornâ€“Markov approximation without a rotating wave approximation. Redfield equations are phenomenologically known to give accurate descriptions of the system dynamics at arbitrary system-environment coupling although they do not guarantee a positive time evolution of the state operator see e.g. refs. In "Methods", we outline the derivation of a Redfield equation from the exact dynamics of two non-interacting qubits in contact with a boson environment. The result is an equation of the form (1) with only two Lindblad operatorssatisfying the commutation relationsThe corresponding weights are time independentwhere Î³ 1, Î³ 2, Îº are real numbers. We refer to Methods for the explicit expressions of H and. Inspection of (20) shows that Î» 1 is negative definite. Figure 4 shows how the influence martingale reproduces the predictions of the Redfield equation.

Fig. 4
Illustration of the influence martingale for a Redfield equation.

The parameters for the model (20) are Î³ 1 = 1, Î³ 2 = 4, Î± = 3 and Îº = 1. The initial condition iswhere g is the ground state, and. The crosses, diamonds, and x's show the result from Monte Carlo simulations after 10 4 realizations. The full lines are master equation predictions and the shaded regions have the same meaning as in Fig. 2. The time mesh is d t = 0.0125. The diamonds show that Ï t is non positive definite for t > 3.

---

### Standards of care in diabetes â€“ 2025 [^1f4eb314]. Diabetes Care (2025). High credibility.

Regarding diagnostic investigations for diabetes mellitus type 1, more specifically with respect to assessment for hyperglycemic crises, ADA 2025 guidelines recommend to provide structured education on the recognition, prevention, and management of hyperglycemic crisis to patients with T2DM having experienced these events and patients at high risk for these events.

---

### Optimization of breeding program design through stochastic simulation with evolutionary algorithms [^bfcc418f]. G3 (2025). Medium credibility.

Step 3: Select parameter settings

Based on the results from the previous step, we want to identify the most promising areas of the search space to investigate further in later steps. For this, we select the most promising parameter settings to be uses as "parents" for the next iteration, considering three strategies. The number of selected parents based on each step varies based on the iterations of the EA with values given below for iteration 11 onwards (see Table 1). In these iterations, 30 out of 300 parameter settings are selected.

Table 1.
Number of parameter settings selected as parents (step 3) and generated settings (step 4) during the EA optimization process.

Step 3.1: Highest value of the objective function

In the first sub-step, 20 of the 30 most promising parameter settings are identified ("selected") based on the value of the objective function that was derived solely based on the simulation of the parameter settings itself. This guarantees that the best-performing parameter settings are prioritized in the reproduction process, subsequently enhancing the likelihood of generating successful parameter settings.

Step 3.2: Highest expected value of the objective function

In the second sub-step, seven parameter settings are identified based on the highest expected value of the objective function. For this, we are employing the kernel regression method suggested in. By this, instead of evaluating each parameter setting individually, kernel regression computes a weighted average of performance values for multiple parameter settings. In contrast to step 3.1, this approach avoids bias towards region with more parameter settings tested overall. In contrast to, we are here proposing the use of an adaptive bandwidth by using the empirical standard deviation in the individual parameter in the current iteration.

Step 3.3: Highest value of the objective function from the previous iteration

In the third sub-step, three parameter settings from previous iterations are used. This typically refers to the best-performing parameter settings from the second-to-last iteration; however, if the expected performance of any previously suggested optima from any iteration based on kernel regression (see step 5) is higher than all parameter settings of the current iteration, these are added instead. Hereby, the risk of discarding potentially promising parameter settings due to stochasticity is reduced.

Management of parameter setting variability/heterogeneity/value distribution

We implement a strategy to manage variation within the EA, ensuring that highly similar parameter settings are not selected. For additional details on this approach, refer to.

---