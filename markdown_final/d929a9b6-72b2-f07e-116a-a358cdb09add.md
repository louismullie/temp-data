# Medical Question & Answer

**Sample ID**: d929a9b6-72b2-f07e-116a-a358cdb09add
**Dataset Index**: 40133

---

## Question

What's the gold standard diagnostic test?

---

## Answer

> Let's see… What do we have here? The user is asking what constitutes a "gold standard" diagnostic test and how to identify it for a given condition. Let's break this down step-by-step. First, I need to think about the formal definition of a gold standard and its role in diagnostic accuracy studies. Then, I should verify the key characteristics that make a test a good reference standard. Next, I will examine common limitations and biases that arise when the gold standard is imperfect. After that, I should review how to handle situations where no perfect gold standard exists. Finally, I will synthesize a practical approach to identifying the gold standard for a specific disease and illustrate with concrete examples, while double-checking for exceptions and controversies.

> Let me first confirm the definition. A gold standard, also called a reference standard, is the best available diagnostic method against which new tests are compared to estimate sensitivity, specificity, and related accuracy metrics; ideally, it assigns disease status without error, though in practice it is rarely perfect and may comprise a composite of tests or expert adjudication rather than a single assay [^1133fdTg] [^113YM4WH] [^116qdH7e].

> I need to check the core characteristics that make a test a credible gold standard. Validity, reliability, responsiveness, and feasibility are the principal attributes; importantly, the reference should be independent of the index test and applied blindly to avoid incorporation bias, and it should be feasible to perform in the target population and setting where the new test will be used [^1133fdTg] [^115RsKF8].

> Hold on, let's not jump to conclusions about perfection. Most gold standards are imperfect to some degree, which can misclassify patients and bias accuracy estimates for the index test; common problems include partial verification bias, differential verification, incorporation bias, and spectrum bias, all of which can inflate sensitivity or specificity or both depending on the design and analysis choices [^112su5pq] [^116oAXm8] [^1151BaFT].

> Wait, let me verify how to handle missing or imperfect reference data. When the gold standard is invasive, costly, or unethical in some patients, researchers may use alternative reference standards such as clinical follow-up, composite criteria, or latent class models; however, these approaches can introduce bias if not planned prospectively and analyzed appropriately, so transparency and pre-specified handling of missing reference outcomes are essential [^111cR2u6] [^1137qyRF] [^116qdH7e].

> I should confirm the hierarchy of evidence for identifying a gold standard. High-quality evidence typically comes from prospective studies that enroll a broad spectrum of patients with suspected disease, apply the reference standard to all participants, and interpret both index and reference tests blinded to each other; systematic reviews and meta-analyses of such studies further strengthen confidence in the chosen reference standard and its performance characteristics [^114AQZxs] [^116uGzst] [^112o3ar2].

> Let me consider concrete examples to anchor this. For COVID-19, nucleic acid amplification testing (RT-PCR/NAAT) is the accepted gold standard for confirming active infection, though it is imperfect and performance varies with timing and specimen quality; serology is not a substitute for diagnosing acute infection but complements understanding of prior exposure and immune response [^1133gSGL] [^114s1af5]. For cystic fibrosis, quantitative sweat chloride testing remains the diagnostic gold standard despite the availability of genetic testing, reflecting its historical validation and clinical utility [^1163y9Kj]. For COPD, post-bronchodilator spirometry with FEV1/FVC less than 0.70 is the guideline-endorsed reference for persistent airflow limitation, acknowledging that some experts debate the best threshold and that composite clinical assessments can outperform single parameters in certain contexts [^113eVjqR] [^111oBrck]. For asthma, there is no single gold standard; diagnosis relies on a composite of symptoms, objective tests such as FeNO or bronchodilator response, and, when needed, methacholine challenge, reflecting disease heterogeneity and test performance across settings [^116gMrC4].

> But wait, what if there is truly no accepted gold standard at all. In some conditions, such as disseminated intravascular coagulation and certain psychiatric disorders, no single definitive test exists, and diagnosis rests on integrated clinical criteria or expert consensus; in such cases, researchers may use latent class analyses, composite reference standards, or outcome-based adjudication to approximate truth and evaluate new tests, while explicitly stating assumptions and limitations [^114hjtJr] [^115wp7Fx] [^116qdH7e].

> I should double-check the practical steps to identify the gold standard for a specific disease. First, review authoritative guidelines and high-quality systematic reviews to see which test is used as the reference in most accuracy studies. Second, appraise the methodological quality of those studies for biases like partial verification and incorporation. Third, consider whether the reference reflects the clinical question and setting in which the new test will be used. Finally, if the reference is imperfect, plan for composite standards, follow-up outcomes, or statistical adjustments and pre-specify how missing reference data will be handled [^114AQZxs] [^116uGzst] [^111cR2u6].

> In summary, I need to ensure the message is precise: a gold standard is the best available, independently applied, and ideally blinded reference method used to define disease status when evaluating a new diagnostic test; it is rarely perfect, so recognizing and mitigating biases and, when necessary, substituting validated alternatives or latent class approaches are critical to accurate inference and clinical decision-making [^114AQZxs] [^112su5pq] [^112zgsW4].

---

The gold standard diagnostic test is the **best available method** for confirming a disease, against which new tests are compared [^114AQZxs]. It is not always perfect [^114oY2xo]; when no single perfect test exists, a composite reference standard or expert consensus is used [^116qdH7e] [^113YM4WH]. Examples include **RT-PCR for COVID-19** [^1133gSGL], **spirometry for COPD** [^113eVjqR], and **polysomnography for sleep apnea** [^111QmAT7]. Because gold standards can be imperfect, studies should report sensitivity/specificity, use blinding, and account for bias [^115RsKF8] [^112su5pq].

---

## Definition and characteristics

A gold standard is the **reference test** or criterion standard used to establish the presence or absence of a disease [^114AQZxs]. It is the benchmark against which new diagnostic tests are evaluated for accuracy, typically expressed as sensitivity and specificity [^111yQ8iv]. Key characteristics include:

- **High accuracy**: High sensitivity and specificity, minimizing false positives and false negatives [^notfound].
- **Independence**: The reference standard should be independent of the index test being evaluated to avoid incorporation bias [^114AQZxs].
- **Blinded interpretation**: Interpretation of the reference standard should be blinded to the results of the index test to prevent bias [^114AQZxs].
- **Clinical relevance**: The reference standard should reflect the clinical context and population in which the test will be used [^114AQZxs].

---

## Examples of gold standard tests

| **Disease/condition** | **Gold standard test** | **Description** |
|-|-|-|
| COVID-19 | Reverse transcription polymerase chain reaction (RT-PCR) | Detects viral RNA and is considered the gold standard for diagnosing active infection [^1133gSGL] [^114s1af5]. |
| Chronic obstructive pulmonary disease (COPD) | Spirometry | Measures airflow limitation and is the gold standard for diagnosing COPD [^113eVjqR]. |
| Obstructive sleep apnea syndrome (OSAS) | Polysomnography | Comprehensive sleep study that monitors breathing, oxygen levels, and sleep stages [^111QmAT7]. |
| Cystic fibrosis | Sweat chloride test | Measures chloride concentration in sweat and remains the gold standard for diagnosis [^1163y9Kj]. |
| Contact dermatitis | Patch testing | Identifies allergens responsible for allergic contact dermatitis [^115y2NcU]. |

---

## Limitations and challenges

Despite their importance, gold standard tests are not always perfect and may have limitations:

- **Imperfect accuracy**: Some gold standards have limited sensitivity or specificity, leading to misclassification of disease status [^114PbobT].
- **Invasiveness**: Certain gold standards, such as biopsy or surgical exploration, are invasive and carry risks [^113ybFhK].
- **Cost and accessibility**: Gold standard tests may be expensive or unavailable in resource-limited settings, limiting their practical use [^113ybFhK].
- **Ethical considerations**: In some cases, it may be unethical to perform the gold standard test, necessitating alternative approaches [^113ybFhK].

---

## Alternative approaches when no gold standard exists

In situations where no single gold standard test exists, researchers employ alternative methods:

- **Composite reference standards**: Combining multiple tests or clinical criteria to define disease status [^116QvYn5].
- **Latent class analysis**: Statistical method that estimates disease prevalence and test accuracy in the absence of a perfect reference standard [^112eASKu].
- **Expert consensus**: Clinical expert panels provide a reference diagnosis based on available evidence [^116qdH7e].

---

## Impact on clinical decision-making

Gold standard tests significantly influence clinical decision-making by providing a **definitive diagnosis**, guiding treatment decisions, and serving as a benchmark for evaluating new diagnostic tests. However, clinicians must consider the limitations of gold standards and interpret test results within the clinical context [^116vHvKQ].

---

## Quality assessment of diagnostic studies

When evaluating diagnostic studies, it is essential to assess the quality of the reference standard used. The **QUADAS-2 tool** is widely used to evaluate the risk of bias and applicability of diagnostic accuracy studies, including the appropriateness of the reference standard [^113eQtJY].

---

The gold standard diagnostic test is the **best available method** for confirming a disease, but it is not always perfect. Researchers and clinicians must carefully consider the limitations of gold standards and use appropriate alternative approaches when necessary.

---

## References

### How to assess quality of primary research studies in the medical literature? [^114AQZxs]. Seminars in Nuclear Medicine (2019). Medium credibility.

Accuracy studies are the cornerstone of the evaluation of new diagnostic tests. In an accuracy study, patients with a clinical suspicion of disease undergo both the new tests that is being evaluated, and the reference test or "gold-standard" test for the disease. Patients are stratified according to the test result and to whether or not the diagnosis was confirmed by the reference test. Main accuracy indices include the sensitivity (proportion of patients who test positive among those who have the disease), specificity (proportion of patients who test negative among those without the disease), as well as the positive and negative predictive values (proportion of patients with the disease among those with a positive test, and of patients without the disease among those with a negative test, respectively). In appraising an accuracy study, the reader should check that the study design and analysis follow methodological standards. The study population should be representative of the population in which the test will be used in practice. The new test should be interpreted in a blinded and independent fashion from the reference standard to avoid expectation bias, and should not be used to establish the diagnosis (incorporation bias). The reproducibility should be verified. Interpretation criteria and technical aspects should be described with enough details to allow replication. Provided that these conditions are met, the next step is to decide whether the test may be used for patient care. The clinical setting in which the test will be used, and the corresponding pretest probability of disease, will determine how best can the test be used in practice.

---

### When should a new test become the current reference standard? [^1179B8zu]. Annals of Internal Medicine (2008). Low credibility.

The evaluation of claims that a new diagnostic test is better than the current gold standard test is hindered by the lack of a perfect reference judge. However, this problem may be sidestepped by focusing on the clinical consequences of the decision rather than on estimation of accuracy. Consequences can be assessed by use of a "fair umpire" test that is not perfect yet can discriminate between disease and nondisease cases and is not biased in favor of 1 test. This article discusses 3 principles to aid judgments about the value of new tests. First, the consequences are best examined in cases with disagreement between the current and new tests. Second, resolving these disagreements requires a fair, but not necessarily perfect, umpire test. Finally, umpire tests include consequences, such as prognosis and response to treatment, as well as causal exposures and other test results.

---

### Developing a new reference standard: is validation necessary? [^114oY2xo]. Academic Radiology (2010). Low credibility.

Rationale and Objectives

A gold standard is often an imperfect diagnostic test, falling short of achieving 100% accuracy in clinical practice. Using an imperfect gold standard without fully comprehending its limitations and biases can lead to erroneous classification of patients with and without disease. This will ultimately affect treatment decisions and patient outcomes. Therefore, validation is essential before implementing a reference standard into practice. Performing a comprehensive validation process is discussed, along with its advantages and challenges. The different types of validation methods are reviewed. An example from our work in developing a new reference standard for vasospasm diagnosis in aneurysmal subarachnoid hemorrhage patients is provided.

Conclusion

Employing a new reference standard may result in a definitional shift of the disease and classification scheme of patients; therefore, it is important to also assess the impact of a new reference standard on patient outcomes and its clinical effectiveness.

---

### Assessing diagnostic tests I: you can' T Be too sensitive [^114JpcSU]. The Journal of Foot and Ankle Surgery (2015). Low credibility.

Clinicians and patients are always interested in less invasive, cheaper, and faster diagnostic tests. When introducing such a test, physicians must ensure that it is reliable in its diagnoses and does not commit errors. In this article, I discuss several ways that new tests are compared against gold standard diagnostics.

---

### Clinically adjudicated reference standards for evaluation of infectious diseases diagnostics [^116qdH7e]. Clinical Infectious Diseases (2023). Medium credibility.

Lack of a gold standard can present a challenge for evaluation of diagnostic test accuracy of some infectious diseases tests, particularly when the test's accuracy potentially exceeds that of its predecessors. This approach may measure agreement with an imperfect reference, rather than correctness, because the right answer is unknown. Solutions consist of multitest comparators, including those that involve a test under evaluation if multiple new tests are being evaluated together, using latent class modeling, and clinically adjudicated reference standards. Clinically adjudicated reference standards may be considered as comparator methods when no predefined test or composite of tests is sufficiently accurate; they emulate clinical practice in that multiple data pieces are clinically assessed together.

---

### Chapter 9: options for summarizing medical test performance in the absence of a "gold standard" [^112zgsW4]. Journal of General Internal Medicine (2012). Low credibility.

The classical paradigm for evaluating test performance compares the results of an index test with a reference test. When the reference test does not mirror the "truth" adequately well (e.g. is an "imperfect" reference standard), the typical ("naïve") estimates of sensitivity and specificity are biased. One has at least four options when performing a systematic review of test performance when the reference standard is "imperfect": (a) to forgo the classical paradigm and assess the index test's ability to predict patient relevant outcomes instead of test accuracy (i.e., treat the index test as a predictive instrument); (b) to assess whether the results of the two tests (index and reference) agree or disagree (i.e., treat them as two alternative measurement methods); (c) to calculate "naïve" estimates of the index test's sensitivity and specificity from each study included in the review and discuss in which direction they are biased; (d) mathematically adjust the "naïve" estimates of sensitivity and specificity of the index test to account for the imperfect reference standard. We discuss these options and illustrate some of them through examples.

---

### Assessing diagnostic tests II: grading on a curve [^117GRF64]. The Journal of Foot and Ankle Surgery (2015). Low credibility.

In this Investigators' Corner, I continue discussing how to introduce and assess new diagnostic tests to replace older tests that are considered gold standards. Specifically, I talk about how to assess a "family" of diagnostics, and how to choose an optimal "family member".

---

### Estimating and comparing diagnostic tests' accuracy when the gold standard is not binary [^113Q4EWA]. Academic Radiology (2005). Low credibility.

Rationale and Objectives

Investigators often need to assess the accuracies of diagnostic tests when the gold standard is not binary-scale. The objective of this article is to describe nonparametric estimators of diagnostic test accuracy when the gold standard is continuous, ordinal, and nominal scale.

Materials and Methods

A nonparametric method of estimating and comparing the area under receiver operating characteristic (ROC) curves, proposed by DeLong et al, is extended to situations in which the gold standard is not binary. Two examples illustrate the methods.

Results

Measures of diagnostic test accuracy, their variance, and tests for comparing two diagnostic tests' accuracies in paired designs are presented for situations in which the gold standard is continuous, ordinal, and nominal scale. These summary measures of diagnostic test accuracy are analogous in form and interpretation to the area under the ROC curve.

Conclusion

Dichotomizing the outcomes of a gold standard so that traditional ROC methods can be applied can lead to bias. The methods described here are useful for assessing and comparing summary test accuracy when the gold standard is not binary scale. They have limitations similar to other summary indices.

---

### Systematic reviews and meta-analyses of diagnostic test accuracy [^116uGzst]. Clinical Microbiology and Infection (2014). Low credibility.

Systematic reviews of diagnostic test accuracy summarize the accuracy, e.g. the sensitivity and specificity, of diagnostic tests in a systematic and transparent way. The aim of such a review is to investigate whether a test is sufficiently specific or sensitive to fit its role in practice, to compare the accuracy of two or more diagnostic tests, or to investigate where existing variation in results comes from. The search strategy should be broad and preferably fully reported, to enable readers to assess the completeness of it. Included studies usually have a cross-sectional design in which the tests of interest, ideally both the index test and its comparator, are evaluated against the reference standard. They should be a reflection of the situation that the review question refers to. The quality of included studies is assessed with the Quality Assessment of Diagnostic Accuracy Studies-2 checklist, containing items such as a consecutive and all-inclusive patient selection process, blinding of index test and reference standard assessment, a valid reference standard, and complete verification of all included participants. Studies recruiting cases separately from (healthy) controls are regarded as bearing a high risk of bias. For meta-analysis, the bivariate model or the hierarchical summary receiver operating characteristic model is used. These models take into account potential threshold effects and the correlation between sensitivity and specificity. They also allow addition of covariates for investigatation of potential sources of heterogeneity. Finally, the results from the meta-analyses should be explained and interpreted for the reader, to be well understood.

---

### Diagnostic methods I: sensitivity, specificity, and other measures of accuracy [^111yQ8iv]. Kidney International (2009). Low credibility.

For most physicians, use of diagnostic tests is part of daily routine. This paper focuses on their usefulness by explaining the different measures of accuracy, the interpretation of test results, and the implementation of a diagnostic strategy. Measures of accuracy include sensitivity and specificity. Although these measures are often considered fixed properties of a diagnostic test, in reality they are subject to multiple sources of variation such as the population case mix and the severity of the disease under study. Furthermore, when evaluating a new diagnostic test, it must be compared to a reference standard, although the latter is usually not perfect. In daily practice diagnostic tests are not used in isolation. Several issues will influence the interpretation of their results. First, clinicians have a prior assumption about the patient's chances of having the disease under investigation, based on the patient's characteristics, symptoms, and the disease prevalence in similar populations. Second, diagnostic tests are usually part of a diagnostic strategy. Therefore, it is not sufficient to determine the accuracy of a single test; one also needs to determine its additional value to the patient's diagnosis, treatment, or outcome as part of a diagnostic strategy.

---

### Anticipating missing reference standard data when planning diagnostic accuracy studies [^111cR2u6]. BMJ (2016). Excellent credibility.

The problem: missing reference standard data

Diagnostic studies typically evaluate the accuracy of one or more tests, markers, or models by comparing their results with those of, ideally, a "gold" reference test or standard. In such studies, the outcome — that is, the presence or absence of the target disease as determined by the chosen reference standard — is often missing in some of the study participants. This is known as partial verification. When only the participants who received the reference standard are included in the analysis (complete case analysis), estimates of the accuracy of the diagnostic test(s), marker(s), or model(s) under study, such as the sensitivity, specificity, predictive values, likelihood ratios, or C index, can be biased.

There are many reasons why missing reference standard results may occur in diagnostic studies, as well as various approaches to deal with these missing outcomes in the statistical analysis. Ideally, how missing outcome data will eventually be dealt with is determined during the design phase of the study as opposed to later during the data analysis phase.

Here, we build on previous research on methods for dealing with missing outcomes during the data analysis phase to look at specific measures that can be taken when designing or conducting a diagnostic accuracy study. This paper focuses on prospective studies in which all included patients suspected of having the disease of interest receive all tests under study as well as the reference standard. It does not cover alternative designs, such as separate sampling of diseased participants and healthy controls, or retrospective studies in which patients who have received both the index test and reference standard are identified in hospital databases. Firstly, we discuss the various reasons for missing reference standard results, then we consider the proposed solutions to handle patterns of missing data, and we end with an overview of specific measures that can be considered in the design phase of a prospective diagnostic study to improve the proposed solutions.

---

### Comparing sensitivity and specificity of medical imaging tests when verification bias is present: the concept of relative diagnostic accuracy [^115TScDB]. European Journal of Radiology (2018). Low credibility.

Medical imaging plays a key role in all stages of cancer management. In evaluating a new imaging modality, the optimal design involves a comparison with standard test results as well as a gold standard, such as a pathological evaluation to determine disease status. However, when both the standard and experimental test results are negative, a gold standard may not always be performed, especially if it involves an invasive and/or costly procedure. In this situation, true disease status cannot be verified, which creates an estimation problem for sensitivity and specificity. The aim of this article is to present the concept of relative accuracy which permits to remove the bias when only patients with at least one positive test receive the gold standard.

---

### Chapter 8: meta-analysis of test performance when there is a "gold standard" [^111kNxbB]. Journal of General Internal Medicine (2012). Low credibility.

Synthesizing information on test performance metrics such as sensitivity, specificity, predictive values and likelihood ratios is often an important part of a systematic review of a medical test. Because many metrics of test performance are of interest, the meta-analysis of medical tests is more complex than the meta-analysis of interventions or associations. Sometimes, a helpful way to summarize medical test studies is to provide a "summary point", a summary sensitivity and a summary specificity. Other times, when the sensitivity or specificity estimates vary widely or when the test threshold varies, it is more helpful to synthesize data using a "summary line" that describes how the average sensitivity changes with the average specificity. Choosing the most helpful summary is subjective, and in some cases both summaries provide meaningful and complementary information. Because sensitivity and specificity are not independent across studies, the meta-analysis of medical tests is fundamentaly a multivariate problem, and should be addressed with multivariate methods. More complex analyses are needed if studies report results at multiple thresholds for positive tests. At the same time, quantitative analyses are used to explore and explain any observed dissimilarity (heterogeneity) in the results of the examined studies. This can be performed in the context of proper (multivariate) meta-regressions.

---

### Diagnostic testing and decision-making: beauty is not just in the eye of the beholder [^116FxJYY]. Anesthesia and Analgesia (2018). Low credibility.

DESIGN OF A DIAGNOSTIC ACCURACY STUDY

Rigorous study design is essential for a diagnostic accuracy study. First, the study objective must be clearly stated. Because the chosen population greatly influences the diagnostic accuracy results, as well as their meaning and applicability, researchers must describe the exact patient population about which they want to make inference.

Questions to consider include the following: Which patients are targeted? Those who already have had a positive result on a certain prescreening test? Those with a certain background predisposing them to have or not have the disease or outcome of interest? Is the goal one of estimation of diagnostic accuracy, comparison between biomarkers, or comparison between populations? Is the biomarker or modality of interest new or well-established?

Choice of the gold standard method used to define diseased versus nondiseased patients should be carefully considered. Many times there is no perfect gold standard — a clear study limitation. An imperfect gold standard raises important questions of how the data will be analyzed and how the results can be interpreted. Nevertheless, statistical methods can attempt to account for an imperfect gold standard. Reliability of the biomarker or medical test being evaluated should also be assessed and reported.

Calculation of the appropriate sample size depends on the goal of the study being either estimation of diagnostic accuracy or comparing biomarkers or groups on diagnostic accuracy. When estimating diagnostic accuracy, the goal is typically to estimate the parameter of interest with a desired precision, measured by the expected width of the CI. When comparing biomarkers or groups on diagnostic accuracy, the difference to detect between the biomarkers or populations needs to be specified, and the sample size determined accordingly.

Of note, if the prevalence of the disease is expected to be low in the study sample (< 50%), then estimation of or detecting differences in sensitivity would drive the sample size because the truly diseased would have a smaller overall sample size compared to the nondiseased, and sufficient power or precision for the smaller sample (the diseased) would guarantee it for the larger sample (the nondiseased). Likewise, specificity would drive the calculations if prevalence was expected to be > 50%.

---

### A unified framework for diagnostic test development and evaluation during outbreaks of emerging infections [^116m1DS2]. Communications Medicine (2024). Medium credibility.

Introduction

Newly emerging infectious agents present a particular challenge for diagnostic test development and evaluation. These agents often surface in the form of an outbreak, an epidemic or a pandemic with high urgency for targeted infection control, but minimal knowledge about the infectious agents themselves. Rapid availability of diagnostic tests, along with information on their accuracy, however limited, is critical in these situations.

Traditional diagnostic study designs and quality assessment tools developed for individual patient care, as proposed in existing guidelines for diagnostic tests –, are difficult to apply in a volatile environment in which there are continuously evolving research questions, infectious agents, and intervention options. These challenges were particularly apparent during the SARS-CoV-2 pandemic, where the quality of diagnostic studies available in the field was generally limited. The Cochrane review on rapid, point-of-care (POC) antigen and molecular-based tests for diagnosing SARS-CoV-2 infection found a high risk of bias in different domains in 66 of the 78 studies considered (85%). The most frequent potential source of bias was identified in the reference standard domain, including potential of imperfect gold/reference standard bias, incorporation bias, and diagnostic review bias (an explanation of these biases is given in Table 1). In the Cochrane review on antibody tests for identification of current and past infection with SARS-CoV-2, the most frequent potential source of bias was identified in the patient selection domain, due to selection or spectrum bias (48 of 54 studies, 89%). A particular issue is that biases with regard to the reference standard or the index test can lead to an overestimation or underestimation of sensitivity and specificity.

---

### Methodology for ACOEM's occupational medicine practice guidelines-2025 revision [^115aJbcE]. ACOEM (2025). High credibility.

Methodology for ACOEM's Occupational Medicine Practice Guidelines — diagnostic studies evaluation — searches for new test accuracy use key terms such as Sensitivity and Specificity and Predictive Value of Tests, and delimiters Humans and English; relevant systematic reviews and meta-analyses and their references are also evaluated. Diagnostic studies are summarized in evidence tables and quality graded using a procedure different from randomized controlled trials (RCTs), with emphasis on the test comparator and on having data to calculate specificity and sensitivity. Studies comparing a new test with an established Gold Standard are evaluated first, comparisons to another non–Gold Standard test are also evaluated, and researchers consult panel members and screen references to ensure all relevant studies are included.

---

### STARD 2015: an updated list of essential items for reporting diagnostic accuracy studies [^115RsKF8]. BMJ (2015). Excellent credibility.

As researchers, we talk and write about our studies, not just because we are happy — or disappointed — with the findings, but also to allow others to appreciate the validity of our methods, to enable our colleagues to replicate what we did, and to disclose our findings to clinicians, other health care professionals, and decision makers, all of whom rely on the results of strong research to guide their actions.

Unfortunately, deficiencies in the reporting of research have been highlighted in several areas of clinical medicine. Essential elements of study methods are often poorly described and sometimes completely omitted, making both critical appraisal and replication difficult, if not impossible. Sometimes study results are selectively reported, and other times researchers cannot resist unwarranted optimism in interpretation of their findings. These practices limit the value of the research and any downstream products or activities, such as systematic reviews and clinical practice guidelines.

Reports of studies of medical tests are no exception. A growing number of evaluations have identified deficiencies in the reporting of test accuracy studies. These are studies in which a test is evaluated against a clinical reference standard, or gold standard; the results are typically reported as estimates of the test's sensitivity and specificity, which express how good the test is in correctly identifying patients as having the target condition. Other accuracy statistics can be used as well, such as the area under the receiver operating characteristics (ROC) curve or positive and negative predictive values.

Despite their apparent simplicity, such studies are at risk of bias. If not all patients undergoing testing are included in the final analysis, for example, or if only healthy controls are included, the estimates of test accuracy may not reflect the performance of the test in clinical applications. Yet such crucial information is often missing from study reports.

It is now well established that sensitivity and specificity are not fixed test properties. The relative number of false positive and false negative test results varies across settings, depending on how patients present and which tests they have already undergone. Unfortunately, many authors also fail to completely report the clinical context and when, where, and how they identified and recruited eligible study participants. In addition, sensitivity and specificity estimates can differ because of variable definitions of the reference standard against which the test is being compared. Thus this information should be available in the study report.

---

### Can you handle the truth (and know it when you see it)? Understanding sensitivity, specificity, predictive values, and ROC curves [^1167djLg]. Journal of Minimally Invasive Gynecology (2005). Low credibility.

The interpretation of diagnostic and screening tests requires a basic knowledge of sensitivity, specificity, and predictive values. This article provides an overview of these measures of test performance, a brief description of receiver operating characteristic curves, and relevant illustrations from the medical literature.

---

### Practice parameter: diagnosis of dementia (an evidence-based review). Report of the quality standards subcommittee of the American Academy of Neurology [^112o3ar2]. Neurology (2001). Medium credibility.

Evidence classification and recommendation definitions — diagnostic study evidence is categorized from Class I to Class IV, ranging from "a well designed prospective study in a broad spectrum of persons with the suspected condition, using a 'gold standard'… in a blinded evaluation" to "any design in which test is not applied in blinded evaluation OR evidence provided by expert opinion alone or in descriptive case series (without controls)". Practice recommendations are defined as Standard (principle reflecting "a high degree of clinical certainty" usually requiring Class I evidence or overwhelming Class II evidence), Guideline (recommendation reflecting "moderate clinical certainty" based on Class II or strong consensus of Class III evidence), Practice Option ("clinical utility is uncertain"), and Practice Advisory ("emerging and/or newly approved therapies or technologies" based on at least one Class I study, with possible modest effect, limited response, or cost-benefit concerns).

---

### Performance of A1c for the classification and prediction of diabetes [^112V3PCG]. Diabetes Care (2011). Low credibility.

Although A1C is now recommended for the diagnosis of diabetes, its precise test performance is uncertain. The lack of a single, clear "gold standard" poses a challenge for determining the performance of A1C. Previous diagnostic studies of A1C have relied exclusively on a single elevated fasting or 2-h glucose values as gold standards. However, because glucose determinations are inherently more variable than A1C, these convenient gold standards are likely to reduce the apparent accuracy of A1C as a diagnostic test. A stronger gold standard would rely on repeated glucose determinations on different days, i.e. the recommended approach to diagnosis of diabetes in clinical practice. Alternatively, A1C and fasting glucose can be compared head-to-head against the subsequent development of clinically diagnosed diabetes as the gold standard. We hypothesized that 1) A1C would perform well as a diagnostic and prognostic test for diabetes across its full range and at the American Diabetes Association–recommended threshold of 6.5% and 2) that its performance would be best when judged against stronger, most clinically relevant gold standards.

---

### Recognising bias in studies of diagnostic tests part 1: patient selection [^1151BaFT]. Emergency Medicine Journal (2019). Medium credibility.

In this two-part series on sources of bias in studies of diagnostic test performance, we outline common errors and optimal conditions during three study phases: patient selection, interpretation of the index test and disease verification by a gold standard. Here in part 1, biases associated with suboptimal participant selection are discussed through the lens of partial verification bias and spectrum bias, both of which increase the proportion of participants who are the 'sickest of the sick' or the 'wellest of the well'. Especially through retrospective methodology, partial verification introduces bias by including patients who are test positive by a gold standard, since patients with a positive index test are more likely to go on to further gold standard testing. Spectrum bias is frequently introduced through case-control design, dropping of indeterminate results or convenience sampling. After reading part 1, the informed clinician should be better able to judge the quality of a diagnostic test study, its inherent limitations and whether its results could be generalisable to their practice. Part 2 will describe how interpretation of the index test and disease verification by a gold standard can contribute to diagnostic test bias.

---

### Diagnostic accuracy studies [^112UDYCc]. Seminars in Nuclear Medicine (2019). Medium credibility.

The medical community often assumes that the tests we use to diagnose various diseases are accurate, safe, and effective. However, the study designs traditionally used to determine whether such a diagnostic test is indeed accurate, safe, and effective are often at a higher risk of bias and are of lower methodological quality than those evaluating efficacy of therapeutic interventions. Several designs can be used to study diagnostic tests such as diagnostic accuracy cross-sectional studies, diagnostic accuracy case-control studies, and diagnostic accuracy comparative studies. Clinicians, researchers, and policy-makers may wish to consider moving toward higher quality study designs when studying new diagnostic modalities prior to their implementation in routine practice and diagnostic randomized trials are one such alternative.

---

### The quality of paediatric asthma guidelines: evidence underpinning diagnostic test recommendations from a meta-epidemiological study [^116vPsSJ]. Family Practice (2024). Medium credibility.

No prior studies have assessed the quality and reporting of diagnostic testing guidelines for childhood asthma. One study evaluated diagnostic guidelines in an adult primary care setting. Of interest, the authors found that recommendations based on high-quality evidence had greater rates of clinician adherence to CPGs. Two reviews specifically compared childhood asthma recommendations across international guidelines, focusing on management. For diagnostic questions, Level 1 evidence comprises prospective studies where diagnostic tests are compared with a gold standard test in all patients in the study group. Conducting such research in asthma is challenging due to the absence of a gold-standard test; it is even more difficult to assess the diagnostic accuracy of a sequential testing strategy.

Our study sought to determine: (i) the methodological quality and reporting of paediatric asthma guidelines for primary care and (ii) the strength of evidence supporting diagnostic test recommendations for childhood asthma.

---

### Why has it taken so long for biological psychiatry to develop clinical tests and what to do about it? [^115wp7Fx]. Molecular Psychiatry (2012). Low credibility.

Patients with mental disorders show many biological abnormalities which distinguish them from normal volunteers; however, few of these have led to tests with clinical utility. Several reasons contribute to this delay: lack of a biological 'gold standard' definition of psychiatric illnesses; a profusion of statistically significant, but minimally differentiating, biological findings; 'approximate replications' of these findings in a way that neither confirms nor refutes them; and a focus on comparing prototypical patients to healthy controls which generates differentiations with limited clinical applicability. Overcoming these hurdles will require a new approach. Rather than seek biomedical tests that can 'diagnose' DSM-defined disorders, the field should focus on identifying biologically homogenous subtypes that cut across phenotypic diagnosis — thereby sidestepping the issue of a gold standard. To ensure clinical relevance and applicability, the field needs to focus on clinically meaningful differences between relevant clinical populations, rather than hypothesis-rejection versus normal controls. Validating these new biomarker-defined subtypes will require longitudinal studies with standardized measures which can be shared and compared across studies — thereby overcoming the problem of significance chasing and approximate replications. Such biological tests, and the subtypes they define, will provide a natural basis for a 'stratified psychiatry' that will improve clinical outcomes across conventional diagnostic boundaries.

---

### When is measuring sensitivity and specificity sufficient to evaluate a diagnostic test, and when do we need randomized trials? [^112PxYig]. Annals of Internal Medicine (2006). Low credibility.

The clinical value of using a new diagnostic test depends on whether it improves patient outcomes beyond the outcomes achieved using an old diagnostic test. When can studies of diagnostic test accuracy provide sufficient information to infer clinical value, and when do clinicians need to wait for results from randomized trials? The authors argue that accuracy studies suffice if a new diagnostic test is safer or more specific than, but of similar sensitivity to, an old test. However, if a new test is more sensitive than an old test, it leads to the detection of extra cases of disease. Results from treatment trials that enrolled only patients detected by the old test may not apply to these extra cases. Clinicians need to wait for results from randomized trials assessing treatment efficacy in cases detected by the new diagnostic test, unless they can be satisfied that the new test detects the same spectrum and subtype of disease as the old test or that treatment response is similar across the spectrum of disease.

---

### Fool's gold: why imperfect reference tests are undermining the evaluation of novel diagnostics: a reevaluation of 5 diagnostic tests for leptospirosis [^113QKgLx]. Clinical Infectious Diseases (2012). Low credibility.

We hypothesized that the gold standard for diagnosing leptospirosis is imperfect. We used Bayesian latent class models and random-effects meta-analysis to test this hypothesis and to determine the true accuracy of a range of alternative tests for leptospirosis diagnosis.

---

### Recognising bias in studies of diagnostic tests part 2: interpreting and verifying the index test [^116oAXm8]. Emergency Medicine Journal (2019). Medium credibility.

Multiple pitfalls can occur with the conduct and analysis of a study of diagnostic tests, resulting in biased accuracy. Our conceptual model includes three stages: patient selection, interpretation of the index test and disease verification. In part 2, we focus on (1) Interpretation bias (or workup bias): where the classification of an indeterminate index test result can bias the accuracy of a test or how lack of blinding can bias a subjective test result, and (2) Disease verification bias: where the index test result is incorporated into the gold standard or when the gold standard is applied only to a select population as the gold standard is an invasive test. In an example with age-adjusted D-dimer for pulmonary embolism, differential verification bias was a limitation due to the use of two gold standards-CT for a high-risk population and follow-up for symptoms in a low-risk population. However, there are circumstances when certain choices in study design are unavoidable, and result in biased test characteristics. In this case, the informed reader will better judge the quality of a study by recognising the potential biases and limitations by being methodical in their approach to understanding the methods, and in turn, better apply studies of diagnostic tests into their clinical practice.

---

### How to avoid describing your radiological research study incorrectly [^113YGub1]. European Radiology (2020). Medium credibility.

In DTA designs, a new test is evaluated against an independent standard test; i.e. participants receive both the new imaging test and a reference test. The best reference is a true, independent "gold standard" but this can be difficult to achieve so researchers often evaluate against standard practice instead. This poses specific analytical challenges because a new test cannot, by definition, outperform the reference (i.e. the new test can never be better). Paradoxically, a new test that, in reality, outperforms the existing standard will appear worse after analysis. A solution might require a composite/consensus reference derived from multiple sources (and risking incorporation bias), or an outcome-based assessment, where the eventual clinical diagnosis defines whether the target pathology was present or not. For example, the diagnostic "rate" of a cancer imaging test can be determined first (what proportion of cases did the new test diagnose compared with standard practice?) and then a cancer registry used to provide a definitive reference standard so sensitivity and specificity can be calculated after adequate follow-up. The obvious disadvantage is that this takes time. In "reference standard positive" designs, only those individuals testing positive by the reference standard receive the new test.

"Diagnostic test pathway" designs are increasingly popular and, like diagnostic accuracy studies, participants receive both intervention and standard tests. Results from the new test are revealed only when clinical decisions have been made and documented based on results from the standard test alone (i.e. reflecting standard clinical practice in the absence of the new test): By then revealing results from the new test, it is possible to determine how the new test impacts on patients' clinical trajectory. More advanced designs compare multiple tests, with all evaluated against a reference. Where the same participants receive all the tests being compared as well as the reference standard, then biases due to differences in participants, study design, and methods are minimised providing a higher level of evidence for a test comparison. Healthcare providers are increasingly interested in not only diagnostic accuracy but also how different trajectories following a test result consume evermore scarce resource. For example, in the UK, the National Institute for health and Clinical Excellence (NICE) Diagnostics Advisory Committee recommends tests for adoption not only on the basis of diagnostic accuracy but also on whether they are cost-effective.

---

### The quantitative science of evaluating Imaging evidence [^116vHvKQ]. JACC: Cardiovascular Imaging (2017). Low credibility.

Cardiovascular diagnostic imaging tests are increasingly used in everyday clinical practice, but are often imperfect, just like any other diagnostic test. The performance of a cardiovascular diagnostic imaging test is usually expressed in terms of sensitivity and specificity compared with the reference standard (gold standard) for diagnosing the disease. However, evidence-based application of a diagnostic test also requires knowledge about the pre-test probability of disease, the benefit of making a correct diagnosis, the harm caused by false-positive imaging test results, and potential adverse effects of performing the test itself. To assist in clinical decision making regarding appropriate use of cardiovascular diagnostic imaging tests, we reviewed quantitative concepts related to diagnostic performance (e.g., sensitivity, specificity, predictive values, likelihood ratios), as well as possible biases and solutions in diagnostic performance studies, Bayesian principles, and the threshold approach to decision making.

---

### "GOLD or lower limit of normal definition? A comparison with expert-based diagnosis of chronic obstructive pulmonary disease in a prospective cohort-study" [^111oBrck]. Respiratory Research (2012). Low credibility.

Introduction

Chronic obstructive pulmonary disease (COPD) is among the leading causes of disability and death in developed countries. The prevalence of COPD is still on the rise, and costs for the health system are substantial. Airflow limitation that is not fully reversible after bronchodilator application is a key feature of COPD, and spirometry is the routine diagnostic procedure of choice recommended to diagnose COPD. However, the degree of obstruction that establishes the diagnosis of COPD is still under debate. The Global Initiative for chronic Obstructive Lung Disease (GOLD) defined COPD as a fixed post-bronchodilator ratio of forced expiratory volume in 1 second and forced vital capacity (FEV1/FVC ratio) of less than 0.70. This definition is widely accepted, mainly because of its practicability.

Since the FEV1 value decreases more quickly with age than the (F)VC, the GOLD definition tends to overdiagnose COPD in the elderly. Therefore, some authors suggested using the lower limit of normal (LLN) procedure to diagnose COPD. The LLN is based on age-stratified pre-bronchodilator cut-off values of the FEV1/FVC ratio, and a value below the lower fifth percentile of an aged-matched healthy reference group is considered abnormal and consistent with a diagnosis of COPD. Multiple studies showed that application of any population-derived LLN will result in lower prevalence estimates of COPD compared to the GOLD definition in the elderly. The question remains however which method should be preferred. Another question is, whether there are other pulmonary function test variables that could improve the diagnostic accuracy of GOLD or LLN. The final question is, which definition predicts best prognosis and thus is useful for treatment decisions in COPD. To answer all three questions, the two criteria should be compared with an alternative, and acceptable reference standard, applied in the relevant domain, that is, a population suspected of COPD. An expert panel diagnosis of COPD, based on all available diagnostic information from the clinical assessment, smoking habits, and a complete pulmonary function test (PFT) could be regarded as such a reference standard.

To the best of our knowledge, our study is the first to validate GOLD and LLN criteria against an expert panel diagnosis in patients suspected of COPD and to assess their prognostic ability.

Since, in daily practice, establishing the diagnosis of COPD is usually not based on a single PFT parameter, we furthermore assessed whether the addition of other PFT parameters to the GOLD or LLN criteria increases diagnostic accuracy compared to either definition alone.

---

### An evidence-based perspective to commonly performed erectile dysfunction investigations [^111LFC6Q]. The Journal of Sexual Medicine (2008). Low credibility.

Introduction

Currently there is no universally accepted gold standard diagnostic test to differentiate psychogenic from physical erectile dysfunction (ED). Instead, sexual health specialists rely on a detailed history, a focused physical examination, and specialized diagnostic tests, to decide if the etiology of the ED is mainly psychogenic or organically caused. Aim. In this review we point out the status of evidence-based principles in the area of diagnosis in Sexual Medicine.

Methods

We review the concepts of evidence-based medicine (EBM) in the area of medical diagnostic tests. We highlight four of the well-known diagnostic tests (penile duplex, pharmacoarteriography, pharmacocavernosometry/cavernosography [PHCAS/PHCAG], and nocturnal penile tumescence [NPT monitoring]) for ED evaluation within an evidence-based perspective.

Main Outcome Measures

Assessment of diagnostic tests for ED using principles of EBM.

Results

Several good diagnostic tests are useful in the evaluation of men with ED. However, modern evidence-based concepts-mainly the likelihood ratio-have not yet been applied to these tests to obtain their maximum clinical benefits.

Conclusions

While penile duplex/color Doppler has good evidence of supporting its use in the diagnosis of arteriogenic ED, data supporting its diagnosis of a physical disorder associated with cavernous venous occlusion dysfunction are lacking. PHCAS/PHCAG's main drawback is an unknown positive predictive value and a possibility of frequent false-positive results. NPT has many advantages when differentiating psychogenic from organic ED, however, several questions related to its physiological mechanisms do exist. Ghanem H, and Shamloul R. An evidence-based perspective to commonly performed erectile dysfunction investigations.

---

### EMBASE search strategies for identifying methodologically sound diagnostic studies for use by clinicians and researchers [^116Zfb5f]. BMC Medicine (2005). Low credibility.

We compiled an initial list of search terms, including index terms and textwords from clinical studies. Input was then sought from clinicians and librarians in the United States and Canada through interviews of known searchers, and requests at meetings and conferences. We compiled a list of 5,385 terms of which 4,843 were unique and 3,524 returned results (list of terms tested provided by the authors upon request). Examples of the search terms tested are 'criterion standard', 'cut point', 'sensitivity', and 'ROC curve', all as textwords; 'diagnosis', the index term, and the index term 'diagnostic test', exploded (that is, including all of this term's indexing subheadings).

As part of a larger study, research staff performance was rigorously calibrated before reviewing the journals and inter-rater agreement for identifying the purpose of articles was 81% beyond chance (kappa statistic, 95% confidence interval (CI) 0.79 to 0.84). Inter-rater agreement for which articles met all methodological criteria was 89% (CI 78% to 99%) beyond chance. Six research assistants then hand-searched all articles in each issue of the 55 journals and applied methodological criteria to determine whether the article was methodologically sound for evaluation of a diagnostic test. The methodological criteria applied for studies of diagnosis were as follows: Inclusion of a spectrum of participants; objective diagnostic ("gold") standard or current clinical standard for diagnosis; participants received both the new test and some form of the diagnostic standard; interpretation of diagnostic standard without knowledge of test result and vice versa; and analysis consistent with study design.

---

### Understanding the direction of bias in studies of diagnostic test accuracy [^112su5pq]. Academic Emergency Medicine (2013). Low credibility.

Ordering and interpreting diagnostic tests is a critical part of emergency medicine (EM). In evaluating a study of diagnostic test accuracy, emergency physicians (EPs) need to recognize whether the study uses case-control or cross-sectional sampling and account for common biases. The authors group biases in studies of test accuracy into five categories: incorporation bias, partial verification bias, differential verification bias, imperfect gold standard bias, and spectrum bias. Other named biases are either equivalent to these biases or subtypes within these broader categories. The authors go beyond identifying a bias and predict the direction of its effect on sensitivity and specificity, providing numerical examples from published test accuracy studies. Understanding the direction of a bias may permit useful inferences from even a flawed study of test accuracy.

---

### The Infectious Diseases Society of America guidelines on the diagnosis of COVID-19: molecular diagnostic testing (December 2023) [^1133gSGL]. Clinical Infectious Diseases (2024). High credibility.

Discussion — Molecular tests designed to detect SARS-CoV-2 nucleic acids have become the gold standard technology for confirming a diagnosis of COVID-19.

---

### Evaluating diagnostic accuracy in the face of multiple reference standards [^11432cc9]. Annals of Internal Medicine (2013). Low credibility.

A universal challenge in studies that quantify the accuracy of diagnostic tests is establishing whether each participant has the disease of interest. Ideally, the same preferred reference standard would be used for all participants; however, for practical or ethical reasons, alternative reference standards that are often less accurate are frequently used instead. The use of different reference standards across participants in a single study is known as differential verification. Differential verification can cause severely biased accuracy estimates of the test or model being studied. Many variations of differential verification exist, but not all introduce the same risk of bias. A risk-of-bias assessment requires detailed information about which participants receive which reference standards and an estimate of the accuracy of the alternative reference standard. This article classifies types of differential verification and explores how they can lead to bias. It also provides guidance on how to report results and assess the risk of bias when differential verification occurs and highlights potential ways to correct for the bias.

---

### Rotator cuff tendinopathy diagnosis, nonsurgical medical care, and rehabilitation: a clinical practice guideline [^1154hXhS]. The Journal of Orthopaedic and Sports Physical Therapy (2025). High credibility.

Clinical assessment evidence for the painful shoulder — Eligibility criteria target adults with shoulder pain or suspected rotator cuff (RC) tendinopathy, assessing clinical tests, imaging tests, measurement tools (range of motion and strength) and self-reported questionnaires or mixed tools against gold standards (imaging tests, surgery, etc). Outcomes include diagnostic accuracy (sensitivity, specificity, positive/negative likelihood ratio [LR]) or metrological quality (validity, reliability, sensitivity to change). Eligible designs are systematic reviews with or without meta-analysis, and studies must be published in English or French in a scientific peer-reviewed journal.

---

### Coefficient of fat absorption to measure the efficacy of pancreatic enzyme replacement therapy in people with cystic fibrosis: gold standard or coal standard? [^1133fdTg]. Pancreas (2022). Medium credibility.

Early in the twentieth century, autopsy reports of patients with purulent lung infections and dramatic scarring and destruction of the pancreas led to the identification of a new disease entity, cystic fibrosis (CF). Before the recognition that excessive secretion of chloride in sweat could be used as a diagnostic test for CF, direct measurement of duodenal enzymes was used to diagnose exocrine pancreatic insufficiency (EPI) as a way to identify children with this new disease. There was debate among leading clinicians about how to treat people with CF (pwCF), most of whom died in infancy or early childhood, often as a direct consequence of malnutrition due to fat malabsorption. Opinions differed as to whether exogenous pancreatic extracts should be used as a therapy for CF, although ultimately this became an important contributor to extending survival.

Eventually, the coefficient of fat absorption (CFA) came to be used as the gold standard for diagnosis of EPI and as a measurement of treatment protocols to control fat malabsorption. – To calculate a CFA, the amount of fat in the diet is recorded and stool is analyzed for fat content. For a research-quality CFA, a diet is consumed that contains 100 g of fat each day over a 3-day period. Oral dye markers are ingested at the start and end of this period. All stools are collected from the appearance of first dye marker to second. Fat is measured in the pooled stool; the CFA (%) = 100 × ([grams fat intake − grams fat excretion]/grams fat intake). The accepted normal values are 85% or greater in infants up to 6 months of age and 93% or greater in children older than 6 months and adults. Fat malabsorption also has been expressed as a coefficient of fat excretion, which is the inverse of CFA.

A gold standard, also known as a criterion standard, is the best diagnostic measurement for a condition against which new tests or results and protocols are compared. It should be valid, reliable, responsive, and ideally feasible (Fig. 1).

FIGURE 1
Factors to balance when choosing a criterion standard.

In practice, the criterion standard is rarely a perfect test but merely the best available test. Several recent reviews have examined the utility of CFA and other tests to diagnose EPI, – so that application of CFA will not be explored here. Our objectives are to review the literature for the second purpose, CFA as a measure of pancreatic enzyme replacement therapy (PERT) efficacy in pwCF, to assess its attributes for this purpose, and to determine whether other tests may improve upon these qualities.

---

### Correctly using sensitivity, specificity, and predictive values in clinical practice: how to avoid three common pitfalls [^115VXoa7]. AJR: American Journal of Roentgenology (2013). Low credibility.

Objective

Radiology is the specialty of imaging-based diagnostic tests. Understanding the science behind evaluating diagnostic test performance is essential for radiologists because we provide care to patients and interact with our colleagues.

Conclusion

Here, we review the key terminology used and common pitfalls encountered in the literature and in day-to-day discussions of diagnostic test performance.

---

### Search strategies to identify diagnostic accuracy studies in MEDLINE and EMBASE [^114vQmk1]. The Cochrane Database of Systematic Reviews (2013). Low credibility.

Background

A systematic and extensive search for as many eligible studies as possible is essential in any systematic review. When searching for diagnostic test accuracy (DTA) studies in bibliographic databases, it is recommended that terms for disease (target condition) are combined with terms for the diagnostic test (index test). Researchers have developed methodological filters to try to increase the precision of these searches. These consist of text words and database indexing terms and would be added to the target condition and index test searches. Efficiently identifying reports of DTA studies presents challenges because the methods are often not well reported in their titles and abstracts, suitable indexing terms may not be available and relevant indexing terms do not seem to be consistently assigned. A consequence of using search filters to identify records for diagnostic reviews is that relevant studies might be missed, while the number of irrelevant studies that need to be assessed may not be reduced. The current guidance for Cochrane DTA reviews recommends against the addition of a methodological search filter to target condition and index test search, as the only search approach.

Objectives

To systematically review empirical studies that report the development or evaluation, or both, of methodological search filters designed to retrieve DTA studies in MEDLINE and EMBASE.

Search Methods

We searched MEDLINE (1950 to week 1 November 2012); EMBASE (1980 to 2012 Week 48); the Cochrane Methodology Register (Issue 3, 2012); ISI Web of Science (11 January 2013); PsycINFO (13 March 2013); Library and Information Science Abstracts (LISA) (31 May 2010); and Library, Information Science & Technology Abstracts (LISTA) (13 March 2013). We undertook citation searches on Web of Science, checked the reference lists of relevant studies, and searched the Search Filters Resource website of the InterTASC Information Specialists' Sub-Group (ISSG).

Selection Criteria

Studies reporting the development or evaluation, or both, of a MEDLINE or EMBASE search filter aimed at retrieving DTA studies, which reported a measure of the filter's performance were eligible.

Data Collection and Analysis

The main outcome was a measure of filter performance, such as sensitivity or precision. We extracted data on the identification of the reference set (including the gold standard and, if used, the non-gold standard records), how the reference set was used and any limitations, the identification and combination of the search terms in the filters, internal and external validity testing, the number of filters evaluated, the date the study was conducted, the date the searches were completed, and the databases and search interfaces used. Where 2 x 2 data were available on filter performance, we used these to calculate sensitivity, specificity, precision and Number Needed to Read (NNR), and 95% confidence intervals (CIs). We compared the performance of a filter as reported by the original development study and any subsequent studies that evaluated the same filter.

Main Results

Ninteen studies were included, reporting on 57 MEDLINE filters and 13 EMBASE filters. Thirty MEDLINE and four EMBASE filters were tested in an evaluation study where the performance of one or more filters was tested against one or more gold standards. The reported outcome measures varied. Some studies reported specificity as well as sensitivity if a reference set containing non-gold standard records in addition to gold standard records was used. In some cases, the original development study did not report any performance data on the filters. Original performance from the development study was not available for 17 filters that were subsequently tested in evaluation studies. All 19 studies reported the sensitivity of the filters that they developed or evaluated, nine studies reported the specificities and 14 studies reported the precision. No filter which had original performance data from its development study, and was subsequently tested in an evaluation study, had what we defined a priori as acceptable sensitivity (> 90%) and precision (> 10%). In studies that developed MEDLINE filters that were evaluated in another study (n = 13), the sensitivity ranged from 55% to 100% (median 86%) and specificity from 73% to 98% (median 95%). Estimates of performance were lower in eight studies that evaluated the same 13 MEDLINE filters, with sensitivities ranging from 14% to 100% (median 73%) and specificities ranging from 15% to 96% (median 81%). Precision ranged from 1.1% to 40% (median 9.5%) in studies that developed MEDLINE filters and from 0.2% to 16.7% (median 4%) in studies that evaluated these filters. A similar range of specificities and precision were reported amongst the evaluation studies for MEDLINE filters without an original performance measure. Sensitivities ranged from 31% to 100% (median 71%), specificity ranged from 13% to 90% (median 55.5%) and precision from 1.0% to 11.0% (median 3.35%). For the EMBASE filters, the original sensitivities reported in two development studies ranged from 74% to 100% (median 90%) for three filters, and precision ranged from 1.2% to 17.6% (median 3.7%). Evaluation studies of these filters had sensitivities from 72% to 97% (median 86%) and precision from 1.2% to 9% (median 3.7%). The performance of EMBASE search filters in development and evaluation studies were more alike than the performance of MEDLINE filters in development and evaluation studies. None of the EMBASE filters in either type of study had a sensitivity above 90% and precision above 10%.

Authors' Conclusions

None of the current methodological filters designed to identify reports of primary DTA studies in MEDLINE or EMBASE combine sufficiently high sensitivity, required for systematic reviews, with a reasonable degree of precision. This finding supports the current recommendation in the Cochrane Handbook for Systematic Reviews of Diagnostic Test Accuracy that the combination of methodological filter search terms with terms for the index test and target condition should not be used as the only approach when conducting formal searches to inform systematic reviews of DTA.

---

### Assessing FDG-PET diagnostic accuracy studies to develop recommendations for clinical use in dementia [^116vGeVr]. European Journal of Nuclear Medicine and Molecular Imaging (2018). Low credibility.

Background

FDG-PET is frequently used as a marker of synaptic damage to diagnose dementing neurodegenerative disorders. We aimed to adapt the items of evidence quality to FDG-PET diagnostic studies, and assess the evidence available in current literature to assist Delphi decisions for European recommendations for clinical use.

Methods

Based on acknowledged methodological guidance, we defined the domains, specific to FDG-PET, required to assess the quality of evidence in 21 literature searches addressing as many Population Intervention Comparison Outcome (PICO) questions. We ranked findings for each PICO and fed experts making Delphi decisions for recommending clinical use.

Results

Among the 1435 retrieved studies, most lacked validated measures of test performance, an adequate gold standard, and head-to-head comparison of FDG-PET and clinical diagnosis, and only 58 entered detailed assessment. Only two studies assessed the accuracy of the comparator (clinical diagnosis) versus any kind of gold-/reference-standard. As to the index-test (FDG-PET-based diagnosis), an independent gold-standard was available in 24% of the examined papers; 38% used an acceptable reference-standard (clinical follow-up); and 38% compared FDG-PET-based diagnosis only to baseline clinical diagnosis. These methodological limitations did not allow for deriving recommendations from evidence.

Discussion

An incremental diagnostic value of FDG-PET versus clinical diagnosis or lack thereof cannot be derived from the current literature. Many of the observed limitations may easily be overcome, and we outlined them as research priorities to improve the quality of current evidence. Such improvement is necessary to outline evidence-based guidelines. The available data were anyway provided to expert clinicians who defined interim recommendations.

---

### Male bladder outlet obstruction: time to re-evaluate the definition and reconsider our diagnostic pathway? ICI-RS 2015 [^116RTXaV]. Neurourology and Urodynamics (2017). Low credibility.

The diagnosis of bladder outlet obstruction (BOO) in the male is dependent on measurements of pressure and flow made during urodynamic studies. The procedure of urodynamics and the indices used to delineate BOO are well standardized largely as a result of the work of the International Continence Society. The clinical utility of the diagnosis of BOO is however, less well defined and there are several shortcomings and gaps in the currently available medical literature. Consequently the International Consultation on Incontinence Research Society (ICI-RS) held a think tank session in 2015 entitled "Male bladder outlet obstruction: Time to re-evaluate the definition and reconsider our diagnostic pathway?" This manuscript details the discussions that took place within that think tank setting out the pros and cons of the current definition of BOO and exploring alternative clinical tests (alone or in combination) which may be useful in the future investigation of male patients with lower urinary tract symptoms. The think tank panel concluded that pressure-flow studies remain the diagnostic gold-standard for BOO although there is still a lack of high quality evidence. Newer, less invasive, investigations have shown promise in terms of diagnostic accuracy for BOO but similar criticisms can be levelled against these tests. Therefore, the think tank suggests further research with regard to these alternative indicators to determine their clinical utility.

---

### How to read articles that use machine learning: users' guides to the medical literature [^117HSWxf]. JAMA (2019). Excellent credibility.

In recent years, many new clinical diagnostic tools have been developed using complicated machine learning methods. Irrespective of how a diagnostic tool is derived, it must be evaluated using a 3-step process of deriving, validating, and establishing the clinical effectiveness of the tool. Machine learning-based tools should also be assessed for the type of machine learning model used and its appropriateness for the input data type and data set size. Machine learning models also generally have additional prespecified settings called hyperparameters, which must be tuned on a data set independent of the validation set. On the validation set, the outcome against which the model is evaluated is termed the reference standard. The rigor of the reference standard must be assessed, such as against a universally accepted gold standard or expert grading.

---

### Antiphospholipid antibody testing: which are most useful for diagnosis? [^114j4s3F]. Rheumatic Diseases Clinics of North America (2006). Low credibility.

Laboratory diagnosis of APS relies on the demonstration of a positive test for aPL. In clinical practice, the gold standard tests are those that detect beta2GPI-dependent aCL or LA. The question on the use of anti-beta2GPI asa routine diagnostic test remains unanswered, and testing for these antibodies should be only performed in very selected cases and not as an alternative to aCL or LA testing. Clinical utility and standardization are still lacking for other aPL specificities; therefore, their application as routine diagnostic tools is not recommended.

---

### Imperfect gold standards for kidney injury biomarker evaluation [^114PbobT]. Journal of the American Society of Nephrology (2012). Low credibility.

Clinicians have used serum creatinine in diagnostic testing for acute kidney injury for decades, despite its imperfect sensitivity and specificity. Novel tubular injury biomarkers may revolutionize the diagnosis of acute kidney injury; however, even if a novel tubular injury biomarker is 100% sensitive and 100% specific, it may appear inaccurate when using serum creatinine as the gold standard. Acute kidney injury, as defined by serum creatinine, may not reflect tubular injury, and the absence of changes in serum creatinine does not assure the absence of tubular injury. In general, the apparent diagnostic performance of a biomarker depends not only on its ability to detect injury, but also on disease prevalence and the sensitivity and specificity of the imperfect gold standard. Assuming that, at a certain cutoff value, serum creatinine is 80% sensitive and 90% specific and disease prevalence is 10%, a new perfect biomarker with a true 100% sensitivity may seem to have only 47% sensitivity compared with serum creatinine as the gold standard. Minimizing misclassification by using more strict criteria to diagnose acute kidney injury will reduce the error when evaluating the performance of a biomarker under investigation. Apparent diagnostic errors using a new biomarker may be a reflection of errors in the imperfect gold standard itself, rather than poor performance of the biomarker. The results of this study suggest that small changes in serum creatinine alone should not be used to define acute kidney injury in biomarker or interventional studies.

---

### BTS / NICE / SIGN guideline for asthma 2024: diagnosis, monitoring and chronic asthma management. how does this compare to GINA 2024? [^116gMrC4]. NPJ Primary Care Respiratory Medicine (2025). Medium credibility.

Peak flow variability is defined as the mean daily variability of twice daily peak flow readings taken over a 2-week period. Daily variability is calculated as highest-lowest reading/mean daily reading x100%.

However, this method does rely on the reliability of patient readings and the mean variability is time-consuming to calculate manually in a short consultation. However, calculators are available on line which can significantly shorten the process.

Pragmatically, but without evidence, both GINA and BTS/NICE/SIGN recognise that reversibility of peak expiratory flow rate (PEFR) post-bronchodilator ≥ 20% (> 15% in children GINA) during an acute attack can be used as a confirmatory diagnostic test.

As highlighted, no single test serves as a "gold-standard" test for asthma diagnosis and in many parts of the world, including the United Kingdom, there are problems in accessing objective tests such as FeNO.

There is agreement that a diagnosis of asthma should be based on a characteristic history, exam and backed up by objective tests where possible. However, where access to objective tests is limited then a more pragmatic approach is needed. The International Primary Care Group(IPCRG) has produced a consensus document "The 'jigsaw puzzle' approach" to building a diagnostic picture of asthma in primary care over timewhich offers a pragmatic solution to the lack of universal availability of recommended tests. It emphasises that there is no one gold standard objective test and a diagnosis may not be able to made in a single consultation, but over time. In particular if an initial test is negative then it may need to be repeated when the patient is symptomatic. Methacholine challenge testing has high specificity and sensitivity, but is relatively patient unfriendly and not readily available in primary care.

It is good practice to record the basis of diagnosis in the medical records.

---

### Dry eye syndrome preferred practice pattern ® [^113KsqsT]. Ophthalmology (2024). High credibility.

Regarding diagnostic investigations for dry eye disease, more specifically with respect to history and physical examination, AAO 2024 guidelines recommend to obtain clinical assessment as the gold standard for diagnosing DED, recognizing that no single test is adequate for establishing the diagnosis of DED.

---

### Methodology for ACOEM's occupational medicine practice guidelines-2025 revision [^117QaafE]. ACOEM (2025). High credibility.

Quality scoring for diagnostic studies (Table C-1) — two example criteria and ratings are provided. For "Disorder clearly defined", 1.0 = disorder definition supported by history, physical findings or other diagnostic testing that is reproducible; 0.5 = disorder labeled with narrow scope of evidence; 0.0 = case definition labeled without supporting evidence or not reproducible. For "Test compared with gold standard", 1.0 = comparison made with gold standard (panel consensus); 0.5 = comparison to established alternative test; 0.0 = comparison to inferior diagnostic test.

---

### Global strategy for the diagnosis, management, and prevention of chronic obstructive pulmonary disease (2025 report) [^113eVjqR]. GOLD (2025). High credibility.

Regarding screening and diagnosis for chronic obstructive pulmonary disease, more specifically with respect to diagnosis, GOLD 2025 guidelines recommend to obtain spirometry to establish the diagnosis of COPD by eliciting a post-bronchodilator FEV1/FVC < 0.70 confirming the presence of persistent airflow limitation.

---

### Diagnosis and management of childhood obstructive sleep apnea syndrome [^111QmAT7]. Pediatrics (2012). Medium credibility.

Childhood obstructive sleep apnea syndrome (OSAS) — diagnostic testing states that polysomnography is the gold standard but may not be readily available; alternative diagnostic tests have weaker positive and negative predictive values than polysomnography, yet objective testing is preferable to clinical evaluation alone, and if an alternative test is negative in a high pretest probability patient, full polysomnography should be sought; the reported strength is an option.

---

### Systematic reviews of diagnostic test accuracy [^111yMWHU]. Annals of Internal Medicine (2008). Low credibility.

More and more systematic reviews of diagnostic test accuracy studies are being published, but they can be methodologically challenging. In this paper, the authors present some of the recent developments in the methodology for conducting systematic reviews of diagnostic test accuracy studies. Restrictive electronic search filters are discouraged, as is the use of summary quality scores. Methods for meta-analysis should take into account the paired nature of the estimates and their dependence on threshold. Authors of these reviews are advised to use the hierarchical summary receiver-operating characteristic or the bivariate model for the data analysis. Challenges that remain are the poor reporting of original diagnostic test accuracy studies and difficulties with the interpretation of the results of diagnostic test accuracy research.

---

### Guidance for diagnosis and treatment of DIC from harmonization of the recommendations from three guidelines [^114hjtJr]. Journal of Thrombosis and Haemostasis (2013). Medium credibility.

Regarding screening and diagnosis for disseminated intravascular coagulation, more specifically with respect to diagnosis, ISTH 2013 guidelines recommend to recognize that there is no gold standard for the diagnosis of DIC and a single test that is, by itself, capable of accurately diagnosing DIC.

---

### How to appraise a diagnostic test [^113xHbGr]. World Journal of Surgery (2005). Low credibility.

Clinicians frequently confront challenges when using diagnostic tests to help them decide whether the patient before them suffers from a particular target condition or diagnosis. The primary issues to consider when determining the validity of a diagnostic test study are how the authors assembled the patients and whether they used an appropriate reference standard in all patients to determine whether the patients did or did not have the target condition. Surgeons should be interested in the characteristics of the test that indicates the direction and magnitude of change in the probability of the target condition associated with a particular test result. The likelihood ratio best captures the link between the pretest probability of the target condition and the probability after the test results are obtained (also called the posttest probability). Many studies, however, present the properties of diagnostic tests in less clinically useful terms: sensitivity and specificity. Sensitivity denotes the proportion of people with the disorder in whom the test result is positive. Specificity denotes the proportion of people without the disorder in whom the test result is negative. Application of the guides presented in this article can allow surgeons to assess critically studies regarding a diagnostic test.

---

### Verification bias an underrecognized source of error in assessing the efficacy of medical imaging [^113J6Fo6]. Academic Radiology (2011). Low credibility.

Rationale and Objectives

Diagnostic tests are validated by comparison against a "gold standard" reference test. When the reference test is invasive or expensive, it may not be applied to all patients. This can result in biased estimates of the sensitivity and specificity of the diagnostic test. This type of bias is called "verification bias", and is a common problem in imaging research. The purpose of our study is to estimate the prevalence of verification bias in the recent radiology literature.

Materials and Methods

All issues of the American Journal of Roentgenology (AJR), Academic Radiology, Radiology, and European Journal of Radiology (EJR) between November 2006 and October 2009 were reviewed for original research articles mentioning sensitivity or specificity as endpoints. Articles were read to determine whether verification bias was present and searched for author recognition of verification bias in the design.

Results

During 3 years, these journals published 2969 original research articles. A total of 776 articles used sensitivity or specificity as an outcome. Of these, 211 articles demonstrated potential verification bias. The fraction of articles with potential bias was respectively 36.4%, 23.4%, 29.5%, and 13.4% for AJR, Academic Radiology, Radiology, and EJR. The total fraction of papers with potential bias in which the authors acknowledged this bias was 17.1%.

Conclusion

Verification bias is a common and frequently unacknowledged source of error in efficacy studies of diagnostic imaging. Bias can often be eliminated by proper study design. When it cannot be eliminated, it should be estimated and acknowledged.

---

### How to create PICO questions about diagnostic tests [^112nqJo8]. BMJ Evidence-Based Medicine (2021). High credibility.

Current convention

One PICO for questions about medical tests has been advocated by many: Population, Index test, Comparator test and test accuracy as Outcome. The index test is the new diagnostic or screening test under investigation, and the comparator test is the best available (reference) method for diagnosing the disease of interest. Although this PICO has strong face validity, it is ambiguous when studied in more detail. The index test as well as comparator test can be positive and negative and so it is unclear which results are compared. Additionally, the proposed outcome is not a health state but a statistical measure, such as sensitivity and specificity.

Moreover, the aim of some diagnostic research is to investigate the effect that offering a test has on patient health instead of test accuracy. This research acknowledges that the test result will determine which care the patient gets, and this will influence his health. It has been suggested that a PICO for a medical test should represent this type of research.

Finally, although reviews about interventions are generally guided by an objective in terms of a PICO, reviews about test accuracy are not. Cochrane recommends to describe the objective of such a review as follows: 'To determine the diagnostic accuracy of [index test] for detecting [target condition] in [participant description]'. This is obviously a clear and concise formulation, but the use of a PICO would promote consistency across reviews.

---

### Anticipating missing reference standard data when planning diagnostic accuracy studies [^114cuYtz]. BMJ (2016). Excellent credibility.

Results obtained using a reference standard may be missing for some participants in diagnostic accuracy studies. This paper looks at methods for dealing with such missing data when designing or conducting a prospective diagnostic accuracy study

---

### Heterogeneity in systematic reviews of medical imaging diagnostic test accuracy studies: a systematic review [^11392q8B]. JAMA Network Open (2024). High credibility.

Introduction

A rigorous systematic review provides a meticulous summary of available primary research on a given topic, and is underpinned by features such as clear objectives, definition of reproducible methodology a priori and systematic presentation of findings. These characteristics serve to minimize bias and thereby enhance the credibility of conclusions drawn from the review findings. Systematic reviews of diagnostic test accuracy (DTA) studies aim to provide a complete interpretation of existing data regarding diagnostic test performance in a clear and concise way that has obvious relevance to clinical practice.

The component primary studies brought together for systematic review or meta-analysis typically inherently differ. Although the results of component primary studies will always differ to some degree, heterogeneity or between-study variability describe differences in underlying study parametersand can be further classified into 3 main subtypes. Clinical heterogeneity refers to differences in the study population, intervention, diagnostic test applied, or outcomes assessed. Methodological heterogeneity refers to heterogeneity in study design and risk of bias. Statistical heterogeneity refers to differences in the measured effect size of an intervention or accuracy of a diagnostic test beyond what would be expected to be attributable to random, within-study variability alone. Statistical heterogeneity is the product of clinical and methodological variability and randomly occurring effect-size modifiers and only becomes apparent after analysis of the results.DTA reviews are more likely to be affected by high levels of heterogeneity than reviews of interventions. Reasons for this difference include the common presence of threshold effects in DTA studies, the tendency for there to be greater differences in clinical parameters (eg, study design, patient population, and test protocol,) between DTA studies, as well as there being few randomized clinical trials in DTA evaluation despite the ability of randomized clinical trials to confer more thorough control of other variables compared with the observational study designs that are more commonly adopted in DTA studies. Additionally, diagnostic performance is often expressed as a pair of accuracy measures (eg, sensitivity and specificity), whose inherent negative correlation should be accounted for in analyses.

---

### Congress of neurological surgeons systematic review and evidence-based guidelines for patients with Chiari malformation: diagnosis [^11439FzG]. Neurosurgery (2023). High credibility.

Classification of evidence on diagnosis — Class I evidence with Level I (or A) recommendation is "Evidence provided by one or more well-designed clinical studies of a diverse population using a "gold standard" reference test in a blinded evaluation appropriate for the diagnostic applications and enabling the assessment of sensitivity, specificity, positive and negative predictive values, and, where applicable, likelihood ratios"; Class II evidence with Level II (or B) recommendation is "Evidence provided by one or more well-designed clinical studies of a restricted population using a "gold standard" reference test in a blinded evaluation appropriate for the diagnostic applications and enabling the assessment of sensitivity, specificity, positive and negative predictive values, and, where applicable, likelihood ratios"; Class III evidence with Level III (or C) recommendation is "Evidence provided by expert opinion or studies that do not meet the criteria for the delineation of sensitivity, specificity, positive and negative predictive values, and, where applicable, likelihood ratios".

---

### Diagnostic sweat testing: the Cystic Fibrosis Foundation guidelines [^1163y9Kj]. The Journal of Pediatrics (2007). Medium credibility.

Cystic fibrosis (CF) diagnosis — gold standard: Despite the availability of genetic testing, a quantitative pilocarpine iontophoresis sweat chloride test remains the gold standard for the diagnosis of CF.

---

### Screening and diagnostic tools for complex regional pain syndrome: a systematic review [^113eQtJY]. Pain (2021). Medium credibility.

Risk of bias of individual studies was assessed using the Quality Assessment of Diagnostic Accuracy Studies tool (QUADAS-2) by 2 raters (A.H. and K.B.).QUADAS-2 is a recommended tool for use in systematic reviews of diagnostic accuracy studies. The tool evaluates risk of bias and applicability in 4 domains: patient selection, index test, reference standard, and flow and timing. Each of the 4 domains receives a rating of high, low, or unclear for risk of bias, with the first 3 also rated for applicability. Risk of bias in each domain is assessed using signaling questions to identify potential risk of bias concerns. For example, study patient selection should be done without inappropriate exclusion criteria, and the interpretation of the index and reference standard test must be done without knowledge of the result of the other. Applicability is rated based on whether the study matched the review questions. The index test is the novel test that is being evaluated for diagnostic accuracy, and the reference standard is a test that is used as a comparator. Ideally, a reference standard is 100% sensitive and specific and reveals the absolute truth about a diagnosis, positive or negative (sometimes referred to as a gold standard test). For this review, selection of the reference standard test was based on study design where an existing tool or physician diagnosis was typically used as the reference test. Given that no widely accepted and evaluated reference standard exists, no tool used as the reference standard received higher than an "unclear" score for risk of bias within the QUADAS-2 tool. Discrepancies were resolved by consensus.

With respect to the secondary search specific to the pediatric population, data extracted from each study included tool name, study setting, type of study, and study sample characteristics (including number of participants and age range).

---

### Correcting for the absence of a gold standard improves diagnostic accuracy of biomarkers in Alzheimer's disease [^116REUyM]. Journal of Alzheimer's Disease (2015). Low credibility.

Background

Studies investigating the diagnostic accuracy of biomarkers for Alzheimer's disease (AD) are typically performed using the clinical diagnosis or amyloid-β positron emission tomography as the reference test. However, neither can be considered a gold standard or a perfect reference test for AD. Not accounting for errors in the reference test is known to cause bias in the diagnostic accuracy of biomarkers.

Objective

To determine the diagnostic accuracy of AD biomarkers while taking the imperfectness of the reference test into account.

Methods

To determine the diagnostic accuracy of AD biomarkers and taking the imperfectness of the reference test into account, we have developed a Bayesian method. This method establishes the biomarkers' true value in predicting the AD-pathology status by combining the reference test and the biomarker data with available information on the reliability of the reference test. The new methodology was applied to two clinical datasets to establish the joint accuracy of three cerebrospinal fluid biomarkers (amyloid-β 1–42, Total tau, and P-tau181p) by including the clinical diagnosis as imperfect reference test into the analysis.

Results

The area under the receiver-operating-characteristics curve to discriminate between AD and controls, increases from 0.949 (with 95% credible interval [0.935,0.960]) to 0.990 ([0.985,0.995]) and from 0.870 ([0.817,0.912]) to 0.975 ([0.943,0.990]) for the cohorts, respectively.

Conclusions

Use of the Bayesian methodology enables an improved estimate of the exact diagnostic value of AD biomarkers and overcomes the lack of a gold standard for AD. Using the new method will increase the diagnostic confidence for early stages of AD.

---

### Contact dermatitis: a practice parameter-update 2015 [^115y2NcU]. The Journal of Allergy and Clinical Immunology: In Practice (2015). Medium credibility.

Contact dermatitis: a practice parameter–update 2015 — diagnostic confirmation emphasizes that in patients suspected of ACD, patch testing is the gold standard to confirm the diagnosis [Strength of Recommendation: Strong; C Evidence], and patch testing is indicated in any patient with acute or chronic, often pruritic, dermatitis if underlying or secondary ACD is suspected. Although medical history can strongly suggest the cause of ACD, it has moderate sensitivity (76%) and specificity (76%) in establishing the diagnosis; for nickel allergy, a positive PT to nickel sulfate is demonstrable in only 60% of patients with a positive history (ie, positive predictive value 60%), whereas 12.5% to 15% of persons reporting a negative history of metal allergy had a positive PT response to nickel sulfate. Patch testing identifies contact sensitizers in nearly 50% of patients presenting with scattered generalized dermatitis; although sensitization occurring after patch testing is rare, this has been reported and the possibility of active sensitization can be minimized by testing with dilute solutions, and skin prick testing has no role in the evaluation of ACD.

---

### Diagnostic accuracy of serological tests for COVID-19: systematic review and meta-analysis [^111LXQAg]. BMJ (2020). Excellent credibility.

Conclusion and future research

Future studies to evaluate serological tests for covid-19 should be designed to overcome the major limitations of the existing evidence base. This can be readily accomplished by adhering to the fundamentals of the design for diagnostic accuracy studies: a well defined use-case (ie, specific purpose for which the test is being used); consecutive sampling of the target population within the target use-case; performance of the index test in a standardised and blinded manner using the same methods that will be applied in the specialty; and ensuring the reference test is accurate, performed on all participants, and interpreted blind to the results of the index test. To reduce the likelihood of misclassification, the reference standard should consist of RT-PCR performed on at least two consecutive specimens, and, when feasible, include viral cultures. To reduce variability in estimates and enhance generalisability, sensitivity and specificity should be stratified by setting (outpatient versus in-patient), severity of illness, and the number of days elapsed since symptom onset.

In summary, we have found major weaknesses in the evidence base for serological tests for covid-19. The evidence does not support the continued use of existing point-of-care serological tests for covid-19. While the scientific community should be lauded for the pace at which novel serological tests have been developed, this review underscores the need for high quality clinical studies to evaluate these tools. With international collaboration, such studies could be rapidly conducted and provide less biased, more precise, and more generalisable information on which to base clinical and public health policy to alleviate the unprecedented global health emergency that is covid-19.

---

### Methodology for ACOEM's occupational medicine practice guidelines-2025 revision [^112MzDVk]. ACOEM (2025). High credibility.

Diagnostic terminology and reference standards — Split-sample validation is achieved by collecting all the information in a single tranche, then artificially dividing this into "derivation" and "validation" samples; an "Absolute SpPin" is a diagnostic finding whose Specificity is so high that a Positive result rules- in the diagnosis; an "Absolute SnNout" is a diagnostic finding whose Sensitivity is so high that a Negative result rules out the diagnosis; good, better, bad and worse refer to comparisons between treatments in terms of clinical risks and benefits; good reference standards are independent of the test and applied blindly or objectively to applied to all patients; poor reference standards are haphazardly applied, but still independent of the test.

---

### Trial and error. how to avoid commonly encountered limitations of published clinical trials [^115Sn5De]. Journal of the American College of Cardiology (2010). Low credibility.

The randomized controlled clinical trial is the gold standard scientific method for the evaluation of diagnostic and treatment interventions. Such trials are cited frequently as the authoritative foundation for evidence-based management policies. Nevertheless, they have a number of limitations that challenge the interpretation of the results. The strength of evidence is often judged by conventional tests that rely heavily on statistical significance. Less attention has been paid to the clinical significance or the practical importance of the treatment effects. One should be cautious that extremely large studies might be more likely to find a formally statistically significant difference for a trivial effect that is not really meaningfully different from the null. Trials often employ composite end points that, although they enable assessment of nonfatal events and improve trial efficiency and statistical precision, entail a number of shortcomings that can potentially undermine the scientific validity of the conclusions drawn from these trials. Finally, clinical trials often employ extensive subgroup analysis. However, lack of attention to proper methods can lead to chance findings that might misinform research and result in suboptimal practice. Accordingly, this review highlights these limitations using numerous examples of published clinical trials and describes ways to overcome these limitations, thereby improving the interpretability of research findings.

---

### A guide for diagnostic evaluations [^116Ra24Y]. Nature Reviews: Microbiology (2010). Medium credibility.

Accurate diagnostic tests have a key role in patient management and the control of most infectious diseases. Unfortunately, in many developing countries, clinical care is often critically compromised by the lack of regulatory controls on the quality of these tests. The information available on the performance of a diagnostic test can be biased or flawed because of failings in the design of the studies which assessed the performance characteristics of the test. As a result, diagnostic tests are sold and used in much of the developing world without evidence of effectiveness. Misdiagnosis leading to failure to treat a serious infection or wasting expensive treatment on people who are not infected remains a serious obstacle to health.

---

### GOLD or lower limit of normal definition? A letter and authors' response [^1169QsW9]. Respiratory Research (2012). Low credibility.

Authors' response

Gülmisal Güder, Stefan Störk, Arno W Hoes and Frans H Rutten.

We thank Quanjer and colleagues for their insightful comments regarding our article. Some points, however, deserve further clarification. The critique of current COPD definitions was not intended to specifically address the GOLD guidelines or the lower limit of normal (LLN) approach, but was a general caveat to base the diagnosis onto one single ratio derived from pulmonary function testing (PFT), ie FEV/FVC. Using the Stanojevic equation as proposed by Quanjer might indeed increase the specificity of the LLN definition, but it will not improve the sensitivity since the LLN approach as well as the GOLD definition fails to acknowledge false negative spirometric test results. Important diagnostic needs will still remain unsolved. E.g. how should we classify patients who – according to the GOLD or LLN definition – are (borderline) normal and have abnormalities at diffusion testing or RV/TLC ratio, especially those with a smoking history and symptoms.

All 405 patients in our study had a GP's diagnosis of COPD and were treated accordingly. The study team did not attempt to modify the pulmonary treatment. We also did not exclude patients with co-morbidities as this might provoke selection bias, nor did we consider healthy never-smokers as controls, since they hardly pose diagnostic problems in daily routine. According to the GOLD definition, about 60% of all patients suffered from COPD, and about 40% depending on the type of LLN definition. Hence, the challenge was not only to differentiate true positive from false positive COPD patients but to also consider the possibility of false negatives. We therefore used an expert diagnosis of a pulmonologist and GP (validated internally and externally) as the reference standard. We agree that this is not a perfect standard. However, we are still convinced that an experienced physician will outperform a single and rather rigid mathematical model in the complex process of integrating very heterogeneous informations from various tests, including other PFT results as well as signs and symptoms. For many diseases that lack an irrefutable single test as a reference, the use of an expert panel (using all diagnostic information) is an excepted alternative, e.g. in heart failure.

---

### Quantitative sensory testing: report of the therapeutics and technology assessment subcommittee of the American Academy of Neurology [^114JrJqc]. Neurology (2003). Medium credibility.

Classification of evidence — The appendix defines Class I as "Evidence provided by a prospective study in a broad spectrum of persons with the suspected condition, using a 'gold standard' for case definition, when test is applied in a blinded evaluation, and enabling the assessment of appropriate tests of diagnostic accuracy"; Class II as "Evidence provided by a prospective study of a narrow spectrum of persons with the suspected condition, or a well designed retrospective study of a broad spectrum of persons with an established condition (by 'gold standard') compared with a broad spectrum of controls, where test is applied in a blinded evaluation, enabling the assessment of appropriate tests of diagnostic accuracy"; Class III as "Evidence provided by a retrospective study where either persons with the established condition or controls are of a narrow spectrum, and where test is applied in a blinded evaluation"; and Class IV as "Any design where test is not applied in blinded evaluation OR evidence provided by expert opinion alone or in descriptive case series (without controls)".

---

### The Infectious Diseases Society of America guidelines on the diagnosis of COVID-19: molecular diagnostic testing (December 2023) [^115oQLtA]. Clinical Infectious Diseases (2024). High credibility.

Infectious Diseases Society of America (IDSA) COVID-19 molecular diagnostic testing — regulatory status and evidence scope: Multiple commercial test manufacturers and clinical laboratories, including academic medical centers, have received emergency use authorization (EUA) for SARS-CoV-2-specific molecular diagnostic tests, including direct to consumer and over the counter tests manufactured for home use, while one multiplex nucleic acid amplification test (NAAT) has received full FDA approval under the traditional premarket review process. EUA guidance differs from the standard FDA approval process; in a public health emergency the FDA only requires test developers to establish acceptable analytical accuracy, and clinical test performance (clinical sensitivity and specificity) has yet to be determined or comprehensively compared across EUA platforms. Given growing test availability and literature, IDSA formed a multidisciplinary panel to critically appraise the existing literature and develop evidence-based diagnostic test recommendations, and anticipates that these guidelines will continue to be updated as substantive new information becomes available.

---

### An evidence-based approach to assessing surgical versus clinical diagnosis of symptomatic endometriosis [^1125arYT]. International Journal of Gynaecology and Obstetrics (2018). Low credibility.

Challenges intrinsic to the accurate diagnosis of endometriosis contribute to an extended delay between the onset of symptoms and clinical confirmation. Intraoperative visualization, preferably with histologic verification, is considered by many professional organizations to be the gold standard by which endometriosis is diagnosed. Clinical diagnosis of symptomatic endometriosis via patient history, physical examination, and noninvasive tests, though more easily executed, is generally viewed as less accurate than surgical diagnosis. Technological advances and increased understanding of the pathophysiology of endometriosis warrant continuing reevaluation of the standard method for diagnosing symptomatic disease. A review of the published literature was therefore performed with the goal of comparing the accuracy of clinical diagnostic measures with that of surgical diagnosis. The current body of evidence suggests that clinical diagnosis of symptomatic endometriosis is more reliable than previously recognized and that surgical diagnosis has limitations that could be underappreciated. Regardless of the methodology used, women with suspected symptomatic endometriosis would be well served by a diagnostic paradigm that is reliable, conveys minimal risk of under- or over-diagnosis, lessens the time from symptom development to diagnosis, and guides the appropriate use of medical and surgical management strategies.

---

### Meta-analysis of diagnostic and screening test accuracy evaluations: methodologic primer [^114hza2q]. AJR: American Journal of Roentgenology (2006). Low credibility.

Objective

Interest in evidence-based diagnosis is growing rapidly as diagnostic and screening techniques proliferate. In this article we provide an overview of systematic reviews of diagnostic performance and discuss in detail statistical methods for the most common variant of the problem: meta-analysis of studies in which a pair of estimates of sensitivity and specificity is reported. The need to account for possible variations in threshold for test positivity across studies led to the formulation of the Summary ROC (SROC) curve method. We discuss graphical and model-based ways to estimate, summarize, and compare SROC curves, and we present an example from a meta-analysis of data on techniques for staging cervical cancer. We also present a brief survey of the methodologic literature for addressing heterogeneity, correlated data, multiple thresholds per study, and systematic reviews of ROC studies. We conclude with a discussion of the significant methodologic challenges that continue to face investigators in this area of diagnostic medicine research.

Conclusion

Systematic reviews of diagnostic performance are a rigorous approach to examining and synthesizing evidence in the evaluation of diagnostic and screening tests. The information from such reviews is needed by clinicians, health policy makers, researchers in diagnostic medicine, developers of diagnostic techniques, and the general public. However, despite progress in study quality and reporting and in methodologic development, major challenges confront investigators undertaking these reviews.

---

### Fool's gold: why imperfect reference tests are undermining the evaluation of novel diagnostics: a reevaluation of 5 diagnostic tests for leptospirosis [^116G7PsR]. Clinical Infectious Diseases (2012). Low credibility.

We began to question the accuracy of the recommended approach after becoming increasingly aware that some patients with suspected leptospirosis who had positive results of alternative tests, such as a real-time polymerase chain reaction (PCR) targeting the gene encoding the 16S ribosomal RNA (rRNA) subunit, a lateral flow test, and/or an immunofluorescence assay (IFA), were negative for leptospirosis on the basis of our reference standard. This was a consistent finding across different studies in which we had obtained a convalescent-phase specimen from the majority of cases. We hypothesized that the reference standard was imperfect and that the accuracy of alternative diagnostic tests, estimated using the gold standard, was biased. We sought an appropriate statistical model with which to determine the true accuracy of alternative tests in this situation.

Bayesian latent class models have been increasingly used to evaluate the true accuracy of diagnostic tests and do not require the assumption that any test or combination of tests is perfect. The objective of this study was to use Bayesian latent class models to reanalyze individual-level data from 4 existing data sets gathered during studies of patients presenting to the hospital with suspected leptospirosis. On the basis of these findings, we estimated the true accuracy of a range of serological and molecular diagnostic tests for leptospirosis and determined the impact of an imperfect gold standard on the reported accuracies of alternative tests. Information from all studies was combined using Bayesian random-effects meta-analysis to further support the observations from individual studies.

---

### Diagnostic accuracy of serological tests for COVID-19: systematic review and meta-analysis [^114s1af5]. BMJ (2020). Excellent credibility.

Introduction

Accurate and rapid diagnostic tests will be critical for achieving control of coronavirus disease 2019 (covid-19), a pandemic illness caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). Diagnostic tests for covid-19 fall into two main categories: molecular tests that detect viral RNA, and serological tests that detect anti-SARS-CoV-2 immunoglobulins. Reverse transcriptase polymerase chain reaction (RT-PCR), a molecular test, is widely used as the reference standard for diagnosis of covid-19; however, limitations include potential false negative results, changes in diagnostic accuracy over the disease course, and precarious availability of test materials. Serological tests have generated substantial interest as an alternative or complement to RT-PCR in the diagnosis of acute infection, as some might be cheaper and easier to implement at the point of care. A clear advantage of these tests over RT-PCR is that they can identify individuals previously infected by SARS-CoV-2, even if they never underwent testing while acutely ill. As such, serological tests could be deployed as surveillance tools to better understand the epidemiology of SARS-CoV-2 and potentially inform individual risk of future disease.

Many serological tests for covid-19 have become available in a short period, including some marketed for use as rapid, point-of-care tests. The pace of development has, however, exceeded that of rigorous evaluation, and important uncertainty about test accuracy remains. We undertook a systematic review and meta-analysis to assess the diagnostic accuracy of serological tests for SARS-CoV-2 infection. Our objectives were to evaluate the quality of the available evidence, to compare pooled sensitivities and specificities of different test methods, and to identify study, test, and patient characteristics associated with test accuracy.

---

### Laboratory testing for the diagnosis of HIV infection: updated recommendations [^115K5xmX]. HIV.gov (2014). Medium credibility.

Laboratory testing for the diagnosis of HIV infection — tables of evidence key — states that the tables identify specific studies and their limitations and strengths for sensitivity, specificity, and accuracy, and lists limitations including "Possible selection bias", "Incomplete information or lack of peer review", "Indirect comparisons of test performance", "Comparison with different, composite, or unspecified reference standards", "Uncertainty about specimen integrity", and "Small size of specimen collections", noting that "Testing of < 100 specimens yields very wide confidence intervals around point estimates and might be more affected by selection bias"; strengths include "Specimens prospectively collected for diagnostic testing".

---

### AGA clinical practice update on the diagnosis and management of extraesophageal gastroesophageal reflux disease: expert review [^111cmmDg]. Clinical Gastroenterology and Hepatology (2023). High credibility.

Diagnostic testing for EER — Best Practice Advice 3 states that "Currently, there is no single diagnostic tool that can conclusively identify GER as the cause of EER symptoms" and that "Determination of the contribution of GER to EER symptoms should be based on the global clinical impression derived from patients' symptoms, response to GER therapy, and results of endoscopy and reflux testing". The text adds that "there is not a single gold standard test for the diagnosis of EER" and that "a diagnosis of EER requires incorporating the global clinical evaluation involving patients' symptoms, endoscopic findings, esophageal reflux monitoring, and response to treatments".

---

### How to: evaluate a diagnostic test [^117LKsYk]. Clinical Microbiology and Infection (2019). Medium credibility.

Background

The development of an in vitro diagnostic test from a good idea to a clinically relevant tool takes several steps, with more stringent requirements at every step.

Objectives

This article aims to summarize the necessary questions to be asked about a test and to illustrate study designs answering these questions. We also aim to relate Regulation (EU) 2017/746 to the needs of evidence-based diagnostic testing, where applicable.

Sources

We used literature on evidence-based diagnostics, a text book on clinical trials in the development and marketing of medical devices and the English version of Regulation 2017/746 of the European Parliament and of the Council on in vitro diagnostic medical devices.

Content

The combination of different test uses and different stages of development determine the required test characteristics and suitability of study designs. In an earlier stage of test development it may be crucial to know whether a test can differentiate diseased persons from healthy controls, although this tells us little about how a test will perform in practice. Later stages focus on the diagnostic accuracy of a test in a clinically relevant situation. However, a test that perfectly distinguishes between patients with and without a certain condition may still have little effect on patient outcomes. Therefore, randomized controlled trials of testing may be needed, as well as post-marketing monitoring.

Implications

Both researchers and users of tests need to be aware of the limitations of diagnostic test accuracy and realize that accuracy is only indirectly linked to people's health status.

---

### The Infectious Diseases Society of America guidelines on the diagnosis of COVID-19: molecular diagnostic testing (December 2023) [^116M2JAV]. Clinical Infectious Diseases (2024). High credibility.

Infectious Diseases Society of America (IDSA) molecular COVID-19 diagnostic testing guideline — Because EUA submissions require analytical but not clinical performance data, the panel identified few studies reporting clinical performance and based most recommendations on diagnostic test accuracy. Accuracy metrics include sensitivity and specificity, and positive and negative predictive values are essential; calculating predictive values requires prevalence or pre-test probability, yet true community prevalence may be ill-defined or underestimated when testing is limited or results are not reported. Accordingly, the panel varied estimates of prevalence/pre-test probability and assay sensitivity and specificity in analyses using available literature and public dashboards to mirror what may be encountered in clinical practice.

---

### Diagnostic accuracy of maternal serum multiple marker screening for early detection of gestational diabetes mellitus in the absence of a gold standard test [^112eASKu]. BMC Pregnancy and Childbirth (2020). Medium credibility.

In clinical practice, for assessing the performance of a new diagnostic test, the result should be compared with the outcome of a gold standard. Ideally, when a gold standard is available, estimating the accuracy measures such as sensitivity, specificity, and area under the receiver operating characteristic (ROC) curve (AUC) is straightforward and without error. However, problems arise when the true disease status of a person could not be identified with certainty due to several reasons including ethical issues, GS test is too expensive, invasive, or it cannot practically be performed, etc. Some investigators argued that the absence of a GS might lead to misclassification of disease status and biased estimates of tests accuracy parameters. In such situations, obtaining a definitive verification of diagnosis for each subject becomes challenging. Hence, it is vital to use the more advanced statistical techniques for evaluating the performance of the new test when a GS is not available.

In the case of more than a single diagnostic test, it is usually desirable to combine the results of multiple tests into a composite diagnostic test in order to obtain more accurate disease classifications. It is worth to note that combining test results may help clinicians make better diagnostic judgment and increased clinical benefits. In such settings, to evaluate the diagnostic performance, one can compare the diagnostic accuracy of the combined tests as opposed to the accuracy of a single test. Latent class or finite mixture models have been increasingly used to combine the results from multiple diagnostic tests through a statistical model to get estimates of disease prevalence and test accuracy in the absence of a gold standard. Clearly, in this modeling approach, the unobserved disease status serves as a latent variable and observed associations among the diagnostic tests are explained by the latent variable. To estimate accuracy parameters of tests, most of the literature had accomplished within the Bayesian framework. This is a well-established method for robust assessment of diagnostic tests.

The goal of the current study was to explore whether combining the results obtained from different biomarkers could aid in prediction of GDM between 14 and 17 weeks of gestation when the true disease status is unknown. For this purpose, we applied the Bayesian latent class models (LCMs) for estimating sensitivity, specificity, and AUC adjusting for some confounding factors.

---

### How to use an article about a diagnostic test [^111ZDRMB]. The Journal of Urology (2008). Low credibility.

Purpose

Urologists frequently confront diagnostic dilemmas, prompting them to select, perform and interpret additional diagnostic tests. Before applying a given diagnostic test the user should ascertain that the chosen test would indeed help decide whether the patient has a particular target condition. In this article in the Users' Guide to the Urological Literature series we illustrate the guiding principles of how to critically appraise a diagnostic test, interpret its results and apply its findings to the care of an individual patient.

Materials and Methods

The guiding principles of how to evaluate a diagnostic test are introduced in the setting of a clinical scenario. We propose a stepwise approach that addresses the question of whether the study results are likely to be valid, what the results are and whether these results would help urologists with the treatment of their individual patients.

Results

Some of the issues urologists should consider when assessing the validity of a diagnostic test study are how the authors assembled the study population, whether they used blinding to minimize bias and whether they used an appropriate reference standard in all patients to determine the presence or absence of the target disorder. Urologists should next evaluate the properties of the diagnostic test that indicate the direction and magnitude of change in the probability of disease for a particular test result. Finally, urologists should ask a series of questions to understand how the diagnostic test may impact the care of their patients.

Conclusions

Application of the guides presented in this article will allow urologists to critically appraise studies of diagnostic tests. Determining the study validity, understanding the study results and assessing the applicability to patient care are 3 fundamental steps toward an evidence-based approach to choosing and interpreting diagnostic tests.

---

### A standardized framework for the validation and verification of clinical molecular genetic tests [^113YM4WH]. European Journal of Human Genetics (2010). Low credibility.

Validation

Full validation is required when there is no suitable performance specification available, for example, with novel tests or technologies. This process involves assessing the performance of the test in comparison with a 'gold standard' or reference test that is capable of assigning the sample status without error (ie, a test that gives 'true' results). In simple terms, validation can be seen as a process to determine whether we are 'performing the correct test'. In the field of medical genetics, with the almost complete absence of reference tests or certified reference materials, the reference should be the most reliable diagnostic method available. It is worth noting that the gold standard does not have to comprise results from a single methodology; different techniques could be used for different samples and in some cases the true result may represent a combination of results from a portfolio of different tests. To avoid introducing bias, the method under validation must not, of course, be included in this portfolio.

Validation data can be used to assess the accuracy of either the technology (eg, sequencing for mutation detection) or the specific test (eg, sequencing for mutation detection in the BRCA1 gene). Generally speaking, the generic validation of a novel technology should be performed on a larger scale, ideally in multiple laboratories (interlaboratory validation), and include a much more comprehensive investigation of the critical parameters relevant to the specific technology to provide the highest chance of detecting sources of variation and interference.

---

### General principles of diagnostic testing as related to painful lumbar spine disorders: a critical appraisal of current diagnostic techniques [^114bRH8x]. Spine (2002). Low credibility.

Study Design

The literature on diagnostic tests available to the spine clinician for the evaluation of chronic low back pain was reviewed.

Objectives

To review critically the available information and data on invasive diagnostic tests used for evaluation of chronic low back pain.

Summary Of Background Information

Numerous published studies have described the technique and clinical results of diagnostic blocks for chronic low back pain. There are various methodologies, but most lack of an adequate "gold standard" with which to compare the results of the diagnostic test.

Methods

The available published studies of diagnostic tests commonly used in the evaluation of chronic low back pain were reviewed, with a focus on invasive techniques. The techniques were evaluated on the basis of the data available to support the conclusions that could be drawn for each of these techniques. The principles of diagnostic testing, including specificity and sensitivity, were reviewed and applied in the context of the data available for each of these invasive tests.

Results

The essential features the clinician seeks in a diagnostic test are accuracy, safety, and reproducibility. It is essential to have a gold standard with which to compare the accuracy of a given diagnostic test. There is no completely reliable gold standard with which to compare a diagnostic test (or injection) when the absence of pain is the end point. The clinical setting in which the test is used directly affects the test results. The prevalence of the disease therefore affects the meaningfulness of the test results. Imaging studies have their greatest value in the exclusion of other conditions. These studies alone were not adequate for predicting the patients who would respond to controlled diagnostic blocks of the facet joint. Facet joint diagnostic blockade probably is most accurately performed by median nerve branch block. The greatest specificity for a positive response to a facet denervation procedure is achieved when the diagnosis is established via highly controlled anesthetic blocks. Over the past few decades, the sacroiliac joint has received varying degrees of interest as an important pain generator of low back pain. Despite testimonials to the contrary, no diagnostic physical examination has correlated with sufficient specificity to diagnose this condition reliably from a clinical standpoint. Lumbar discography has been one of the single most controversial subjects in the management of degenerative, painful lumbar spine conditions. The specificity and sensitivity are high for the diagnosis of disc degeneration. The question that revolves around discography concerns the accuracy of this test for the diagnosis of discogenic pain. An integral part of the problem is the lack of an adequate gold standard. In a comparison of nerve root blockade, sciatic nerve block, posterior ramus block, and subcutaneous injection in a cohort of patients with sciatica, the sensitivity of nerve root block was very high, with only a moderate level of specificity. In the case of diagnostic selective nerve blocks used for evaluation of complex or protean nerve compression, surgical confirmation and clinical results should be a reliable gold standard. Conflicting results have been presented depending on the target lesion and method of study.

Conclusions

There are inherent limitations in the accuracy of all diagnostic tests. The tests used to diagnose the source of a patient's chronic low back pain require accurate determination of the abolition or reproduction of the patient's painful symptoms.

---

### Effects of disease severity distribution on the performance of quantitative diagnostic methods and proposal of a novel'V-plot' methodology to display accuracy values [^112gwahX]. Open Heart (2018). Low credibility.

Conclusions

For any given clinical test being compared with a gold standard, there is no universal value of diagnostic accuracy, sensitivity, specificity, predictive values, likelihood ratios or ROC curves. Accuracy will always vary progressively from almost 100% at the extremes (of health and disease) to approximately 50% (close to pure chance) near the diagnostic cut-point. Disease prevalence and the precise distribution of values in the underlying sample (extremes versus intermediate) can therefore completely control the obtained value for a test's diagnostic accuracy. A test should not be chosen by clinicians based on a reported high accuracy value, unless the disease distribution of the study sample is known to be clinically relevant.

The V-plot of accuracies presented here exposes the variation of diagnostic accuracy along the spectrum of disease and is therefore a truly sample-independent display of categorical agreement between two methods of clinical measurement. Once derived for the relationship between two methods of measurement, the V-plot allows for the overall diagnostic accuracy to be estimated in separate samples where frequency distribution is known.

---

### Defining and measuring diagnostic uncertainty in medicine: a systematic review [^112kHyRH]. Journal of General Internal Medicine (2018). Low credibility.

CONCLUSION

Diagnostic uncertainty, while prevalent, has not been comprehensively evaluated in current literature and medical practice. Although various methods have been used to study diagnostic uncertainty in clinical practice, evidence is limited as to which of these is the most useful or relevant, and no comprehensive measurement framework exists. On the basis of this review, we propose that diagnostic uncertainty be defined as " subjective perception of an inability to provide an accurate explanation of the patient's health problem". As next steps, we need to adopt a uniform definition of diagnostic uncertainty and work toward methodological advances in measuring diagnostic uncertainty in medical practice. The scientific foundation created in this review can inform future interventions aimed at improving the management of diagnostic uncertainty, thereby helping to reduce both under-diagnosis and overuse of health care resources.

---

### Anticipating missing reference standard data when planning diagnostic accuracy studies [^1137qyRF]. BMJ (2016). Excellent credibility.

Conclusion

Despite efforts to assess the outcome in all participants in a diagnostic accuracy study, missing reference standard results (that is, missing outcomes) are often inevitable and should be anticipated in any prospective diagnostic accuracy study. Analyses that include only the participants in whom the reference standard was performed are likely to produce biased estimates of the accuracy of the index tests. Several analytical solutions for dealing with missing outcomes are available; however, these solutions require knowledge about the pattern of missing data, and they are no substitute for complete data. Researchers should anticipate the mechanisms that generate missing reference standard results before the start of a study, so that measures and actions can explicitly be taken to reduce the potential for biased estimates of the accuracy of the tests, markers, or models under study, as well as to facilitate correction in the analysis phase. In all cases, researchers should include in their study report how missing data on the index test and reference standard were handled, as invited by the STARD reporting guideline.

---

### Distal symmetric polyneuropathy: a definition for clinical research: report of the American Academy of Neurology, the American association of electrodiagnostic medicine, and the American Academy of Physical Medicine and Rehabilitation [^111B5w4v]. Neurology (2005). Medium credibility.

AAN/AAEM/AAPM&R distal symmetric polyneuropathy — diagnostic glossary standardizes key terms for case definition and evidence assessment: a predictor (diagnostic predictor) is a symptom, examination finding, or test result presumably predicting the presence of a distal symmetric polyneuropathy; the target disorder is the condition or disease being sought and, in this context, is a specific type of distal symmetric polyneuropathy (e.g., diabetic peripheral neuropathy); the reference standard (the gold standard) is the test or procedure performed to determine the actual presence or absence of a distal symmetric polyneuropathy; the nominal group process is a formalized, iterative method for achieving expert consensus while preserving individual input; and the ROC (receiver-operating-characteristic) curve is a standardized graph of sensitivity (true positive rate) by specificity (true negative rate) designed to depict diagnostic accuracy and the trade-off between increasing sensitivity and decreasing specificity.

---

### Methodology for ACOEM's occupational medicine practice guidelines-2025 revision [^114vMXod]. ACOEM (2025). High credibility.

ACOEM Occupational Medicine Practice Guidelines — diagnostic test study scoring delineates criteria for test application, blinding, and statistical reporting at three levels (1.0, 0.5, 0.0). Investigative test conducted on all patients is scored as 1.0 = 95% or more of patients received investigative test; 0.5 = 80% or more of patients received investigative test; 0.0 = Less than 80% of patients received investigative test. Gold standard (control) test conducted on all patients is scored with 1.0 = 95% or more of patients received investigative test; 0.5 = 80% or more of patients received investigative test; 0.0 = Less than 80% of patients received investigative test. Assessor of investigative test result blinded to disorder and assessor of gold standard test result blinded to disorder are each graded with 1.0 = Blinding of assessor defined and plausible, 0.5 = Blinding not clearly stated but probable, and 0.0 = No blinding or blinding improbable based on methods. For statistical analysis, 1.0 = Sensitivity and specificity stated or can be readily determined, 0.5 = One but not both provided, and 0.0 = Not provided, unable to calculate; similarly, for positive predictive value (PPV) and negative predictive value (NPV), 1.0 = PPV and NPV stated or can be readily determined, 0.5 = One but not both provided, and 0.0 = Not provided, unable to calculate. Normal range of investigative test includes a criterion where 1.0 = Normal range of test defined and appropriate for study.

---

### Anticipating missing reference standard data when planning diagnostic accuracy studies [^113ybFhK]. BMJ (2016). Excellent credibility.

Data missing due to infeasibility

When performing the reference standard in any of the participants in specific subgroups is explicitly decided against or even impossible — for example, no biopsy of the breast in women without any abnormality on mammography (table 1) — some alternative measure of the target disease should be obtained. In the design phase of a study, the decision can be made to use an alternative reference standard in these participants, a common choice being clinical follow-up. Rather than focusing on how well the index test results correspond to the preferred reference standard, it may be more relevant to focus on whether the index test provides information about clinically relevant outcomes. If so, the clinical relevance of this alternative reference standard should be discussed. One should then focus on the accuracy estimates of the index tests across strata of the index test results — that is, presenting predictive values.

Considerations for study design

Although we have provided guidance for how to handle missing data on the reference standard, we stress that situations exist in which these approaches to deal with missing reference standard data may not be possible or cannot remove the bias, even when researchers anticipate the missing reference standards before the study starts. Additionally, although unbiased estimates of diagnostic test accuracy help to evaluate potential clinical value, cross sectional accuracy studies do not always provide the information needed when forming a conclusion about whether a test improves the care of patients. Hence, in some situations, it may be necessary to go beyond accuracy studies and opt for alternative designs that focus on estimating or comparing the clinical value of tests in terms of their ability to improve actual outcomes for patients. This may often be the case when missing outcomes are unavoidable or the new index test is hypothesised to outperform the reference standard.

---

### Creutzfeldt-jakob disease: updated diagnostic criteria, treatment algorithm, and the utility of brain biopsy [^111kmktd]. Neurosurgical Focus (2015). Low credibility.

Creutzfeldt-Jakob disease (CJD) is a rare neurodegenerative condition with a rapid disease course and a mortality rate of 100%. Several forms of the disease have been described, and the most common is the sporadic type. The most challenging aspect of this disease is its diagnosis-the gold standard for definitive diagnosis is considered to be histopathological confirmation-but newer tests are providing means for an antemortem diagnosis in ways less invasive than brain biopsy. Imaging studies, electroencephalography, and biomarkers are used in conjunction with the clinical picture to try to make the diagnosis of CJD without brain tissue samples, and all of these are reviewed in this article. The current diagnostic criteria are limited; test sensitivity and specificity varies with the genetics of the disease as well as the clinical stage. Physicians may be unsure of all diagnostic testing available, and may order outdated tests or prematurely request a brain biopsy when the diagnostic workup is incomplete. The authors review CJD, discuss the role of brain biopsy in this patient population, provide a diagnostic pathway for the patient presenting with rapidly progressive dementia, and propose newer diagnostic criteria.

---

### Patellofemoral pain [^112CttYp]. The Journal of Orthopaedic and Sports Physical Therapy (2019). High credibility.

Patellofemoral pain — identified research gap notes uncertainty in the optimal reference standard for diagnostic accuracy studies. Additional research is needed to determine the best reference standard to be utilized for studies of diagnostic test accuracy using individual or a combination of tests.

---

### What is the optimal timing for reading the leukocyte esterase strip for the diagnosis of periprosthetic joint infection? [^111VmDRC]. Clinical Orthopaedics and Related Research (2021). Medium credibility.

Gold Standard for Diagnosis of PJI: ICM Criteria

The definition of PJI was based on the ICM 2018 diagnostic criteria, which consist of major criteria and minor criteria. The minor criteria include the C-reactive protein or D-dimer level (2 points); erythrocyte sedimentation rate (1 point); elevated synovial white blood cell count, LE, or alpha-defensin (3 points); synovial polymorphonuclear percentage (2 points); culture (2 points); histologic analysis (3 points); and intraoperative purulence (3 points).

A total score of 6 or more indicated infection, a score between 3 and 5 was inconclusive, and a score of 2 or less indicated no infection. Forty-nine patients were classified as having PJI (score ≥ 6) and the other 35 were classified as having no PJI (score ≤ 2) according to the definition. All patients were followed regularly for more than 1 year, and 35 patients without PJI did not report an occurrence of PJI during the period. For this study, the ICM 2018 diagnostic criteria turned out to be robust, serving as the gold standard for PJI diagnosis. We scored all patients included in this study against the ICM 2018 diagnostic criteria, regardless of their LE strip test results. We used white blood cell count instead of LE strip test results in the gold standard test because they are equivalent in the diagnostic criteria.

Acquisition and Processing of Synovial Fluid

All diagnostic arthrocentesis procedures were performed in the outpatient operating room of our hospital, and all operating steps followed the standardized anatomic positioning procedure. After arthrocentesis, we divided the harvested joint fluid into two portions, one of which was put into a normal centrifuge tube by using a syringe and centrifuged as recommended by Aggarwal et al. (6600 rpm, 180 seconds; D3024, SCILOGEX, Pittsburgh, PA, USA). For one patient, the sample was not centrifuged because the collected synovial fluid was less than 1.5 mL. Of the 84 samples before centrifugation and the 83 samples after centrifugation, most of the LE strips gradually deepened in color over time after sample application.

---

### Adult outpatients with acute cough due to suspected pneumonia or influenza: CHEST guideline and expert panel report [^111MgD3U]. Chest (2019). High credibility.

Acute cough with suspected pneumonia — diagnostic indicators and reference standard: Physician judgment alone frequently led to overestimation of the probability of pneumonia. Although individual signs and symptoms alone cannot rule in or rule out pneumonia, combinations of signs and symptoms can improve overall diagnostic accuracy, and the absence of any vital sign abnormalities has a high negative predictive value for ruling out pneumonia. However, even with the highest cutoff points for decision rules, the positive predictive value is rarely > 50%. Studies assessed clinical algorithms against chest radiography as the gold standard for pneumonia diagnosis, although chest radiography is described as an imperfect gold standard because a significant proportion of pneumonia cases initially diagnosed based on higher-resolution imaging of the chest are not detected on chest radiographs.

---

### "GOLD or lower limit of normal definition? A comparison with expert-based diagnosis of chronic obstructive pulmonary disease in a prospective cohort-study" [^115mTkMD]. Respiratory Research (2012). Low credibility.

Data analysis

Continuous data are expressed as mean (standard deviation, SD) or median (quartiles), as appropriate. Comparisons between groups were made with Fisher's exact test or Mann-Whitney U -test. Sensitivity, specificity, positive and negative predictive values and kappa (κ) statistics with 95% confidence intervals (CI) were calculated for each COPD definition with the 'expert panel diagnosis of COPD' of the initial panel as the reference test. The classification system proposed by Landis and Koch was used to determine the level of concordance (a κ of 0.81–1.00 is considered almost perfect). A bootstrap method was used for calculating 95% CI of κ and to assess statistical significance for correlated κ. The diagnostic ability of different PFT parameters for predicting COPD according to the reference standard was tested using ROC curve with C-statistics with 95% CI. The two PFT parameters predicting best according to the C-statistics were incorporated into a new 'modified' GOLD- or LLN-based definition, and diagnostic performance (false positives, false negatives, kappa) of these extended models were compared to the original definitions (Figure 1).

Figure 1
Flow chart for diagnostic algorithm. In clinical practice the diagnosis of COPD is based on multiple variables. As the simplest model we chose a three PFT parameters approach in which an initial COPD YES/NO diagnosis based on FEV/FVC levels was corrected if FEV1 and RV/TLC levels were altered counterintuitively*. * As thresholds for FEV1 and RV/TLC levels different cut-off levels were used and kappa statistics calculated for all alternatives. Each change in COPD diagnosis only materializes if both parameters deviate by ≤ 5/7.5/10/12.5/15/20% from 100% of the predicted value. Example: If deviations of 10% (from 100%) are chosen as thresholds for both FEV1 and RV/TLC (as % of predicted) in order to change the GOLD-COPD diagnosis from 1) 'yes' into 'no' (i.e. FEV1 ≥ 90% and RV/TLC ≤ 110%; [2) or vice versa, from 'no' into 'yes', FEV1 < 90% and RV/TLC > 110%]), then the number of misclassified patients (false positives + false negatives) is reduced from 69 to 33, and κ- statistics improve from 0.64 to 0.83. Abbreviations: as in table 1.

---

### A unified framework for diagnostic test development and evaluation during outbreaks of emerging infections [^116QvYn5]. Communications Medicine (2024). Medium credibility.

Diagnostic test evaluation

IVDs must be evaluated in phase III diagnostic accuracy studies that ideally start by including all individuals who will be tested in clinical practice to avoid selection bias (all-comer studies). Individuals fulfilling the inclusion criteria should be enrolled consecutively, without judging how likely this person is to test positive or negative. In such prospective diagnostic studies, to minimise variability and thus increase statistical power, all study participants ideally undergo all tests under investigation (index tests) as well as the reference standard to assign their final diagnosis.

The reference standard must be sufficiently reliable to differentiate between people with and without the target condition, but it is usually not perfect. This imperfectness has to be taken into account when interpreting the results. Suppose a POC antigen test for SARS-CoV-2 is evaluated with a PCR test as reference standard resulting in a sensitivity of 90%. This does not mean that 90% of people with SARS-CoV-2 will be detected but that the POC test will be positive in 90% of cases with a positive PCR test. Solutions to this may include follow-up data or composite reference standards, which use all tests or clinical criteria available for a diagnosis. However, if the test under evaluation is part of this composite reference standard, this may lead to incorporation bias.

Depending on the phase of the epidemic or pandemic, recruitment speed can vary considerably due to changes in incidence. The guideline on clinical evaluation of diagnostic agents of the European Medicine Agencydemands sample size specification in a confirmatory diagnostic accuracy study in the study protocol. The required sample size is highly dependent on the prevalence of the target condition, which may change during the recruitment phase, making a priori sample size calculations inappropriate at the time of recruitment.

---

### The Infectious Diseases Society of America guidelines on the diagnosis of COVID-19: antigen testing (January 2023) [^117Dfxh9]. Clinical Infectious Diseases (2024). High credibility.

COVID-19 antigen testing — patients having symptoms for more than 5 days with nucleic acid amplification testing as the reference standard — is presented in a GRADE evidence profile that assesses test accuracy across pretest probabilities of 5%, 20%, and 50%, includes 15 studies using cohort and cross-sectional study designs, and rates risk of bias and indirectness as Not serious; inconsistency/imprecision as Serious; and publication bias as None.

---

### Simple statistical measures for diagnostic accuracy assessment [^116HFDhT]. Journal of Thoracic Oncology (2010). Low credibility.

The aim of diagnostic medicine research is to estimate and compare the accuracy of diagnostic tests to provide reliable information about a patient's disease status and thereby influencing patient care. When developing screening tools, researchers evaluate the discriminating power of the screening test by using simple measures such as the sensitivity and specificity of the test, as well as the positive and negative predictive values. In this brief report, we discuss these simple statistical measures that are used to quantify the diagnostic ability of a test.

---

### Research methods: diagnostic test characteristics [^112XCXRU]. Hospital Pediatrics (2023). Medium credibility.

The goal of a diagnostic test is to provide information on the probability of disease. In this article, we review the principles of diagnostic test characteristics, including sensitivity, specificity, positive and negative predictive value, receiver operating characteristics curves, likelihood ratios, and interval likelihood ratios. We illustrate how interval likelihood ratios optimize the information that can be obtained from test results that can take on > 2 values, how they are reflected in the slope of the receiver operating characteristics curve, and how they can be easily calculated from published data.