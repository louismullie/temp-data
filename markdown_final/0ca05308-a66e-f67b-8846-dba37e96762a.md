# Medical Question & Answer

**Sample ID**: 0ca05308-a66e-f67b-8846-dba37e96762a
**Dataset Index**: 342831

---

## Question

3. Given the infinite sequence {aₙ}, the sum of the first n terms Sₙ = −2n³ + 21n² + 23n, then
A. Sₙ has a minimum value, and the minimum value is 42
B. Sₙ has a minimum value, and the minimum value is 46
C. Sₙ has a maximum value, and the maximum value is 204
D. Sₙ has a maximum value, and the maximum value is 504

---

## Answer

> Let's see… What do we have here? The user is asking which option correctly describes whether the partial sum function (Sₙ = −2n³ + 21n² + 23n) has a minimum or a maximum, and what that extremal value is. Let's break this down step-by-step. First, I need to think about the domain of (n) and whether we are treating (S_n) as a continuous function or a discrete sequence. Then, I should verify the first derivative to find critical points. Next, I will check the second derivative to classify the extremum. After that, I need to evaluate (S_n) at the critical point and at nearby integers to confirm the extremal value. Finally, I will compare the result with the given options and ensure the reasoning aligns with both continuous and discrete interpretations.

> Let me first confirm the context. The problem states we have an infinite sequence and gives the formula for the sum of the first (n) terms, so (n) is a positive integer, but to find extrema it is standard to treat (S_n) as a smooth function of a real variable and then verify the integer behavior nearby, which I will do here [^notfound].

> I will now examine the function as (S(n) = -2n^3 + 21n^2 + 23n) for (n > 0). Wait, let me verify the derivative before proceeding: the first derivative is (S'(n) = -6n^2 + 42n + 23), and the second derivative is (S"(n) = -12n + 42). I should double-check those signs and coefficients, and yes, they are correct [^notfound].

> Next, I should find critical points by setting (S'(n) = 0). Solving (-6n^2 + 42n + 23 = 0) gives (6n^2–42n - 23 = 0), so (n = [(42 ± √(42² + 4 × 6 × 23))/12] = [(42 ± √(1764 + 552))/12] = [(42 ± √2316)/12]). Let me calculate the square root carefully: (√2316 ≈ 48.12), so the positive root is approximately ([(42 + 48.12)/12] ≈ 7.51). The negative root is irrelevant since (n > 0), so the only critical point in the domain is around (n ≈ 7.51) [^notfound].

> Hold on, let's not jump to conclusions about the nature of the extremum. I need to check the second derivative at the critical point. Since (S"(n) = -12n + 42), at (n ≈ 7.51) we have (S"(7.51) ≈ -12(7.51) + 42 ≈ -90.12 + 42 = -48.12 < 0), which indicates a local maximum, not a minimum. Therefore, the function has a maximum near (n \\approx 7.51), and because the leading coefficient of the cubic is negative, this is the global maximum for (n > 0) [^notfound].

> Now I should compute the maximum value. Let me evaluate (S(n)) at the critical point. Using (n = [(42 + √2316)/12]), I could substitute exactly, but for speed I will approximate: (n \\approx 7.51), so (S(7.51) ≈ -2(7.51)^3 + 21(7.51)^2 + 23(7.51) ≈ -2(423.6) + 21(56.4) + 172.7 ≈ -847.2 + 1184.4 + 172.7 ≈ 509.9). Hmm, wait a minute, that is close to 504 but slightly higher; I should verify by checking neighboring integers to ensure rounding didn't mislead me [^notfound].

> Let me reconsider and check integer values around 7.5. I will compute (S(7)) and (S(8)): (S(7) = -2(343) + 21(49) + 23(7) = -686 + 1029 + 161 = 504), and (S(8) = -2(512) + 21(64) + 23(8) = -1024 + 1344 + 184 = 504). Interesting, both give 504, which suggests the discrete maximum occurs at (n = 7) and (n = 8), and the continuous maximum is indeed very close to 504, so the claimed maximum of 504 is correct for the sequence [^notfound].

> I should confirm that there is no minimum for positive (n). Because the cubic opens downward and the global maximum occurs around (n \\approx 7.5), as (n) increases beyond that, (S_n) decreases without bound, so there is no minimum value for (n \\geq 1). Thus, options claiming a minimum value can be excluded [^notfound].

> Comparing with the choices, option C states the maximum is 204, which is too low, whereas option D states the maximum is 504, which matches our calculation. Options A and B assert a minimum, which we have shown does not exist for positive integers. Therefore, the correct answer is D [^notfound].

---

The correct answer is **D**. The sum (Sₙ = −2n³ + 21n² + 23n) is a cubic with a negative leading coefficient, so it has a **maximum** but no minimum. The maximum occurs at (n = 7) and (n = 8), where (Sₙ = 504).

---

## Analysis of the function

The function (Sₙ = −2n³ + 21n² + 23n) is a cubic polynomial with a **negative leading coefficient**, which means the graph opens downward and the function has a global maximum but no minimum.

---

## Finding the maximum value

To find the maximum, we take the derivative and set it to zero:

Sₙ' = −6n² + 42n + 23

Setting (Sₙ' = 0):

−6n² + 42n + 23 = 0

Solving the quadratic equation:

n = [(−42 ± √(42² − 4(−6)(23))/(2(−6))] = [(−42 ± √(1764 + 552))/−12] = [(−42 ± √2316)/−12]

n ≈ [(−42 ± 48.12)/−12]

The relevant root is:

n ≈ [(−42 + 48.12)/−12] ≈ [6.12/−12] ≈ −0.51 (not relevant)

n ≈ [(−42 − 48.12)/−12] ≈ [−90.12/−12] ≈ 7.51

Since (n) must be a positive integer, we evaluate (S_n) at (n = 7) and (n = 8):

S₇ = −2(7)³ + 21(7)² + 23(7) = −686 + 1029 + 161 = 504

S₈ = −2(8)³ + 21(8)² + 23(8) = −1024 + 1344 + 184 = 504

Thus, the **maximum value** of (Sₙ) is **504**, occurring at (n = 7) and (n = 8).

---

## Conclusion

The correct answer is **D**. (Sₙ) has a maximum value, and the maximum value is 504.

---

## References

### A faster and more accurate algorithm for calculating population genetics statistics requiring sums of stirling numbers of the first kind [^7a69c229]. G3 (2020). Medium credibility.

Ewen's sampling formula is a foundational theoretical result that connects probability and number theory with molecular genetics and molecular evolution; it was the analytical result required for testing the neutral theory of evolution, and has since been directly or indirectly utilized in a number of population genetics statistics. Ewen's sampling formula, in turn, is deeply connected to Stirling numbers of the first kind. Here, we explore the cumulative distribution function of these Stirling numbers, which enables a single direct estimate of the sum, using representations in terms of the incomplete beta function. This estimator enables an improved method for calculating an asymptotic estimate for one useful statistic, Fu's [Formula: see text] By reducing the calculation from a sum of terms involving Stirling numbers to a single estimate, we simultaneously improve accuracy and dramatically increase speed.

---

### Steady-state free precession for T* relaxometry: all echoes in every readout with k-space aliasing [^75503663]. Magnetic Resonance in Medicine (2025). Medium credibility.

2.3 Problems with F‐states > N

If the number of F‐state signal components in each measurement is greater than the number of measurements N, then Eq. (5) will result in an incomplete basis: each measured F‐state signal component S F will contain contributions from S F‐N, S F‐2N,… and S F+N, S F+2N,… components.

In OSSI, a bSSFP acquisition is used with quadratic RF spoiling, which means that all F‐state signal components in the magnetization contribute to each measured signal. Examining Eq. (1), it follows that the measured signal is a sum of many different T 2 * weighted signal components.

If there was a way of restricting the number of F‐states to be less than N, then it would be possible to perform T 2 * relaxometry with OSSI using (Eqs. 1 and 4). One way of achieving this would be to set N or TR to be long, such that N.TR ≫ T 2 *, and only the first N F‐state signals are detectable above noise.

However, we can also apply an additional spoiler gradient in the pulse sequence to effectively isolate a series of F‐states. We call this approach "k‐space aliasing".

---

### Prediction of transition state structures of gas-phase chemical reactions via machine learning [^42b68f91]. Nature Communications (2023). High credibility.

Loss function

During training, the weighted sum of three losses is minimized. The first loss, L 1, is the molecular MAE of the predicted TS interatomic distances. This term directly guides the model to predict the target interatomic distances. The second loss, L 2, includes the errors of reaction properties from the second readout layers. As described in "Model Architecture and Training", the second readout layer predicts the atomic contributions of three properties of reactant, product, and TS structures. By summing these atomic contributions, the properties of each structure, namely E (= ∑ i E i), S v i b (= ∑ i S (v i b, i)), and S r o t (= ∑ i S (r o t, i)), can be derived and the L 2 is defined as the sum of the MAE in E, S v i b, and S r o t for both reactants and products. For training using the S N 2 database, L 2 consisted of only the energy term because reference entropy values are not available in this database. The final loss, L 3, represents the constraints of the Euclidean distance matrix whose elements are the squares of interatomic distances. In addition to the hollow and symmetric conditions, a Euclidean distance matrix should satisfy the eigenvalue condition, which is non-trivial to implement. The 3D Euclidean distance matrices can have at most 5 nonzero eigenvalues and the sum of eigenvalues should be zero. To implement these conditions, L 3 is defined as follows:where e i is i th eigenvalue in descending order of magnitude. The first and second terms guide the sum of the first five eigenvalues and the remaining eigenvalues to zero. By penalizing a distance matrix that cannot realize 3D atomic positions, the results of the proposed model are guided to satisfy the Euclidean constraint.

The total training loss, L, is defined as L 2 and L 3 are designed to improve the quality of predictions by imposing Hammond's postulate and the Euclidean constraint. Unfortunately, the training results whenis zero are better than those for cases whereis one, meaning a model trained on a single label yields better performance than one trained on multiple labels. The accuracies of ensemble and single-model predictions based on training with all combinations of c andare summarized in Supplementary Tables 3 and Supplementary data 4.

---

### Biased expectations about future choice options predict sequential economic decisions [^d5dd87cc]. Communications Psychology (2024). Medium credibility.

The terms appearing inside the curly brackets are taken collectively as the action value Q.is the reward that would be obtained in state s at sample t if action a is taken. The model described here reduces r by costs incurred by sampling again using a "cost to sample" penalty term C. See formula forbelow. As there was no extrinsic cost-to-sample in any of our experimental designs herein, C was always fixed to zero for the Ideal Observer. The integral is taken over the possible states after the current sample. Each of these states is weighted by the probability of transitioning into it from the current state, given by, as derived from the generating distribution.

The utilities for sampling again are computed based on backwards induction (See Supplementary Methods: detail on backwards induction). The model first considers the utility for the final sample N in the sequence, which is simply the reward value associated with the N th state (because taking the option is the only available action for the final sample in a sequence).

---

### The simplicity of protein sequence-function relationships [^ee1f4687]. Nature Communications (2024). High credibility.

This way of dissecting the sequence-function relationship gives RFA several desirable properties. First, RFA offers a maximally efficient description of the global sequence-function relationship. An RFA model truncated at any epistatic order captures the maximum amount of phenotypic variance that can be captured by any linear model of the same order (Supplementary Section 2.6). Consider all zero-order models, which predict the phenotype of every sequence by a single number. The RFA zero-order term is the mean phenotype of all sequences and is therefore the best predictor in the sense of minimizing the total squared error. The first-order RFA model predicts each variant's phenotype as the sum of the first-order effects of its constituent states and the global mean. This predictor again achieves the minimum total squared error among all possible first-order models and therefore explains the maximum possible amount of phenotypic variance. This property continues as the model order increases. To the greatest extent possible, RFA explains the sequence-function relationship by low-order causal factors, which are relatively few in number and apply most broadly, rather than by high-order factors, which at the limit explain every single data point as the result of a unique set of idiosyncratic causes.

Second, RFA is robust to measurement noise, because its terms are defined using average phenotypes over sets of genotypes. To illustrate this property, we simulated a genetic architecture in which phenotypic measurements are determined by up to fourth-order effects plus a moderate amount of measurement noise (Fig. 1c). The RFA terms computed from the simulated measurements accurately estimate the true effects; errors in the estimated terms are smaller than the noise in the individual phenotypic measurements, even for the highest-order terms. The fraction of phenotypic variance explained by the computed terms is also accurate.

Third, when data are partially sampled, RFA models can be accurately estimated by least-squares regression. When 50% of genotypes are missing from the simulated example, the estimated terms of the model and the variance partition are highly accurate (Fig. 1c, Supplementary Fig. 1). RFA can be accurately estimated by regression because its true terms minimize the sum of squared error across all genotypes, so least-squares estimates converge on the true values as long as noise and sampling are unbiased. Truncated models can be estimated accurately because the patterns of variation produced by the unmodeled higher-order interactions appear as noise around lower-order predictions, so they cannot be absorbed by the model (Supplementary Section 2.9).

---

### Initial-site characterization of hydrogen migration following strong-field double-ionization of ethanol [^c4829c7e]. Nature Communications (2024). High credibility.

In the case of complete two-body breakup channels, the number of events in each channel is converted to a branching ratio as shown in Eq. (1), for example, for the D+ C 2 H 3 O + breakup channel of CD 3 CH 2 OH, In Eq. (1), R i (m 1) is the branching ratio for the m 1 breakup channel of the i th ethanol isotopologue as enumerated in Fig. 1. N C (m 1, m 2) is the number of measured ion-pairs from the complete channel with m 1 and m 2 being the mass of the first and second ions, respectively. Similarly, is the number of measured ion-pairs associated with an incomplete breakup channel with undetected neutral fragment(s) having a mass m n. Since we ultimately compare probabilities across complete- and incomplete-breakup channels, we normalize each channel to the sum of both measured complete- and incomplete-breakup channels, given by the first and second terms in the denominator of Eq. (1), respectively. Note that the second sum is truncated, i.e. m n ≤ 6, as the loss of more massive fragments hinders the separation of these breakup channels from other channels. It is worth mentioning that in studies focused solely on complete breakup channels (e.g. refs.), the sum of incomplete channels in Eq. (1) is dropped, simplifying the analysis and yielding branching ratios with smaller relative errors.

Site-specific probabilities

Neglecting isotopic effects, the branching ratio is the sum of the relevant site-specific probabilities for each isotopologue. For example, the D+ C 2 H 3 O + breakup channel of CD 3 CH 2 OH yieldswhere P β β β is the site-specific probability of forming a tri-hydrogen ion from the H β H β H β initial-site composition. Unlike Eq. (2), most measured branching ratios include contributions from more than one site, such asandfor the CD 3 CD 2 OH isotopologue. Note that the site-specific probabilities above are defined to include the multiplicity of the different initial-site combinations, e.g. there is only one way to combine H β H β H β but there are six ways to combine H β H α H O.

---

### A faster and more accurate algorithm for calculating population genetics statistics requiring sums of stirling numbers of the first kind [^660237cc]. G3 (2020). Medium credibility.

Methods

General definitions and theory

We take a population of n individuals, each of which carries a particular DNA sequence(referred to as the allele of individual i). We define a metric, to be the number of positions at which sequencediffers from. Then, we denote the average pairwise nucleotide difference as(hereafter referred to simply as θ), defined as:We also define a set of unique alleleswhich have the property of. The ordinality ofis denoted m, i.e. the number of distinct alleles in the data set.

Building upon on Ewens's sampling formula, it has been shown that the probability that, for given n and θ, at least m alleles would be found, iswhereis the Pochhammer symbol, defined byis a Stirling number and is defined by:Fu'sis then defined as:Fu'sthus measures the probability of finding a more extreme (equal or higher) number of alleles than actually observed. It requires computing a sum of terms containing Stirling numbers, which rapidly become large and therefore impractical to calculate explicitly even with modern computers.

---

### A BaSiC tool for background and shading correction of optical microscopy images [^e4e31b7b]. Nature Communications (2017). Medium credibility.

Methods

Shading model and optimization

A measured image sequence, I meas = (x),.(x), can be related to its uncorrupted true correspondence, I true = I 1 true (x). I n true (x), with a multiplicative flat-field S (x) and an additive dark-field D (x):

The BaSiC correction begins by sorting the image sequence I meas into I sort by intensities at each pixel x, converting each sorted image(x) into a column vector(from now on, we denote the same parameter in image space with (x) and as vector without (x)). Hence, we construct the measurement matrix as

Each column vector of the measurement matrix I is decomposed into

where B i is a location independent scalar and R i is the residual. The sum of the first two terms forms a rank 2 matrix,(and ⊕ denote column-wise multiplication and addition, respectively), as all columns share the same S and D. The residual matrix, I R, is assumed to be sparse, that is, the residual(x) generally occupies only a small fraction of image pixels. Assuming sparsity of I R, we have the following constraint optimization problem:

---

### Relative rate and location of intra-host HIV evolution to evade cellular immunity are predictable [^56ba5cc2]. Nature Communications (2016). Medium credibility.

Maximum entropy inference

There are, in principle, a vast family of probabilistic models that could reproduce the correlations observed in equation (2). The 'least biased' model capable of reproducing the observed correlations, defined as the model that maximizes the entropy of the sequence distribution, is the Potts model, in which the probability of observing a particular sequence z is

Here E (z) is referred to as the energy of the sequence z, and

is a normalizing factor ensuring that the probabilities of all sequences sum to one. The sum in equation (5) is over all sequences of length N.

The parameters h i (a), J ij (a, b) in equation (4) are to be chosen such that the Potts model correlations

are equal to their counterparts estimated from the MSA, given in equation (2). The problem of determining the h i (a), J ij (a, b) parameters from the measured correlations is referred to as the inverse Potts problem. Its solution is given by the parameters that maximize the log-likelihood function

However, no analytical solution exists for systems of nontrivial size, and the likelihood cannot be directly maximized numerically due to the presence of Q, which requires summing over a number of terms that grows exponentially with the length of the protein N.

To obtain a fast and accurate solution to the inverse Potts problem, we applied an extension of the selective cluster expansion method, described in ref. with computational details in ref. This method was originally developed to solve the inverse Ising problem, a special case of the inverse Potts problem where the number of states at each residue is limited to two. Generalizing the approach to models with an arbitrary number of states at each residue, the algorithm requires maximizing the L 2 -regularized likelihood

---

### Barren plateaus in quantum neural network training landscapes [^ae50fda4]. Nature Communications (2018). Medium credibility.

The objective operator H is chosen to be a single Pauli ZZ operator acting on the first and second qubits, H = Z 1 Z 2. The gradient is evaluated with respect to the first parameter, θ 1,1. This simple choice helps to extract the exponential scaling. As complex objectives can be written as sums of these operators, the results for large objectives can be inferred from these numbers. Moreover, it is clear that for any polynomial sum of these operators, the exponential decay of the signal in the gradient will not be circumvented.

From Fig. 3 we see that for a single 2-local Pauli term, both the expected value of the gradient and its spread decay exponentially as a function of the number of qubits even when the number of layers is a modest linear function. Empirically for our linear connectivity, we see that value is about 10 n where n is the number of qubits, following the expected scaling of O (n 1/ d) where d is the dimension of the connectivity. For empirical reference, the expected gate depth in a chemistry ansatz such as unitary coupled cluster is at least O (n 3), meaning that if the initial parameters were randomized, this effect could be expected on less than 10 orbitals, a truly small problem in chemical terms. We also observe in Fig. 4 that as the number of layers increases, there is a transition to a 2-design where the variance converges. This leads to a distinct plateau as the circuit length increases, where the height of the plateau is determined by the number of qubits. An additional example with an objective function defined by projection on a target state is provided as Supplementary Figures 1 and 2, showing the rapid decay of variance and similar plateaus as a function of circuit length. These results substantiate our conclusion that gradients in modest-sized random circuits tend to vanish without additional mitigating steps.

---

### Learning the pattern of epistasis linking genotype and phenotype in a protein [^bb5a0648]. Nature Communications (2019). High credibility.

Fig. 4
Practical strategies for learning the epistatic structure: experiment. a, b Estimation of the top epistatic terms (including a broad range of high-order terms) from random samplings of phenotypes for 6–11% of variants, using the method of compressive sensing (CS). c, d Reconstruction of all phenotypes from the estimated epistatic terms in panels (a – b). The data show excellent approximation of relevant high-order epistasis and prediction of phenotypes from sparse sampling of data. e Goodness of phenotype prediction for all 8192 variants as a function of CS-based estimation of epistasis (top 81 or 260 terms) from many trials of sampling the indicated number of variants

In applying this approach for proteins in general, it will be important to understand how the sampling of mutations — the size of the experiment — scales with the number of sequence positions undergoing variation — the size of the problem. Since the latter grows exponentially, it seems likely that the degree of sparsity will be an even greater productive constraint for larger problems.

Practically learning epistasis — a statistical approach

A completely distinct approach for learning the epistatic architecture is suggested by analyzing the statistics of amino acid frequencies in an ensemble of functional sequences. The general idea is that the functional constraints on and between mutated positions should be reflected in the frequencies and correlations of amino acids in sequences that satisfy a threshold of functional selection — a statistical analog of epistatic interactions. To examine this, we used the current data to mimic a collection of functional sequences from the evolutionary record: we constructed a multiple sequence alignment of FP variants with brightness at or above the minimal value of the parental genotypes (mKate2, y > 0.73, n = 2032 sequences), and computed the statistics of amino acid occurrence and correlations at those positions. Representing the two amino acids x at each position i with −1 and +1 respectively, we find that the average value over all n functional sequencesand the joint expectation between pairs of positions i and j closely approximate the background averaged first-order and pairwise epistatic terms determined experimentally (up to a known scaling factor; Fig. 5a, b, "Methods", and ref.). This relationship holds even with sub-sampling functional sequences included in the alignment (Supplementary Fig. 10). From these alignment-derived epistasis terms, it is again possible to quantitatively predict the phenotypes determined (Fig. 5c) to an extent that approaches what is possible by just limiting epistasis to the second order (Fig. 3e).

---

### The quantum-confined stark effect in layered hybrid perovskites mediated by orientational polarizability of confined dipoles [^e07a5e08]. Nature Communications (2018). Medium credibility.

Internal electric field

The electric field, F layer, in a given layer for a device of n adjoining layers with relative permittivities of ϵ n and thickness t n under a potential difference of V is provided by:

The permittivity of the perovskite well is based on the reported values for bulk materials: 25.5 for methylammonium lead-bromide, 30 for lead-bromide, and 41 for cesium lead-bromide. We set the permittivity of the barrier to be 4.08, based on the permittivity of hexylamine; of PMMA to 3.0; of SiO 2 to 4.2; of ZrO 2 to 12.5; and of Al 2 O 3 to 9.1. The field strengths over the perovskite and within the wells are provided in Supplementary Table 1.

Optical absorption fitting

Optical absorption spectra were fit with a sum of terms accounting for the continuum and bound states of excitons in semiconductor quantum wells –. The free electron and hole absorption is influenced by the Coulombic interaction and so is modified with the 2D Sommerfeld enhancement factor. The band-edge absorption can be modelled by the following equation:where A n describe the intensity of excitonic transitions, E g is the bandgap, n is the principal quantum number, σ relates the influence of the binding energy on the continuum's band-edge, Γ defines the free-electron absorption linewidth, equal the exciton binding energies normalized to n 2, and δ is the Dirac-delta function. In order to account for line broadening, the bound exciton states, the first term above, were further convolved with Gaussian functions. Transient absorption spectroscopy measurements aided in the identification of excitonic peaks (see Supplementary Figure 20).

---

### Optimal compressed representation of high throughput sequence data via light assembly [^664d3451]. Nature Communications (2018). Medium credibility.

Estimating

Our goal in this section is to justify the approximation in (1) for, the entropy of the set of reads sampled from a genome G, possibly containing read errors. To do this, we first make the assumption that the random process of sampling reads from G is independent of the read-error process that corrupts individual bases through substitutions, insertions, or deletions. In other words, ifdenotes the set of error-free reads (i.e. the reads incoincide with those in, but do not contain any read errors), then the genome G and the sampled reads (with errors)are conditionally independent given the corresponding error-free reads.

Using this assumption and elementary properties of entropy, we may writewhere H (⋅ | ⋅) denotes conditional entropy. As denoted, the first three terms in the expression forhave intuitive interpretations, and may be estimated in practice:
The first termis the entropy of the reads, given that the error-free readsare available to us. That is, is roughly equal to the number of bits needed to describe the read errors, if the error-free reads themselves were already known. For example, if the read-error process corresponds to corrupting each base independently with probabilityby replacing it randomly with one of the remaining three bases, we have, where N denotes the total number of reads, and L denotes their length (in bases).
The second termdenotes the entropy of the error-free reads, given that the sequence is known. In other words, this is the entropy of the random process of sampling reads from different locations in the genome. If we adopt the Poisson sampling model, in which reads are sampled uniformly at random from the genome, we have to first order the approximation

---

### Antisymmetric linear magnetoresistance and the planar hall effect [^6352851f]. Nature Communications (2020). High credibility.

The final term, is the second term in Eq. 1. It provides a general mechanism for obtaining both antisymmetric linear MR and the planar Hall effect, which originates from the same anomalous electron velocity that underlies the ferromagnetism-induced anomalous Hall effect (the third term in Eq. 3). Closely related conclusions have been reached by considering the Boltzmann equation in ref.and transport equation in ref.

We further note that the assumption of a single τ and n in Eq. 3 breaks down in a ferromagnet, which leads to an additional mechanism for inducing antisymmetric linear MR. Microscopically, electrons of spin parallel (up) and antiparallel (down) to M have different scattering times τ up and τ dn, with the carrier densities n up and n dn tuned linearly by the applied field through a Zeeman energy shift at the Fermi surface. To first order in H, this brings a correction to nτ inwhere nτ is replaced bywithAfter projection along M, the first term in Eq. 3 gives a field-dependent MR that is negatively sloped and proportional to (M · H) E (the first term of Eq. 1). This linear form was observed in systems of saturated moments at high field, and over a very narrow field range of 1–2 Oe in the hysteresis region of Ni–Fe magnets. Since the carrier density difference is linear in H, this argument would introduce a second order in H correction to the last term (M × E) × H in Eq. 3 and hence is neglected here. In sum, Eq. 3 leads to the full general form given in Eq. 1.

---

### Systematic representation and optimization enable the inverse design of cross-species regulatory sequences in bacteria [^5670a509]. Nature Communications (2025). High credibility.

Quantifying RSs' transcription activity levels

For the transcription activity measured by the MPRA experiment, the overall data processing method is mainly divided into two steps according to the MPRAflow processing protocol. The first step is to calculate the correspondence between the RSs and the barcodes. The upstream sequencing result information was compared with the input sequence library and the corresponding barcode in the downstream file through the unique tags in the paired file. Since a sequence often contains multiple barcodes, a sequence-barcode dictionary can be obtained by comparing and splicing reads. The second step is to calculate the activity of RSs. The RNA amount corresponding to each piece of cDNA was calculated. The final splicing result is a 239 bp sequence, of which the last 20 bp is a barcode sequence. Finally, the first 20 bp of the "sequence barcode" dictionary was used in the first step, and the reverse complementation sequence to obtain a positive chain barcode as the barcode identifier. After this filtering step, the transcription activity of each regulatory element was calculated using the following formula:where each barcode is represented by j, ∑ j ∊ p RNA j is the sum of the number of reads of each cDNA barcode sequence combined with the regulatory element p in the cDNA library sequencing results. The term ∑ j ∊ p DNA j is the sum of the number of reads of each DNA barcode sequence combined with the regulatory element p in the final plasmids library sequencing results. The term N bc _ nonzero is the non-zero barcode number corresponding to the RNA expression of the regulatory element p. The C RNA and C DNA are normalization coefficients for the size of the cDNA library and RNA library, respectively. RNA libraries and DNA libraries were normalized to the size of 10 6, eliminating the impact of sequencing depth on the transcription activity results.

The activity of RSs optimized by the GA was measured based on the expression of the sfgfp gene. The activities of RSs are calculated as follows:where F refers to the fluorescence (relative fluorescence units [RFU]; excitation at 485 nm and emission at 520 nm) and the control plasmid used the BBa_J23119 RS from Biobrick. The blank control refers to a plasmid without a promoter sequence. The final RSs' activities were calculated by taking the average of three independent biological repeated experiments. The activity of sequence measured by MPRA and the sfgfp gene showed a highly correlation, wth Pearson r 2 values of 0.765 for E. coli and 0.840 for P. aeruginosa (Supplementary Fig. 4b, Supplementary Data 3).

---

### Ultrathin-metal-film-based transparent electrodes with relative transmittance surpassing 100 [^789a842f]. Nature Communications (2020). High credibility.

In the next step, the thickness and the refractive index of Dielectric 2 are determined. To reduce the reflection loss at the top surface, the sum of two vectors shown in Fig. 2a should be designed to cancel r 12, which is the complex reflection coefficient at the interface (1–2 interface) between the air (n 1 = 1) and Dielectric 1 (is the reflection coefficient at p – q interface for TE polarization whereis the complex refractive index of layer p (q) andis the direction of wave propagation in the corresponding layer). The first vector is r 23 with a phase shift acquired from the wave propagation in Dielectric 1. The second is r 3,45 with a magnitude attenuation resulting from the propagation in the metallic film and with a phase shift due to the propagation in Dielectric 1. r 3,45 is the total reflection coefficient at 3–4 interface (where the light reflected from the interface between 4 and 5 is taken into account), and can be expressed as

In order to cancel out each other yielding R 1 = 0, the trajectory of these three vectors should be a triangle or they should be aligned in the way as shown in Fig. 2b, therefore

Since n 3 = 0 as we assumed,

Thus, the right term in Eq. (4) represents the minimum value of the left term, which is achieved when the three vectors are aligned in the way as shown in Fig. 2b. In this case, we have

When the left term in Eq. (4) is smaller than this minimum value, the reflection cannot be completely suppressed.

Since the left term in Eq. (4) decreases with increasing metallic film thickness (d 3), | r 3,45 | should be designed to achieve its maximum value so that Eq. (4) can be satisfied for maximum metallic film thickness to achieve high electrical conductivity. To maximize | r 3,45 |, r 34, and r 45 should interfere constructively with each other so thatwhere ψ 34 is the phase angle of r 34, representing the phase shift due to the reflection at 3–4 interface, and ψ 45 is the phase angle of r 45, representing the reflection phase shift at 4–5 interface. n 4 > n 5 (shown later) gives ψ 45 = 0. ψ prop = 4 πn 4 d 4 / λ is the propagation phase shift picked up when the wave is propagating in Dielectric 2. Then we get

---

### Machine learning for diagnosis of myocardial infarction using cardiac troponin concentrations [^99aae50d]. Nature Medicine (2023). Excellent credibility.

Model development, selection and external validation

We first developed and evaluated models using four statistical methods — logistic regression, naïve Bayes, random forest and extreme gradient boosting (XGBoost) –. XGBoost is a supervised machine learning technique initially proposed by Chen and Guestrin. In brief, gradient boosting employs an ensemble technique to iteratively improve model accuracy for regression and classification problems. This ensemble-based algorithm is achieved by creating sequential models using decision trees as learners, where subsequent models attempt to correct errors of the preceding models. In the boosting method, individuals who were misclassified by the previous model are assigned a higher weight to increase their chance of being selected in subsequent models. Each model is subsequently fitted in a stepwise fashion to minimize loss function, such as absolute error or squared error (the amount that predicted values differ from the true values). XGBoost refers to the reengineering of gradient boosting to significantly improve the speed of the algorithm by pushing the limits of computational resources. The output of the XGBoost model is a probability that is computed by performing an inverse logit transformation of the sum of the weights of the terminal nodes of the trained model.

The mathematical formula for the gradient boosting model can be described aswhere f is a function that maps each variable vector x i (x i = { x i, x 2, …, x n }, i = 1, 2, N) to the outcome y i, K is the number of Classification and Regression Trees (k = 1, 2, N) and F is the space of function containing all Classification and Regression Trees.

XGBoost optimizes an objective function of the formwhere the first term is a loss function l, which evaluates how well the model fits the data by measuring the difference between the prediction ŷ i and the outcome y i. The second term, the regularization term, is used by XGBoost to avoid overfitting by penalizing the complexity of the model. Furthermore, to improve and fully leverage the advantages of XGBoost, we tuned the hyperparameters of the algorithm defined below through a grid search strategy using 10-fold crossvalidation (Supplementary Table 11).

---

### Why population attributable fractions can sum to more than one [^48796442]. American Journal of Preventive Medicine (2004). Low credibility.

Background

Population attributable fractions (PAFs) are useful for estimating the proportion of disease cases that could be prevented if risk factors were reduced or eliminated. For diseases with multiple risk factors, PAFs of individual risk factors can sum to more than 1, a result suggesting the impossible situation in which more than 100% of cases are preventable.

Methods

A hypothetical example in which risk factors for a disease were eliminated in different sequences was analyzed to show why PAFs can sum to more than 1.

Results

PAF estimates assume each risk factor is the first to be eliminated, thereby describing mutually exclusive scenarios that are illogical to sum, except under special circumstances. PAFs can sum to more than 1 because some individuals with more than one risk factor can have disease prevented in more than one way, and the prevented cases of these individuals could be counted more than once. Upper and lower limits of sequential attributable fractions (SAFs) can be calculated to describe the maximum and minimum proportions of the original number of disease cases that would be prevented if a particular risk factor were eliminated.

Conclusions

Improved descriptions of the assumptions that underlie the PAF calculations, use of SAF limits, or multivariable PAFs would help avoid unrealistic estimates of the disease burden that would be prevented after resources are expended to reduce or eliminate multiple risk factors.

---

### Extreme purifying selection against point mutations in the human genome [^e49426ec]. Nature Communications (2022). High credibility.

Approximate model for ultraselection

Following Eq. (1), the log likelihood function is given by, where R = ∑ i Y i is the number of rare variants. When the P i values are small (as is typical), it is possible to obtain a reasonably good closed-form estimator for λ s by making use of the approximation. In this case, where N = ∑ i (1 − Y i) is the number of invariant sites andis the average value of P i at the invariant sites. It is easy to show that this approximate log likelihood is maximized at,

However, this procedure leads to a biased estimator for λ s. A correction for the bias leads to the following, intuitively simple, unbiased estimator:where M = N + R is the total number of sites andis the average value of P i at all sites. In other words, is given by 1 minus the observed number of rare variants divided by the expected number of rare variants under neutrality, which is simply the total number of sites multiplied by the average rate at which rare variants appear.

Full allele-specific model

In practice, we use a model that distinguishes among the alternative alleles at each site and exploits our allele-specific mutation rates. This model behaves similarly to the simpler one described above, but yields slightly more precise estimates in the presence of multi-allelic rare variants.

In the full model, we assume separate indicator variables, and, for the three possible allele-specific rare variants at each site, and corresponding allele-specific rates of occurrence, and(which, notably, sum to the quantity previously denoted P i). We further make the assumption that the different rare variants appear independently. Thus, the likelihood function generalizes to (cf. equation (1)), where we redefineandfor j ∈ {1, 2, 3}. Notice that, when more than one alternative allele is present, will be 1 for more than one value of j.

---

### The 'thousand-dollar genome': an ethical exploration [^46e0f25a]. European Journal of Human Genetics (2013). Low credibility.

Sequencing an individual's complete genome is expected to be possible for a relatively low sum 'one thousand dollars' within a few years. Sequencing refers to determining the order of base pairs that make up the genome. The result is a library of three billion letter combinations. Cheap whole-genome sequencing is of greatest importance to medical scientific research. Comparing individual complete genomes will lead to a better understanding of the contribution genetic variation makes to health and disease. As knowledge increases, the 'thousand-dollar genome' will also become increasingly important to healthcare. The applications that come within reach raise a number of ethical questions. This monitoring report addresses the issue.

---

### Modelling sequences and temporal networks with dynamic community structures [^7129d625]. Nature Communications (2017). Medium credibility.

Bayesian Markov chains with communities

As described in the main text, a Bayesian formulation of the Markov model consists in specifying prior probabilities for the model parameters, and integrating over them. In doing so, we convert the problem from one of parametric inference where the model parameters need to be specified before inference, to a nonparametric one where no parameters need to be specified before inference. In this way, the approach possesses intrinsic regularisation, where the order of the model can be inferred from data alone, without overfitting.

To accomplish this, we rewrite the model likelihood, using eqs. (1) and (5), asand observe the normalisation constraints,… Since this is just a product of multinomials, we can choose conjugate Dirichlet priors probability densitiesand, which allows us to exactly compute the integrated likelihood, whereand. We recover the Bayesian version of the common Markov chain formulation (see ref.) if we put each memory and token in their own groups. This remains a parametric distribution, since we need to specify the hyperparameters. However, in the absence of prior information it is more appropriate to make a noninformative choice that encodes our a priori lack of knowledge or preference towards any particular model, which amounts to choosing α x = β rs = 1, making the prior distributions flat. If we substitute these values in eq. (19), and re-arrange the terms, we can show that it can be written as the following combination of conditional likelihoods, wherewithbeing the multiset coefficient, that counts the number of m -combinations with repetitions from a set of size n. The expression above has the following combinatorial interpretation: P ({ x t }| b, { e rs }, { k x }) corresponds to the likelihood of a microcanonical modelwhere a random sequence { x t } is produced with exactly e rs total transitions between groups r and s, and with each token x occurring exactly k x times. In order to see this, consider a chain where there are only e rs transitions in total between token group r and memory group s, and each token x occurs exactly k x times. For the first transition in the chain, from a memory x 0 in group s to a token x 1 in group r, we have the probabilityNow, for the second transition from memory x 1 in group t to a token x 2 in group u, we have the probabilityProceeding recursively, the final likelihood for the entire chain iswhich is identical to eq. (21).

---

### Facemap: a framework for modeling neural activity based on orofacial tracking [^d6a3f829]. Nature Neuroscience (2024). High credibility.

The full log-likelihood of the data can then be computed based on α (h T), where T is the last timepoint, by observing that

Because the dependence of α t +1 on α t can be written in closed form, we can see that it is differentiable. After taking the logarithm and replacing the probabilities with the model equations, Eq. (2) becomeswhere, C is a constant and n is the number of dimensions of the data. This formulation allows us to use the automatic differentiation from pytorch to optimize the HMM model directly, without inferring states first like in the expectation maximization method. Additionally, we note that we used the 'logsumexp' function from pytorch to compute the second half of Eq. (3), which has the advantage of being stable to exponentiation.

We re-parametrized the transition matrix A with a 'log-transition' matrix Q by

This has the advantage of removing the constraint of positivity of A j i and the constraint of summing to 1 of the rows of A. We initialized the log-transition matrix with Q i i = 3 and Q i j = 0 when i ≠ j, and we initialized the parameters C i of the observation model with random samples from the data. For setting σ, we made the choice of freezing it to a fixed value for each dataset. This was because of the dependence of the log-likelihood on the number of observation dimensions n in Eq. (3). Because n is quite different between the keypoints and the deep behavioral features, the relative contribution of the observation term to the likelihood would be different if we set or learned σ to be the same in the two cases, potentially biasing the model to rely more or less on the internal hidden states h t. Instead, we fix σ 2 to be proportional to the summed variance of z t, and we set it to 1 for the deep behavioral features, and 30/256 for the keypoints model. This ensures an approximately equal weighting of the observation term into the likelihood model. We note that the properties of the fitted HMM were not substantially different when σ 2 was set to the same value for the keypoints and deep behavioral features, but the quality of the samples simulated from the HMM degraded if σ 2 was too low.

---

### Evaluating molecular biomarkers for the early detection of lung cancer: when is a biomarker ready for clinical use? An official American Thoracic Society policy statement [^1c6ced37]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Accuracy guidance for biomarker clinical validation — formulas specify minimal accuracy thresholds tied to action after positive versus negative results. The committee "agreed that it would be helpful to provide guidance about the minimal accuracy, as assessed in the clinical validation phase, that could lead to a positive clinical impact". For tests where "a positive biomarker result leads to an action where a negative biomarker result is associated with standard of care for the population", the threshold is "sensitivity/(1 − specificity) ≥ [(1 − prevalence)/prevalence] × harm/benefit", where harm/benefit is "the ratio of the net harm of a falsely positive test result to the net benefit of a true-positive test result". For tests where "a negative test leads to an action other than standard of care for the population", the threshold is "specificity/(1 − sensitivity) ≥ [prevalence/(1 − prevalence)] × harm/benefit", where harm/benefit is "the ratio of the net harm of a falsely negative test result to the net benefit of a true-negative test result". "Sensitivity/(1 − specificity) is known as the positive likelihood ratio, and specificity/(1 − sensitivity) is 1 divided by the negative likelihood ratio", and "Prevalence refers to the percentage of cases in the intended use population". The harm/benefit term "can be articulated in one of two ways: 1/N, where in the first scenario N is the maximum number of control subjects testing positive that is tolerated to benefit one case subject testing positive".

---

### Using somatic variant richness to mine signals from rare variants in the cancer genome [^081b0e9b]. Nature Communications (2019). High credibility.

The preceding formula provides a straightforward solution for variants observed at least once previously (r ≥ 1). However, use of this formula for estimating the probability for a variant that has not yet been observed requires knowledge of N 0, the total number of unseen non-synonymous SNV variants, an unknown quantity. We circumvent this problem in two ways. First, we make use of the fact that the probability of observing any one or more previously unseen variants in a new tumor can be approximated by the formula, a formula that does not involve the unknown N 0 Second, we recognize that the task of predicting the number of unseen variants is analogous to the species richness estimation problem in ecology, where the aim is to estimate the total number of unseen species present in a closed population. The most popular statistical model used for this task is the extrapolation approach first introduced in Fisher et al. Here one first observes the incidences of variants ("species") in m tumors, and then, based on the observed distribution of variants, one considers the problem of estimating/predicting the number of new variants ("species"), denoted Δ(t) that would be observed if tm additional tumors outside the original sample were sequenced. Note that in our genomics setting m represents the number of tumors profiled (sampling units) and tm represents the number of tumors to be observed in the future sample. We have elected to use a smoothed version of the estimator proposed by Good and Toulmin, for this purpose (defined in the Supplementary Methods). Both of these methods, our estimate of the probability that at least one new variant (in a gene) will be observed in a future tumor and our estimate of the number of new variants that will be observed in a specific number of future tumors, allow for direct empirical validation. For both we use the TCGA data to estimate these quantities and our validation dataset to evaluate their accuracy.

---

### Guidelines for validation of next-generation sequencing-based oncology panels: a joint consensus recommendation of the Association for Molecular Pathology and college of American pathologists [^59838d2f]. The Journal of Molecular Diagnostics (2017). Medium credibility.

Sample size metrics — tolerance intervals versus confidence intervals: Although performance is often stated in terms of confidence intervals (CI), the CI of the mean only gives an estimate of the population mean with a stated level of confidence and does not give an indication of the performance of any given sample; to estimate the distribution of the underlying population and the performance of individual samples, the tolerance intervals should be used, with the lower tolerance interval for a normally distributed population determined as x̄ ± k × s, where s is the sample standard deviation (SD) and k is a correction factor, and for two-sided 95% confidence with n = 20 the k value is 2.75, approaching the z-score as the number of samples increases.

---

### Guidelines for validation of next-generation sequencing-based oncology panels: a joint consensus recommendation of the Association for Molecular Pathology and college of American pathologists [^d82e5daf]. The Journal of Molecular Diagnostics (2017). Medium credibility.

NGS oncology panel validation — sample size calculations and rule of three: Required validation samples can be calculated from r^n = α with n = ln(α)/ln(r); for 95% confidence (α = 0.05) and at least 95% reliability (r = 0.95) the minimum number of samples is 59. For a 95% confidence level, the rule of three applies: counting 100 cells with no positives allows a claim that < 1% of cells have the event, but to be 95% confident that < 1% of cells have it, 300 cells with no positives are needed; to claim reliability ≥ 95% with 95% confidence requires 3 × 20 or 60 samples.

---

### Xrare: a machine learning method jointly modeling phenotypes and genetic evidence for rare disease diagnosis [^85769426]. Genetics in Medicine (2019). Medium credibility.

ERIC is a new measure of similarity between phenotype terms

The poor performance of existing phenotype similarity measures in the presence of imprecision and noise motivated us to develop ERIC. The information content (IC) of a phenotype term t is computed as:where N denotes the total number of genes under consideration, and n denotes the number of genes with phenotype t. The ERIC similarity between t 1 and t 2 is calculated as:where t MICA is the most informative common ancestor (MICA) of t 1 and t 2. In addition, the ERIC score is set to have a minimum value of zero.

Here we briefly describe the rationale for the ERIC score formula. First, we define the distance between term t and its ancestral term t ancestor as:

Then we use this formula to calculate how much IC emitted from t 1 is received by t 2 :In addition, to make the score symmetric between two phenotype terms, we replaced IC (t 2) with the minimum of IC (t 1) and IC (t 2) in this formula. In addition, the ERIC score is set to have a minimum value of zero, when the minimum Dist (t, t MICA) of t 1 and t 2 is larger than IC (t MICA). Intuitively, this at t MICA with a radius of IC (t MICA) are considered noise. The final definition of ERIC similarity can be written as:

Other IC-based measures tested in the study, such as Resnik, Lin, and Jiang-Conrath(JC) measures, are calculated as:

Similarity between phenotype sets

Best match sum(BMS) was used to calculate the similarity between two sets of phenotype terms, the query phenotype set T 1 and the annotation phenotype set T 2:

More details are described in Supplementary Methods.

---

### Direct determination of high-order transverse ligand field parameters via µ SQUID-EPR in a EtN [GdPc] SMM [^c2933297]. Nature Communications (2023). High credibility.

Fig. 1
Crystal structure of Et4N[160GdPc2] and µSQUID loops.

a Side view and b top view of the unit cell of Et 4 N[160 GdPc 2] showing two molecules residing within the unit cell. Hydrogen atoms have been removed for clarity. Colour code: Gd, dark blue; O, red; N, cyan; C, grey. c Hysteresis loops of the isotopically purified 5%Et 4 N[160 GdPc 2]@Et 4 N[YPc 2] determined by μSQUID measurements between 0.025 and 0.3 K and at a fixed sweeping rate of 128 mT/s and d at a fixed temperature of 25 mK and different sweeping rates.

A [Xe]4 f electronic configuration characterises the Gd 3+ ion embedded in Et 4 N[160 GdPc 2], which in the Russell-Saunders coupling scheme leads to the 8 S 7/2 ground state separated by more than 30 × 10 3 cm −1 from the first exited 6 P multiplet. The ground 8 S 7/2 state is isotropic, however, the degeneracy can be removed only by an odd perturbation, e.g. the coupling to an external magnetic field through the Zeeman term (where≈ 2 is the value of the free electron). For the pure 8 S 7/2 state, an isotropicvalue is expected. Nevertheless, when a small admixture with the first excited state 6 P is present, the ground state can be written as:with the corresponding g-value being:where. Spin-orbit admixture in the ground state is responsible for the ligand field splitting effects around two orders of magnitude smaller than 〈 L 〉≠0 in lanthanide SMMs. The Steven's operators formalism allows to model the ligand field interaction (3):whereare Steven's operators andare the ligand field parameters. Note that the ligand field Hamiltonian contains only even terms in order to account for time-reversal symmetry. The axial terms, are invariant with respect to the point symmetry of the Gd 3+ site (D 4d), while the non-axial termsandare the result of the deviation from the ideal square antiprismatic symmetry of the molecule. Further symmetry-breaking requires an additional orthorhombic termin (3). The complete Hamiltonian for Et 4 N[160 GdPc 2] is therefore the sum of the Zeeman and ligand field interaction:

---

### A combined variational and diagrammatic quantum monte carlo approach to the many-electron problem [^4f6cecab]. Nature Communications (2019). High credibility.

Fig. 1
Grouping of Feynman diagrams. The grouping is achieved by leveraging the fermionic crossing symmetry and the free-energy generating functional. Orange top box shows the number of Feynman/Hugenholtz/Free-energy Hugenholtz diagrams at orders 3–6, excluding the Hartree–Fock sub-diagrams (see Supplementary Note 1). The green panel on the left and the right shows an example of the free-energy Hugenholtz diagram, and how is the Hugenholtz vertex related to the standard Feynman diagram. Note that a single Hugenholtz diagram with N vertices (black dots) represents up to 2 N standard Feynman diagrams with alternating signs. By attaching two external vertices to different propagators in the Hugenholtz free-energy diagram in the green box, one generates four topologically distinct groups of standard Feynman diagrams for the polarization function. Two of them are shown in the blue and orange box below. By the process of attaching external vertices to a single Hugenholtz free-energy diagram, we generate 10 out of 11 standard Feynman diagrams for the polarization at the third order. The color lines represent our choice for momentum loops, which are uniquely determined by the choice of the loops in the free-energy Hugenholtz diagram. The external momentum is added through the shortest path connecting two external vertices. Note that such grouping of diagrams allows us to calculate the weight of all diagrams in this figure with only eight different electron propagators, instead of expected 36. The above protocol can generate multiple copies of the same Feynman diagram. We tested two approaches: (a) we keep one copy to avoid double counting; (b) we keep all copies and precompute the symmetry factors, and we notice somewhat better performance of scheme (b)

Finally, beyond variationally optimizing the zeroth order term (L 0) we can also look for improvement of the high-order vertex functions. One of our choices is to sum up all ladder diagrams dressing the vertices (see Supplementary Fig. 3). We call this scheme the Vertex Corrected Constant Fermi Surface (VCCFS). The original diagrammatic expansion is here called Constant Fermi Surface (CFS) scheme. The name originates in the above described principle that electron potential v k is determined in such a way that L and L 0 share the same physical Fermi surface volume.

---

### Transformer-based long-term predictor of subthalamic beta activity in Parkinson's disease [^d4cea312]. NPJ Parkinson's Disease (2025). Medium credibility.

Architecture

The proposed architecture aims to improve long-term DBS treatment for PD. Our aim is to predict a certain daily distribution of beta oscillatory activity based on recordings from previous days. This has very important clinical implications because it represents the first step toward IPGs that can automatically adjust stimulation parameters and set them based not on the immediate readout of the brain signal but on its evolution over time (see Discussion).

We used a Transformer model (3.6 M parameters) to address our time series forecasting problem, due to the multi-head attention (MHA) core, based on a general notion of sparsity of interactions within the sequence (Fig. 2 b). The input 'x' to the Transformer (see previous subsection) is a tensor of size [N, n _ b i n s] where N is the number of consecutive distributions used as input, n _ b i n s is considered as the embedding size. This means that each distribution in the sequence would represent a token, for which we can perform the one-step-ahead and the multi-step-ahead prediction. Given the input sequence, the Transformer processes it through three encoder layers and six decoder layers, followed by an FC layer with a sigmoid function as output non-linearity. The output of each layer for each position in the input sequence is a representation of that position after processing. When using Transformers for sequence-to-sequence tasks, the choice of x [−1] as input for the FC layer stands for using the representation of the last position in the sequence as the final representation for the entire sequence. Since we are interested in predicting the distribution for the (N + K)th day, with K ∈ [1, 6], based on the information from the previous six days, using x [−1] as input to the FC layer captures the model's understanding of the entire sequence. The FC layer then projects this representation to the output space, with size n _ b i n s, to achieve the final prediction.

We need to set specific hyperparameters for our framework. We distinguish between dataset-related, architecture-related, and training-related hyperparameters, all listed in Supplementary Table 2 and Supplementary Table 3.

---

### All-to-all reconfigurability with sparse and higher-order ising machines [^3efd0c9b]. Nature Communications (2024). High credibility.

The optimal median time to solution is defined as:where q represents the median (q = 0.5), the first (q = 0.25) and the second quartiles (q = 0.75) of TTS over the instances. f p (n) is a parallelization factor that counts how many runs of the same instance can be executed in parallel in a single solver. Even though we could fit multiple master graphs at lower n, we used f p = 1 for the FPGA-based p-computer at all n. Figure 7 focuses on the median optimal TTS, i.e. 〈TTS〉 q = 0.5 for all solvers, and Table 1 reports the scaling exponent γ and the prefactor η for each quartile. A detailed description of TTS calculation is mentioned in the "Methods" section.

---

### Germline de novo mutation rates on exons versus introns in humans [^ac10c4e6]. Nature Communications (2020). High credibility.

Comparison of sequence context models

We selected the sequence context dependency model that best explains mutations across our set of exonic and intronic sequences by means of the AIC. We interrogated each of the 191,361,633 (~6.4% of the whole-genome length) exonic and intronic sites on the 95,633 2001-nt windows for the state in the observed data: mutated (and type) or not mutated. For recurrent sites, we chose one observed mutation at random. For a given model, we computed the log-likelihood as the sum across sites of the logarithm of the estimated probability of the observed state at the site. Probabilities were estimated with mutations from the entire dataset, through the direct or the decomposition approach as stated above.

Internal exon-centered mutational analysis

A total of 95,633 stacked 2001-nt sequences centered on the middle position of internal meta-exons were used to compare the observed and expected mutational profiles across exons and introns. We computed the frequency of mutation at a site l with reference core sequenceand flanking sequence X l aswhere L = 2001 denotes the total number of considered sites. Then, each frequency was normalized by the total frequency on the sequence:

Finally, the total number of observed mutations n s on each of the 2001-nt sequence s, of a total of S = 95,633 stacked sequences, was redistributed across both middle exonic and flanking intronic sites according to the normalized frequencies:thus yielding the expected number of mutations at site l of a given sequence s. By adding up the values of all the stacked sequences, we obtain the cumulative number of expected mutations at site l,

For the internal exon-centered analysis on synonymous or nonsynonymous mutations, we separated all possible exonic mutations in middle exon sequences into two groups: those with synonymous consequence and those with a consequence ranking higher than synonymous in the Ensembl Variation hierarchy. Then we computed the expected numbers by only adding frequencies for either synonymous or nonsynonymous mutations.

Computation of effect size and statistical significance

We performed 1000 random permutations of the observed mutations in each stacked sequence based on the probability of each site to acquire a mutation. The effect size, defined as the relative increase or decrease in observed exonic mutations with respect to the expected number, was computed based on the simulation mean expected value. The error of this estimate is given as one standard deviation derived from the 1000 permutations. Moreover, we computed an empirical one-sided p value as the fraction of the simulations with more (or fewer) exonic mutations than the observed number of exonic mutations.

---

### Determining sequencing depth in a single-cell RNA-seq experiment [^ce3ab87a]. Nature Communications (2020). High credibility.

An underlying question for virtually all single-cell RNA sequencing experiments is how to allocate the limited sequencing budget: deep sequencing of a few cells or shallow sequencing of many cells? Here we present a mathematical framework which reveals that, for estimating many important gene properties, the optimal allocation is to sequence at a depth of around one read per cell per gene. Interestingly, the corresponding optimal estimator is not the widely-used plug-in estimator, but one developed via empirical Bayes.

---

### Controlling network ensembles [^0fba599f]. Nature Communications (2021). High credibility.

Table 1
The distributions from which the edge weights are drawn for the graph in Fig. 4 A.

The edge weights may be constant, δ, drawn from a uniform distribution, drawn from a triangular distribution, or drawn from a truncated normal distribution.

For this network, we choose nodes 1 and 2 to be driver nodes and nodes 5 and 6 to be target nodes so thatand C = [O 2×4 7D1 I 2]. The final vector value is chosen to be y f = [17D11] T and t f is chosen to be large enough such thatis sufficiently close to zero to be ignored. The largest eigenvalue, μ 0, and associated values, as a function of N, are shown in Fig. 4 B, C where we see the linear increase required by Assumptions 1 and 2. For N = 50, all of the eigenvalues, μ k, and associated values, for 25 realizations, are shown in Figs. 4 D, E, respectively. Again, it is apparent that the behavior agrees with the requirements laid out in Assumptions 1 and 2. As both assumptions hold, we can be sure that D N (b)/ N p, E N (b), and J N (b) all approach constant values in the N → ∞ limit. The particular values approached in this limit depend on the choice of b. The deviation is shown in Fig. 4 F and the control energy is shown in Fig. 4 G. We see that, since, as b grows, the slope of the deviation decreases. Similarly, since, as b grows, so does the control energy. Finally, the total cost is shown in Fig. 4 H, where the different growth rates are due to the coefficientthat appears in the approximate expressions derived in Supplementary Note 1.4.

---

### Sennosides a and b (senna-lax) [^a3a77faf]. FDA (2012). Low credibility.

Active ingredient (in each tablet)

Sennosides 8.6 mg

---

### Modelling sequences and temporal networks with dynamic community structures [^4f105116]. Nature Communications (2017). Medium credibility.

Results

Inference of markov chains

Here we consider general time-series composed of a sequence of discrete observations { x t }, where x t is a single token from an alphabet of size N observed at discrete time t, and x t −1 = (x t −1, …, x t − n) is the memory of the previous n tokens at time t (Fig. 1). An n th-order Markov chain with transition probabilities p (x t | x t −1) generates such a sequence with probabilitywhere a x, x is the number of transitions x → x in { x t }. Given a specific sequence { x t }, we want to infer the transitions probabilities p (x | x). The simplest approach is to compute the maximum-likelihood estimate, that iswhere a x = ∑ x a x, x, which amounts simply to the frequency of observed transitions. Putting this back into the likelihood of eq. (1), we haveThis can be expressed through the total number of observed transitionsand the conditional entropyas. Hence, the maximisation of the likelihood in eq. (1) yields the transition probabilities that most compress the sequence. There is, however, an important caveat with this approach. It cannot be used when we are interested in determining the most appropriate Markov order n of the model, because the maximum likelihood in eq. (3) increases with n. In general, increasing number of memories at fixed number of transitions leads to decreased conditional entropy. Hence, for some large enough value of n there will be only one observed transition conditioned on every memory, yielding a zero conditional entropy and a maximum likelihood of 1. This would be an extreme case of overfitting, where by increasing the number of degrees of freedom of the model it is impossible to distinguish actual structure from stochastic fluctuations. Also, this approach does not yield true compression of the data, since it does not describe the increasing model complexity for larger values of n, and thus is crucially incomplete. To address this problem, we use a Bayesian formulation, and compute instead the complete evidencewhich is the sum of all possible models weighted according to prior probabilities P (p) that encode our a priori assumptions. This approach gives the correct model order for data sampled from Markov chains as long as there are enough statistics that balances the structure present in the data with its statistical weight, as well as meaningful values when this is not the case.

---

### 2017 ACC / AHA / HRS guideline for the evaluation and management of patients with syncope: a report of the American college of cardiology / American Heart Association task force on clinical practice guidelines and the Heart Rhythm Society [^2b979233]. Heart Rhythm (2017). Medium credibility.

Regarding diagnostic investigations for syncope, more specifically with respect to initial ECG, ACC/AHA/HRS 2017 guidelines recommend to obtain a resting 12-lead ECG as part of the initial evaluation of patients with syncope.

---

### Protein degradation by human 20S proteasomes elucidates the interplay between peptide hydrolysis and splicing [^26c1c0a0]. Nature Communications (2024). High credibility.

Computation of the theoretical sequence space size

The number of possible unmodified spliced and non-spliced peptides that could be derived from a protein sequence in sequence-agnostic fashion constituted the size of the theoretical peptide sequence space. The number X of non-spliced peptides of length N that could theoretically arise from a substrate of length L was:

To derive the positions of all spliced peptides, we defined four indices i, j, k and n that denoted the first (i, j) and second (k, n) splice-reactant, respectively. The corresponding number of peptides was calculated via summing over interval ranges that form valid spliced peptides. Cis -spliced peptides can be formed via forward or reverse ligation. The number of all forward cis -spliced peptides of length N that could theoretically arise from a substrate of length L was:

L ext denoted the minimal splice-reactant length and was set to 1 per default. Analogously, the number of theoretically possible reverse cis -spliced peptides was calculated as:

To calculate the number of theoretical homologous trans -spliced peptides in an in vitro scenario where a single protein was digested with purified proteasomes, the following formula was used:

Computation of substrate-specific cleavage and splicing strengths

SCS-P 1 and PSP-P 1 were calculated using the MS1 intensities (quantities) of peptides after four hours of digestion. For α-Synuclein, the MS1 intensity after three hours of digestion was taken. In the case of a multi-mapper peptide, i.e. a peptide that maps to several substrate locations (see above), the intensity value of this peptides was adjusted by the number of possible peptide origins.

For each amino acid residue of a given substrate sequence, the summed quantity of all non-spliced peptides containing this residue at their P 1 position was denoted as site-specific cleavage strength (SCS-P 1). Similarly, the summed intensity of all spliced peptides carrying this residue at the C-terminus of their first splice-reactant (P1) was denoted as site-specific splicing strength (PSP-P 1) (Supplementary Fig. 15). Cleavage and splicing strength values for a given residue were then normalized by their respective sums in a local, 29 amino acid long, window surrounding the current residue, resulting in percentage distributions of SCS-P 1 and PSP-P 1 over the substrate sequence.

---

### Summary benchmarks-full set – 2024 [^c9abe316]. AAO (2024). High credibility.

Preferred Practice Pattern (PPP) guidelines — GRADE evidence quality ratings are defined for forming recommendations for care as follows: "Good quality (GQ): Further research is very unlikely to change our confidence in the estimate of effect", "Moderate quality (MQ): Further research is likely to have an important impact on our confidence in the estimate of effect and may change the estimate", and "Insufficient quality (IQ): Further research is very likely to have an important impact on our confidence in the estimate of effect and is likely to change the estimate; any estimate of effect is very uncertain".

---

### 2014 ESC guidelines on diagnosis and management of hypertrophic cardiomyopathy: the task force for the diagnosis and management of hypertrophic cardiomyopathy of the European Society of Cardiology (ESC) [^11206c8f]. European Heart Journal (2014). Medium credibility.

Regarding diagnostic investigations for syncope, more specifically with respect to initial ECG, ESC 2014 guidelines recommend to obtain a 12-lead ECG to identify the cause of symptoms in patients with unexplained syncope.

---

### Sub-poissonian loading of single atoms in a microscopic dipole trap [^d24c0ca4]. Nature (2001). Excellent credibility.

The ability to manipulate individual atoms, ions or photons allows controlled engineering of the quantum state of small sets of trapped particles; this is necessary to encode and process information at the quantum level. Recent achievements in this direction have used either trapped ions or trapped photons in cavity quantum-electrodynamical systems. A third possibility that has been studied theoretically is to use trapped neutral atoms. Such schemes would benefit greatly from the ability to trap and address individual atoms with high spatial resolution. Here we demonstrate a method for loading and detecting individual atoms in an optical dipole trap of submicrometre size. Because of the extremely small trapping volume, only one atom can be loaded at a time, so that the statistics of the number of atoms in the trap, N, are strongly sub-poissonian (DeltaN2 approximately 0.5N). We present a simple model for describing the observed behaviour, and we discuss the possibilities for trapping and addressing several atoms in separate traps, for applications in quantum information processing.

---

### Evaluation after a first seizure in adults [^b94f7748]. American Family Physician (2022). High credibility.

Regarding diagnostic investigations for first seizure in adults, more specifically with respect to initial assessment, AAFP 2022 guidelines recommend to evaluate for provoking factors after a first seizure, such as inflammatory, infectious, structural, toxic, or metabolic causes.

---

### Measuring single cell divisions in human tissues from multi-region sequencing data [^8932d0e1]. Nature Communications (2020). High credibility.

Bayesian parameter inference

We use a MCMC to recover the mutational distance μL and the cell survival rate β given a measured distribution of mutational distances. More precisely we implemented a standard Metropolis-Hastings-algorithm following below steps:
(i) Create a new random set of model parameters w given the current set of parameters v from a defined probability density Q, such that Q (x | y) = Q (y | x).
(ii) Calculate the likelihood L (P (w)) of the model distribution P (w) given the data.
(iii) Calculate the ratio of the new and old likelihood ρ = L (P (w))/ L (P (v)). Accept the new parameter set with probability ρ otherwise reject.
(iv) Repeat.

In our case the model distribution is given by Eq. (7) in the main text. To calculate the likelihood of Eq. (7) given the data, we have to choose a cut off for the infinite sums. However, real data always has a maximum mutational distance. Higher terms of the infinite sums contribute to higher mutational distances. The distribution of interest does not change for a sufficiently high cut off and each observed data set only requires finite many terms. Here we used r = i = 30 as upper cut-off, which is a conservative choice. We used uninformed uniform prior distributions for mutational distance μL and the per-cell survival rate β in all cases. Point estimates were extracted as sample medians from the MCMC inferences. The ranges of the uniform priors were adjusted to optimise acceptance rates and computational time. In our implementation, a new set of parameters is relative to the previously accepted parameter set w New = w old + Φ(w), where Φ is the prior parameter distribution. A typical range used in our inference scheme is Φ uniform (β) = [–0.06, +0.06] and Φ uniform (μ) = [–0.15, +0.15]. We also tested Gamma prior distributions and did not see differences in convergence. One numerical realisation of the Log-Likelihood function is shown in Supplementary Fig. 18 and example traces of the MCMC algorithm are shown in Supplementary Fig. 17. We also tested the influence of sequencing depth and spatial sampling strategies on the performance of the MCMC inference framework (Supplementary Figs. 19 and 20). The code for the MCMC inference is available at.

---

### 2025 ESC guidelines for the management of myocarditis and pericarditis [^36c33c3b]. European Heart Journal (2025). High credibility.

Regarding follow-up and surveillance for myocarditis, more specifically with respect to clinical and imaging follow-up, ESC 2025 guidelines recommend to obtain follow-up with clinical assessment, biomarkers, ECG, exercise test, Holter-ECG monitoring, echocardiography, and cardiac MRI at least within 6 months after the index hospitalization in all patients with myocarditis to identify potential progression or new risk factors.

---

### Standards of care in diabetes – 2025 [^31be65a6]. Diabetes Care (2025). High credibility.

Regarding diagnostic investigations for hypoglycemia, more specifically with respect to screening for cognitive impairment, ADA 2025 guidelines recommend to simplify diabetes treatment plans as much as possible for patients with cognitive impairment and tailor to minimize the risk of hypoglycemia.

---

### Beyond the traditional simulation design for evaluating type 1 error control: from the "theoretical" null to "empirical" null [^12866e2f]. Genetic Epidemiology (2019). Medium credibility.

1 INTRODUCTION

Type 1 error (T1E) control evaluation using simulations is always the first step in understanding the performance of any newly developed statistical test. To formulate the problem more precisely, let us consider the current large‐scale genome‐wide association studies (GWAS) or next‐generation sequencing (NGS) studies of complex and heritable traits. These studies scan through millions or more genetic markers (s) across the genome for the ones associated with a trait of interest, while accounting for environmental effects; without loss of generality we assume these genetic markers are single‐nucleotide polymorphisms (SNPs). Manyassociation tests have been developed, and they often require the assumption of (approximately) normally distributed errors to maintain an accurate T1E, with some tests being more robust than others. For example, Bartlett test for variance heterogeneity has been shown to have large inflated T1E rates when the error termfollows a t ‐ or‐distribution (Struchalin, Dehghan, Witteman, van Duijn, & Aulchenko, 2010), and the likelihood ratio test (LRT) is similarly sensitive to nonnormal errors (Cao, Wei, Bailey, Kauwe, & Maxwell, 2014), while Levene's test appears to be more robust (Soave & Sun, 2017; Soave et al. 2015).

---

### Deep flanking sequence engineering for efficient promoter design using deepSEED [^393af296]. Nature Communications (2023). High credibility.

Results

Overview of the AI-aided flanking sequence optimization method DeepSEED

DeepSEED constructed a probabilistic view to unify the expert knowledge to define the 'seed' of promoters and deep learning methods to fill up the flanking sequences that match the 'seed' to improve the promoter performance. The promoter design problem can be formulated in probabilistic terms as maximizing the joint probability of the promoter sequenceand the target property, i.e.

The promoter sequenceis divided into two parts: 'seed' sequencesderived from expert knowledge and flanking regions of 'seed'. By applying the chain rule, the following is obtained:

The formula suggests that the maximization of the probability could be achieved in two stages (Fig. 1a). The first termindicates the process of assigningcompatible with the target property according to expert knowledge (Stage I, Expert Knowledge Integration). The second termrepresents optimizing flanking regionsconditioned on the 'seed' sequencesand the property(Stage II, Sequence Optimization, see "Promoter design approach" in "Methods").

Fig. 1
An overview of the DeepSEED approach.

a The learning procedure of DeepSEED. The model was able to automatically design the promoter sequences by means of the two stages of the learning process. b Training process of the generator and predictor. The details of the sequence construction in the three promoter design tasks are shown in Supplementary Fig. 6. c Sequence generation process in the generator and genetic algorithm. The functional regions represent the functional promoter sequence space learned by the generator. N 1 and N 2 represent the natural sequences from the dataset. S 0, S 1, S 2, and S 3 represent the sequences in the optimization process. d The promoter design tasks in this work. The protein binding sites, as prior expert knowledge, were used to design constitutive promoters, IPTG-inducible promoters, and Dox-inducible promoters.

---

### Sodium sulfacetamide 9.8% and sulfur 4.8% (Sodium sulfacetamide and sulfur) [^dfba513c]. FDA (2025). Medium credibility.

HOW SUPPLIED:

This product is supplied in the following size(s):

10 oz. (285 g) bottles, NDC 44523-748-10

---

### European Stroke Organisation (ESO) and European Association of Neurosurgical Societies (EANS) guideline on stroke due to spontaneous intracerebral haemorrhage [^a24b784e]. European Stroke Journal (2025). High credibility.

Regarding diagnostic investigations for intracerebral hemorrhage, more specifically with respect to initial evaluation, EANS/ESO 2025 guidelines recommend to consider using algorithms such as the DIAGRAM for targeted investigation of the cause of spontaneous ICH to improve the performance of prediction regarding the underlying cause, compared to standard care.

---

### Diagnosis, evaluation, and management of ascites, spontaneous bacterial peritonitis and hepatorenal syndrome: 2021 practice guidance by the American Association for the Study of Liver Diseases [^02b228e7]. Hepatology (2021). High credibility.

Regarding diagnostic procedures for ascites, more specifically with respect to diagnostic paracentesis, AASLD 2021 guidelines recommend to establish the diagnosis of SBP/spontaneous bacterial empyema with a fluid polymorphonuclear leukocyte count > 250/mm³.

---

### Alpha-1-antitrypsin deficiency targeted testing and augmentation therapy: a Canadian Thoracic Society meta-analysis and clinical practice guideline [^46380d55]. Chest (2025). High credibility.

Regarding diagnostic investigations for alpha-1 antitrypsin deficiency, more specifically with respect to AAT levels, CTS 2025 guidelines recommend to consider obtaining initial measurement of serum α-1 antitrypsin levels, followed by genetic testing if α-1 antitrypsin level is < 23 mcmol/L (< 1.2 g/L), in patients with moderate clinical suspicion of α-1 antitrypsin deficiency.

---

### A draft human pangenome reference [^a7eb09fb]. Nature (2023). Excellent credibility.

Step 2: fitting the mixture model

In the second step, the frequencies of coverages were calculated using cov2counts. The output file with the.count suffix is a two-column tab-delimited file: the first column shows coverages and the second column shows the frequencies of those coverages.

The python script fit_gmm.py takes a file.counts suffix, fits a Gaussian mixture model and finds the best parameters through expectation-maximization. This mixture model consists of four main components and each component represents a specific type of region:
Erroneous component, which is modelled by a Poisson distribution. To avoid overfitting, this mode only uses the coverages below 10 so its mean is limited to be between 0 and 10. It represents the regions with very low read support.
(Falsely) duplicated component, which is modelled by a Gaussian distribution, the mean of which is constrained to be half of mean of the haploid component. It should mainly represent the falsely duplicated regions.
Haploid component, which is modelled by a Gaussian distribution. It represents blocks with the coverages that we expect for the blocks of an error-free assembly.
Collapsed component, which is actually a set of components each of which follows a Gaussian distribution, the mean of which is a multiple of the mean of the haploid component. It represents regions that have additional copies present in the underlying genome that have been 'collapsed' into a single copy.

It was noted that the model components may change for different regions owing to regional coverage differences and that the resulting systematic differences affect the accuracy of the partitioning process. To make the coverage thresholds more sensitive to the local patterns, the diploid assembly was split into windows of length (5–10 Mb) and a distinct model was fit for each window. Before fitting, we split the whole-genome coverage file produced in the first step into multiple coverage files for each window. We implemented and ran split_cov_by_window for splitting:

This produced a list of coverage files, each of which ends with ${CONTIG_NAME}_${WINDOW_START}_${WINDOW_END}.cov

We then repeated the above-described steps for each resulting coverage file.

One important observation is that for short contigs, the coverage distribution is generally too noisy to satisfactorily fit the mixture model. To address this issue, we performed the window-specific coverage analysis only for the contigs longer than 5 Mb. For the shorter contigs, we used the results of the whole-genome analysis.

---

### Evaluation, treatment, and prevention of vitamin D deficiency: an endocrine society clinical practice guideline [^ae2db7e8]. The Journal of Clinical Endocrinology and Metabolism (2011). Medium credibility.

Regarding screening and diagnosis for vitamin D deficiency, more specifically with respect to definitions, ES 2011 guidelines recommend to define vitamin D deficiency and insufficiency according to the level of serum 25-hydroxyvitamin D as follows:

- **Deficiency**: < 20 ng/mL (50 nmol/L)

- **Insufficiency**: 21–29 ng/mL (52.5–72.5 nmol/L).

---

### U-sleep: resilient high-frequency sleep staging [^2b2ada2c]. NPJ Digital Medicine (2021). Medium credibility.

For each segment of 1.5 h of sleep we counted the number of occurrences of sleep stage transition triplets. A triplet is a sequence (s 1, s 2, s 3) ∈ {W, N1, N2, N3, REM} 3. We considered only triplets for which s 1 ≠ s 2 and s 2 ≠ s 3. This leaves 80 different triplets in which a fast transition to stage s 2 occur (e.g. (N3, W, N1)) ignoring more typical triplets such as (N2, N2, N2).

We fit a random forest classifier(using the sklearn implementation) to the triplet frequencies (occurrences per time). We fit the classifier to 79 out of the 80 subjects and predicted whether the last subject suffers from OSA or not, repeating the process for all subjects (leave-one-out cross validation). We repeated the whole experiment 50 times for each frequency bin with a small randomization in the hyperparameters of the random forest classifier. The latter is done to increase our confidence that any observed correlation is not related to a very specific set of hyperparameters. Specifically, in each repetition of the experiment we trained a random forests with 200 trees with respect to the Gini impurity measure and class weights w c = n /(k ⋅ n c), where w c is the weight associated with class c, n is the total number of samples, n c the number of samples of class c, and k = 2 is the number of classes ('balanced' mode in sklearn notation). A random value was chosen for the following hyperparameters: maximum_tree_depth ∈ {2, …, 7}, min_samples_leaf ∈ {2. 7}, min_samples_split ∈ {2. 7} and max_features ∈ {sqrt, log2}. We refer tofor a detailed description of those parameters.

We determined the overall OSA classification for each subject by the majority vote over the predictions of the model across all segments of 1.5 h of sleep, ties were broken at random.

---

### Summary benchmarks-full set – 2024 [^8c93cd3b]. AAO (2024). High credibility.

Keratorefractive surgery — postoperative care and follow-up specifies that the operating surgeon is responsible for the preoperative assessment and postoperative management; for surface ablation techniques, examination on the day following surgery is advisable and every several days thereafter until the epithelium is healed; and for uncomplicated LASIK, examine within 36 hours following surgery, a second visit 1 to 4 weeks postoperatively, and further visits thereafter as appropriate.

---

### Predicting short-to long-term breast cancer risk from longitudinal mammographic screening history [^0bd49d9c]. NPJ Breast Cancer (2025). Medium credibility.

Problem formulation

Risk calculation can be treated as a multi-class classification problem, which is common in breast imaging, such as the classification of breast density, the Breast Imaging Reporting and Data System (BI-RADS) score, the type of malignancy, and the BC molecular subtype. In this study, we first discretize the time interval between the screening mammograms of the patients and future cancer diagnoses into various 1-year time periods and treated each period as an independent class. Additionally, we also include one more class which defines whether the patients are cancer-free in the time interval. We define the sum of the probabilities for patients getting BC at each year and the probability of staying healthy throughout the total time interval to be equal to 1. To evaluate the overall risk ofyears, the probabilities of each year are summed together from the first year up to theyear. The formulas are defined as follows Eq. 1 :where, means the predicted probability of an exam getting a BC diagnosis atyear, which is calculated by inputting a sequence of mammograms and corrected risk factors (m) into the model F and using thefunction for the probability generalization. For example, to predict theof a patient getting BC withinyears from the checked images, it can be calculated as the sum of the probability of the first year and second year. Thus, the risk of a patient getting BC within 5 or 10 years from the available data can be calculated as the cumulative sum of the probability from the first year up to the 5th or 10th year. Importantly, the prediction results of our model can guarantee that the risk is monotonically increasing and self-consistent. This avoids the situation, that can occur with separately trained models, where long-term risk may be lower than short-term risk. Moreover, this formulation also learns the inherent relationship between risks at different time points. In this study, the model is trained to predict the risk of BC at each of the 15 years and is validated by predicting BC within a ten-year timeframe. Therefore, our model's design allows for straightforward extension toward longer-term predictions (up to 15 years), provided that a sufficient amount of longer-term follow-up data becomes available.

---

### Colorectal cancer screening and prevention [^390b568a]. American Family Physician (2025). High credibility.

Regarding screening and diagnosis for colon cancer, more specifically with respect to indications for screening, high-risk individuals, family history, AAFP 2025 guidelines recommend to obtain CRC screening in patients with ≥ 1 first-degree relatives with CRC or adenomatous polyps, starting at 40 years of age or 10 years before the age of the youngest relative at the time of their diagnosis.

---

### Vaccines for the twenty-first century society [^bcd53627]. Nature Reviews: Immunology (2011). Medium credibility.

Vaccines have been one of the major revolutions in the history of mankind and, during the twentieth century, they eliminated most of the childhood diseases that used to cause millions of deaths. In the twenty-first century, vaccines will also play a major part in safeguarding people's health. Supported by the innovations derived from new technologies, vaccines will address the new needs of a twenty-first century society characterized by increased life expectancy, emerging infections and poverty in low-income countries.

---

### Diagnosis, evaluation, and management of ascites, spontaneous bacterial peritonitis and hepatorenal syndrome: 2021 practice guidance by the American Association for the Study of Liver Diseases [^04259c82]. Hepatology (2021). High credibility.

Regarding screening and diagnosis for spontaneous bacterial peritonitis, more specifically with respect to diagnostic criteria, AASLD 2021 guidelines recommend to establish the diagnosis of SBP with a fluid polymorphonuclear leukocyte count > 250/mm³.

---

### Guidelines on the management of ascites in cirrhosis [^d8a7ad99]. Gut (2021). High credibility.

Regarding diagnostic investigations for liver cirrhosis, more specifically with respect to evaluation of ascites, BASL/BSG 2021 guidelines recommend to obtain total protein concentration and calculate serum-ascites albumin gradient in the initial ascitic fluid analysis.

---

### Diagnosis, management and treatment of the Alport syndrome-2024 guideline on behalf of ERKNet, ERA and ESPN [^45eede36]. Nephrology, Dialysis, Transplantation (2025). High credibility.

Regarding diagnostic investigations for Alport syndrome, more specifically with respect to hearing assessment, ERA/ERN ERKNet/ESPN 2025 guidelines recommend to consider obtaining a hearing evaluation at diagnosis or upon reaching adulthood in female patients with X-linked alport syndrome, and then every 5 years in the absence of hearing loss symptoms.

---

### A generative network model of neurodevelopmental diversity in structural brain organization [^c4ac635e]. Nature Communications (2021). High credibility.

The optimal η and γ parameters are significantly negatively correlated with each other, such that subjects with large γ parameters tend to have larger negative η (Best N = 1 network: r = −0.284, p = 2.07 × 10 −6; N = 10 networks: r = −0.403, p = 6.08 × 10 −12; r = −0.460, p = 1.58 × 10 −15; N = 100 networks: p = −0.460, p = 1.58 × 10 −15, N = 500 networks: r = −0.497, p = 3.21 × 10 −18) (Supplementary Fig. 3 f). Optimally simulated networks, using this simple wiring equation, are so similar to the actual networks that a support vector machine is unable to distinguish them using the parameters from the energy Eq. (2) (mean accuracy = 50.45%, SD = 2.85%).

---

### Diagnosis, evaluation, and management of ascites, spontaneous bacterial peritonitis and hepatorenal syndrome: 2021 practice guidance by the American Association for the Study of Liver Diseases [^97c5aadf]. Hepatology (2021). High credibility.

Regarding diagnostic investigations for liver cirrhosis, more specifically with respect to evaluation of SBP, AASLD 2021 guidelines recommend to establish the diagnosis of SBP with a fluid polymorphonuclear leukocyte count > 250/mm³.

---

### Pediatric and adult brain death / death by neurologic criteria consensus guideline [^c31b8a7b]. Neurology (2023). High credibility.

Regarding diagnostic investigations for brain death, more specifically with respect to neurological evaluation (numbers of examinations), AAN/AAP/CNS/SCCM 2023 guidelines recommend to obtain ancillary testing to complete BD/death by neurologic criteria determination when the accurate evaluation of a component of the BD/death by neurologic criteria neurologic examination cannot be safely assessed.

---

### Probabilistic classification of late treatment failure in uncomplicated falciparum malaria [^ee7eecfa]. Nature Communications (2025). High credibility.

Deriving moments of allele counts

The locus-wise probability (4) of observed genotypes for recurrent sample r is formulated as a multinomial expression of the population allele frequencies. Convolving Equation (4) over the modelled distributions of(5) and(6), which are conditioned on per-sample allele counts, yields a multinomial expression of baseline allele counts (Supplementary Equation (12)).

Because individual clones and the alleles they carry are not directly observable in multiclonal samples genotyped using standard methods (e.g. not single-cell genotyping), allele counts for multiclonal samples must be derived under an appropriate model –. By Jensen's inequality, the expected per-sample allele counts(which are straightforward to compute) cannot be substituted directly forin Supplementary Equation (12). Convolving the locus-wise probability given by Supplementary Equation (12) over compatible allelic configurations in baseline samples b and i = 1, …, n requires calculating moments of the per-sample allele countsand the pooled-sample allele counts (i.e. per-sample allele counts summed over one or more baseline samples)aggregated over allelic subsets A ⊆ H ℓ (Supplementary Note 1.2).

In Supplementary Note 1.3, we derive these moments from first principles under our framework. In brief, we use a simple combinatorial argument based on ordered partitions to derive probability mass functions and consequently cumulants of the per-sample allele counts. To adjust per-sample allele counts for genotyping error, we adopt a Poisson binomial model: the number of distinct alleles in a given set A ⊂ H ℓ that are harboured by clones in a baseline sample is modelled as a sum of ∣ G ℓ ∣ independent, but not identically distributed, Bernoulli random variables with respective success probabilities ∑ α ∈ A δ ℓ (g, α).

To compute moments of the pooled-sample allele counts (under the assumed independence of clones within and between baseline samples), we first sum cumulants of the per-sample allele counts to recover cumulants of the pooled-sample allele counts. We then exploit complete exponential Bell polynomials to map cumulants of the pooled-sample allele counts to moments of the pooled-sample allele counts, as required. We adopt this construction because the order of these moments (up to M (r)) is likely, in practical settings, to be smaller than the number of baseline samples n over which allele counts are aggregated.

---

### The minimal work cost of information processing [^927873c9]. Nature Communications (2015). Medium credibility.

Classical mappings and dependence on the logical process

Our result, which is applicable to arbitrary quantum processes, applies to all classical computations as a special case. Classically, logical processes correspond to stochastic maps, of which deterministic functions are a special case. As a simple example, consider the AND gate. This is one of the elementary operations computing devices can perform, from which more complex circuits can be designed. The gate takes two bits as input, and outputs a single bit that is set to 1 exactly when both input bits are 1, as illustrated in Fig. 2a.

The logical process is manifestly irreversible, as the output alone does not allow to infer the input uniquely. If one of the inputs is zero, then the logical process effectively has to reset a three-level system to zero, forgetting which of the three possible inputs 00, 01 or 10 was given; this information can be viewed as being discarded, and hence dumped into the environment. We can confirm this intuition with our main result, using the fact that a general classical mapping is given by the specification of the conditional probability p (x ′| x) of observing x ′ at the output if the input was x. Embedding the classical probability distributions into the diagonals of quantum states, the infinity norm in expression (2) becomes simply

---

### Diagnosis, evaluation, and management of ascites, spontaneous bacterial peritonitis and hepatorenal syndrome: 2021 practice guidance by the American Association for the Study of Liver Diseases [^5407197a]. Hepatology (2021). High credibility.

Regarding diagnostic investigations for liver cirrhosis, more specifically with respect to evaluation of ascites, AASLD 2021 guidelines recommend to establish the diagnosis of SBP/spontaneous bacterial empyema with a fluid polymorphonuclear leukocyte count > 250/mm³.

---

### Clinical practice guidelines for healthy eating for the prevention and treatment of metabolic and endocrine diseases in adults: cosponsored by the American Association of Clinical Endocrinologists / the American college of endocrinology and the obesity society [^2288b87f]. Endocrine Practice (2013). Medium credibility.

Regarding preventative measures for vitamin D deficiency, more specifically with respect to vitamin D requirements, AACE/ACE/OS 2013 guidelines recommend to prescribe vitamin D supplementation of at least 1,000–2,000 IU of vitamin D2 or vitamin D3 daily in most patients, to keep the plasma 25-hydroxyvitamin D level > 30 ng/mL.

---

### Early nasal microbiota and subsequent respiratory tract infections in infants with cystic fibrosis [^fa0fb4b5]. Communications Medicine (2024). Medium credibility.

Effect of a "first hit" on the microbiota in infants with CF

We investigated the role of first antibiotic treatment and the role of number of antibiotic treatments in the first year of life analogous to 1.−4. of the first part of the study. Infants with CF were divided into three groups depending on the number of antibiotic treatments: "Never" (0 antibiotic treatments, n = 17), " < 75th Percentile" (1 antibiotic treatment, n = 25), and " ≥ 75th Percentile" (2 or more antibiotic treatments, n = 8). We investigated the role of first RTI in the first year of life analogous to 1.−4. of the first part of the study.

Statistics and reproducibility

Statistics were performed with statistical software R (version 4.1.2). For normal distributed microbiota exposures, generalized additive mixed models (gamm function, mgcv package) were performed with autoregressive structure (lag 1) to account for temporal correlation within subjects (random effect) and corrected for before named possible confounders (fixed effect). The age of the infant in weeks was added in a smooth term. β-diversity was tested with permutational multivariate analysis of variance (PERMANOVA). Terms were added with the option "marginal" to account for all predictor variables equally (adonis2 function, vegan package). Differential abundance analysis of bacterial families was calculated with MaAsLin2 after total sum scaling normalization. We focused on the nine most abundant families for the main manuscript and summarized the rest ("others") due to 16S rRNA sequencing resolution. Reported q -values were corrected for multiple testing with Benjamini-Hochberg procedure as recommended in MaAsLin2 default. To control for confounding, we corrected for sweat chloride levels in infants with CF, mode of delivery, feeding type (breastfeeding vs formula fed), parental smoking, siblings, season, and antibiotic treatment. Due to only two infants with CF attending childcare in the first year of life, we matched control cases that did not attend childcare either without correcting in the further analyses. Results with p < 0.05, q < 0.05 respectively, were considered significant.

Reporting summary

Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article.

---

### AASLD practice guidance on the clinical assessment and management of nonalcoholic fatty liver disease [^faf606eb]. Hepatology (2023). High credibility.

Regarding follow-up and surveillance for metabolic dysfunction-associated steatotic liver disease, more specifically with respect to laboratory and imaging follow-up, AASLD 2023 guidelines recommend to obtain routine surveillance for esophageal varices and monitoring for decompensation in patients with MASH cirrhosis.

---

### Colorectal cancer screening and prevention [^c270f773]. American Family Physician (2025). High credibility.

Regarding screening and diagnosis for colon cancer, more specifically with respect to indications for screening, general population, aged 76–85 years, AAFP 2025 guidelines recommend to consider obtaining screening for CRC in adults aged 76–85 years at average risk based on overall health status, prior screening history, and patient preferences.

---

### Diagnosis and management of Barrett esophagus: European Society of Gastrointestinal Endoscopy (ESGE) guideline [^7e8a3fce]. Endoscopy (2023). High credibility.

Regarding follow-up and surveillance for Barrett's esophagus, more specifically with respect to surveillance endoscopy, choice of modality, ESGE 2023 guidelines recommend to obtain p53 immunohistochemistry to support reproducibility of dysplasia diagnosis and aid the assessment of atypia of uncertain significance.

---

### Diagnosis and management of Barrett esophagus: European Society of Gastrointestinal Endoscopy (ESGE) guideline [^e401f1ce]. Endoscopy (2023). High credibility.

Regarding follow-up and surveillance for Barrett's esophagus, more specifically with respect to surveillance endoscopy, post-treatment, ESGE 2023 guidelines recommend to perform four-quadrant random biopsies just distal to a normal-appearing neo-squamocolumnar junction to detect dysplasia in the absence of visible lesions after successful endoscopic eradication therapy.

---

### Three-dimensional wave breaking [^e1be3bd3]. Nature (2024). Excellent credibility.

We defined the amplitude spectrum of the wave groups we created using the JONSWAP spectrum:The parametric form of the JONSWAP spectrum in equation (2) generally corresponds to an energy spectrum (and, thus, the amplitude of discrete wave components. Instead, we set the amplitude spectrum to be proportional to the JONSWAP spectrum itself, a n ∝ E (f n), as this gives the correct shape of extreme (and, thus, breaking) waves in an underlying random Gaussian sea. In equation (2), f = ω /2π is frequency, and we set the peak frequency f p = 0.75 Hz. Here, σ = 0.9 and 0.7 for f < f p and f > f p, respectively. We chose a JONSWAP spectrum as this spectral shape represents well typical ocean conditions. We set γ = 1 to give a broad underlying spectrum. A broad spectrum results in a wave group that is well dispersed and less steep at the wavemakers, which minimizes the errors associated with generating linear waves. Moreover, our preliminary experiments found that wave groups with broad underlying spectra exhibited only a single breaking crest, whereas focused wave groups based on narrower spectra were more likely to break several times.

---

### Current diagnostic and treatment strategies for specific dilated cardiomyopathies: a scientific statement from the American Heart Association [^76d7d8b8]. Circulation (2016). Medium credibility.

Regarding medical management for myocarditis, more specifically with respect to immunosuppressive therapy, AHA 2016 guidelines recommend to initiate immunosuppressive therapy that includes calcineurin inhibitors and corticosteroids for the treatment of acute dilated cardiomyopathy caused by giant cell myocarditis.

---

### Diagnosis, evaluation, and management of ascites, spontaneous bacterial peritonitis and hepatorenal syndrome: 2021 practice guidance by the American Association for the Study of Liver Diseases [^15509609]. Hepatology (2021). High credibility.

Regarding diagnostic investigations for liver cirrhosis, more specifically with respect to evaluation of ascites, AASLD 2021 guidelines recommend to obtain the following in the initial laboratory investigation of ascitic fluid:

- neutrophil count

- ascitic fluid total protein

- ascitic fluid albumin

- serum albumin (to calculate the serum-ascites albumin gradient).

---

### Cardiac rehabilitation: indications, benefits, and new approaches [^5a62c202]. American Family Physician (2025). High credibility.

Regarding follow-up and surveillance for non-ST-elevation myocardial infarction, more specifically with respect to cardiac rehabilitation, AAFP 2025 guidelines recommend to recognize that post-coronary heart disease, patients benefit from virtual, hybrid, or in-person cardiac rehabilitation programs.

---

### Standards of practice for nutrition support nurses [^32cfd922]. Nutrition in Clinical Practice (2007). Medium credibility.

A.S.P.E.N. standards — disclaimer emphasizes that these standards of practice for the NSN do not constitute medical or other professional advice and should not be taken as such.

---

### Tuberous sclerosis complex surveillance and management: recommendations of the 2012 international tuberous sclerosis complex consensus conference [^022d3e01]. Pediatric Neurology (2013). Medium credibility.

Regarding diagnostic investigations for tuberous sclerosis complex, more specifically with respect to evaluation of cerebral involvement, TSCi 2013 guidelines recommend to educate parents to recognize infantile spasms during infancy even if none have occurred at time of first diagnosis.

---

### AGA clinical practice update on the role of noninvasive biomarkers in the evaluation and management of nonalcoholic fatty liver disease: expert review [^56749473]. Gastroenterology (2023). High credibility.

Regarding follow-up and surveillance for metabolic dysfunction-associated steatotic liver disease, more specifically with respect to surveillance for HCC, AGA 2023 guidelines recommend to consider obtaining surveillance for HCC in patients with MASLD and noninvasive test results suggestive of advanced fibrosis (F3) or cirrhosis (F4).

---

### Pediatric and adult brain death / death by neurologic criteria consensus guideline [^badb48ae]. Neurology (2023). High credibility.

Regarding diagnostic investigations for brain death, more specifically with respect to neurological evaluation (numbers of examinations), AAN/AAP/CNS/SCCM 2023 guidelines recommend to perform at least one examination for BD/death by neurologic criteria.
Consider performing a separate and independent examination for BD/death by neurologic criteria in adult patients by a second clinician.

---

### 2025 ESC guidelines for the management of myocarditis and pericarditis [^b050a21b]. European Heart Journal (2025). High credibility.

Regarding nonpharmacologic interventions for myocarditis, more specifically with respect to physical activity restrictions, ESC 2025 guidelines recommend to restrict physical exercise until remission for at least 1 month in athletes and non-athletes after inflammatory myopericardial syndrome, using an individualized approach to accelerate recovery.

---

### Evaluation and management of well-appearing febrile infants 8 to 60 days old [^d4d81c16]. Pediatrics (2021). High credibility.

Consensus recommendations — agreement process and rationale: The guideline's recommendations reflect universal agreement or a strong consensus among committee members, and in the one situation with only majority agreement, additional members were appointed and added; subsequently, consensus was achieved. The major reason for disagreement was varying levels of risk tolerance, and for these recommendations the Key Action Statements provide more detailed explanations of uncertainties and attempts to derive numbers needed to test and treat.

---

### Renagel [^c81613fc]. FDA (2018). Low credibility.

3 DOSAGE FORMS AND STRENGTHS

800 mg and 400 mg tablets.

Tablets: 800 mg and 400 mg (3)

---

### Learning dynamical information from static protein and sequencing data [^d0f71c33]. Nature Communications (2019). High credibility.

Topology-preserving dimensionality reduction

To ensure that the inference protocol can be efficiently applied to larger systems with a high-dimensional energy landscape, we derive a general method for reducing the dimensionof an energy landscape while preserving its topology. A PDF withwell-separated Gaussians indimensions can be projected onto thedimensional hyperplane spanning the Gaussian means using principal component analysis (PCA); projecting onto a hyperplane of dimensionrisks losing information about the relative positions of the Gaussian means and, in general, does not allow a correct recovery of the MFPTs (Supplementary Methods). In practice, it suffices to chooseto be larger than the number of energy minima if their number is not known in advance.

To preserve the topology under such a transformation — which is essential for the correct preservation of energy barriers and MEPs in the reduced-dimensional space — one needs to rescale GMM components in the low-dimensional space depending on the covariances of the Gaussians in theneglected dimensions (Fig. 1 c). Explicitly, one finds that within the subspace spanned by the retained principal components (Supplementary Methods)as long assatisfies certain minimally restrictive conditions (Supplementary Methods). Here, denotes the firstcolumns of the matrix of sorted eigenvectorsof the covariance matrix of the Gaussian means, and, andare the mixing components, reduced-dimensional PDF, and the covariance matrix of each individual Gaussian in the mixture, respectively (Supplementary Methods). Neglecting the determinant scale factors in Eq. (3), as is often done when GMM models are fitted to PCA-projected data, leads to inaccurate MFPT estimates (Fig. 1 c, bottom). It is noteworthy that Eq. (3) does not represent inversion of the transformation performed on the data by PCA, unless alldimensions are retained; if some dimensions are neglected, Eq. (3) represents a rescaling of the marginal distribution in the retained dimensions to reconstruct the PDF in the original dimension. In other words, the transition rates are best recovered from the conditional — not marginal — distributions, which are given by Eq. (3) up to a constant factor that does not affect energy differences.

---

### Considerations for severe acute respiratory syndrome coronavirus 2 genomic surveillance: a joint consensus recommendation of the Association for Molecular Pathology and association of public health laboratories [^77510db5]. The Journal of Molecular Diagnostics (2025). High credibility.

SARS-CoV-2 whole-genome sequencing quality metrics — Table 3 presents a consensus recommendation for quality metrics based on the workflow purposes. Recommended metrics include: Average coverage depth ≥ 100×, Coverage at a single base to make a call ≥ 30×, % of Genome coverage ≥ 90, % Mapped reads to MN_908,947.3 reference No metrics, Total reads per sample ≥ 100,000, % of Missing data ≤ 10, % Ambiguous base calls < 10, Coverage of MN_908,947.3 reference in negative control, % < 10, and S-gene coverage No metrics.

---

### Transformation of doped graphite into cluster-encapsulated fullerene cages [^f16c03c9]. Nature Communications (2017). Medium credibility.

Fig. 4
Influence of the encapsulated cluster on growth of icosahedral C 80. Isomerically pure a Sc 3 N@ I h -C 80 after laser desorption (~2 mJ) without carbon vapor and b after reaction with graphite vapor in He (10 mJ) and c Sc 3 N@C 2n + n C 2 formation distribution. Comparison of isomerically pure d Y 3 N@ I h -C 80 after laser desorption (~2 mJ) and e reaction with graphite vapor under identical conditions (10 mJ) and f Y 3 N@C 2n + n C 2 formation distribution. C 2 -elimination events are not readily observed for either M 3 N@ I h -C 80

To probe cluster-cage size effects with respect to carbon insertion reactions for the I h -C 80 cage, molecular reactivity studies are performed whereby Y 3 N is substituted for Sc 3 N. Figure 4d–f shows products that result after exposure of isomerically pure Y 3 N@ I h -C 80 to graphite vapor, conducted under identical conditions to those for the Sc 3 N@ I h -C 80 (and Sc 3 N@ D 3 -C 68) growth experiments. Y 3 N@C 82, formed by C 2 incorporation, is formed in high relative abundance; however, Y 3 N@C 84 and Y 3 N@C 2n compounds as large as Y 3 N@C 100 are present in substantial abundance. In addition, Y 3 N@ I h -C 80, like Sc 3 N@ I h -C 80, does not readily exhibit carbon loss events or top-down behavior (Supplementary Fig. 14) when subjected to high energy laser ablation in absence of carbon vapor (i.e. low carbon density, high energy conditions). Extensive SORI-CID experiments strongly support that the major growth products are homogeneous Y-based NCFs (Supplementary Fig. 15). The more extensive bottom-up growth of Y 3 N@ I h -C 80 is in agreement with the formation distribution of Y 3 N@C 2n generated from Y-doped and N-doped graphite (Fig. 1). Thus, the bottom-up mechanism is attributed to also operate in the Y-containing graphite starting material plasma.

---

### Practical guideline on obesity care in patients with gastrointestinal and liver diseases-joint ESPEN / UEG guideline [^a26d293f]. Clinical Nutrition (2023). High credibility.

Regarding diagnostic investigations for metabolic dysfunction-associated steatotic liver disease, more specifically with respect to initial evaluation, ESPEN/UEG 2023 guidelines recommend to consider obtaining MRI-proton density fat traction to confirm the diagnosis of MASLD in patients with grade II/III obesity or suspected MASLD.

---

### Acute lymphoblastic leukemia, version 2.2024, NCCN clinical practice guidelines in oncology [^61c50342]. Journal of the National Comprehensive Cancer Network (2024). High credibility.

Regarding follow-up and surveillance for acute lymphoblastic leukemia - NCCN, more specifically with respect to assessment of minimal residual disease, NCCN 2024 guidelines recommend to use minimal residual disease quantification as an essential component of patient evaluation over the course of sequential acute lymphoblastic leukemia therapy.

---

### Clinical practice guidelines treatment of acute hyperkalaemia in adults [^e1aaa9a9]. UKKA (2023). High credibility.

Regarding medical management for hyperkalemia, more specifically with respect to indications for urgent treatment, UKKA 2023 guidelines recommend to consider obtaining hospital assessment in acutely unwell patients with mild (serum potassium 5.5–5.9 mmol/l) or moderate hyperkalemia (serum potassium 6.0–6.4 mmol/l), particularly in the presence of AKI.

---

### A research roadmap for next-generation sequencing informatics [^127cc27e]. Science Translational Medicine (2016). Low credibility.

Next-generation sequencing technologies are fueling a wave of new diagnostic tests. Progress on a key set of nine research challenge areas will help generate the knowledge required to advance effectively these diagnostics to the clinic.

---

### A sample selection strategy for next-generation sequencing [^8ab2ae1a]. Genetic Epidemiology (2012). Low credibility.

Next-generation sequencing technology provides us with vast amounts of sequence data. It is efficient and cheaper than previous sequencing technologies, but deep resequencing of entire samples is still expensive. Therefore, sensible strategies for choosing subsets of samples to sequence are required. Here we describe an algorithm for selection of a sub-sample of an existing sample if one has either of two possible goals in mind: maximizing the number of new polymorphic sites that are detected, or improving the efficiency with which the remaining unsequenced individuals can have their types imputed at newly discovered polymorphisms. We then describe a variation on our algorithm that is more focused on detecting rarer variants. We demonstrate the performance of our algorithm using simulated data and data from the 1000 Genomes Project.

---

### Self-condensation-assisted chemical vapour deposition growth of atomically two-dimensional MOF single-crystals [^a849ca90]. Nature Communications (2024). High credibility.

Independent monolayer MoS 2 is highly sensitive to environmental active stimuli, as their atoms are almost completely exposed; however, it is theoretically impossible to distinguish interfering substances with similar chemical properties by monolayer MoS 2. Amine gases/vapours such as NH 3, tert-butylamine (TBA), tert-pentylamine (TPA), isopropylamine (IPA) and 2-butylamine (2-BA) are electron donors; When expose them to a monolayer MoS 2, the absorbed molecules on the surface of MoS 2 would shift the Fermi level to the conduction band, leading to electrical resistance decreases of MoS 2. As shown in Fig. 4e, obvious electrical resistance decreases were all observed in a monolayer MoS 2 sensor to exposure of NH 3, TBA, TPA, IPA and 2-BA, respectively, with S of −5.45% for NH 3, −1.82% for TBA, −2.63% for TPA, −4.21% for IPA and −2.74% for 2-BA (at the same concentration). However, when exposing the above analytes to sensors fabricated with the Fe n (bim) 2n /MoS 2 heterostructure, a rapid drop in device resistance only occurs as gaseous NH 3 is injected, and almost no change in the resistance is observed with an injection of other amine vapours. The S of the Fe n (bim) 2n /MoS 2 sensor for the same concentrations of NH 3, TBA, TPA, IPA and 2-BA are about −4.14%, −0.11%, −0.08%, −0.10% and −0.09%, respectively. Calculating selectivity coefficient of the NH 3 that is defined as the ratio of responses of NH 3 and another interfered gas/vapour, that is = / S i, there is at least one order of magnitude increase infor the Fe n (bim) 2n /MoS 2 (Fig. 4f), wherewas 2.99 to TBA, 2.07 to TPA, 1.29 to IPA and 1.99 to 2-BA for the monolayer MoS 2 sensor, and 36.95 to TBA, 50.19 to TPA, 40.18 to IPA and 43.32 to 2-BA for the Fe n (bim) 2n /MoS 2 sensor. This proves a clearly enhanced NH 3 selective response of the Fe n (bim) 2n /MoS 2 heterostructure among amine vapours/gases. In addition, the NH 3 selectivity of the Fe n (bim) 2n /MoS 2 sensor has good reproducibility (Supplementary Fig. 19).

---

### Evolutionary signatures of human cancers revealed via genomic analysis of over 35, 000 patients [^c1317d70]. Nature Communications (2023). High credibility.

For each dataset, we then simulated cancer cell fractions (CCF). To do so, we randomly generated a total order consistent with the generative structure for each (possibly noisy) sample. This total order may represent a set of time observations such as follow-ups through time of patients. For each node in the ordering, we generated a random value within (0,1) with higher values representing cancer cell fractions of early mutated genes and lower values representing late mutations. We then added noise to the simulated CCF by sampling from a Gaussian distribution with mean being the simulated CCF and variance of 0.00, 0.01, 0.05, 0.10, and 0.20. With this, we obtained 22,500 configurations for the NGS sequencing of a single biopsy data type.

---

### Pediatric and adult brain death / death by neurologic criteria consensus guideline [^f023ea66]. Neurology (2023). High credibility.

Regarding diagnostic investigations for brain death, more specifically with respect to prerequisites for evaluation (hypotension), AAN/AAP/CNS/SCCM 2023 guidelines recommend to target an SBP and mean arterial pressure approximating the known chronic baseline for the individual patient if the baseline BP varies significantly from the age-based normal range.