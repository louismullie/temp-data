# Medical Question & Answer

**Sample ID**: 6bc55149-05fe-65da-0e12-dd107bd77e6e
**Dataset Index**: 47807

---

## Question

Example 1 Given
[x²/(2² − 1²)] + [y²/(2² − 3²)] + [z²/(2² − 5²)] + [w²/(2² − 7²)] = 1,
[x²/(4² − 1²)] + [y²/(4² − 3²)] + [z²/(4² − 5²)] + [w²/(4² − 7²)] = 1,
[x²/(6² − 1²)] + [y²/(6² − 3²)] + [z²/(6² − 5²)] + [w²/(6² − 7²)] = 1,
[x²/(8² − 1²)] + [y²/(8² − 3²)] + [z²/(8² − 5²)] + [w²/(8² − 7²)] = 1.

Find the value of x² + y² + z² + w².

---

## Answer

> Let's see… What do we have here? The user is asking how to find the value of (x² + y² + z² + w²) given a system of four equations. Let's break this down step-by-step. First, I need to think about the structure of the equations and whether they can be viewed as evaluations of a rational function. Then, I should define a function that captures the pattern and evaluate it at the given points. Next, I will construct a polynomial whose roots correspond to the denominators and use polynomial identities to extract the desired sum. Finally, I will verify the arithmetic and state the result clearly.

> Let me first confirm the pattern. Each equation has the form (∑(k = 1 to 4) [v_k/(t² − a_k²)] = 1), where (v_k) stands for (x², y², z², w²) and the denominators are (t² − 1², t² − 3², t² − 5², t² − 7²) evaluated at (t = 2, 4, 6, 8), respectively. This suggests defining a rational function (f(t) = ∑(k = 1 to 4) [v_k/(t² − a_k²)]) and noting that (f(t) = 1) at (t = 2, 4, 6, 8).

> Wait, let me verify the denominators. For the first equation, the denominators are (2² − 1² = 3), (2² − 3² = −5), (2² − 5² = −21), and (2² − 7² = −45), which matches the structure. Similarly, the other equations follow the same pattern with (t = 4, 6, 8) in place of 2, so the template holds across all four equations.

> I will now define the rational function explicitly as (f(t) = [x²/(t² − 1)] + [y²/(t² − 9)] + [z²/(t² − 25)] + [w²/(t² − 49)]), and I know (f(t) = 1) when (t ∈ {2, 4, 6, 8}). This means the equation (f(t) − 1 = 0) has those four values as roots, which is the key insight I need to exploit.

> Next, I should review how to convert this into a polynomial equation. Multiplying (f(t) − 1) by the common denominator ((t² − 1)(t² − 9)(t² − 25)(t² − 49)) yields a polynomial in (t²) of degree 4, say (P(t²) = 0), whose roots are (2², 4², 6², 8²). Let me denote (u = t²), so (P(u)) has roots (4, 16, 36, 64).

> Hold on, I should verify the form of (P(u)). Expanding (f(t) − 1 = 0) gives (P(u) = (u − 1)(u − 9)(u − 25)(u − 49) − ∑(k = 1 to 4) v_k × ∏(j ≠ k) (u − a_j²) = 0), where (a_j²) are (1, 9, 25, 49). This is a monic quartic in (u) with the same roots as ((u − 4)(u − 16)(u − 36)(u − 64)), so I can equate coefficients to extract information about the (v_k).

> I need to check the coefficient of (u³) on both sides. On the left, the coefficient of (u³) in (P(u)) is the sum of the roots of the denominator product minus the sum of the numerators, i.e., (−(1 + 9 + 25 + 49) + (x² + y² + z² + w²)). On the right, the coefficient of (u³) in ((u − 4)(u − 16)(u − 36)(u − 64)) is (−(4 + 16 + 36 + 64)). Equating these gives (−(1 + 9 + 25 + 49) + (x² + y² + z² + w²) = −(4 + 16 + 36 + 64)).

> Let me compute the arithmetic carefully. The left side sum of constants is (1 + 9 + 25 + 49 = 84), and the right side sum is (4 + 16 + 36 + 64 = 120). So the equation becomes (−84 + (x² + y² + z² + w²) = −120), which implies (x² + y² + z² + w² = 84 − 120 = −36). Hmm, wait a minute, a sum of squares cannot be negative, so I must have made a sign error in the coefficient mapping.

> Let me reconsider the coefficient of (u³) in (P(u)). The polynomial (P(u) = (u − 1)(u − 9)(u − 25)(u − 49) − ∑ v_k ∏_{j ≠ k} (u − a_j²)) is monic, so its (u³) coefficient is (−(1 + 9 + 25 + 49) − (x² + y² + z² + w²)), because each (v_k) term contributes a cubic term with coefficient (−v_k). Equating this to the right side coefficient (−(4 + 16 + 36 + 64)) gives (−84 − (x² + y² + z² + w²) = −120), which corrects the earlier mistake and yields (x² + y² + z² + w² = 120 − 84 = 36).

> I should double-check that this result is consistent with the structure. The sum of squares equals the difference between the sum of the squared roots of the right-hand polynomial and the sum of the squared poles of the rational function, which is exactly 120 minus 84 equals 36, and this aligns with the corrected coefficient matching.

> Final answer: (x² + y² + z² + w² = 36).

---

The system can be viewed as evaluating a rational function at four points. Define

f(t) = [x²/(t² − 1²)] + [y²/(t² − 3²)] + [z²/(t² − 5²)] + [w²/(t² − 7²)].

Given (f(2) = f(4) = f(6) = f(8) = 1), we consider the function

g(t) = f(t) − 1 = [x²/(t² − 1)] + [y²/(t² − 9)] + [z²/(t² − 25)] + [w²/(t² − 49)] − 1.

Multiplying through by the common denominator yields a degree-4 polynomial in (t²) with roots at (t² = 4, 16, 36, 64). By Vieta's formulas, the sum of the roots equals the negative of the coefficient of the (t⁶) term divided by the coefficient of the (t⁸) term, which gives

4 + 16 + 36 + 64 = 1 + 9 + 25 + 49 + (x² + y² + z² + w²).

Solving for the sum of squares, we obtain

x² + y² + z² + w² = (4 + 16 + 36 + 64) − (1 + 9 + 25 + 49) = 120 − 84 = 36.

---

The value of (x² + y² + z² + w²) is **36**.

---

## References

### Evolution of dosage compensation under sexual selection differs between X and Z chromosomes [^f6e0ceae]. Nature Communications (2015). Medium credibility.

Since selection is weak, the first (equations (9) and (10)) and second moments of the distribution of infinitesimal change in male and female expression are sufficient to describe the evolution of male and female expression over many substitutions. In continuous time, and ignoring the time taken for segregation to occur and the time between mutations, we obtain that the random variables Z m (t) and Z f (t), which describe male and female expression at time t, satisfy the stochastic differential equation,

where d W m and d W f are standard independent Brownian motions, and b 1 and b 2 are scaled variance terms given byand.

Unless selection on male and female expression is very weak (very small N eX S m, N eX S f), and evolution is dominated by genetic drift, the qualitative features of the evolution of dosage compensation are captured by the expected trajectory of equations (11) and (12). Writing the expected male and female expression as z m and z f, we find that their evolution is given by equations (1) and (2), where time is rescaled according to τ = 2 μt /30. This scaling eases the comparison between X and Z expression when differences between mutation rates in the two systems are ignored.

The expected values for male and female expression through time, z m (τ) and z f (τ), may be found exactly by solving equations (1) and (2). If we assume that degradation of the Y-linked gene copy leads to an initial diminution in expression in males of − z 0, and female expression is unperturbed by degradation and remains at the ancestral optimal level zero, we have

where. Similar expressions for Z-linked genes can be found by replacing N eX by N eZ, and m subscripts by f and vice versa. Plots in Fig. 1 correspond to equations (13) and (14) with respect to t rather than τ to compare more easily the deterministic paths with the stochastic replicates of equations (11) and (12) that are shown in Fig. 2.

---

### Multiple imputation for analysis of incomplete data in distributed health data networks [^0364fca3]. Nature Communications (2020). High credibility.

Multiple imputation

An MI method replaces each missing value multiple times from its predictive distribution based on the observed data, accounting for the uncertainty of imputation. Each of the imputed datasets is analyzed separately as if it were fully observed. The results across all imputed datasets are then combined following Rubin's rule. For example, if X 1 which has missing values is continuous, we can use a Bayesian linear regression model for imputationwherewith priorswhereandrefer to the inverse gamma distribution and the multivariate Gaussian distribution, respectively. Let Z = [1, y, x 2, …, x p], and let Z c be the N c × (p + 1) submatrix of Z loaded with the complete cases only. Similarly, let x 1, c be the subvector of x 1 with the complete cases. The posterior distribution of (τ 2, α) is given bywhere. The MI method samples (τ 2, α) from Equation (3), imputes the missing values of X 1 according to Eq. (2) with random errors added, and fits the analysis model (1) using the imputed full data. This procedure is repeated multiple times.

When X 1 that has missing values is binary, we can use a Bayesian logistic regression model for imputation with prior. Letbe the maximum A posteriori estimator. Note that, as N c tends to infinity, we have, where W is a diagonal matrix with, W c is the sub-diagonal-matrix of W for the complete cases,… Therefore, the MI method samples α from, imputes the missing values according to the Bernoulli distribution, and fits the analysis model using the imputed full data. This procedure is repeated multiple times. A regularization parameter λ can be used to avoid numerical difficulties particularly when the sample size at a site is less than the dimension of the parameters. We choose the value of λ to be as small as possible so that the bias caused by regularization can be negligible.

---

### An alternative to the breeder's and lande's equations [^10a6a292]. G3 (2014). Low credibility.

The breeder's equation for the evolution of quantitative traits for additive genetic effects, introduced by Lush, is widely used both in artificial and natural selection theory and experiments and appears in all textbooks of quantitative genetic. This equation can be stated as follow: consider a continuous phenotypic trait Z subject to selection. Noting the mean phenotype in parental, selected parents and the progeny by E (Z 0), E (Z W) and E (Z 1), we can define the selection differential S = E (Z w) − E (Z 0) and the response R = E (Z 1) − E (Z 0). The scalar breeder's equation reads R = h 2 S and ascertains that the response to selection and the selection differential are related through a proportionality relation that is the ratio of genotype to phenotype variances, h 2. The equation naturally extends to selection on multiple traits and its vectorial version reads

Use of the breeder's equation and its underlying assumptions has been criticized by many authors. One fundamental assumption of the breeder's equation is the normal (Gaussian) distribution of the breeding value (genotype) and environment factors. Authors who demonstrate the linear relation assume normal distribution for the aforementioned quantities or the analogous hypothesis of linearity of the parent−offspring regression (see Appendix/Parent−offspring regression). When this assumption is relaxed, the breeder's equation is no longer valid, and one has to resort to a system of hierarchical moment (or alternatively, cumulant) equations to describe the changes; in general, this system is not closed, and the moments of a given order depend on moments of higher order.

---

### [^7c88bcc5]. Developmental Medicine and Child Neurology (2019). Medium credibility.

5.3 干预方式：（组）场景设置

5.3.1 个人因素

自2011年以来，训练方式发生了转折性改变，以小组为基础的干预被加入治疗领域中。总而言之，小组干预对运动表现产生了很大的影响。

虽然尚不能通过数据分析结果来决定最佳的小组规模，但由一名治疗师引导下，4～6名儿童的小组规模的干预是易于管理和有效的模式，必要时可增加一名助理。 268, 273, 280, 281 限制规模的小组训练可以方便指导者在参与者间走动，以此监控小组动态和个人进步情况。一项研究发现， 280 运动能力较差的儿童在大组活动中会感到更焦虑。然而，这些儿童在大组中处理同伴问题的能力确实得到了提高，这是非常有用的生活技能。因此，应该仔细考虑小组规模的设置，但也要根据年龄、疾病的严重程度、小组成员和干预的目标谨慎设定。

---

### Using both qualitative and quantitative data in parameter identification for systems biology models [^fc71653e]. Nature Communications (2018). Medium credibility.

Results

An illustration of the potential value of qualitative data

To demonstrate the potential value of qualitative data, we consider a simple case of solving for the coefficients of polynomial functions.

We consider two polynomial functions: y 1 = ax 2 − bx + c and y 2 = dx + e. Suppose we want to solve for the coefficients a, b, c, d, and e, which we will take to be positive. As the ground truth coefficients to be determined, we choose (a, b, c, d, e) = (0.5, 3, 5, 1, 1.5).

Suppose that a limited amount of quantitative information is available. Namely, it is known that the parabola y 1 contains the points (2, 1) and (8, 13), and the line y 2 contains the point (3.5,5). This is not enough information to solve for any of the coefficients because three points are required to specify a parabola, and two points are required to specify a line (Fig. 1a).

Fig. 1
A simple illustration using polynomial functions. We use qualitative and quantitative information to determine the unknown coefficients. a Visualization of the problem. We seek to find the coefficients of equations for a parabola and a line, with the ground truth shown (blue solid curves). Two points on the parabola and one point on the line are known (black dots). These three points are consistent with infinitely many possible solutions (e.g. orange dashed curves). Qualitative information (colored circles, x -axis) specifies whether the parabola is above (+) or below (−) the line. This information limits the possible values of intersection points x 1 and x 2 to the green shaded segments of the x -axis. b Bounds on coefficient values as a function of the number of qualitative points known. Shaded areas indicate the range of possible values of each coefficient

---

### Adherence to public institutions that foster cooperation [^193d8bdf]. Nature Communications (2021). High credibility.

In summary, for a population following the Stern Judging norm, the equilibrium frequencies of good individuals in each strategic type from the perspective of an arbitrary observer satisfy the following equations:An analogous derivation for Simple Standing yieldsLikewise, for Scoring we haveFinally, for Shunning we haveThese equations correspond to the expressions under private assessment with complete empathy from prior studies, with the exception that g = ∑ i f i g i, the proportion of the population with a good reputation in the eyes of an arbitrary observer, is replaced here by G = ∑ i f i G i, the proportion of the population with a good institutional reputation.

For each social norm, the system of equations above is closed once we specify how G depends on g X, g Y, g Z and on the size and strictness of the institution. We consider two limiting cases: very strict institutions that broadcast an individual as good only if all members agree she is good (q > (Q − 1)/ Q) and very tolerant institutions that broadcast an individual as good provided at least one member views her as good (q < 1/ Q). In these two respective cases we haveFor completeness' sake, we provide the expression for arbitrary q and Q :The resulting system of equations for g X, g Y, and g Z can be solved by radicals when Q ≤ 2. More generally, a unique feasible solution exists for any Q, and it can be computed numerically by iterating the above system of equations after choosing any initial value∈ (0, 1) 3. Successive iteratesremain in [0, 1] 3 and form a contraction: the equations for g X, g Y, g Z are each convex combinations of elements in (0, 1), provided e 2 > 0 and e 1 > 0. Solving this system determines the equilibrium frequency of good individuals of each strategic type from the eyes of an arbitrary observer (g i) and therefore also yields the institutional broadcast (G i, from Eq. (7), and G = ∑ i f i G i). Substituting these expressions into the replicator equation (Eqs. (1) and (2)) provides the dynamics of strategy frequencies f i, allowing us to compute selection gradients, strategic equilibria, and basins of attraction, as shown in Fig. 2.

---

### Adaptive resetting for informed search strategies and the design of non-equilibrium steady-states [^9a9db8d1]. Nature Communications (2025). High credibility.

The final value theorem for Z-transforms states that. By using it, we get (see section 6 of the Supplementary Information)where 〈 N R 〉 is the mean number of time steps between consecutive resetting events. Note that the steady-state in Equation (14) is well defined whenever 〈 N R 〉 is finite, regardless of whether or not the process without resetting has a steady-state. This is a generalization of a well-known result in the theory of standard resetting to state- and time-dependent resetting.

To estimate the NESS, we first sample a set of N trajectories without resetting of length M Δ t. We stress that M should be large enough such that, had we used resetting, the probability of surviving M steps without resetting would be negligible, i.e. Then, we use Equations (12) and (14), and the definition of the Z-transform, to obtainThis estimation results in an unnormalized distribution, which should be normalized. The normalization factor provides an estimate for the mean time between consecutive resetting events, 〈 N R 〉. Equation (15) shows that the estimation of the NESS with resetting, from trajectories without resetting, is done by averaging the histogram of positions over time and trajectories, but reweighing each trajectory, at every time step, by its survival probability.

Prediction and design of non-equilibrium steady-states

The above results can be used to predict and design NESS of spatially-dependent resetting protocols. We demonstrate this using two examples.

It is well known that for free diffusion with a constant resetting rate, a Laplace distributed NESS emerges. An analytical solution for the NESS of diffusion with a parabolic resetting rate r (x) = r 0 x 2 is also known. Interestingly, in both cases, the tails of the NESS decay as, with α = 1 for the constant resetting rate, and α = 2 for the parabolic resetting rate. This raises a more general question: what is the asymptotics of the NESS for diffusion with a power-law resetting rate r (x) = r 0 ∣ x ∣ λ. While there are currently no known closed-form solutions for the NESS with λ ≠ {0, 2}, we can easily estimate the resulting NESS using the procedure described in the previous section.

---

### Longitudinal gut microbiome changes in immune checkpoint blockade-treated advanced melanoma [^b6f40802]. Nature Medicine (2024). Excellent credibility.

Without including any of the 'peripheral' independent variables, which we adjusted for (that is, center, time to/since first injection, other forms of irAEs, patient identification, age, sex and BMI), we can write our linear regression model aswhere Z and W 1–3 are binary variables dummy coded to be either 0 or 1, always with 0 as the reference category. Thus, the β 2 coefficient for Z (PFS12: 0 is PFS < 12, 1 is PFS ≥ 12) represents the value when all treatment characteristics of interest (W 1–3) are at their reference level (that is, monotherapy (W 1), no colitis (W 2) and no PPIs (W 3)) and when the independent variable X has a value of zero (that is, baseline). We can further rewrite equation (8) to illustrate that the relationship between X and Y is conditional on Z and W 1–3 as follows:where the first and second parentheses represent the intercepts and the slopes graphing Y against X.

Post hoc contrasts to compute the comparisons of interest

To create the relevant comparisons between cases and controls, we constructed so-called post hoc contrasts (linear combinations of coefficients) directly from the fitted model. To compute these, we first constructed reference grids (Supplementary Information), which contain all relevant combinations of the categorical independent variables that we wanted to average over. Based on these reference grids, we computed marginal means of cases and controls, which we then could statistically compare. Because we already mean centered all 'peripheral' independent (continuous and categorical) variables, we only consider the coefficients associated with the treatment characteristic of interest (W 1–3), which is shown in equation (8). The post hoc contrasts we computed were (1) PFS ≥ 12 versus PFS < 12 months, (2) colitis versus no colitis, (3) PFS ≥ 12 months with and without colitis, (4) patients on combination versus monotherapy with colitis, (5) PFS ≥ 12 versus PFS < 12 months on monotherapy without colitis and no PPIs, (6) PFS ≥ 12 versus PFS < 12 months on combination therapy without colitis and no PPIs and (7) PFS ≥ 12 versus PFS < 12 months on PPIs, monotherapy and without colitis. In Supplementary Information, we show the mathematical procedure to compute these post hoc contrasts for (1), (2) and (3), but the same logic applies when computing to the remaining contrasts.

---

### An alternative to the breeder's and lande's equations [^a7d3dba1]. G3 (2014). Low credibility.

Stretched exponentials

We see, however, that even if the strict condition (16) is fulfilled, the proportionality constant need not be h 2. Consider, for example, the class of stretched exponential functions φ (k) = exp(−| k | α), which generalizes Gaussians (case α = 2). Set. The inverse FT of these functions gives the distribution of the genotype Y and environment effect E and it is straightforward to show that as for the Gaussian case. Condition (16) however is satisfied this time withand therefore the realized heritability h α = R / S isThe aforementioned examples were to emphasize the fact that selection-independent proportionality is achieved only for particular pairs of genotype/environment distributions. In general, as shown in Figure 2, the realized heritability is not constant and depends critically on the selection function W (z).

Figure 2
A simple example in which R / S ≠ h 2. (A) The parental breeding value distribution (thin red line) is a double Gaussian p 0 (y) = ((m, s; y) +(− m, s; y))/2; the environmental effects distribution (thin blue line) follows a normal distribution f (x) = (0, σ E; x). The phenotype distribution q (z) (equation 5), (thick black line), has the appearance of a normal distribution. The result of a truncation selection, selecting only and all individuals with phenotype value > z 0 is shown in (B). (B) Right scale: The Response R (red line, circle) and the Selection differential S (orange line, triangle) as a function of the truncation selection z 0. Left scale: the value of R / S (thick black line) as a function of z 0 and its comparison to h 2 (thin dashed line). All integrations (equations 11 and 12) can be performed exactly for this case: R (z 0) and S (z 0) are combination of Gaussian and erf(z 0) functions, their exact expressions are given in Appendix/Computation of truncation selection. The parameters of the figures are m = 3, s = 2 and, therefore h 2 = 1/2.

---

### State estimation of a physical system with unknown governing equations [^794152e0]. Nature (2023). Excellent credibility.

State estimation is concerned with reconciling noisy observations of a physical system with the mathematical model believed to predict its behaviour for the purpose of inferring unmeasurable states and denoising measurable ones 1,2. Traditional state-estimation techniques rely on strong assumptions about the form of uncertainty in mathematical models, typically that it manifests as an additive stochastic perturbation or is parametric in nature 3. Here we present a reparametrization trick for stochastic variational inference with Markov Gaussian processes that enables an approximate Bayesian approach for state estimation in which the equations governing how the system evolves over time are partially or completely unknown. In contrast to classical state-estimation techniques, our method learns the missing terms in the mathematical model and a state estimate simultaneously from an approximate Bayesian perspective. This development enables the application of state-estimation methods to problems that have so far proved to be beyond reach. Finally, although we focus on state estimation, the advancements to stochastic variational inference made here are applicable to a broader class of problems in machine learning.

---

### Computational modeling of the temporal influences between cues, craving and use in addiction: a dynamical system analysis based on ecological momentary assessment data [^90590bea]. Translational Psychiatry (2025). Medium credibility.

DST models

Using successive simulations based on different parameterization of the z (cues) and y (craving) variables of the DST model, we sought to represent in a non-linear way (unlike SARIMAX models) the mutual interactions between craving and cues. We did not seek to modify the other two variables (f variable for long-term temporality or x variable for use). The parameter values of the equations of y (craving) and z (cues) equations are modified in order to fit with each of the two SARIMAX models. We defined two sets of parameters: the first set of parameters of the DST-model 1, corresponds to the dynamical system representation of the 154 patients who fit with the SARIMAX-model 1; the second set of parameters of the DST-model 2, corresponds to the dynamical system representation of the 57 patients who fit with the SARIMAX-model 2.

The details of the parameter values of the two DST models are provided in Table 4. Five parameter values were necessarily modified to recreate the two SARIMAX-models: L, R b, P, τ x, and τ y (in red in Table 4). Firstly, the value of the L parameter (corresponding to the vulnerability level in the y equation) was barely modified (L = 1.01 in the DST-model 1 and L = 1.00 in the DST-model 2) to obtain these two profiles of influence of craving on cues. Secondly, the value of the parameter R b (referring to the sensitivity of the internal elements) was necessarily modified (R b = 1.04 in the DST-model 1 and R b = 0.904 in the DST-model 2): for craving to have an influence on sensitivity to cues (DST-model 2), a patient must have a lower level of sensitivity. Thirdly, the value of the parameter P (representing potentiation, or the maximal rate of internal elements amplifying symptoms) was necessarily increased for DST-model 1 (P = 10) compared to DST-model 2 (P = 1), indicating that craving (which drives DST-model 1) serves as an internal mechanism that amplifies use. Finally, τ x and τ y refer solely to the time scales specific to each variable (e.g. τ x
- dx/dt) without clinical interpretation, and their increase in DST-model 1 does not alter the interpretation of the other parameter values.

---

### Conceptualizing productive engagement in a system dynamics framework [^7f938192]. Innovation in Aging (2017). Low credibility.

Theory Development

The goal of theory development is to increase confidence that the SD model reflects the actual structure of the system. Once a preliminary conceptual model is developed (such as in Figure 3), we can then build confidence in the model by identifying errors and omissions through an iterative process of qualitative review using the most current literature. This iterative process can be enhanced through the development a simulation model based on this qualitative structure.

SD provides a set of meta-theoretical rules for formulating causal relationships in a similar way as multivariate regression analysis provides a set of meta-theoretical rules for formulating statements about the associations among variables. The chief difference is that SD models focus on feedback relationships that are represented as a system of coupled and ordinary differential equations. In other words, SD relies heavily on calculus-based mathematics instead of the more traditional statistics-based mathematics used in the social sciences. The iterative process of model specification begins in a manner similar to specifying "priors" (initial values and conditions) in Bayesian statistics, in which an initial, quantified, and simulatable model is tested to establish the internal logic and validity, as well as to check for gaps in the reasoning. Essentially, we are trying to ground the model in rough estimates from empirical data and the literature.

---

### Demography and the emergence of universal patterns in urban systems [^36ce729d]. Nature Communications (2020). High credibility.

Methods

Probability solution for geometric random growth with boundary conditions

The Fokker-Planck equation for random geometric growth without drift iswhere P = P [x (t), t ∣ x (0), t 0] is the conditional probability of observing state x of the random variable at time t, given the initial state x (t 0) at time t 0. For simplicity of notation, we have dropped the i indices in x and σ and write x (t 0) as x 0.

Equation (17) has some similarities with the diffusion equation in physics and can analogously be solved exactly. First, it is useful to change variables so as to eliminate the non-linear term x 2. We setand. By changing the variables in Eq. (17) we findThis equation is now linear and can be solved in two ways. The first way is using factorization and solve it as a heat equationandThe second way is to solve it directly via a Fourier transform, so thatwhich leads toThis equation can be solved via a separation of variables, P [k, τ] = f (k) T (τ), which leads to an eigenvalue problem:Solving for k we obtain:withwhere P [y, 0] = f (k) T (0). In particular, there are two stationary solutions, for w = 0, with k = k 0 = 2 i and k = k 1 = i. Substituting k 0, we see that the solution corresponds to, which is Zipf's distribution. The other solution corresponds to the existence of a constant probability current up or down the urban hierarchy associated with different boundary conditions, see main text.

---

### Maternal transmission as a microbial symbiont sieve, and the absence of lactation in male mammals [^467b80e3]. Nature Communications (2024). High credibility.

Differential equation model

We note that a system of ordinary differential equations can be constructed for the mean behaviour of the stochastic process. We have used this to check the results of the stochastic realisations, to gain further understanding of the dynamics, and to check the robustness of the results to changes in parameter values.

In the absence of evolution (Figs. 2, 3, and 5 a, b), the equations are:assuming a 1:1 sex ratio to reduce the dimensionality of the system from four to two. The state variables are the density of female hosts x + with the symbiont community S, and the density x − with the community s. These state variables come from dividing the number of females with and without the additional symbiont (n +, n −) by a scaling parameter ν (x + = n + / ν, x − = n − / ν). The mating classes are as defined in Table 1, the proportion of females (respectively males) passing community S on to the next host generation being α (respectively β). We include here a term e 0, describing the per-capita rate at which hosts take up the new symbiont from the environment (i.e. horizontal transmission). Such transmission converts the host's symbiont community from s to S. Under strict vertical transmission, e 0 = 0.

We used the differential equations to describe the dynamics under biparental transmission (setting α = 1, β = 1), and under maternal transmission (setting α = 1, β = 0). In the absence of horizontal transmission, the additional symbiont was added close to a boundary equilibrium point I of the host population(Equilibrium points are calculated by setting the above system of differential equations equal to 0.) The initial per-capita rate of increase of hosts carrying the symbiont at this equilibrium point isgiving a threshold fitness w 0 above which invasion of the symbiont happens with values w 0 = 1/2 for biparental transmission, and w 0 = 1 for maternal transmission. A second equilibrium point I I occurs atwhere every host carries the symbiont. This is unaffected by the mode of vertical transmission, but, unless the symbiont is neutral (w = 1), the symbiont changes the equilibrium host population size when it is present in all hosts. There is a threshold fitness w 1 at which the symbiont causes extinction of the host population at w 1 = 2 d 0 / b 0.

---

### Why are we regressing? [^fd8ada5d]. The Journal of Foot and Ankle Surgery (2012). Low credibility.

In this first of a series of statistical methodology commentaries for the clinician, we discuss the use of multivariate linear regression.

---

### Evolution of dosage compensation under sexual selection differs between X and Z chromosomes [^a1882357]. Nature Communications (2015). Medium credibility.

The probability of fixation determines the expected change in expression over one particular substitution in each sex. If the mutant is lost, expression does not change, if the mutant is fixed, male and female expression become z m + δ m and z f +2 δ f, respectively (Table 2). To calculate the expected change in expression, we consider all possible mutational sizes and their frequencies,

where 3 N is the total number of genes in the population, μ is the mutation rate and f (δ m, δ f) is the probability density function for the effects of mutants on male and female expression. We assume that f (δ m, δ f) is a bivariate normal distribution with mean (0, 0) and covariance matrix

In this case, equations (6) and (7) read

and the second moments of the change in male and female expressions are approximately E[(Δ z m) 2 | z m, z f] = E[(Δ z f) 2 | z m, z f] = σ 2, and E[(Δ z m)(Δ z f)| z m, z f] = ρ σ 2. For simplicity, we set σ 2 = 0.1. Setting different values for σ 2 changes the impact of mutations, but since it does so with the same intensity in males and females, it does not change our results. However, increasing σ 2 increases the overall rate of evolution.

---

### Spin dephasing under nonlinear gradients: implications for imaging and field mapping [^eaf2803c]. Magnetic Resonance in Medicine (2012). Low credibility.

This work examines the prototypical MR echo that would be expected for a voxel of spins evolving in a strong nonlinear field, specifically focusing on the quadratic z(2) - ½(x(2) + y(2)) field. Dephasing under nonlinear gradients is increasingly relevant given the growing interest in nonlinear imaging, and here, we report several notable differences from the linear case. Most notably, in addition to signal loss, intravoxel dephasing under gradients creating a wide and asymmetric frequency distribution across the voxel can cause skewed and nonlinear phase evolution. After presenting the qualitative and analytical origins of this difference, we experimentally demonstrate that neglecting these dynamics can lead to significant errors in sequences that assume phase evolution is proportional to voxel frequency, such as those used for field mapping. Finally, simplifying approximations to the signal equations are presented, which not only provide more intuitive forms of the exact expression but also result in simple rules to predict key features of the nonlinear evolution.

---

### Article 5. An introduction to estimation – 2: from z to T [^fb82b806]. Emergency Medicine Journal (2001). Low credibility.

Provided the sample size is large enough (that is, n greater than 100), the z statistic can be used to determine the confidence interval estimation of the population mean even when the sigma is not known. In these cases the estimation of the standard error of the mean is used. The z statistic is also valid when determining the population's proportion based upon a large sample. However, when dealing with smaller samples, the z statistic is replaced by the t statistic. This makes it possible to estimate, in a population with an unknown standard deviation: The probability of getting a sample mean greater than or equal to a particular value The value of a sample mean with a particular probability of occurring The probability of getting a sample mean between two particular values The confidence interval for the estimation of the population mean can also be determined using the t statistic.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^cbffc27e]. CDC (2011). Medium credibility.

Appendix G. Mathematical formula for sample size — sample size required to detect the difference of GM between two subpopulation groups defines a z test with null hypothesis GM1 = GM2 or GMmax/GMmin = 1 and alternative GM1 ≠ GM2 or GMmax / GMmin > 1, where GMmax = max(GM1, GM2) and GMmin = min(GM1, GM2). It considers power W (say 80% chance) to detect that one group's GM is at least k fold higher and gives the p-value as p = Prob(Z ≥ z) = 1 − φ−1(z) with Z ~ Normal (0,1). Given α (e.g., 0.05) and target ratio k, power is W = Prob (Z > φ−1(1 − α) − log10(k)/(S√(1/n1 + 1/n2))), where S is the expected standard deviation of log10(VL) in the population of interest; when n1 = n2 = n, W = Prob (Z > φ−1(1 − α) − log10(k)/(S√(2/n))). The required sample size is n = 2[(φ−1(1 − α) − φ−1(1 − W))S/log10(k)]², and tables are provided for W = 0.8 and W = 0.9 with α = 0.05.

---

### KDIGO 2024 clinical practice guideline for the evaluation and management of chronic kidney disease [^b40fce01]. Kidney International (2024). High credibility.

Selection of GFR estimating equations — use of validated eGFR: We recommend using a validated GFR estimating equation to derive GFR from serum filtration markers (eGFR) rather than relying on the serum filtration markers alone (1D). Practice Point 1.2.4.1 advises to use the same equation within geographical regions (as defined locally [e.g., continent, country, and region] and as large as possible), noting that within such regions, equations may differ for adults and children. The recommendation places a high value on using an estimating equation validated in the population of interest, and the key points are to use an equation validated and most suited to the population of interest; there is potential for harm if people get different eGFR values when receiving care in different settings, and there is benefit to clinical care, research, and public health with the use of validated equations such that decisions, research findings, and public policy are informed by accurate estimates of CKD.

---

### A space-time tradeoff for implementing a function with master equation dynamics [^57fa10b0]. Nature Communications (2019). High credibility.

Master equations are commonly used to model the dynamics of physical systems, including systems that implement single-valued functions like a computer's update step. However, many such functions cannot be implemented by any master equation, even approximately, which raises the question of how they can occur in the real world. Here we show how any function over some "visible" states can be implemented with master equation dynamics-if the dynamics exploits additional, "hidden" states at intermediate times. We also show that any master equation implementing a function can be decomposed into a sequence of "hidden" timesteps, demarcated by changes in what state-to-state transitions have nonzero probability. In many real-world situations there is a cost both for more hidden states and for more hidden timesteps. Accordingly, we derive a "space-time" tradeoff between the number of hidden states and the number of hidden timesteps needed to implement any given function.

---

### Anticipating regime shifts by mixing early warning signals from different nodes [^4c2ed7d1]. Nature Communications (2024). High credibility.

Two nodes connected by a directed edge

We consider a network composed of two nodes and a directed edge of weight w (≥ 0); see Fig. 1 a for a schematic. We assume that node 1 influences node 2 but not vice versa. We also assume that, as a bifurcation parameter, denoted by r, gradually increases, node 1 undergoes a saddle-node bifurcation and that node 2 also undergoes a saddle-node bifurcation either almost at the same time as node 1 or after r has further increased. The model is given bywhere Δ r (≥ 0) is a constant, σ 1 and σ 2 are the intensities of dynamical noise applied to nodes 1 and 2, respectively, and f (x) satisfies the following conditions. First, we assume f (x, r) = r + x 2 when r ≤ 0 and, where Δ x (> 0) is a small constant. This condition guarantees that, in the absence of coupling and dynamical noise, Eqs. (8) and (9) are both the topological normal form of the saddle-node bifurcation. In other words, d x /d t = f (x, r) with r < 0 has a stable equilibriumand an unstable equilibrium, which collide at x ✱ = 0 when r = 0. In Fig. 2, we show an example bifurcation diagram of single-node deterministic dynamics given by d x /d t = f (x, r). If σ 1 = 0, then x 1 (t) undergoes a saddle-node bifurcation at r = 0 as r increases starting with a negative value. If σ 2 = 0 and w = 0, then x 2 (t) undergoes a saddle-node bifurcation at r = Δ r. Second, we assume that f (x, r) is continuous in terms of x and r for simplicity. Third, we assume that f (c, r) = 0 for ∀ r ≥ 0 for a unique positive value of c, which is larger than Δ x. This implies that, in the absence of noise, x = c is the unique stable equilibrium after a node undergoes a saddle-node bifurcation as r gradually increases. This assumption in combination with the continuity assumption for f (x, r) also implies that the stable equilibrium apart frompersists for some r < 0 although its position changes from x = c in general. Therefore, there are two stable equilibria at least in some range of x < 0 near x = 0, as shown in Fig. 2.

---

### Models of physiology and physiologically based models in clinical pharmacology [^fa104488]. Clinical Pharmacology and Therapeutics (2012). Low credibility.

As science matures, it becomes more mathematical, progressing from enumeration to the use of equations to the formulation of models. Clinical pharmacology has developed to the stage where models play an increasingly important role in predicting and analyzing drug pharmacokinetics and pharmacodynamics, and even in characterizing disease progression and therapeutic response. Useful models have two characteristics that are in ostensible conflict: (i) they must accurately represent the essential features of the underlying system and (ii) the representation must be sufficiently simplified to enable its salient features to be identified and investigated through further experimentation.

---

### A mechanistic model of tau amyloid aggregation based on direct observation of oligomers [^59997389]. Nature Communications (2015). Medium credibility.

Seeding predictions

The effects of seeding are taken into account straightforwardly, by setting the initial population concentrations in equations (7, 8, 9) at t = 0 according to the nature of the seeds. All rate constants are set according to the fitting procedure discussed above. For the simulations described in the main text, we investigate seeding by oligomers; x (0) is set to the initial seed concentration, while f (0) remains negligible. The rate of aggregation is monitored via the formation of fibril mass; if the unseeded reaction gives a final fibril residue concentration f a · f (∞) = W, and at time t W /2 has produced a fibril residue concentration of W /2, any increase in reaction rate on seeding is quantified by considering the time s t W /2 at which the seeded reaction produces a fibril residue concentration of W /2. The seeding required to double the reaction rate is thus found by setting s t W /2 / t W /2 = 1/2. For the K18 wild-type construct, this criterion is very similar to simply halving the reaction half-time. For the ΔK280 deletion mutant, seeding leads to a non-negligible increase in the total fibril mass produced, thus motivating this more careful definition. Intracellular conditions are simulated by setting m (0) = 2 μM.

Inferring activation free energies

For a given rate constant k describing a particular reaction step, the activation free energy Δ G ‡ of the process at a temperature T can be inferred from transition state theory, via the equation

where R represents the molar gas constant. The prefactor Γ has units matching those of k, and is approximately constant for a given process; it has a weak temperature dependence that to a good approximation is assumed negligible in the studied range. The change in activation energy ΔΔ G ‡ for a given process on mutation can thus be foundindependently of Γ by applying equation (10) to both the WT (k WT) and the Mutant (k Mut) system and subtracting:

The values quoted in Table 1 are thus found by application of equation (11) to the four different rate constants in the conversion model.

---

### Effects of changing population or density on urban carbon dioxide emissions [^07cf0996]. Nature Communications (2019). High credibility.

We summarize all these properties calculated for Cobb–Douglas (Eq. (3)) and translog (Eq. (5)) models in Supplementary Table 2.

Fitting models with the ridge regression approach

As we have discussed in the main text, multicollinearity is present in the models of Eqs. (3) and (5). This effect happens when at least two predictors in a multiple linear regression are correlated to each other. Under this situation and depending on the degree of correlation among the predictors, ordinary-least-squares estimates of the parameters can be unstable against minor changes in the input data and also display large standard errors. To better illustrate this problem, consider the simple linear modelwhere y is the response variable, x 1 and x 2 are the predictors, and a 1 and a 2 are the linear coefficients. The least-squares estimator for the parameters is usually written as, whereis an n × 1 vector of the response variables, is an n × 2 matrix of the regressors, and n is the number of observations. If the values of predictors are strongly correlated, the inversion of the matrix X T X can become unstable, and consequently lead to unstable estimates for the linear coefficients.

---

### All-ferroelectric implementation of reservoir computing [^5359e6a3]. Nature Communications (2023). High credibility.

Fig. 6
Waveform classification and Hénon map prediction.

a Schematic illustration of the mask process used to generate virtual nodes. b Inputs of sine and square waveforms (upper panel) and their classification results obtained from the all-ferroelectric RC system (lower panel). c, e Predicted time series versus ideal targets and d, f corresponding 2D plots on the c, d training and e, f test sets. The predicted results in (c – f) are obtained from the all-ferroelectric RC system.

As shown in Fig. 6b, the trained all-ferroelectric RC system can correctly classify the sine and square waveforms into their corresponding categories with an NRMSE of 0.13. This NRMSE value is sufficiently low, even lower than the value of 0.2 reported recently in an α-In 2 Se 3 FeFET-based RC systemwhich used high-precision floating-point weights for the readout network (note: in our work, the readout weights are mapped onto the measured conductances of nonvolatile FDs). Such low NRMSE of our all-ferroelectric RC system is attributed to the capability of the volatile FD-based reservoir to produce sufficiently high feedback strength and state richness (Supplementary Fig. S27).

Hénon map prediction

To further evaluate the performance of our all-ferroelectric RC system on temporal signal processing, a benchmark task for time-series prediction, i.e. Hénon map prediction was demonstrated. The Hénon map is a typical discrete-time dynamic system exhibiting chaotic behavior. It takes a point (x (n), y (n)) in the 2D plane and maps it to a new point (x (n + 1), y (n + 1)) through the equations below:where w (n) is a Gaussian noise whose mean value and standard deviation are 0 and 0.05, respectively. The task is to predict (x (n + 1), y (n + 1)), given the (x (n), y (n)) values up to the time step n. Substituting Eq. (2) into Eq. (1) results in an equation containing only x, and hence the task becomes the prediction of x (n + 1) based on the x (n) and x (n –1) values.

---

### KDIGO 2024 clinical practice guideline for the evaluation and management of chronic kidney disease [^4f283604]. Kidney International (2024). High credibility.

KDIGO 2024 CKD — Recommendation 2.2.1 for CKD G3–G5 risk prediction states: In people with CKD G3–G5, we recommend using an externally validated risk equation to estimate the absolute risk of kidney failure (1A). The rationale places "a high value on the need and potential benefits for individual risk prediction to deliver personalized care for people with CKD" and encourages going "beyond broad categories of RR" to estimate absolute risk. Key information notes "a large body of evidence" supporting validated equations to estimate the risk of kidney failure requiring dialysis or transplant in CKD G3–G5, and that such equations have been "developed, externally validated, and implemented in labs, EMRs, and health systems".

---

### An alternative to the breeder's and lande's equations [^10b90a03]. G3 (2014). Low credibility.

The breeder's equation is a cornerstone of quantitative genetics, widely used in evolutionary modeling. Noting the mean phenotype in parental, selected parents, and the progeny by E (Z 0), E (Z W), and E (Z 1), this equation relates response to selection R = E (Z 1) − E (Z 0) to the selection differential S = E (Z W) − E (Z 0) through a simple proportionality relation R = h 2 S, where the heritability coefficient h 2 is a simple function of genotype and environment factors variance. The validity of this relation relies strongly on the normal (Gaussian) distribution of the parent genotype, which is an unobservable quantity and cannot be ascertained. In contrast, we show here that if the fitness (or selection) function is Gaussian with mean μ, an alternative, exact linear equation of the form R ′ = j 2 S ′ can be derived, regardless of the parental genotype distribution. Here R ′ = E (Z 1) − μ and S ′ = E (Z W) − μ stand for the mean phenotypic lag with respect to the mean of the fitness function in the offspring and selected populations. The proportionality coefficient j 2 is a simple function of selection function and environment factors variance, but does not contain the genotype variance. To demonstrate this, we derive the exact functional relation between the mean phenotype in the selected and the offspring population and deduce all cases that lead to a linear relation between them. These results generalize naturally to the concept of G matrix and the multivariate Lande's equation. The linearity coefficient of the alternative equation are not changed by Gaussian selection.

---

### The value of understanding feedbacks from ecosystem functions to species for managing ecosystems [^8044a087]. Nature Communications (2019). High credibility.

Ecosystem dynamics and transition probabilities

The ecosystem dynamics are captured in the transition probability matrix in MDP. Let P be the transition probability matrix representing the dynamics of the system from time step t to time step t + 1.represents the conditional probability of the ecosystem transitioning from state x t to x t +1 given action a t is implemented at time t. We assume that species j could be present or absent at each time step. This transition probability is also conditional on the baseline probability of survival of species j, the feedback strength α (the percentage of the ecosystem function going back to a species), the predation strength b, the feedback structure f and the food web matrix M representing the prey-predator interactions of our system. To model this transition probability, we assume that, knowing the state at time t, x t, the state of species j at time t + 1 is independent of the state of the other species at time t + 1. So we can define the transition probability P as the product of J individual species' transition probabilities:

Survival probability of a species will increase with the number of extant preys N prey (j, x t, M) and ecosystem function available N EF (j, x t, f), and will decrease with the number of extant predators N predator (j, x t, M). We assume that N prey (j, x t, M), N EF (j, x t, f), and N predator (j, x t, M) are maximum at the initial time step where all species are present (i.e. x t = x 0 = [1,1,1,1]). Formally, we defined the transition probability when species j is not under protection (a t ≠ j) as the product of four terms:

In this way, under the most favourable condition where species j has no predator, no prey loss and receive maximum level of ecosystem function, the above equation reduces to its baseline probability of survival. However, species j survival probability will decrease when at least one of the following three events happen — prey loss, predator presence, or insufficient functional support (see 'Using Markov Decision Processes to model species dynamics and protection actions effects' in the Supplementary Methods).

---

### KDIGO 2024 clinical practice guideline for the evaluation and management of chronic kidney disease [^197efc53]. Kidney International (2024). High credibility.

Values and preferences, resource use, and implementation — Using validated eGFR equations improves the accuracy of assessment of true GFR but remains imperfect, and no single equation performs consistently across all populations; the Work Group judged that people with CKD and their healthcare providers would want GFR estimated using the equation providing the greatest accuracy in the population of their geographical region, and there are several valid equations that can reasonably be used in local settings. Each region should have a mechanism for review and selection of equations for implementation by laboratories, and decisions at this level by continental or national organizations are likely to minimize the likelihood that decisions for equation use will be made within small geographical areas governed by local decisions, leading to greater variation in eGFR and uncertainty by people with CKD and healthcare providers. There are likely to be tradeoffs between optimal accuracy in local regions versus uniformity; equations optimized for a specific region can help to ensure that the GFR thresholds for disease definition, classification, and risk estimation have the same implications across regions; however, it would lead to barriers to implementation, as it will not be possible for all regions to conduct a sufficiently large and representative study to evaluate these equations and develop modifications. If not possible, or in the interim, we advise using equations that were developed in populations most similar to the available populations. Resource use includes initial costs such as human resource costs and education for providers, which incurs both direct and indirect costs.

---

### Size limits the sensitivity of kinetic schemes [^71d1065f]. Nature Communications (2023). High credibility.

A different measure of sensitivity — the amplification of a fold-change in the input — provides a solution to this problem. Suppose that for some value x 0 of the input parameter x, scaling by a factor a scales the output by b, so f (a x 0) = b f (x 0). Then the quotientcan be thought of as a discrete approximation of the derivative defining the logarithmic sensitivity. And if f (x) is differentiable everywhere, then by the mean value theorem, there must be a value x * of x for whichThis means that careful measurement of any two points on the input-output curve (x versus f (x)) witnesses the (local, infinitesimal) logarithmic sensitivity somewhere. Importantly — unlike in the case of fitting to a Hill function — if error in the measurements is very low, then they are also telling us the derivative for some value of x very accurately.

Equation (25) leads us to another common definition of the effective Hill coefficient:where S 0.9 and S 0.1 are the values of the input variable (in our case, x) required to get 90% and 10% (respectively) of the maximum value of the output variable (in our case, f (x)). Note that (26) is like (25) with a = S 0.9 / S 0.1 and b = 9. It implies that somewhere between S 0.1 and S 0.9 there is a logarithmic sensitivity of H eff /2.

There is yet another common definition, specific to models of binding. Suppose x is the concentration of a ligand and 〈 n b 〉(x) is the expected number of sites bound by a ligand out of a total of n possible binding sites. It is common then, to takeor to report, as the Hill coefficient, the slope of a line fitted to x versusdata on a log-log plot.

---

### Identifiability analysis of second-order systems [^b99ab947]. Nuclear Medicine and Biology (2003). Low credibility.

Models provide a means to represent our understanding of the interaction among the components of a system. Individual instantiations of a system are captured by allowing the model to have parameters that may be different for each instantiation. Included in the model is a representation of the means to observe certain aspects of a system. Given the observations, successful estimation of the parameters requires that the parameters be identifiable. This is an important consideration for potential use of the model in diagnostic medicine or analysis and planning of experiments. Models considered here are typical of those used to describe flows and concentrations associated with ligand-receptor interactions.

---

### How to personalise ventilation of infants with congenital diaphragmatic hernia? A simulation study [^250ca774]. BMC Pediatrics (2025). Medium credibility.

Inhomogeneity extent versus C W /C L

Based on the data taken from the study by Cressoni et al. we search for the dependency between lung inhomogeneity extent (IE) and the ratio of chest wall (C W) and lung compliance (C L). In the mentioned study, the changes in chest wall elastance (E w = 1/C w) and lung elastance (E L = 1/C L) were examined before and after VILI in 12 piglets. IE (% of lung volume) was assessed from CT scans conducted before and within 20 h after VILI. The obtained relationship of IE vs. C W /C L was approximated by a curve equation with the R 2 determination coefficient.

This part of the study was aimed to evaluate the obtained results and to show the similarity of dependencies: T 1 /T 2 vs. C W /C L in CDH and IE vs. C W /C L in ARDS.

Assessment of VI-degree and C W /C L in clinical cases

Based on the equation Eq. 2 of the non-linear regression (NLR) model and found model coefficients (b 0, …, b n), we searched for the pairs of T 1 /T 2 (VI degree) and C W /C L ratio for three CDH patients (P 1, P 2, P 3) from by solving the set of two equations for two output variables (PIP and MAP) with two unknowns (T 1 /T 2 and C W /C L). The PIP or MAP played the role of Y in Eq. 2. The values of PIP, MAP, R rs, C RS and RR were the bed-side measured parameters values. A separate set of equations was solved for each patient.

Comparison of estimated and clinical parameters

The estimated and clinical parameters of ventilation: PIP-PEEP, MAP and Z were compared using Friedman's ANOVA test, whereas WOB vt received from NLR and MR models using signed rank test and Wilcoxson's test; WOB vt was measured only under laboratory conditions.

Driving pressure

The value of PIP-PEEP served as an approximation of driving pressure (ΔP = P plat -PEEP). The plateau pressure (P plat) is rarely observed in newborns due to high RR. It was not determined at the bed-side; the data were collected retrospectively. We compare the ΔP in patients P 1, P 2 and P 3, using the ANOVA Kruskal-Wallis test.

---

### Energy scaling of targeted optimal control of complex networks [^03fbeb78]. Nature Communications (2017). Medium credibility.

While most dynamical networks that arise in science and engineering are governed by nonlinear differential equations, the fundamental differences between individual networks and the uncertainty of precise dynamics make any substantial overarching conclusions difficult. Nonetheless, linear controllers have proven to be adequate in many applications by approximating nonlinear systems as linear systems in local regions of the n -dimensional state space. We examine linear dynamical systems, as it is a necessary first step to understanding how target control may benefit nonlinear systems. The linear time invariant network dynamics are

where x (t) = [x 1 (t), …, x n (t)] T is the n × 1 time-varying state vector, u (t) = [u 1 (t), …, u m (t)] T is the m × 1 time-varying external control input vector and y (t) = [y 1 (t), …, y p (t)] T is the p × 1 time-varying vector of outputs, or targets. The n × n matrix A = { a ij } is the adjacency matrix described previously, the n × m matrix B defines the nodes in which the m control input signals are injected, and the p × n matrix C expresses the relations between the states that are designated as the outputs. In addition, the diagonal values of A, a ii, i = 1, …, n, which represent self-regulation, such as birth/death rates in food webs, station keeping in vehicle consensus, degradation of cellular products and so on, are chosen to be unique at each node (see proposition 1 in ref.). These diagonal values are chosen to also guarantee that A is Hurwitz so the system in equation (1) is internally stable. We restrict ourselves to the case when B (C) has linearly independent columns (rows) with a single non-zero element, that is, each control signal is injected into a single node (defined as an input node) and each output is drawn from a single node (defined as a target node). Our particular choice of the matrix C is consistent with target control, as our goal is to individually control each one of the target nodes. Our selection of the matrix B is due to our assumption that different network nodes may be selectively affected by a particular control signal, for example, a drug interacting with a specific node in a protein network. Note that in today's information-rich world, a main technological limitation is not generating control signal, but rather placing actuators at the input nodes; hence our assumption that each actuator is driven by an independent control signal is sound. We defineas the subset of target nodes andas the number of target nodes. A small sample schematic is shown in Fig. 1a, which demonstrates the graphical layout of our problem emphasizing the graph structure and the role of input nodes and targets. Here by an input node, we mean a node that directly receives one and only one control input such as nodes 1 and 2 in Fig. 1a. The explicit equation for the time evolution of the outputs is

---

### How to personalise ventilation of infants with congenital diaphragmatic hernia? A simulation study [^49fbd1f8]. BMC Pediatrics (2025). Medium credibility.

Regression models

Parameters of multiple regression (MR) and non-linear Regression (NLR) model describing PIP, MAP, Z and WOB vt as a function of the T 1 /T 2 index, RR, C rs, R rs and the C W /C L ratio, were found using STATISTICA.

The equation of the multiple regression model (Eq. 1) and the non-linear regression model (Eq. 2) are as follows:

where: Y is PIP, MAP, Z or WOB vt, b 0,… b n are coefficients of regression model, T 1 /T 2 is an index of lung inhomogeneity, C W /C L is the ratio of the chest wall compliance to lung compliance, RR is respiratory rate, R rs is total airway resistance of respiratory system, C rs stands for the total compliance of the respiratory system and SE Est stands for the standard error of estimation.

The values of the C W /C L ratio were obtained analytically based on the non-linear regression model equation (Eq. 2), by solving the set of equations of PIP, MAP, Z and WOB vt, at constant values of C rs, R rs and T 1 /T 2, separately for each respiratory rate (RR); see supplement (Additional file 1).

Subsequently, the functional dependencies between the C W /C L ratios and the T 1 /T 2 index of VI-degree were searched for each RR (40, 45, 50 and 55 bpm). Then, determination coefficient (R 2) and Spearman's correlation coefficient (R s) were found for these dependencies.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^dee6cacc]. CDC (2011). Medium credibility.

Community viral load sample size calculations — sample size to detect differences in geometric mean (GM) viral load depends on power and standard deviation (S). The table specifies "α = 0.05 and W = 0.8", with S columns 1 to 1.5; for k = 3 the minimum sample sizes by S are 54, 66, 78, 92, 106, and 122, and "1.2 is the standard deviation of national VL data", so at S = 1.2 the needed sample size is 78.

---

### Recommendations for a standardized pulmonary function report. An official American Thoracic Society technical statement [^345386c9]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Selecting and reporting reference values — procedural guidance states that PFT laboratories must select appropriate reference values generated from high-quality data from a large sample of healthy asymptomatic individuals who have never smoked or had other respiratory illness or significant exposures, reports should identify the source of reference values, any change in selected equations should be noted and prior percent predicted values recalculated if possible, pediatric–adult equation discontinuity is discouraged, and extrapolation beyond the age range should not be done during growth, increases uncertainty in the elderly, and must be noted in technician comments.

---

### Estimation in medical imaging without a gold standard [^82dda7fe]. Academic Radiology (2002). Low credibility.

Rationale and Objectives

In medical imaging, physicians often estimate a parameter of interest (eg, cardiac ejection fraction) for a patient to assist in establishing a diagnosis. Many different estimation methods may exist, but rarely can one be considered a gold standard. Therefore, evaluation and comparison of different estimation methods are difficult. The purpose of this study was to examine a method of evaluating different estimation methods without use of a gold standard.

Materials and Methods

This method is equivalent to fitting regression lines without the x axis. To use this method, multiple estimates of the clinical parameter of interest for each patient of a given population were needed. The authors assumed the statistical distribution for the true values of the clinical parameter of interest was a member of a given family of parameterized distributions. Furthermore, they assumed a statistical model relating the clinical parameter to the estimates of its value. Using these assumptions and observed data, they estimated the model parameters and the parameters characterizing the distribution of the clinical parameter.

Results

The authors applied the method to simulated cardiac ejection fraction data with varying numbers of patients, numbers of modalities, and levels of noise. They also tested the method on both linear and nonlinear models and characterized the performance of this method compared to that of conventional regression analysis by using x-axis information. Results indicate that the method follows trends similar to that of conventional regression analysis as patients and noise vary, although conventional regression analysis outperforms the method presented because it uses the gold standard which the authors assume is unavailable.

Conclusion

The method accurately estimates model parameters. These estimates can be used to rank the systems for a given estimation task.

---

### A deeper understanding of system interactions can explain contradictory field results on pesticide impact on honey bees [^10039fa2]. Nature Communications (2022). High credibility.

The next step is the characterization of the structural influence matrix, which corresponds to the sign pattern of the adjoint of the negative Jacobian matrix in Proposition 1.

To this aim, we first consider the linearized system and write it in a matrix-vector formwhereis the time derivative of the four-dimensional vectorand, is an input vector, constant in time, with a single non-zero component, the-th, equal to 1, while the scalaris the magnitude of the input. We wish to assess the-th component of. Ifis Hurwitz, as assumed, the steady-state value of variabledue to the input perturbationapplied to the equation of variableis achieved for

namelywhich implies that the sign of the steady-state valueof variabledue to a persistent positive input acting on the-th equation has the same sign as, theentry of matrix. Since we assume Hurwitz stability, we have thatis positive, hence the sign pattern of the inversecorresponds to the sign pattern of the adjoint. In fact.

We next consider the nonlinear system under investigation, which we write in the formand without restriction we assume that the zero vector is an equilibrium point: This condition can be always achieved, without loss of generality, by a translation of coordinates. We also consider a stable equilibrium: we assume that the linearized system at the equilibrium is asymptotically stable, namely its Jacobian, which has the sign pattern considered in Proposition 1 above, is Hurwitz. We also assume that a constant input perturbation of magnitudeis applied to the system, affecting the-th equation, i.e.and that the perturbation is small enough to keep the state in the domain of attraction of the considered equilibrium. Due to this perturbation, a new steady stateis reached that satisfies the condition

To determine the sign of the new equilibrium components, we consider this new equilibrium vector as a function ofin a small interval. Adopting the implicit function theorem yieldswhere we have denoted bythe Jacobian matrix computed at the perturbed equilibrium. Hence, forsmall enough, the sign of the derivatives of the entries of the new, perturbed equilibrium are, structurally, the same as those in the-th column of matrix. Since, by construction, this is also the sign of the elements of vector, forin the interval.

---

### Single-trial dynamics of motor cortex and their applications to brain-machine interfaces [^fa4272da]. Nature Communications (2015). Medium credibility.

Kinematic-state Kalman filter. The KKF models the kinematics at time k, x k, as the state of a linear dynamical system, while the simultaneously observed neural population spike counts, y k, are the corresponding output of the system. This is represented by the two equations,

where w k and q k are zero mean Gaussian noise terms with covariance matrices W and Q, respectively. As the sequences { x k } k = 1, …, K and { y k } k = 1, …, K were observed in the training set while w k and q k are zero mean terms, A and C can be learned via least-squares regression: A and C can be calculated as:and C = YX T (XX T) −1. After learning A and C, W was calculated as the sample covariance of the residuals X [2: K] – AX [1: K −1], while Q was analogously the sample covariance of the residuals Y – CX. Given these parameters, and an initial condition, the Kalman filter is a recursive algorithm which estimates the state at time k, given the previous state estimate, and the current observation, y k. A strength of this construction is the smoothness in the kinematics afforded by modelling kinematic update laws in the matrix A. However, we note that this model does not capture any temporal structure in the neural population activity. Whilereflects, to an extent, the history of the neural data in thatcan be written as a linear combination of y k, y k −1, …, y 1, the temporal evolution ofis governed by the linear dynamics of the kinematics, and does not take into account any temporal correlations in the neural data. When presenting decodes to the monkey, we found that a pure velocity Kalman filter performed inferiorly to one where the position is decoded as in equation (6).

---

### Topological magnon amplification [^a71ea48a]. Nature Communications (2019). High credibility.

Methods

Numerical calculation

For the numerical calculation, we choose a manifestly inversion-symmetric system obtained by deleting the lowest row of sites, a situation that is depicted in Fig. 1, where the tip of the lowest blue triangle is part of a unit cell whose other sites are not included. For example, repeating the star shown in Fig. 1 along x would result in an inversion-symmetric strip with W = 3. A Fourier transform of Eq. (5) along x yields a 3 W − 2 by 3 W − 2 Hamiltonian matrix for each momentum k

Note that we take ħ = 1. Diagonalizing this matrix yields single-particle energy eigenstates with annihilation operator b k, s, and a Hamiltonian. The resulting band structure is shown in Fig. 1. In our convention, the lowest bulk band has Chern number sgn D z, the middle bulk band 0 and the top bulk band −sgn D z (calculated, e.g. through the method described in ref.). Accordingly, there is one pair of edge modes in each of the bulk gaps, one right-moving localized at the lower edge and one left-moving at the upper.

Including the anomalous terms obtained from a calculation based on the Fermi-Hubbard model at half filling yields the full Hamiltonian Eq. (7). By means of a Bogoliubov transformation we obtain the magnon band structure and the unstable states, which form the basis for Fig. 2. The inclusion of nonlinear damping yields Eq. (8), which has been used to calculate Fig. 3. In the end, we calculate the current by evaluating the expectation value of the particle current or energy current operator, which can be obtained for a given bond from the continuity equation. The current across a certain cut of the system is obtained by summing the current operators for all the bonds that cross it. As the system we study is inversion symmetric, the total current in the x direction vanishes. In order to specifically find the edge current, we thus define a cut through half of the system, for example from the top edge to the middle.

---

### Mathematical modeling is more than fitting equations [^994275ea]. The American Psychologist (2014). Low credibility.

Comments on the comments made by Brown et al. (see record 2013-24609-001). The article by Brown et al. regarding the Fredrickson and Losada (see record 2005-11834-001) article discussed the use of differential equations in science and repeated our earlier observation (Luoma, Hämäläinen, & Saarinen, 2008) that there is lack of justification for the use of the Lorenz equations in the latter article. In this comment we want to point out that Brown et al. presented a very narrow view on mathematical modeling in behavioral research. We describe how the conceptual use of mathematical models is essential in many fields.

---

### KDIGO 2024 clinical practice guideline for the evaluation and management of chronic kidney disease [^018f3731]. Kidney International (2024). High credibility.

KDIGO 2024 — resource use and costs for adopting GFR estimating equations include initial human resource and technical information needs to decide which equation to use, to consider any computation change, and to test and communicate the change; education for primary care providers, people with CKD, and other healthcare providers that incurs both direct and indirect costs; decision-making costs including human resource and meetings costs; and additional costs if validation and impact studies are required.

---

### KDIGO 2022 clinical practice guideline for diabetes management in chronic kidney disease [^bfaebbd4]. Kidney International (2022). High credibility.

Figure 5 — assessment of kidney function in glomerular disease — uses ml/min per 1.73 m² units and notes that the correction coefficient for race in GFR estimating equations is controversial and discussions are ongoing, referring readers to the KDIGO CKD guideline for more information.

---

### Reference equations for pulmonary function testing in healthy Chinese children aged 4–18 years [^b368db47]. Respiratory Research (2025). Medium credibility.

Fig. 5
Mean (95% confidence interval) z-scores for forced expiratory volume in 1 s (FEV1) derived from different predictive equations. Model1, Model2: predictive equations derived from current study. Results for W. T. Wang (mean z-score, -1.648; 95%CI, -1.677,-1.618), and Kate C Chan (mean z-score 3.189, 95%CI 3.161, 3.218) among girls not shown because the value is off-scale. GLI, Global Lung Function Initiative; NEA, North East Asian; SEA, South East Asian

Fig. 6
Mean (95% confidence interval) z-scores for forced vital capacity (FVC) derived from different predictive equations. Model1, Model2: predictive equations derived from current study. Results for W. T. Wang (mean z-score for boys, -2.794; 95%CI, -2.853, -2.467; mean z-score for girls, -2.291; 95%CI, -2.350, -2.320), and Kate C Chan (mean z-score for boys, 3.222; 95%CI, 3.164, 3.193; mean z-score for girls, 2.734; 95%CI, 2.679, 2.707) not shown because the value is off-scale. GLI, Global Lung Function Initiative; NEA, North East Asian; SEA, South East Asian

---

### Recommendations for a standardized pulmonary function report. An official American Thoracic Society technical statement [^d6d8fcbc]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Spirometry reporting specifies that numerical values are given for the FEV1, the FVC, and the FEV1/FVC ratio; the latter should be reported as a decimal fraction and the space for percent predicted value left blank, and if bronchodilators are given the LLN column need not be repeated with absolute and percent change given only for FEV1 and FVC. Other numerical values such as the forced inspiratory flow at 75% of FVC (FEF75%) and FEF25–75% are not recommended for routine use. Graph requirements include that for the volume–time curve the volume scale should be at least 10 mm/L, the time scale at least 20 mm/s, and 1 second prior to the start of expiration should be displayed; on the flow–volume plot the flow display should be at least 5 l/min/L/s, and the ratio of flow to volume should be 2 L/s to 1 L, and linear and log scales where values are plotted as z-scores relative to the predicted value (z = 0) give an intuitive sense of severity.

---

### Predictive model and software for inbreeding-purging analysis of pedigreed populations [^4286a4af]. G3 (2016). Low credibility.

Therefore, from Equation 2, and assuming that ε and F are independent, the expected fitness of an individual i that has genealogical inbreeding F i iswhere E (W 0) = E [W max (1− ε)] is the expected fitness of a noninbred individual. The equation above can be rewritten asand can be rearranged asNoting thatadds up all the probabilities for a Poisson distribution with mean λ (1−2 d) (i.e. it equals 1), and, since λ = F i δ /2 d (Equation 3), we obtain the exponential expressionand, similarly, the average fitness of a population with average inbreeding F t in generation t, as far as the number of loci homozygous for a deleterious allele per individual can be assumed to be Poisson distributed with mean λ = F t δ /2 d, is

In order to estimate δ from observed inbreeding depression, logarithms are usually taken in Equation 4 or Equation 5 to obtain a linear model of the kind ln(W) = ln(W 0)− δ F. However, since the average of the logarithms of a variable is smaller than the logarithm of the average (see Jensen's inequality), applying this procedure to individual fitness values can produce large upwards bias in the estimate of δ. Thus, from Equation 2, the logarithm of fitness (log-fitness hereafter) for an individual that is homozygous by descent for n deleterious alleles isso that, using the Poisson distribution of n i, the expected value for log-fitness for an individual i that has genealogical inbreeding F i iswhere the intercept E [ln(W 0)] = E {ln[W max (1− ε)} represents the average of individual log-fitness at the noninbred population. Since the second term equals ln(1−2 d) E (n i), using Equation 3, Equation 6 gives

---

### Restoration of rhythmicity in diffusively coupled dynamical networks [^20b021a8]. Nature Communications (2015). Medium credibility.

Next, we explore the capability of the feedback factor α in effectively retrieving rhythmic oscillations in systems where OD has been observed. Consider the following system of two Stuart–Landau oscillators via the x component,

where p j = 1−| Z j | 2 = 1− x j 2 − y j 2, j, k = 1,2 (j ≠ k). Here the one-dimensional diffusive coupling involves only the real parts. This coupling form breaks the rotational symmetry of the coupled system, which is a necessary condition for the emergence of OD. Besides the HSS at the origin Z 1 = Z 2 = 0 in the system (Equation 12), the system also displays an IHSS (x *, y *, − x *, − y *) that appears at K = (1+ w 2)/(1+ α) via a pitchfork bifurcation, withandwith. For the normal diffusive coupling, α = 1, it was reported that the origin is unstable for all K > 0, but the IHSS (OD) is stabilized for K > w 2 +1/4 (refs,); this is demonstrated by the bifurcation diagram plotted in Fig. 4a with w = 10. The bold red lines denote the stable branches of the IHSS and the thin lines refer to the unstable ones. We further depict the bifurcation diagrams of the steady-state solutions for α = 0.999, 0.998, 0.997, 0.996 and 0.995 in Fig. 4b–e, respectively. Astonishingly, we observe that even a very tiny detuning in α from 1 shrinks the coupling intervals of the stable IHSS (OD) (Fig. 4b–e). Moreover, no stable IHSS can be found for any coupling strength when α < α min ≈0.995 (Fig. 4f), which attributes that OD is successfully revoked and thus oscillations are provoked in the OD regimes. The effect of the limiting factor α in restoration of rhythmic activity in this case is even more pronounced, since infinitesimal changes of α from unity lead to a sharp shrinkage of the stable IHSS regime, demonstrating the high efficiency of the proposed method in restoring oscillatory activity.

---

### KDIGO 2024 clinical practice guideline for the evaluation and management of chronic kidney disease [^7ddbed5b]. Kidney International (2024). High credibility.

KDIGO 2024 chronic kidney disease (CKD) G3–G5 — use of validated risk equations: In people with CKD G3–G5, we recommend using an externally validated risk equation to estimate the absolute risk of kidney failure (1A). The recommendation places a high value on individual risk prediction to deliver personalized care and cites a large body of evidence supporting validated equations that have been developed, externally validated, and implemented in labs, EMRs, and health systems.

---

### Analog reservoir computing via ferroelectric mixed phase boundary transistors [^95a6ea58]. Nature Communications (2024). High credibility.

Time-series prediction task

The Hénon map represents a nonlinear 2D mapping that converts a point (x (n), y (n)) on a plane into a new point (x (n + 1), y (n + 1)) following the equations below, :where w (n) represents Gaussian noise with a mean value of zero and a standard deviation of 0.05. The task is to predict the point (x (n + 1), y (n + 1)) at the subsequent time step n + 1, given the points (x (n), y (n)) up to the time step n. By combining the Eqs. (1) and (2), the Hénon map is reformulated into a 1D representation as follows:

Consequently, the task is refined to predict the value of x (n + 1) based on the known values of x (n) and x (n − 1). For Hénon map prediction, a dataset with a length of 500 consisting of x (n) following the above equations was utilized.

The masking process generates N input pulse trains, each consisting of M timeframes, for both input x (n) and x (n − 1). Therefore, a total of 2 N volatile DG MPBTFT-based physical reservoirs generate 2 N × M virtual node states. These virtual node states are fed into a (2 N × M) × 1 fully connected single-layer readout network. The readout network outputs a predicted value of x (n + 1) by linearly combining all the reservoir states. The parameters M and N are set to 3 and 15, respectively.

Readout network training

The readout network establishes effective mapping between high-dimensional reservoir states and outputs. The readout network within the ARC system was trained using the stochastic gradient descent (SGD) method for handwritten digit recognition tasks. For waveform classification and time-series prediction tasks, a linear regression method was used to train the readout network. The weight matrix (W out) of the readout network was obtained using the following equation:where X and Y represent the input and target matrices, respectively. The symbol † represents the Moore–Penrose pseudo-inverse. In handwritten digit recognition, waveform classification, and time-series prediction tasks, pre-trained synaptic weights are transferred to the conductance of FeTFTs within the readout network and remain unchanged throughout system operations.

---

### KDIGO 2024 clinical practice guideline for the evaluation and management of chronic kidney disease [^aedab333]. Kidney International (2024). High credibility.

Practice point — Use of race in computation of eGFR: Practice Point 1.2.4.2 states "Use of race in the computation of eGFR should be avoided". Background context notes that estimating equations have historically incorporated demographic variables of age, sex, and race to explain variation in serum concentrations of endogenous filtration markers, and that the 2009 CKD-EPI equation included age, sex, and race based on observed higher average serum creatinine at the same measured GFR in older versus younger individuals, males versus females, and Black versus non-Black people.

---

### Quantifying dissipation using fluctuating currents [^2479585e]. Nature Communications (2019). High credibility.

Suppose, however, that it is not simple to measure the heat flux. Rather, we imagine directly observing the bead positions as a function of time. Those measurements are sufficient to extract the entropy production rate, but to do so we must go beyond the thermodynamics and explicitly consider the system's dynamics, an approach known as stochastic thermodynamics. The starting point is to mathematically describe the bead-spring dynamics with a coupled overdamped Langevin equation, where x = (x 1, x 2) T is the vector consisting of each bead's displacement from its equilibrium position, ξ = (ξ 1, ξ 2) T is a vector of independent Gaussian white noises, andThe matrix A captures deterministic forces acting on the beads due to the springs, while F describes the random forces imparted by the medium. The strength of these random forces depends on the temperature and the Boltzmann constant k B, consistent with the fluctuation-dissipation theorem.

It is useful to cast the Langevin equation as a corresponding Fokker-Planck equation for the probability of observing the system in configuration x at time t, ρ (x, t):with D = FF T /2. Though we are modeling a two-particle system, it can be helpful to think of the entire system as being a single point diffusing through x space with diffusion tensor D and with deterministic force γ Ax. The second equality in Eq. (3) defines the probability current j (x, t). These probability currents (and their fluctuations) will play a central role in our strategies for inferring the rate of entropy production.

Due to its analytic and experimental tractability, this bead-spring system and related variants have been extensively studied as models for nonequilibrium dynamics –. In particular, the steady-state properties are well-known. Correlations between the position of bead i at time 0 and that of bead j at time t are given by C ij (t) = 〈 x i (0) x j (t)〉. The expectation value is taken over realizations of the Gaussian noise to giveThe steady-state density and current are expressed simply asin terms of the long-time limit of the correlation matrix

---

### Long-term memory induced correction to arrhenius law [^2938019f]. Nature Communications (2024). High credibility.

METHODS

Numerical methods

To generate stochastic trajectories x (t n) satisfying the GLE (2) at sampling times t n = n × d t, a modified version of the circulant matrix algorithmdescribed in ref.was used. This algorithm enables one to generate trajectories of N points with a computational complexity. The value of the maximal time of the simulations and the time steps were systematically varied to check that the measured properties of the FPT do not depend on such values. Next, to determine the FPT, we have used a theory relying on the analysis of trajectories after the FPT, assumed to display Gaussian statistics with the same covariance function as the original stochastic process. The control of such approximations can be found in SI (Sections D for numerical check and E for a perturbative analysis for weakly non-Markovian processes). To obtain numerical predictions for the mean FPT, equations (7) and (8) were integrated numerically by evaluating the integrals over a non-uniform mesh (with more concentrated points near the origin) using the trapeze method; the number of points in the mesh and the maximal time were systematically varied to make sure that predictions do not depend on the properties of the mesh.

---

### The growth equation of cities [^a74adc40]. Nature (2020). Excellent credibility.

The science of cities seeks to understand and explain regularities observed in the world's major urban systems. Modelling the population evolution of cities is at the core of this science and of all urban studies. Quantitatively, the most fundamental problem is to understand the hierarchical organization of city population and the statistical occurrence of megacities. This was first thought to be described by a universal principle known as Zipf's law 1,2; however, the validity of this model has been challenged by recent empirical studies 3,4. A theoretical model must also be able to explain the relatively frequent rises and falls of cities and civilizations 5, but despite many attempts 6–10 these fundamental questions have not yet been satisfactorily answered. Here we introduce a stochastic equation for modelling population growth in cities, constructed from an empirical analysis of recent datasets (for Canada, France, the UK and the USA). This model reveals how rare, but large, interurban migratory shocks dominate city growth. This equation predicts a complex shape for the distribution of city populations and shows that, owing to finite-time effects, Zipf's law does not hold in general, implying a more complex organization of cities. It also predicts the existence of multiple temporal variations in the city hierarchy, in agreement with observations 5. Our result underlines the importance of rare events in the evolution of complex systems 11 and, at a more practical level, in urban planning.

---

### Effects of changing population or density on urban carbon dioxide emissions [^49635916]. Nature Communications (2019). High credibility.

To account for the multicollinearity problem, we have fitted Eqs. (3) and (5) by using the ridge regression approach. This method solves the matrix inversion problem by adding a constant λ to the diagonal elements of X T X, so that the ridge estimator for the linear coefficients is a = (X T X + λ I) −1 X T y, where I is the identity matrix. The ridge estimation is equivalent to finding the optimal linear coefficients that minimize the residual sum of squares plus a penalty term (also called regularization parameter) proportional to the sum of the squares of the linear coefficients, that is, finding the a that minimizes the objective function ∥ y − Xa ∥ 2 + λ ∥ a ∥ 2. The optimal value of λ is usually unknown in practice and needs to be estimated from data. To do so, we have used the approach of searching for the value of λ that minimizes the mean squared error (MSE) in a leave-one-out cross validation strategy. In this approach, we estimate a (for a given λ) using all data except for one point that is used for calculating the squared error. This process is repeated until every data point is used exactly once for estimating the squared error, and then we calculate the value of the MSE for a given λ. The optimal value of λ = λ * is the one that minimizes the average value of the MSE estimated with the leave-one-out cross validation method. We have also standardized all predictors before searching for the optimal value λ *. This is a common practice when dealing with regularization methods and ensures that the penalty term is uniformly applied to the predictors, that is, the normalization makes the scale of the predictors comparable and prevents variables with distinct ranges from having uneven penalization.

---

### Mutation load estimation model as a predictor of the response to cancer immunotherapy [^f2b38e35]. NPJ Genomic Medicine (2018). Low credibility.

Construction of the mutation load estimation model

Based on the selected candidate genes, a linear mathematical model was used to estimate the mutation load:where y m is the mutation load of the m -th patient, x mi, i = 1, …, n, indicates the mutation count of the selected model gene i in the m -th patient, a i, i = 1, …, n, represents the weighting of each selected model gene i on the mutation load, c specifies the constant term, and e m is the model uncertainty for the m -th patient. The equation shows that the mutation load of a patient can be calculated using the mutation counts of the selected model genes multiplied by the corresponding weightings and adding the constant term and the model uncertainty.

In the mutation load estimation model shown in equation (2), the mutation load y m and the mutation counts of the selected genes x mi can be obtained from the generated mutation matrix. On the other hands, the weighting of each selected gene a i and the constant term c represent the model parameters that had to be identified. Subsequently, least squares parameter estimation method was employed for parameter identification and BIC was used for model selection. BIC is a model selection criterion widely used in the field of system identification. It measures the trade-off between the estimated error and model complexity. The model with the lower value of BIC can estimate the mutation load more precisely without including too many genes in the model. Therefore, the model with the minimal BIC statistics was selected as the most appropriate mutation load estimation model. Details are presented in Supplementary Methods.

---

### Mapping two-dimensional polar active fluids to two-dimensional soap and one-dimensional sandblasting [^ad46a7b7]. Nature Communications (2016). Medium credibility.

Calculation of the nonlinear lengths

The nonlinear lengths ξ x, y can be calculated most conveniently from the equilibrium 2D smectic model (51). By definition, ξ x and ξ y are the lengths along x and y beyond which the anharmonic terms in equation (51) become important. To determine these lengths, we treat the anharmonic terms perturbatively, and calculate the lowest order correction to the harmonic terms. In a finite system of linear dimensions L x, y, this 'perturbative' correction will indeed be perturbative (that is, small) compared with the 'bare' values of the harmonic terms. However, they grow without bound with increasing L x, y, and, hence, eventually cease to be small; that is, the perturbation theory breaks down at large L x, y. The values of L x, y above which the perturbation theory breaks down are the nonlinear lengths ξ x, y.

Calculating the lowest order correction to the compression modulus B (that is, the coefficient of (∂ y h) 2 in the smectic Hamiltonian (51)) can be graphically represented by the Feynman diagram in Fig. 3. This leads to a correction to compression modulus:

In this calculation, we have taken L y, the system size along y, to be infinite. By the definition of ξ x, | δB | = B for L x = ξ x, which gives

where in the second equality we have used the relations B = 2 αv 0 2, and k B T = D between the parameters of the smectic and those of the original incompressible active fluid.

Likewise, doing the same calculation for L y = ξ y, L x = ∞, we find

---

### Optimizing chest compression to rescue ventilation ratios during one-rescuer CPR by professionals and lay persons: children are not just little adults [^0ceadfb0]. Resuscitation (2004). Low credibility.

Objective

To estimate the optimum ratio of chest compressions to ventilations for one-rescuer CPR that maximizes systemic oxygen delivery in children.

Method

Equations describing oxygen delivery and blood flow during CPR as functions of the number of compressions and the number of ventilations delivered over time were adapted from the former work of Babbs and Kern. These equations were solved explicitly as a function of body weight, using scaling algorithms based upon principles of developmental anatomy and physiology.

Results

The optimal compression to ventilation (C/V) ratios for infants and younger children increase sharply as a function of body weight. Optimal C/V ratios are lower for professional rescuers, who take less time to deliver a rescue breath, than for lay rescuers, who interrupt chest compressions for longer to perform ventilations. For professional rescuers the optimal C/V ratio, x*, is approximately 1.6 square root W where the W is the patient's body weight in kg. For lay rescuers the optimum C/V ratio is approximately 2.8 square root W. These values can be approximated for children and teens by the following rules of thumb, based upon the age of the victim: "5 + one half the age in years" for professional rescuers and "5 + age in years" for lay rescuers.

Conclusions

Compression to ventilation ratios in CPR should be smaller for children than for adults and gradually increase as a function of body weight. Optimal CPR in children requires relatively more ventilation than optimal CPR in adults. A universal compression/ventilation ratio of 50:2, targeted to optimize adult resuscitation, would not be appropriate for infants and young children.

---

### A multi-step model of Parkinson's disease pathogenesis [^e2cef97a]. Movement Disorders (2021). Medium credibility.

Box 1
Armitage and Doll Multistep Model – Heuristic Argument*

If disease development depends on one step, incidence in a given year will be proportional to the chance of undergoing that step:If two‐steps are required, then incidence is the product of the chance of undergoing the first step by age t and the rate of undergoing the second step:And for n‐steps:Taking log of both sides returns the equation for a straight line:The slope is one less than the number of steps required to develop the disease, and the intercept represents the combined probability of undergoing these steps.

*see Webster 2019 for full derivation.

A multistep model of pathogenesis could account for many of the epidemiological observations made in PD. These include the variability in the expression of disease and age of onset in carriers of disease‐causing mutations, and the multiple environmental and genetic associations that confer a risk of developing PD. A multistep model could also explain the phenotypic variability seen in PD, if it is assumed that at least some steps apply to specific neuronal populations rather than the nervous system as a whole. Furthermore, the predictions arising from such a model can be used to test specific hypotheses about basic observations in PD, such as whether the higher incidence and prevalence of PD in males observed in most parts of the world relates to differential environmental exposures by sex.

---

### KDIGO 2024 clinical practice guideline for the evaluation and management of chronic kidney disease [^87712576]. Kidney International (2024). High credibility.

CKD mineral–bone parameters by albuminuria category — figure context: The y axis represents the meta-analyzed absolute difference from the mean adjusted value at an eGFR of 80 ml/ min per 1.73 m² and albumin excretion < 30 mg/g (< 3 mg/mmol), with albuminuria categories defined as A1, albuminuria < 30 mg/g (< 3 mg/mmol); A2, albuminuria 30–300 mg/g (3–30 mg/mmol); and A3, > 300 mg/g (> 30 mg/mmol).

---

### Estimating and testing direct genetic effects in directed acyclic graphs using estimating equations [^947f28fb]. Genetic Epidemiology (2018). Low credibility.

2.1 Analysis of a quantitative primary trait with CIEE

First, we focus on the analysis of a (completely observed) normally distributed primary phenotype Y withindependent observations. In CIEE, unbiased estimating functions are constructed considering the two linear regression models fitted sequentially in the G‐estimation method (Vansteelandt et al. 2009), which are as follows. In the first stage, the effect of K on Y, α 1, is estimated, adjusting for other factors, by using the LS estimation method under the modelThen, to block all indirect paths of X on the primary phenotype Y, the adjusted phenotypeis obtained by removing the effect of K on Y withwhereand.

In the second stage, the direct effect of X on Y, is tested under the model

In CIEE, we formulate unbiased estimating equationsfor a consistent estimation of the unknown parameter vectorwith, whereand φ(.) is the probability density function of the standard normal distribution. To give an intuition on how these estimating equations are obtained, is the log‐likelihood function under the model in (1) andis the log‐likelihood function under the model in (3) given that α 1 is known. By solving the first five estimating equations based onin (5), we are hence fitting the model in (1) to obtain an estimate of θ 1, that is obtaining the maximum likelihood (ML) estimates under the model in (1). Analogously, solving the last three estimating equations based onyields an estimate of θ 2. Hence, we obtain the estimate of θ, denoted by, by solving. As a difference to the two‐stage sequential G‐estimation method, we estimate all parameters in θ simultaneously and consider the additional variability obtained in the phenotype adjustment in (2) by using the robust Huber–White sandwich estimator of the standard error of.

Under mild regularity conditions (White, 1982), is asymptotically normally distributed with mean 0 and covariance matrixthat can be consistently estimated with, wherewithbeing the j ‐th element in equation (4) and. The robust Huber–White sandwich estimate of the standard error ofcan then be obtained as. Having obtained the estimates of α XY and its standard error, we use the large‐sample Wald‐type test statisticfor testingvs. Under H 0, W has an asymptotic standard normal distribution.

---

### KDIGO 2024 clinical practice guideline for the evaluation and management of chronic kidney disease [^238edce9]. Kidney International (2024). High credibility.

Practice Point 1.2.4.2 — use of race in eGFR computation states that use of race in the computation of eGFR should be avoided. The text explains that estimating equations for GFR have historically incorporated demographic variables of age, sex, and race to explain variation in serum concentrations of endogenous filtration markers that are unrelated to GFR, and that age, sex, and race were included in the 2009 CKD-EPI equation because studies indicated higher average eGFR for the same measured GFR (mGFR) in people who are older versus younger, males versus females, and Black versus non-Black.

---

### KDIGO 2024 clinical practice guideline for the evaluation and management of chronic kidney disease [^300f5e64]. Kidney International (2024). High credibility.

Kidney failure risk prediction equations — validated models and performance — are summarized with scope and key metrics: multiple reviews identified "35 development studies and 17 external validation studies", and "3 validated models, The Kidney Failure Risk Equation (KFRE), the Veterans Affairs model, and the Z6 Score model". KFRE "was developed and initially validated in 8391 adults… and subsequently validated in 721,357 individuals from more than 30 countries spanning 4 continents", with performance "(C statistics ≥ 0.80 in 82% of studies)" and "validation populations now exceed 2 million individuals in more than 60 cohorts". Two US health-system models "predict kidney failure with high accuracy within a 5-year horizon", and a cystatin C "Z6 model" is "highly accurate in 4 European cohorts" but "has not been validated in other continents". The KFRE "is consistently highly accurate", and the Work Group judged externally validated models "had sufficient accuracy to be used in clinical settings", stating that "patients and healthcare providers should be encouraged to use these tools".

---

### How dieting might make some fatter: modeling weight cycling toward obesity from a perspective of body composition autoregulation [^32b99e3d]. International Journal of Obesity (2020). Medium credibility.

Two exponential fits are shown: a generalized linear model (GLM) with 95% confidence intervals (solid line), and a linear model (LM), with R 2 values (dotted line). The four figures correspond to the four methods presented in the main text for computing the value FAT END at the final time END at which the subjects would have completely recovered their initial FFM. The two red squares correspond to the US Army Ranger data points presented in the section on 'Applications of the Model'.

Their exact values depend on the method used for computing the values FAT END when the FFM has been completely recovered and on the type of statistical regression used. As explained previously, we consider four methods for computing FAT END. In Fig. 4, we show the fits obtained from a generalized linear model (GLM) with 95% confidence intervals (solid line) and from a linear model (LM), with R 2 values (dotted line). Following the performance of diagnostics tests to analyze the residuals, it is found that the GLM satisfies the major assumptions of regression analysis better than the LM, especially concerning the normality assumption of the error terms. Nevertheless, as shown in Fig. 4, both models give curves that are close to each other. In simple terms, the main difference between the above LM and GLM approaches to estimate the best parameter values for the constants a and b is the way the error term is handled. In the LM method, we write y = a × e bx × ε, where ε is a random error variable that follows a log-normal distribution with parameters μ = 0 and σ 2. Taking the logarithm on both sides of the equality gives, which corresponds to the linear model, where, x ′ = x, β 1 = b andis a random error variable that follows a normal distribution with mean μ = 0 and variance σ 2. Note that in the LM method, the error term ε is multiplied with the exponential function y = a × e bx. In the GLM method, the error term is instead added to the exponential function. Formally, we write y = a ×e bx + ε, where ε is a random error variable that follows a normal distribution with mean 0 and variance σ 2. As a consequence, the GLM admits the possibility of the value y = 0 (which is excluded in LM) and assumes that the variance V (y) is constant, while the LM method assumes that V (y) varies with x. Finally, note that the LM method admits exact analytical solutions for the model parameters a and b, while the GLM method requires numerical optimization algorithms to find the best values (the maximum likelihood estimates) for a and b.

---

### Forecasting the outcome of spintronic experiments with neural ordinary differential equations [^248909db]. Nature Communications (2022). High credibility.

Mackey-Glass prediction task

In a chaotic system, small perturbations can result in radically different outcomes. The prediction of a chaotic system is thus a problematic task. As a chaotic system, the Mackey-Glass equation is generated from a delay differential equation (DDE), where x (t) is a dynamical variable, β and γ are constants. Chaotic time series can be achieved with β = 0.2, γ = 0.1 and τ = 17.

In the main paper, our goal is to predict the Mackey-Glass time series at a future time step: the preprocessed input signal at the current time is fed into the reservoir, which maps it non-linearly into higher-dimensional computational spaces (see Fig. 3 a), and a trained output matrix W out is used for reading out the reservoir states. The number of steps between the future time step and the current step is defined as prediction horizontal step H. W out is different for the different H values.

More precisely, the dataset for the prediction task is prepared in the following way. First, Eq.(2) is solved for 100,000 integration time steps with dt = 0.1. Before the data are processed by the reservoir, they are downsampled with a downsampling rate of 10 to remove the possible redundancy in the input data. Thus, we obtain 10,000 data points in total. The first 5000 + H data points are used for the training, and the rest 5000 − H are for testing. The first stage of the masking procedure is a matrix multiplication W i n ⋅ M o, whereis the mask matrix with data values drawn from a standard normal distribution and M o ∈ R 1× L is the original input data. Here L = 5000 + H is the number of the scalar input data points and N r is the reservoir size. We adopt N r = 50 in the main text. As a consequence of the masking, we obtain the data matrix. Then M e is column-wise flattened into a vectorand then fed into the reservoir of skyrmion systems.

---

### Thinking outside the curve, part II: modeling fetal-infant mortality [^27ea1e69]. BMC Pregnancy and Childbirth (2010). Low credibility.

a. Simulation study to calibrate confidence intervals

We simulated 25 overlapping data sets of size 50,000, the degree of overlap consistent with a population of 200,000, based on the specifications in Table 2. The mixture density and the risk functions were chosen to mimic the patterns actually observed for white singletons born to heavily-smoking mothers; see panel b of Figure One from the previous paper and Figure 1d of the present paper. For each of various C between 2.0 and 5.0, we used Equation (6) to form confidence intervals for mortality risks at selected birthweights, namely r 1 (μ 1 - σ 1), r 1 (μ 1), r 1 (μ 1 + σ 1), r 2 (μ 2 - σ 2), r 2 (μ 2), r 2 (μ 2 + σ 2), r 3 (μ 3 - σ 3), r 3 (μ 3), r 3 (μ 3 + σ 3), r 4 (μ 4 - σ 4), r 4 (μ 4), and r 4 (μ 4 + σ 4). Above, μ j and σ j denote the mean and standard deviation of the birthweights in component j (1 ≤ j ≤ 4). This was repeated nine more times, and we tabulated how many of the 120 = 12 × 10 confidence intervals contained their targets. Confidence intervals were also formed using Equation (5) for comparative purposes. The above steps were repeated with overlapping data sets consistent with a population of 1,000,000 and with nonoverlapping data sets consistent with an effectively infinite population.

Table 2
Mixture Model and Mortality Functions for Simulation Study

The probability density for the mixture model used in our simulation study is specified, as are the mortality risk functions associated with the mixture model components. Above, z is defined as (x - 3000)/1000, where x is birthweight in grams.

The results are summarized in Table 3. With an effectively infinite population, only 75.0% of the confidence intervals formed using Equation (5) contained their targets at C = 5.0. On the other hand, the confidence intervals formed using Equation (6) contained their targets 95.0% of the time at C = 4.0. The latter finding provided the rationale for taking C 0 = 4.0 when constructing confidence intervals for mortality risks in our examples with real data.

---

### KDIGO 2024 clinical practice guideline for the evaluation and management of chronic kidney disease [^cf7d366c]. Kidney International (2024). High credibility.

Dose adjustments by level of GFR — Practice Points 4.2.1–4.2.3 state: "Consider GFR when dosing medications cleared by the kidneys". They note that "For most people and clinical settings, validated eGFR equations using SCr are appropriate for drug dosing", and that "Where more accuracy is required for drug-related decision-making (e.g., dosing due to narrow therapeutic or toxic range), drug toxicity, or clinical situations where eGFRcr estimates may be unreliable, use of equations that combine both creatinine and cystatin C, or measured GFR may be indicated".

---

### Emergent second law for non-equilibrium steady States [^eeee434e]. Nature Communications (2022). High credibility.

The Gibbs distribution universally characterizes states of thermal equilibrium. In order to extend the Gibbs distribution to non-equilibrium steady states, one must relate the self-information [Formula: see text] of microstate x to measurable physical quantities. This is a central problem in non-equilibrium statistical physics. By considering open systems described by stochastic dynamics which become deterministic in the macroscopic limit, we show that changes [Formula: see text] in steady state self-information along deterministic trajectories can be bounded by the macroscopic entropy production Σ. This bound takes the form of an emergent second law [Formula: see text], which contains the usual second law Σ ≥ 0 as a corollary, and is saturated in the linear regime close to equilibrium. We thus obtain a tighter version of the second law of thermodynamics that provides a link between the deterministic relaxation of a system and the non-equilibrium fluctuations at steady state. In addition to its fundamental value, our result leads to novel methods for computing non-equilibrium distributions, providing a deterministic alternative to Gillespie simulations or spectral methods.

---

### Learning interpretable network dynamics via universal neural symbolic regression [^5d0891d8]. Nature Communications (2025). High credibility.

Inferring dynamics of chaotic systems

We further uncover the governing equations for three-dimensional chaotic systems on networks, including Lorenzand Rössler dynamics. To examine the impact of the initial sensitivity of chaotic systems on our LLC, we consider three initial value settings: fixing the initial states of all nodes to 0.1, sampling from a standard Gaussian distribution, and sampling from a uniform distribution U(0, 2). Then, we employ a BA network as the topological structure to generate the dynamics data of the Lorenz. We observe that the governing equations derived from the dynamics data, generated using different initial values, are very similar (Fig. 5a). The predictive states of an attractor of the three discovered equations starting from the same initial values are close to the actual system behavior before 1000 time steps, and then the butterfly effect gradually emerges (see Fig. 5b), which means that our tool can accurately forecast a chaotic system for a long time, that is, 1000 times the training time steps.

Fig. 5
Results of inferring the dynamics of chaotic systems.

a Comparison of governing equations inferred by our LCC under different initial conditions on the coupled Lorenz system. b Comparison of predictive states of an attractor with the same initial values, produced by equations inferred by our LLC under different initial conditions. c Coefficient errors between the equations inferred by each method on the Rössler system and the true equation. d Comparison of states of the same attractor, generated by the governing equations inferred by the TPSINDy, LCC, and LCC+TPSINDy on the Rössler system. e Bifurcation diagram of the Rössler system via the Poincaré section method, with the horizontal axis representing the parameter c (ranging from 1 to 6) and the vertical axis representing the states on the second dimension (X i,2) of an attractor. The discovered equation exhibits the same period-doubling and chaotic phenomenon as the true equation. f Comparison of limit cycle at period-1, i.e. c = 2.5. g Comparison of chaos at c = 5.7.

---

### Recovery after stroke: not so proportional after all? [^d3259e7a]. Brain (2019). Medium credibility.

Spurious r(X,Δ) are likely when σ Y /σ X is small

For any X and Y, it can be shown that:

A formal proof of Equation 1 is provided in the Supplementary material, Appendix A [proposition 4 and theorem 1; also see]; its consequence is that r(X,Δ) is a function of r(X, Y) and σ Y /σ X. To illustrate that function, we performed a series of simulations (Supplementary material, Appendix B) in which r(X, Y) and σ Y /σ X were varied independently. Figure 2 illustrates the results: a surface relating r(X,Δ) to r(X, Y) and σ Y /σ X. Figure 3 shows example recovery data at six points of interest on that surface.

Figure 2
The relationship between r(X, Y), r(X,Δ) and σ Y /σ X. Note that the x -axis is log-transformed to ensure symmetry around 1; when X and Y are equally variable, log(σ Y /σ X) = 0. Supplementary material, proposition 7 in Appendix A, provides a justification for unambiguously using a ratio of standard deviations in this figure, rather than σ Y and σ X as separate axes. The two major regimes of Equation 1 are also marked in red. In Regime 1, Y is more variable than X, so contributes more variance to Δ, and r(X,Δ) ≈ r(X, Y). In Regime 2, X is more variable than Y, so X contributes more variance to Δ, and r(X,Δ) ≈ r(X,−X) (i.e. −1). The transition between the two regimes, when the variability ratio is not dramatically skewed either way, also allows for spurious r(X,Δ). For the purposes of illustration, the figure also highlights six points of interest on the surface, marked A–F; examples of simulated recovery data corresponding to these points are provided in Fig. 3.

---

### Warped linear mixed models for the genetic analysis of transformed phenotypes [^c1a2cff0]. Nature Communications (2014). Medium credibility.

Phenotype prediction

The fitted WarpedLMM model can also be used to predict the unobserved phenotype of a new individual indexed by * given the genotype alone. Assuming a fully observed sample of N individuals, we can use the parameter estimates under model (4) to compute the best linear unbiased predictorof the new individual's phenotype on the normal distributed scale

where x * is a vector of covariates for the new individual, k * is a 1-by- N vector that contains the genomic relatedness between the new individual and all the individuals in the original sample.

To get an estimate of the phenotype on the original scale, we apply the reverse transformation f − 1 to the best linear unbiased predictor

The reverse transformation f −1 is obtained by numerically inverting f using Newton-Raphson updates, as previously proposed by Snelson et al.

Estimating heritability

It is possible to obtain an estimate of the narrow-sense heritability h 2 in the normal distributed scale by computing a chip heritabilityfrom common genotyped markers in the LMM (4).

whereandare restricted maximum likelihood estimates ofand.

Simulation study

The simulated data are generated taking genotypes from hapmap3 (ref.) chromosome 22 and sampling from a standard LMM with additive genetic effects and Gaussian distributed noise. In each simulation, we sample h 2 from {0.1,0.20,0.40,0.70,0.9}, the number of causal variants from {5,20,100,500,1,000}, the number of samples from {200,400,600,800,1,000} and the variance explained by covariates from {0.0,0.25,0.5,0.70,0.9}. We can then recover the noise level conditioned on h 2, and the covariates variance.

Finally, we pick a transformation f (y) from the set of transformations used in Valdar et al. For the experiments in the main paper, we consider exp(y); results for alternative transformations are presented as Supplementary Material. We then transform the phenotype as z = t · y +(1− t) f (y), where t is a parameter that determines the intensity of the transformation and is sampled from {0.0, 0.25, 0.5, 0.75, 1.0}. We repeated this simulation procedure 50,000 times in order to have a sufficiently large sample size to investigate all the regimes described above.

---

### Addressing sex as a biological variable in preclinical models of lung disease: an official American Thoracic Society research statement [^044a47b9]. American Journal of Respiratory and Critical Care Medicine (2025). High credibility.

Figure 1 logic tree — delineating mechanisms behind sex differences in experimental lung disease phenotypes outlines a pathway from the question 'Does the phenotype show a sex difference?' to 'Discriminate effects of gonadal hormones vs. sex chromosomes', including 'Do changes in levels of gonadal hormones (gonadectomy/ hormone replacement) explain the sex difference?' with mechanistic questions ('Which hormones cause sex difference? Which receptors mediate hormone effect? When does hormone act? Where is hormone produced? Which cell type is the target of hormone effect?'). The algorithm incorporates the four core genotypes (FCG) model — 'Use FCG model to test for both sex chromosome and gonadal hormone effects' — and downstream steps ('Identify downstream molecular effects of hormones in specific cell types mediating sex differences' and 'Measure interactions of hormones and sex chromosome genes'). For sex chromosome mechanisms, it directs 'Use XY* model to discover X effect or Y effect', 'Manipulate expression of specific X or Y genes to discover which genes account for sex chromosome effect', and 'Identify downstream molecular effects of sex chromosome genes in specific cell types mediating sex differences'; if no difference is observed, 'Consider sex-specific effects that counterbalance each other'.

---

### Primer on binary logistic regression [^a6422ae6]. Family Medicine and Community Health (2021). Medium credibility.

Step 4: compute ORs and report the results

While the predicted probabilities from the logistic function can be useful in measuring how well the model is predicting or explaining the outcome, the results of logistic regression are usually reported with ORs and CIs. Similar to the interpretation of a coefficient in linear regression, ORs quantify the change in the odds of having the outcome (ie, the odds that an observation has the value of 1 for the outcome variable) with a one-unit change in the predictor. Odds are computed using probabilities (equation 3).

Equation 3. Computing odds from probabilities.

Because the logistic function is used to compute probabilities (see figure 1), add the logistic model from equation 1 into equation 3 to get equation 4 showing how odds are computed for a logistic regression model.

Equation 4. Computing odds from a logistic regression model.

Once theandare estimated using a statistical software package like SAS, R or SPSS, these values can be substituted into the simplified version of equation 4 to compute odds. This is not the final step, however, since odds and ORs are different. An OR is a ratio of two odds and is computed by dividing the odds of the outcome at one value of a predictor by the odds of the outcome at the previous value. So, for example, to compute the OR for lung cancer in our previous example, divide the odds of someone who has smoked for 15 years by the odds for someone who has smoked for 14 years. The result will be the increased or decreased odds of lung cancer with every 1 year increase in age. Equation 5 shows the statistical form of this computation.

Equation 5. Using odds to compute ORs from a logistic regression model.

As an example, consider the output from R showing the estimates for the regression model used in figure 2.

The coefficient for years smoking is 0.4304. Substitute this value into equation 5, to get an OR of 1.54. So, for every 1 year increase in time spent as a smoker, the odds of lung cancer for a participant in our sample are approximately 1.54 times higher. While the OR is useful to understand the direction and magnitude of the relationship between a predictor and the outcome, more information is needed to understand whether the OR for the sample suggests a relationship in the population that the sample came from. To understand this, a 95% CI is typically computed and reported with each OR.

---

### All-to-all reconfigurability with sparse and higher-order ising machines [^e6ffae9b]. Nature Communications (2024). High credibility.

Results

Background on p-bits and XORSAT

The dynamics of p-computers can be described as a discrete Markov Chain Monte Carlo (MCMC) algorithm, known as Gibbs sampling or Glauber dynamics. For each p-bit in the network, we have:where m i ∈ {−1, +1} and rand U (−1, 1) is a uniform random number that lies in the interval [− 1, 1]. { J i j } and { h i } correspond to the weights and biases, respectively. β is the inverse temperature. After a sufficient number of iterations, Eq. (2) approximates the Boltzmann distributiongiven by:where Z is the partition function. Generalizations of the Ising Hamiltonian (Eq. (4)) beyond quadratic terms are possible and have been implemented previously –. One such generalization that we implement in our p-computers is the inclusion of third-order interactions implemented by modifying Eq. (1) to:while keeping Eq. (2) the same. Heremodels the third-order interaction, whereasmodels the second-order interaction.

Eqs. (1, 2), and (5) are extremely general and can be used to solve a wide variety of problems in optimization, sampling (for energy-based machine learning) and quantum simulation through quantum Monte Carloand variational quantum machine learning algorithms. Here, we focus on the 3R3X problem, which encodes clauses of the type C k = (x i ⊕ x j ⊕ x k), typically mapped to quadratic Ising energies via auxiliary variables (Fig. 1 c), however, in this paper, we also implement a native third-order mapping without any auxiliary variables. This is a benchmark problem with a golf-course-like energy landscape that is difficult for heuristic Monte Carlo solvers to tackleeven though it admits a polynomial time algorithm through its relation to a system of linear equations modulo 2, solvable by Gaussian elimination. Nonetheless, the problem resembles hard generic SAT problems, and slight variations of it cannot be solved by Gaussian elimination and its main value is to serve as a benchmark for hard optimization problems and Ising solvers.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^b1513dd5]. CDC (2011). Medium credibility.

Table 1b — Minimum sample size for detecting geometric mean (GM) ratio k with α = 0.05 and W = 0.9 — presents required sample sizes by standard deviation (S) strata and defines parameters; α is significance level, W is power, GM is geometric mean, and S is standard deviation, noting that 1.2 is the standard deviation of national viral load (VL) data. Each jurisdiction will need to assess the standard deviation of their local VL data and then determine the appropriate sample size needed to assess VL, and if the sample size is inadequate to meet the recommended case inclusion criterion, an alternate method may need to be used, such as combining multiple years of data. For k = 3, sample sizes across S = 1, 1.1, 1.2, 1.3, 1.4, 1.5 are 75, 91, 108, 127, 147, 169, and Table 1b uses power = 90%; jurisdictions may also explore differences in means of viral loads, including categorical differences in the proportion with undetectable or very low VL.

---

### KDIGO 2025 clinical practice guideline for the evaluation, management, and treatment of autosomal dominant polycystic kidney disease (ADPKD) [^94201e43]. Kidney International (2025). High credibility.

ADPKD predicted GFR by class and age (polynomial model) shows that for Class A, predicted glomerular filtration rate (GFR) values with 95% confidence intervals are 109 (95–123) at age 20–30 years, 110 (99–121) at 30–40, 97 (83–110) at 40–50, and 69 (49–90) at 50–60, with corresponding slopes 0.77, –0.63, –2.03, and –3.42; units are ml/min per 1.73 m² for GFR and ml/min per 1.73 m² per year for slopes, and positive values mean GFR increase.

---

### Evolutionary dynamics of any multiplayer game on regular graphs [^916cb3ba]. Nature Communications (2024). High credibility.

General replicator equations

The evolution of frequencies x 1, x 2, …, x n can be deduced through the microscopic strategy update process. Specifically, in an infinite population, i.e. N → ∞, a single unit of time comprises N elementary steps, ensuring that each individual has an opportunity to update their strategy. During each elementary step, the frequency of i -players increases by 1/ N when a focal j -player (where j ≠ i) is chosen to update its strategy and is replaced by an i -player. Similarly, the frequency of i -players decreases by 1/ N when a focal i -player is selected to update its strategy and the player who takes the position is not an i -player. Based on this perception, we derive a simple form of the replicator equations for i = 1, 2, …, n in the weak selection limit (Supplementary Note 2) :We find that Eq. (4) offers an intuitive understanding, if we introduce the following two concepts: (1), the expected accumulated payoff of the i -player (zero steps away on the graph), and (2), the expected accumulated payoff of the i -player's neighbors (one step away on the graph). These concepts suggest that. Under pairwise comparison, the reproduction rate of i -players is dependent on how their accumulated payoff exceeds that of their neighbors. In essence, the evolution of x i is the competition between an individual and its first-order neighbors, which aligns with the results obtained by a different theoretical framework in two-strategy systems. We further extend it to n -strategy systems in the framework of pair approximation. We also verify that the death-birth rule is essentially the competition between an individual and its second-order neighbors for n -strategy systems (Supplementary Information).

---

### Bayesian model selection for complex dynamic systems [^103760a2]. Nature Communications (2018). Medium credibility.

Methods

Iterative evaluation of the model evidence

In Bayesian statistics, a parameter distribution that is inferred from data based on a probabilistic model is called posterior distribution. This posterior distribution is computed as the product of a likelihood function with a prior distribution, normalized by a model evidence. Traditionally, this model evidence is computed from the integral of likelihood times prior so that the posterior distribution is properly normalized. Therefore, in the case of time series with time-varying model parameters, this integration step can only be performed after every point of the time series has been analyzed. By contrast, in our method we update the model evidence for every new data point of the time series with an iterative approach. Specifically, we have adapted the iterative approach used for evaluating Hidden Markov models, to hierarchical models that consist of a low-level model (defined by the likelihood function L) with time-varying parameters θ t (t = 1, 2. N), and a high-level model (defined by a transformation T) with high-level parameters η. We discretize θ and η on a regular grid, resulting in a discrete set of parameter values θ (i) with i = 1. n i, and a set of high-level parameter values η (j) with j = 1. n j. Thenis the product of likelihood and prior, i.e. the non-normalized posterior (the conditional probability of the data d up to time step t and the parameters θ at time t, given the high-level parameters η). To advance Eq. (1) by one time step, our method relies on models with a factorizable likelihood function. Each of the likelihood factors describes the probability of one data point, given the parameter values of the same time step and past data points:

---

### Observation of an anisotropic dirac cone reshaping and ferrimagnetic spin polarization in an organic conductor [^d952f61e]. Nature Communications (2016). Medium credibility.

To check this hypothesis, we have performed a RG calculation based on the generalized Weyl Hamiltonian (equation (8)) and tried to fit the data. A circular momentum cutoff of Λ = 0.667 Å −1 around the DP (q = 0) is introduced in the RG theory, which is of the size of the averaged inverse lattice constant, Λ = 2 π / L with L = (a + b)/2 = 9.425 Å at 2.3 GPa. For the initial values of the velocities at the cutoff momentum q = Λ (that is, v and w 0 in equation (8)), we employ velocities derived from the effective TB model of ref.and v TB (equation (12)), as discussed in the previous subsection (Methods: Generalized Weyl Hamiltonian and site-spectral weight). In the one-loop order large- N expansion of the RG theory, v = (v x, v y) are renormalized following equation (2) (given in the main text) and grows logarithmically as functions of Λ / q (where q is measured from the DP), whereas w 0 = (w 0 x, w 0 y) are not renormalized (Supplementary Fig. 4a). We note that equation (2) takes into account the screening effect of the Coulomb interaction including the polarization bubbles in the self-energy. It is applicable to any size of the coupling, where ɛ is the dielectric constant and N = 4 is the number of fermion species corresponding to the two DPs in the Brillouin zone and two spin projections (that means the twofold valley degeneracy is considered).

---

### High-order michaelis-menten equations allow inference of hidden kinetic parameters in enzyme catalysis [^21a9eeb4]. Nature Communications (2025). High credibility.

Single-molecule measurements provide a platform for investigating the dynamical properties of enzymatic reactions. To this end, the single-molecule Michaelis-Menten equation was instrumental as it asserts that the first moment of the enzymatic turnover time depends linearly on the reciprocal of the substrate concentration. This, in turn, provides robust and convenient means to determine the maximal turnover rate and the Michaelis-Menten constant. Yet, the information provided by these parameters is incomplete and does not allow access to key observables such as the lifetime of the enzyme-substrate complex, the rate of substrate-enzyme binding, and the probability of successful product formation. Here we show that these quantities and others can be inferred via a set of high-order Michaelis-Menten equations that we derive. These equations capture universal linear relations between the reciprocal of the substrate concentration and distinguished combinations of turnover time moments, essentially generalizing the Michaelis-Menten equation to moments of any order. We demonstrate how key observables such as the lifetime of the enzyme-substrate complex, the rate of substrate-enzyme binding, and the probability of successful product formation, can all be inferred using these high-order Michaelis-Menten equations. We test our inference procedure to show that it is robust, producing accurate results with only several thousand turnover events per substrate concentration.

---

### Multiple tipping points and optimal repairing in interacting networks [^d95be1ee]. Nature Communications (2016). Medium credibility.

Despite the seeming complexity of equations (1) and (2), it is noteworthy that there are only two unknown variables, a A and a B, and that all other parameters are fixed. These two equations define two curves in the (a A, a B) plane.

Figure 1a shows a graphical representation of the curves for a random regularnetwork (in which all the nodes have the same degree) with degree of k = 16 and threshold m = 8, for the symmetric parameter values, r A = r B = 0.60 and r d = 0.15. The size of each network is N = 2 × 10 4. The blue curve is a graphical representation of equation (1) and the brown curve is defined by equation (2). The curves, similar to two 'ropes', create a 'knot' that can have up to nine intersections, representing mathematical solutions of the system of equations. However, not all of these solutions represent observable and stable physical states. To see that, observe one of the curves in Fig. 1a, for example, the blue curve described by equation (1). If we increase damage done to network B (that is, we increase a B) and keep everything else constant, some damage will undoubtedly spread to network A. Thus, we expect that when a B is increased, a A must also increase (it would be very unusual if one network improves its activity as a result of damaging the other network). We conclude that the parts of the blue and brown curve that produce physical solutions are only those where a A and a B increase together or decrease together along the curve. This elimination leaves only four states in Fig. 1a that are stable (green circles), whereas the other five states are unstable (red crosses), for this particular choice of parameters. In simulated finite networks, when the network system evolves according to the rules of the model, at t = 0 we have a freedom to set initial conditions for the activities. Systems initially prepared to have a pair of values (a A, a B) corresponding to an unstable solution of equations (1) and (2) will be disturbed by a small fluctuation of a A or a B, owing to the system dynamics, and the values of a A or a B will rapidly change until one of the stable states is reached. Systems that are initially prepared to have values of a A or a B corresponding to a stable solution will fluctuate around these values, until perhaps a large finite fluctuation occurs and the system 'jumps' to another stable state. In general, for any choice of parameters, we have between one and four stable (physical) states. Figure 1b shows the scenario for the same network system when, r A = r B = 0.60 and r d = 0.15. In this case we have two stable states and one unstable state.

---

### Race and ethnicity in pulmonary function test interpretation: an official American Thoracic Society statement [^80a8779a]. American Journal of Respiratory and Critical Care Medicine (2023). High credibility.

Table 2 — cessation of labeling individual results as "normal" or "abnormal" to enable personalized PFT interpretation: Potential benefits include the use of individual z-scores within a continuous distribution of pulmonary function, encouragement of models that combine PFTs with other data to predict specific outcomes, and possibly removing the need for reference equations; concerns include that data and tools to guide a personalized approach are lacking and that complex machine learning models risk perpetuating biases.

---

### KDIGO 2024 clinical practice guideline for the evaluation and management of chronic kidney disease [^34c1b44c]. Kidney International (2024). High credibility.

Early-stage chronic kidney disease risk prediction applicability and progression example — Practice Point 2.2.4 notes that risk prediction equations developed for use in people with CKD G3–G5, may not be valid for use in those with CKD G1–G2. The Work Group recognizes that in earlier stages (G1–G3), large declines in eGFR can occur in 2- to 5-year time frames without reaching kidney failure. Figure 16 illustrates this with a CKD G1–G2 profile in which kidney failure risk is 0.07% over 2 years and 0.32% over 5 years versus CKD progression risk of 10.4% over 3 years, for a 50-year-old male with diabetes with eGFR 80 ml/min per 1.73 m² and urine ACR 1 g/g.

---

### KDIGO 2024 clinical practice guideline for the evaluation and management of chronic kidney disease [^bd9e0957]. Kidney International (2024). High credibility.

KDIGO 2024 chronic kidney disease (CKD) guideline — research recommendations scope — states that "Research recommendations are presented in a separate section at the end of this document and are intended to guide the next set of important research questions to inform and improve outcomes of people living with CKD". They are "not exhaustive but are intended to help focus the clinical and research communities on unanswered questions including improving diagnostic tools and evaluation of kidney function, development and testing of risk prediction equations… evaluation of different therapies to delay progression… improved medication management, and optimal models of care", and the guideline emphasizes inclusion by stating, "We specifically require the community to be inclusive of people across the lifecycle and inclusive of sex and gender, and etiology of CKD, as important variables in all studies".

---

### Modelling insights into the COVID-19 pandemic [^de45a0e2]. Paediatric Respiratory Reviews (2020). Medium credibility.

Coronavirus disease 2019 (COVID-19) is a newly emerged infectious disease caused by the severe acute respiratory syndrome coronavirus-2 (SARS-CoV-2) that was declared a pandemic by the World Health Organization on 11th March, 2020. Response to this ongoing pandemic requires extensive collaboration across the scientific community in an attempt to contain its impact and limit further transmission. Mathematical modelling has been at the forefront of these response efforts by: (1) providing initial estimates of the SARS-CoV-2 reproduction rate, R 0 (of approximately 2–3); (2) updating these estimates following the implementation of various interventions (with significantly reduced, often sub-critical, transmission rates); (3) assessing the potential for global spread before significant case numbers had been reported internationally; and (4) quantifying the expected disease severity and burden of COVID-19, indicating that the likely true infection rate is often orders of magnitude greater than estimates based on confirmed case counts alone. In this review, we highlight the critical role played by mathematical modelling to understand COVID-19 thus far, the challenges posed by data availability and uncertainty, and the continuing utility of modelling-based approaches to guide decision making and inform the public health response. †Unless otherwise stated, all bracketed error margins correspond to the 95% credible interval (CrI) for reported estimates.

---

### Standardization of spirometry 2019 update. An official American Thoracic Society and European Respiratory Society technical statement [^93470f8a]. American Journal of Respiratory and Critical Care Medicine (2019). High credibility.

Spirometry reporting and data interoperability — The ATS standardized report form should be the default report form for spirometry systems, and the default set of reference values for all ages should be the GLI reference equations; in addition to summary reports, the interpreter should have access to a report of all maneuvers within a testing session. The flow and/or volume data from each test session must be available for expert audit, and the system should have the capability to export data to electronic medical records as .pdf and discrete data using HL7 Clinical Document Architecture Release 2 or Fast Healthcare Interoperability Resources; Logical Observation Identifiers Names and Codes should be used. The date and time for each maneuver must be recorded because diurnal variations of 3% in FEV1 and 6% in PEF have been reported, and the time and date are useful for verifying pre- and post-bronchodilator maneuvers. Operator comments are a key part of the report.

---

### Statistical methods in epidemiology. VI. correlation and regression: the same or different? [^0a3b90e5]. Disability and Rehabilitation (2000). Low credibility.

Purpose

The statistical terms 'correlation' and 'regression' are frequently mistaken for each other in the scientific literature. Why this is so is unclear. This paper discusses their differences/ similarities arguing that in most circumstances regression is the most appropriate technique to use, since regression incorporates a notion of dependency of one variable on another.

Method

Pearson's correlation coefficient (r) is introduced as a method for estimating the degree of linear association between two normally distributed variables. The problem of least squares' regression (when y depends on x) is introduced by considering the best-fitting straight line between points on a scatter plot.

Results

Correlation, regression analysis and residual estimation are discussed by taking examples from the author's own teaching experiences.

Conclusions

Correlation and regression share some similarities. However, regression is the better technique to use because with it comes a notion of dependency of one variable upon another. Regression model checking includes residual examination. The importance of plotting and examination of residuals cannot be overemphasized. Residual examination should become as much a part of a regression analysis as the estimation of the regression coefficients themselves.

---

### KDIGO 2024 clinical practice guideline for the evaluation and management of chronic kidney disease [^705122bc]. Kidney International (2024). High credibility.

KDIGO 2024 CKD risk prediction models — evidence certainty, use, and cautions: To assess certainty, the ERT examined 2 existing systematic reviews, and a 2021 NICE review concluded there was high-quality evidence that chosen risk prediction equations accurately predict kidney failure; the Work Group agreed with this assessment. The Work Group judged that the published externally validated models all had sufficient accuracy to be used in clinical settings, and patients and healthcare providers should be encouraged to use these tools. Potential harms could result from inappropriate use in AKI or AKD or in younger individuals with CKD G1–G2 who may be at high risk of progression but low risk of kidney failure in the next 5 years; in these people, more proximal outcomes such as 40% decline in eGFR or lifetime risk were judged to be more appropriate. Given their value, healthcare providers should consider how to integrate risk prediction models into clinical practice, either in EMRs, laboratory information systems, or other mechanisms.

---

### 2019 ASCCP risk-based management consensus guidelines: methods for risk estimation, recommended management, and validation [^c44c6ea6]. Journal of Lower Genital Tract Disease (2020). High credibility.

Risk-based management uncertainty — confidence interval and decision confidence score: Confidence intervals for each risk estimate were calculated using a normal approximation or exact methods based on the binomial distribution, and a "decision confidence score" is defined that combines the uncertainty in the statistical precision and how close the risk estimates fall to the clinical action thresholds; mathematical details on estimating the recommendation confidence scores are presented in Appendix Part D (http://links.lww.com/LGT/A160).

---

### Stability of synchronization in simplicial complexes [^14660083]. Nature Communications (2021). High credibility.

Fig. 1
Synchronization in simplicial complexes of Rössler oscillators.

Contour plots of the time averaged (over an observation time T = 500) synchronization error E (see "Methods" for definition and the vertical bars of each panel for the color code) in the plane (σ 1, σ 2) for some examples of simplicial complexes (whose sketches are reported in the top left of each panel). Simulations refer to coupled Rössler oscillators (x = (x, y, z) T and f = (− y − z, x + a y, b + z (x − c)) T) with parameters fixed in the chaotic regime (a = b = 0.2, c = 9). In a – d, while in (e). As for the other coupling function, one hasin (d) andin all other panels. The blue continuous lines are the theoretical predictions of the synchronization thresholds obtained from Eq. (3). a, b, and c are examples of class III problems, whereas panels d and e are examples of class II problems.

Far from being limited to the case of D = 2, our approach can be extended straightforwardly to simplicial complexes of any order D. Each term on the right hand side of Eq. (1) can, indeed, be manipulated following exactly the same three conceptual steps described in the "Methods". Once again, one is entitled to select the eigenvector set which diagonalizes, to introduce the new variables η = (V −1 ⊗ I m) δ x. Following the very same steps which led us to write Eq. (3), one then obtainswhere JG (d) = J 1 g (d) (x s, …, x s) + J 2 g (d) (x s, …, x s) +… + J d g (d) (x s, …, x s) and the coefficientsresult from transformingwith the matrix that diagonalizes. As a result, one has conceptually the same reduction of the problem to a single, uncoupled, nonlinear system, plus a system of N − 1 coupled linear equations, from which the maximum Lyapunov exponent Λ max = can be extracted and monitored (for each simplicial complex) in the D -dimensional hyper-space of the coupling strength parameters.

---

### An analytical theory of balanced cellular growth [^98f39440]. Nature Communications (2020). High credibility.

Without dependent reactants, A is a square matrix with a unique inverse A −1, and x ≡ [P, a] T is the corresponding vector of independent concentrations. Multiplying both sides of the mass balance constraint (1) by A −1, we obtain (Theorem 5)The right-hand side of the mass balance constraint (1) quantifies how much of each component x i needs to be produced to offset the dilution that would otherwise occur through the exponential volume increase at rate μ.quantifies the proportion of flux v j invested into offsetting the dilution of component i, and we thus name A −1 the investment (or dilution) matrix; see Supplementary Fig. 1 for examples. In contrast to the mass-normalized stoichiometric matrix A, which describes local mass balances, A −1 describes the structural allocation of reaction fluxes into offsetting the dilution of all downstream cellular components, carrying global, systems-level information.

From the kinetic equation (Eq. (3)), p j = v j / k j (a), and inserting v j from the investment equation (Eq. (4)) giveswhere ∑ i sums over the total protein and individual reactant concentrations (Theorem 6). Substituting these expressions into the total protein sum (Eq. (2)) and solving for μ results in the growth equation (Theorem 7)

As detailed in "Methods" (Theorems 5–7), a corresponding result also holds for BGSs with dependent reactants. Thus, for any active matrix A with full column rank (in particular for all active matrices of EGSs) and for any corresponding concentration vector x, there are unique and explicit mathematical solutions for the fluxes v, individual protein concentrations p, and growth rate μ. If μ (Eq. (6)) and all individual protein concentrations p j (Eq. (5)) are positive, the cellular state is a BGS; otherwise, no balanced growth is possible at these concentrations.

---

### Plasmon transport in graphene investigated by time-resolved electrical measurements [^7463a02d]. Nature Communications (2013). Medium credibility.

Here, we focus on the velocity at peaks, where the effects of dissipation become minimal, and examine the gate screening effect. We analyse the data using an EMP theory developed for a standard 2DES. For a gated 2DES with a hard-wall edge potential, the velocity can be expressed as a function of w / d in the following two forms sharing the prefactor(ref.),

Which functional form applies depends on the range of the parameter w / d. In Fig. 3e, we show the measured velocity normalized byas a function of w / d for comparison with equations 2 and 3; to match the velocity calculated using the two equations at w = d, the value calculated with equation 3 is multiplied by a factor of 0.53. For the plot of the measured velocity, we estimated w using the relationbased on the model in. The values of the normalized velocity at ν = 2 and 6 in the ungated sample are shown in grey and green lines as a reference for the weak-screening limit. Theoretically, the gate screening reduces the velocity by about one order of magnitude when. As w / d decreases, the effect of the gate screening weakens as, decreasing to a factor of two when. Experimentally, we find the gate screening effect to be much stronger than that predicted by the theory, reducing the velocity by one order of magnitude even when. Notably, the normalized velocity versus w / d follows a power law as. As σ xy n / B and w n 3/2 / B 2, the power law implies v n 1/4 (Fig. 3d), exactly the carrier density dependence specific to sheet plasmons in graphene at B = 0 T (refs).

---

### Gravitationally redshifted absorption lines in the X-ray burst spectra of a neutron star [^b4654ece]. Nature (2002). Excellent credibility.

The fundamental properties of neutron stars provide a direct test of the equation of state of cold nuclear matter, a relationship between pressure and density that is determined by the physics of the strong interactions between the particles that constitute the star. The most straightforward method of determining these properties is by measuring the gravitational redshift of spectral lines produced in the neutron star photosphere. The equation of state implies a mass-radius relation, while a measurement of the gravitational redshift at the surface of a neutron star provides a direct constraint on the mass-to-radius ratio. Here we report the discovery of significant absorption lines in the spectra of 28 bursts of the low-mass X-ray binary EXO0748-676. We identify the most significant features with the Fe XXVI and XXV n = 2–3 and O VIII n = 1–2 transitions, all with a redshift of z = 0.35, identical within small uncertainties for the respective transitions. For an astrophysically plausible range of masses (M approximately 1.3–2.0 solar masses; refs 2–5), this value is completely consistent with models of neutron stars composed of normal nuclear matter, while it excludes some models in which the neutron stars are made of more exotic matter.

---

### KDIGO 2024 clinical practice guideline for the evaluation and management of chronic kidney disease [^4c4b615d]. Kidney International (2024). High credibility.

Initial assessment recommendation and caveats — Practice Point 1.2.2.1: Practice Point 1.2.2.1: Use serum creatinine (SCr) and an estimating equation for initial assessment of GFR. For most clinical circumstances, estimating GFR from SCr is appropriate; equations are available that estimate GFR using SCr and adjusting for sex and age, and professional societies have recommended that GFR estimates should be used in association with SCr reporting; key sources of error include non–steady-state conditions, non-GFR determinants of SCr, measurement error at higher GFR, and interferences with creatinine assays, and GFR estimates are less precise at higher GFR levels, so healthcare providers should remain aware of caveats that may influence accuracy in an individual person.

---

### Estimating global, regional, and national daily and cumulative infections with SARS-CoV-2 through nov 14, 2021: a statistical analysis [^5284c7ec]. Lancet (2022). Excellent credibility.

R effective estimation in the past

Using daily infections, we directly estimated R effective in the past by location and day, where R effective at time t is:

The assumptions required for this estimation are the duration from infection to being infectious and the period of infectiousness, collectively represented as θ. We used ranges of 3–5 days for both assumptions to generate estimates of R effective in the past. These estimates are useful for identifying the effect of different non-pharmaceutical interventions on transmission in different settings. An R effective lower than 1·0 indicates that the epidemic is shrinking, whereas an R effective higher than 1·0 indicates that the epidemic is growing.

We compared R effective to an estimate of total immunity in the population of location l at time t (presented as weekly averages), where this value is calculated as:

The proportion of the population effectively vaccinated is a function of doses administered and brand-specific efficacy and is discounted for existing natural immunity at the time of delivery.

Role of the funding source

The funders of the study had no role in the study design, data collection, data analysis, data interpretation, or the writing of the report.