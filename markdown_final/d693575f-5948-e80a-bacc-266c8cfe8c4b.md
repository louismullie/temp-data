# Medical Question & Answer

**Sample ID**: d693575f-5948-e80a-bacc-266c8cfe8c4b
**Dataset Index**: 272887

---

## Question

10,11

The number N is a perfect square and does not end in zero. After erasing the last two digits of this number, another perfect square is obtained. Find the largest number N with this property.

---

## Answer

> Let's see… What do we have here? The user is asking for the largest perfect square N that does not end in zero, such that removing its last two digits leaves another perfect square. Let's break this down step-by-step. First, I need to think about how to represent N and the truncated number mathematically. Then, I should verify the constraints on the last two digits and what that implies for the square root modulo 100. Next, I will set up the core equation and derive bounds on the possible values of the truncated square. After that, I need to check which candidate truncated squares yield valid endings and then reconstruct the full N to find the largest one. Finally, I will double-check that the solution satisfies all conditions and that I haven't missed a larger candidate.

> Let me first formalize the problem. Let N be a perfect square, so N = k² for some integer k, and N does not end in 00, which means k² mod 100 ≠ 0. When we erase the last two digits, we get M, which must also be a perfect square, so M = m² for some integer m. This means N = 100m² + r, where r is the two-digit remainder 0 ≤ r ≤ 99, and r ≠ 0 because N does not end in 00. Wait, let me verify that last point: if r were 0, N would end in 00, which violates the condition, so r must be at least 1 and at most 99.

> I need to ensure the last-two-digit constraint is translated correctly to k. Since N = k² and N mod 100 = r with r ≠ 0, it follows that k² mod 100 ≠ 0, so k cannot be a multiple of 10. Hold on, I should verify the converse: if k is not a multiple of 10, then k² mod 100 is indeed not 0, which preserves the "does not end in zero" condition for N.

> Next, I should review the core equation. Substituting N = 100m² + r into N = k² gives k² = 100m² + r, which rearranges to k² − 100m² = r. This is a generalized Pell-type equation, but I don't need to solve it explicitly; instead, I can bound m and then test candidate values. Since r ≤ 99, we have k² − 100m² ≤ 99, which implies k² ≤ 100m² + 99. Taking square roots, k ≤ √(100m² + 99) ≈ 10m + 4.95, so k ≤ 10m + 4. Similarly, k² > 100m² implies k > 10m, so k is strictly greater than 10m. Combining these, k must satisfy 10m < k ≤ 10m + 4, which means k ∈ {10m + 1, 10m + 2, 10m + 3, 10m + 4}.

> Now I will examine each of these four cases to see which can produce a valid two-digit remainder r = k² − 100m² that is not 00. Let me compute r for each candidate k: if k = 10m + 1, then r = (10m + 1)² − 100m² = 20m + 1; if k = 10m + 2, then r = 40m + 4; if k = 10m + 3, then r = 60m + 9; if k = 10m + 4, then r = 80m + 16. I should confirm that none of these expressions can equal 0 modulo 100 for any m, which aligns with the earlier point that k not divisible by 10 ensures r ≠ 0.

> Hold on, let's not jump to conclusions; I should also ensure that r is a two-digit number, i.e., 1 ≤ r ≤ 99. For k = 10m + 1, r = 20m + 1, so I need 20m + 1 ≤ 99, which gives m ≤ 4.9, hence m ≤ 4. For k = 10m + 2, r = 40m + 4, so 40m + 4 ≤ 99 gives m ≤ 2.375, hence m ≤ 2. For k = 10m + 3, r = 60m + 9, so 60m + 9 ≤ 99 gives m ≤ 1.5, hence m ≤ 1. For k = 10m + 4, r = 80m + 16, so 80m + 16 ≤ 99 gives m ≤ 1.0375, hence m ≤ 1. These bounds constrain how large m can be in each case, which will limit the size of N = k².

> I will now examine the viable candidates systematically, starting with the largest plausible m values to find the largest N. Let me first consider k = 10m + 4 with m = 1, which gives k = 14, r = 80(1) + 16 = 96, and N = 14² = 196. Truncating the last two digits gives 1, which is 1², so this works. But wait, what if m = 2? Then r = 80(2) + 16 = 176, which exceeds 99, so m = 2 is invalid for this case. Thus, the only valid candidate in this branch is N = 196.

> Next, I should review k = 10m + 3 with m = 1, which gives k = 13, r = 60(1) + 9 = 69, and N = 13² = 169. Truncating gives 1, which is 1², so this also works. For m = 2, r = 60(2) + 9 = 129, which is too large, so m = 2 is invalid here. Thus, the only valid candidate in this branch is N = 169.

> For k = 10m + 2, I should check m = 1 and m = 2. With m = 2, k = 22, r = 40(2) + 4 = 84, and N = 22² = 484. Truncating gives 4, which is 2², so this is valid. With m = 1, k = 12, r = 44, and N = 144, truncating to 1, which is 1², also valid. But since we are seeking the largest N, 484 dominates 144 in this branch.

> For k = 10m + 1, I need to check m = 1, 2, 3, 4. With m = 4, k = 41, r = 20(4) + 1 = 81, and N = 41² = 1681. Truncating gives 16, which is 4², so this is valid. With m = 3, k = 31, r = 61, N = 961, truncating to 9, which is 3², valid. With m = 2, k = 21, r = 41, N = 441, truncating to 4, which is 2², valid. With m = 1, k = 11, r = 21, N = 121, truncating to 1, which is 1², valid. Among these, 1681 is the largest in this branch.

> Let me now compare the largest valid N from each branch: 196 from k = 10m + 4, 169 from k = 10m + 3, 484 from k = 10m + 2, and 1681 from k = 10m + 1. Clearly, 1681 is the largest among these, but wait, I should verify that I haven't overlooked a larger possibility by testing higher m values where r remains ≤ 99. I already confirmed that for k = 10m + 4 and k = 10m + 3, m = 2 pushes r beyond 99, so no larger N exists in those branches. For k = 10m + 1, m = 5 would give r = 101, which is invalid, so 1681 is indeed the maximum in that branch as well.

> I should double-check the winning candidate. N = 1681 is 41², so it is a perfect square. It does not end in 00, satisfying the non-zero ending condition. Erasing the last two digits gives 16, which is 4², so the truncated number is also a perfect square. Therefore, 1681 meets all the stated requirements and is the largest such number.

---

The largest perfect square (N) that does not end in zero and becomes another perfect square when its last two digits are removed is **1681**. This is because 1681 is 41², and removing the last two digits gives 16, which is 4². No larger perfect square satisfies this condition, as verified by checking larger squares and confirming that truncating the last two digits never yields another perfect square.

---

## Mathematical formulation

Let (N) be a perfect square such that:

- N = k² for some integer (k).
- (N) does not end in zero.
- Removing the last two digits of (N) yields another perfect square, say m².

This implies:

N = 100 × m² + r

where (r) is the two-digit remainder (i.e. 0 ≤ r ≤ 99) and r ≠ 0 (since (N) does not end in zero).

---

## Solution approach

We need to find the largest (k) such that:

- k² = 100 × m² + r for some integers (m) and (r) with 1 ≤ r ≤ 99.
- m² is a perfect square.

Rearranging gives:

k² − 100 × m² = r

This is a generalized Pell-like equation, but since (r) is small (1–99), we can solve it by testing successive values of (m) and checking whether (100 × m² + r) is a perfect square for some (r) in the valid range.

---

## Verification of the largest solution

Starting from large (m) and working downward, we find:

- For (m = 4), 100 × 4² = 1600. Adding (r = 81) gives 1681 = 41², which is a perfect square. Removing the last two digits yields 16, which is 4².
- For (m = 5), 100 × 5² = 2500. Adding any (r) from 1 to 99 yields 2501–2599, none of which are perfect squares.
- For (m = 6), 100 × 6² = 3600. Adding any (r) from 1 to 99 yields 3601–3699, none of which are perfect squares.

Thus, **1681 is the largest such number**.

---

## Conclusion

The largest perfect square (N) that does not end in zero and becomes another perfect square when its last two digits are removed is **1681**.

---

## References

### Too many digits: the presentation of numerical data [^a41454ae]. Archives of Disease in Childhood (2015). Low credibility.

Thus a decimal places rule that ignores significant digits does not work. But equally, and perhaps surprisingly, a significant digits rule that ignores decimal places does not always work either. Reporting risk ratios to three significant digits for example leads to the largest ratio below 1 being reported as 0.999 and the smallest above 1 as 1.01, with three and two decimal places, respectively. This is clearly unsatisfactory as they differ in precision by a factor of ten. In this instance a combination of significant digits and decimal places, the rule of four, works best: round the risk ratio to two significant digits if the leading non-zero digit is four or more, otherwise round to three.

The rule of four gives three decimal places for risk ratios from 0.040 to 0.399, two from 0.40 to 3.99 and one from 4.0 to 39.9. Applying it to the example of 22.68 above gives 22.7 (95% CI 7.5 to 74). Alternatively one can apply the rule with one less significant digit, giving 23 with CI 8 to 70.

Another example is the reporting of test statistics such as t or F. Specifying one decimal place would permit say t = 30.1, where 30 is clearly sufficient as it is so highly significant. Conversely specifying two significant digits would permit t = −0.13, where again the extra precision is irrelevant as it is far from significant. A suitable rule specifies up to one decimal place and up to two significant digits.

When comparing group means or percentages in tables, rounding should not blur the differences between them. This is the basis for the Hopkins two digits rule, whereby the mean has enough decimal places to ensure two significant digits for the SD. An analogous rule for percentages might be to use enough decimal places to ensure two significant digits for the range of values across groups, eg, if the range is 10% or more use whole numbers, if less than 1% use two decimal places, and otherwise one. In practice percentages are usually given along with their corresponding frequencies, so precision is less critical as the exact values can be calculated.

---

### A critical edge number revealed for phase stabilities of two-dimensional ball-stick polygons [^956ab7b7]. Nature Communications (2024). High credibility.

To clarify this paradox, we perform simulations on a larger system with N = 22,500 monomers, whose results are consistent with the above (details in SI). With the larger simulation scale, we identify that, the translational correlation decays relatively fast locally but exhibits a typical algebraical decaying behavior in the long range, as illustrated by the red line in Fig. 5e, indicating that this solid state is still a crystal. Combined with the above features, we regard it as a distorted square lattice. The translational symmetry is further investigated by a 'renormalization' process, in which we divide the monomers into blocks, with n (= 1, 4, 9, 25, 100, 225) monomers in each block, and then calculate the translational correlation of the block COMs. After rescaling the distance by the characteristic length of the block, which is just the distance of the first peak in the correlation function, different curves collapse together, as shown in Fig. 5e, exhibiting the so-called self-similarity, which guarantees the existence of the translational symmetry. Furthermore, considering the locally disordered nature, this self-similarity strongly implies that it is a critical point for the ordered and disordered solid states, neither totally disordered as the triangular system nor perfectly ordered as a standard crystalline structure for pentagon, hexagon, or octagon. Since this holds at different temperatures (e.g. also T = 2.8 in Supplementary Fig. S 8f), the critical point is defined in the shape space in terms of a critical edge number, rather than in the thermal-parameter space.

When cooling down the distorted square lattice, the Voronoi cells have less dispersive areas but surprisingly higher concentrations of topological defects (Supplementary Fig. S 8b, c), even for the case as low as T = 0.01. This supports the mechanism that the local distortions are originated from the intrinsic properties of the system, in terms of interaction and shape rather than thermal fluctuations, emphasizing its critical nature in the shape space.

We also perform simulations at P = 5 and P = 10. Under different pressures, the square system still follows a solid-liquid transition without the appearance of the tetratic phase and the solid structures also share similar features (see SI).

Pentagon, hexagon, and octagon: entropy-dominant versus enthalpy-dominant

---

### A critical edge number revealed for phase stabilities of two-dimensional ball-stick polygons [^b97fa077]. Nature Communications (2024). High credibility.

In this work, we investigate phase stabilities of 2D regular ball-stick polygons (triangle, square, pentagon, hexagon, and octagon) by molecular dynamics (MD) simulation supplemented with some numerical calculation methods. As summarized in Fig. 1, with increasing edge number, we discover a critical edge numberfor the phase stabilities in this polygonal family below melting points: The triangular system is stabilized in a spin-ice-like glassy state rather than a crystalline structure at any finite temperatures before melting into liquid via a discontinuous phase transition; the square system exhibits a distorted square lattice structure, featured by a self-similar translational symmetry with a high concentration of topological defects; the pentagon, hexagon, and octagon systems are stabilized with the normal crystalline structures analogous to corresponding hard polygons. Moreover, for the crystalline state, is higher for polygons with more edges at a higher pressure but exhibits a crossover for hexagon and octagon at a lower pressure. Detailed analysis suggests that the perfect match of interaction and shape in the hexagon system induces an ultra-stability at low pressures, revealing the entropy-dominant mechanism at a higher pressure and the enthalpy-dominant mechanism at a lower pressure.

Fig. 1
Illustration of the crystalline phase stabilities of the 2D ball-stick polygons with different edge numbers (n).

As n increases, there exists a critical edge numberfor the stability of the crystalline morphology.

---

### A primer on deep learning and convolutional neural networks for clinicians [^1862c2ba]. Insights Into Imaging (2021). Medium credibility.

Why the deep in deep learning?

The depth of a neural network is its number of neuron layers. Deep Learning refers to the fact of having many more layers than in the so-called Machine Learning algorithms together with all the issues arose from it. The philosophy is the same for both cases but, having more layers usually implies further problems. We summarize here three of the most common issues in Deep Learning:
Vanishing gradient: the sigmoid activation function presents derivatives very close to zero for most of its range. This is not important when having a few layers, but for very deep neural networks, the product of many values too close to zero can result in a neural network that is unable to learn. This is easy to understand taking a look at the expressions in Fig. 9 and imagining what happens if the da/dz factor is very small. For Deep Learning algorithms, specific activation functions are used, such as ReLU or leaky ReLU (see Fig. 11), where the derivatives values (and hence the gradients) are greater than for a sigmoid. Each problem is different, but a rule of thumb can be to use ReLU (or leaky ReLU) for the hidden layers and use the sigmoid only for the last layer when working on a binary classification problem.
Overfitting problems: Overfitting occurs when a good fit is achieved on the training data, while the model does not generalize well on new, unseen data. This means that the model has learned patterns specific to the training data, which are irrelevant to other new or different data. This can also happen in Machine Learning algorithms, but are more of an issue for Deep Learning models due to the larger amount of neurons. It can be easily understood thinking about the number of neurons as if it were the degree of a polynomial: the greater the degree, the easier it is to find a curve passing by all the N points. But if we add a new point to the dataset, most probably the perfect N-points fit would fail on the N + 1 point, meaning that the model is not general enough for this specific problem. Several regularization methods have been developed to avoid the overfitting, such as dropout, data augmentation or L1 / L2 regularization. The dropout technique consists of randomly dropping out nodes during the training to avoid the over specialization of some neurons. The data augmentation technique is used to increase the amount of data by adding slightly modified copies of already existing data or newly created synthetic data from existing data, so that the algorithm does not see the same data twice. When working with images, the modifications can be geometric transformations, flipping, color modification, cropping, rotation, noise injection, random erasing, etc. The L 1/ L 2 regularization techniques consist of adding a penalty term to the loss function: the absolute value of the magnitude of the network weights for L1 and the squared magnitude for L 2.000000000000000e+00 When minimizing this modified loss function, the penalty term (that is always positive since it is either a magnitude or its squared value) will advantage network weights as small as possible while still trying to optimize the network performance. This will help the neural network to regularize itself, since it will favor simpler models. This can be understood again in terms of a fit to a polynomial: trying to have smaller polynomial coefficients will lead to a simpler model that will not try to perfectly fit every single outlier in the training dataset, but to give a more general result that will potentially have a better performance when exposed to new data. In any case, the generalization to new data is still one of the major problems of AI, especially in the medical context where datasets are sometimes not as large, varied and balanced as desirable. Fig. 11 ReLU and leaky ReLU activation functions
Convergence problems: a fast convergence of the gradient descent algorithm to the minimum is not guaranteed. The optimization algorithm to be used is another hyperparameter and a correct choice can mean the difference between good results in minutes, hours, or days. Several optimization algorithms have been developed to improve the convergence problems, such as the Adam optimization. This algorithm uses the squared gradients to scale the learning rate and it takes advantage of adaptive momentum by using the moving average of the gradient instead of gradient itself.

---

### A general derivation and quantification of the third law of thermodynamics [^43738480]. Nature Communications (2017). Medium credibility.

The most accepted version of the third law of thermodynamics, the unattainability principle, states that any process cannot reach absolute zero temperature in a finite number of steps and within a finite time. Here, we provide a derivation of the principle that applies to arbitrary cooling processes, even those exploiting the laws of quantum mechanics or involving an infinite-dimensional reservoir. We quantify the resources needed to cool a system to any temperature, and translate these resources into the minimal time or number of steps, by considering the notion of a thermal machine that obeys similar restrictions to universal computers. We generally find that the obtainable temperature can scale as an inverse power of the cooling time. Our results also clarify the connection between two versions of the third law (the unattainability principle and the heat theorem), and place ultimate bounds on the speed at which information can be erased.

---

### Full randomness from arbitrarily deterministic events [^0738cea4]. Nature Communications (2013). Medium credibility.

Do completely unpredictable events exist? Classical physics excludes fundamental randomness. Although quantum theory makes probabilistic predictions, this does not imply that nature is random, as randomness should be certified without relying on the complete structure of the theory being used. Bell tests approach the question from this perspective. However, they require prior perfect randomness, falling into a circular reasoning. A Bell test that generates perfect random bits from bits possessing high-but less than perfect-randomness has recently been obtained. Yet, the main question remained open: does any initial randomness suffice to certify perfect randomness? Here we show that this is indeed the case. We provide a Bell test that uses arbitrarily imperfect random bits to produce bits that are, under the non-signalling principle assumption, perfectly random. This provides the first protocol attaining full randomness amplification. Our results have strong implications onto the debate of whether there exist events that are fully random.

---

### Universal and uniquely human factors in spontaneous number perception [^5cea3268]. Nature Communications (2017). Medium credibility.

Some researchers have suggested that a combination of non-numeric dimensions is used to quantify sets of objects, instead of representing number directly. To test this we implemented a regression analysis, which included the combination of variables suggested (convex hull, density, cumulative surface area and average diameter) and also included number (equation (12)) on our results from Experiment 1. Again group was entered as a random effects term. If number is represented via these alternative dimensions then we should find no significant effect of number when this collection of variables is included in the model. This is not what we found. Instead, number was a significant predictor over and above all other measures (β number = 1.05, P < 0.001, n = 4,785). This persistent effect of number even when other variables are controlled shows that the effect of number cannot be accounted for by alternative variables. Additionally, the effect of number was significantly greater than any of the variables tested (z -tests: Number versus Convex Hull: z = 7.39, P < 0.001, n = 4,785; Number versus Density: z = 2.79, P < 0.01, n = 4,785; Number versus Area: z = 2.15, P < 0.05, n = 4,785; Number versus Dot Diameter: z = 5.51, P < 0.001, n = 4,785). The results suggest that numerical information is perceived directly, rather than indirectly through the representation of other quantitative dimensions.

---

### Dissociation of long-term verbal memory and fronto-executive impairment in first-episode psychosis [^716d3ba7]. Psychological Medicine (2009). Low credibility.

Executive function was measured using the Cambridge Automated Neuropsychological Test Battery (CANTAB; Sahakian & Owen). Working memory spatial span (Owen et al.) was measured by the ability to remember the order of sequences of squares presented on the screen in increasing number. Spatial working memory manipulation (Owen et al.) was measured by the number of errors made on a task in which subjects 'opened' sets of boxes, varying between three and eight in number, to find tokens. Planning (Owen et al.) was measured on a task where subjects moved coloured 'balls' in an arrangement on the screen to match a goal arrangement in problems differing in difficulty; accuracy was measured as the number of perfect solutions. Attentional set-shifting (Owen et al.) was measured in a task where subjects learned a series of visual discriminations in which one of two stimulus dimensions was relevant. On the penultimate extra-dimensional shift (EDS) stage, the rule was reversed so that a previously irrelevant dimension now became relevant. Number of errors at this stage assessed the ability to inhibit the previously correct response set by shifting attention from one dimension to another.

---

### Fault-tolerant error correction with the gauge color code [^496fc07d]. Nature Communications (2016). Medium credibility.

A special property of the gauge color code is that measurement errors, followed by syndrome estimation, will only introduce false defects in locally correctable configurations. Therefore, residual errors remain local to the measurement error. Moreover, the code is such that the probability of obtaining configurations of face outcomes that correspond to faux stabilizer defects decays exponentially with the separation of their cells. This is because the number of measurement errors that must occur to produce a pair of false stabilizer defects is extensive with their cell separation. To this end, the errors introduced from incorrect measurements are local to the measurement error and typically small. This property, coined 'confinement' in ref. is essential for fault-tolerant error correction. Most known codes achieve confinement by performing syndrome measurements many times. We give numerical evidence showing that our error-correction protocol confines errors in the following subsection.

The simulation

We simulate fault-tolerant error correction with encoded statesof linear size L where j indicates the number of error-correction cycles that have been performed and whereis a codeword. We seek to find a correction operator C such thatwhere E is the noise incident toafter N error-correction cycles.

To maintain the encoded information over long durations, we repeatedly apply error-correction cycles to keep the physical noise sufficiently benign. After a short period, the statewill accumulate physical noise E j (p) with error rate p. To correct the noise, we first estimate a stabilizer syndrome, s j, using gauge syndrome data with the syndrome estimation algorithm M q, where measurement outcomes are incorrect with probability q = p. Specifically, we have that. We then use the stabilizer decoding algorithm D to predict a suitable correction operator C j = D (s j), such that we obtain

---

### Experimental demonstration of a skyrmion-enhanced strain-mediated physical reservoir computing system [^704d6303]. Nature Communications (2023). High credibility.

The purpose of the waveform recognition task is to recognize the current (black square), last (green square), and second-to-last (orange square) waveform types (Fig. 3c) by using the sampled n data points from the current period of output signal and the trained weights corresponding to each point (see methods). The waveform sequences '0 1 1' and '0 0 1' are marked by the yellow and blue regions. The last cycle of the input signals in these two regions are different (sine wave in the yellow region and square wave in the blue region). The orange, green, black squares, and the yellow dots correspond to those in Fig. 3c. The coloured lines connecting squares and dots represent the output weight matrix W out. The dotted and solid curves in Fig. 3d show the recognition rates for the saturation and skyrmion state-based strain-mediated RC systems, respectively. The black, green, and orange curves in Fig. 3d show the recognition rates of the current, last, and second-to-last waveforms as a function of n, the number of data points sampled per period at V xy. For the current waveform, the recognition rates reach 100% (perfectly classified) on increasing n. The system will provide better efficiency to run a reservoir computer if a smaller n is used to realize higher accuracy in a task. For the second-to-last waveform, the recognition rates of both RC systems are all around 50%, indicating randomness and unpredictability. Interestingly, the two RC systems have similar trends with n; however, distinctive differences are observed for the last waveform recognition. Upon increasing n, the recognition rates increase rapidly with obvious randomness and then tend to saturate with n > 10. The recognition rates are 94.4% and 99.3% in the saturation and skyrmion states, respectively. The skyrmion-enhanced strain-mediated reservoir shows a better performance regarding the recognition rate for the last waveform. This result demonstrates the short-term memory effect of the strain-mediated reservoir, which is 2-waveform duration time (specifically to our experiment, the time scale is about 9 s × 2 = 18 s). The physical mechanism of the skyrmion enhancement is discussed later (see discussion section and supplementary information S4, S5). The reservoir's computational capabilities are evaluated by further analyzing the experimental data of the waveform classification task, where T delay, max = 2 is used in the calculation since the experimental results of the waveform classification only comprise the current, the last, and the second-to-last waveform. The parity check capacity (C PC) and short-term memory content (C STM) at the skyrmion state are both around 2.31, which are restricted by the slow operation of the present configuration. However, they could be significantly enhanced through involving the fast magnetization dynamics.

---

### Multispectral imaging method for rapid identification and analysis of paraffin-embedded pathological tissues [^84d46615]. Journal of Digital Imaging (2023). Medium credibility.

In the current study, Eq. (12) involves solving not a single system of equations, but one for each point x in the image, with the condition that the abundances must be numbers greater than zero. This non-negative least square (NNLS) algorithm is subject to all the solutions being greater than or equal to zero. Mathematically, obtaining the abundances is equivalent to solving the system, for each point x, where b is the regression vector of the least squares problem; hence, x would be calculated as. However, the U -matrix, in our case the collection of endmembers k, is generally not a square matrix (the number of wavelengths is usually greater than the number of spectra considered relevant); hence, it only makes sense to talk about solving the system approximately by using pseudo-inverses. That is, we use the formulawith the following solution, which is not exact since best is an approximation of b. Furthermore, the solution is restricted to considering only values in which " x " takes positive values. This algorithm is known as NNLS and is fast and requires a small amount of computer memory. In this work, NNLS implemented in the open-source Matlab Hyperspectral Toolbox has been used.

The initial problem of calculating the abundance is to find the spectra to be considered endmember. In this study, we used the structure of the eigenvectors associated with the PCs to obtain endmembers. As can be seen in Fig. 3, the pixel values of the absorbance of three images of a control eye at three wavelengthshave been represented in a "scatter plot" diagram. The directions associated with the eigenvectors of that "scatter plot" have also been represented; their length is proportional to data propagation in these directions. The standard deviation of this dispersion is given by, whereis the eigenvalue associated with the direction α; note that eigenvectors have a unit module. Hence, if we form the vector, we are forming a spectrum that would be represented as a pixel located at the ends of the red arrows shown in Fig. 3. These new vectors continue to be related to the eigenvectors and, at the same time, represent spectra located on the "outside" of the point cloud that forms the scatter plot. The new vectors also satisfy the conditions to be considered endmembers.

---

### Quantifying randomness in real networks [^e1049c29]. Nature Communications (2015). Medium credibility.

Results

General requirements to a systematic series of properties

The introductory remarks above instruct one to look not for a single base property Y, which cannot be unique or universal, but for a systematic series of base properties Y 0, Y 1,… By 'systematic' we mean the following conditions: (1) inclusiveness, that is, the properties in the series should provide strictly more detailed information about the network structure, which is equivalent to requiring that networks that have property Y d (Y d -random graphs), d > 0, should also have properties Y d ′ for all d ′ = 0, 1, …, d −1; and (2) convergence, that is, there should exist property Y D in the series that fully characterizes the adjacency matrix of any given network, which is equivalent to requiring that Y D -random graphs is only one graph — the given network itself. If these Y -series satisfy the conditions above, then whatever property X is deemed important now or later in whatever real network, we can always standardize the problem of explanation of X by reformulating it as the following question: what is the minimal value of d in the above Y -series such that property Y d explains X? By convergence, such d should exist; and by inclusiveness, networks that have property Y d ′ with any d ′ = d, d +1, …, D, also have property X. Assuming that properties Y d are once explained, the described procedure provides an explanation of any other property of interest X.

---

### Insightful classification of crystal structures using deep learning [^adfc71a4]. Nature Communications (2018). Medium credibility.

Despite its rather complicated functional form (see Eqs. (1) and (2)), the descriptor D F is one image for each system being represented (data point); the eight crystal classes considered in this work (see below) and examples of their calculated two-dimensional diffraction fingerprints are shown in Fig. 2b, c, respectively. This descriptor compactly encodes detailed structural information (through Eq. (1)) and — in accordance with scattering theory — has several desirable properties for crystal structure classification, as we outline below.

It is invariant with respect to system size: changing the number of periodic replicas of the system will leave the diffraction peak locations unaffected. This allows to treat extended and finite systems on equal footing, making our procedure able to recognize global and local order, respectively. We exploit this property, and instead of using periodically repeated crystals, we calculate D F using clusters of approximately 250 atoms. These clusters are constructed replicating the crystal unit cell (see Methods). By using finite samples, we explicitly demonstrate the local structure recognition ability of our procedure. The diffraction fingerprint is also invariant under atomic permutations: re-ordering the list of atoms in the system leads to the same D F due to the sum over all atoms in Eq. (1). Moreover, its dimension is independent of the number of atoms and the number of chemical species in the system being represented. This is an important property because machine learning models trained using this descriptor generalize to systems of different size by construction. This is not valid for most descriptors: for example, the Coulomb matrix dimension scales as the square of atoms in the largest molecule considered, while in symmetry functions-based approachesthe required number of functions (and thus model complexity) increases rapidly with the number of chemical species and system size. Being based on the process of diffraction, the diffraction fingerprint mainly focuses on atomic positions and crystal symmetries; the information on the atomic species — encoded in the form factorin Eq. (1) — plays a less prominent role in the descriptor. As a result, materials with different atomic composition but similar crystal structure have similar representations. This is the ideal scenario for crystals classification: a descriptor which is similar for materials within the same class, and very different for materials belonging to different classes. Finally, the diffraction fingerprint is straightforward to compute, easily interpretable by a human (it is an image, see Fig. 2c), has a clear physical meaning (Eqs. (1) and (2)), and is very robust to defects. This last fact can be traced back to a well-known property of the Fourier transform: the field at one point in reciprocal space (the image space in our case) depends on all points in real space. In particular, from Eq. (1) we notice that the field Ψ at point q is given by the sum of the scattering contributions from all the atoms in the system. If, for example, some atoms are removed, this change will be smoothened out by the sum over all atoms and spread over — in principle — all points in reciprocal space. Practically, with increasing disorder new low-intensity peaks will gradually appear in the diffraction fingerprint due to the now imperfect destructive interference between the atoms in the crystal. Examples of pristine and highly defected structures, together with their corresponding diffraction fingerprints, are shown in Fig. 2d–f, respectively. It is evident that the diffraction fingerprint is indeed robust to defects. This property is crucial in enabling the classification model to obtain a perfect classification even in the presence of highly defective structures (see below).

---

### A network-based framework for shape analysis enables accurate characterization of leaf epidermal cells [^49d0fb66]. Nature Communications (2021). High credibility.

We provide quantitative support for these results by calculating the Biological Homogeneity Index (BHI) to measure the quality of clusters (Supplementary Note 4). We find that GraVis has a perfect score for graphs with an equal number of nodes. In addition, using a different number of nodes and with and without reduction, based on modularity, GraVis still shows high cluster homogeneity and outperforms the approach based on Fourier transform (Supplementary Table 1). The rotational distance fares similarly well, whereas the Fourier transform results in the least homogeneous clusters (Supplementary Table 1).

To investigate the sensitivity of the algorithm to the spatial resolution of the node placement, we further used the set of synthetic shapes with the same number of nodes to illustrate the clustering quality based on different node densities. Therefore, we selected the large shapes (3 triangles, 4 squares, 3 circles) with each the equal number of nodes (n = 20) and used the node reduction method based on modularity clustering (Supplementary Fig. 4) to reduce the number of nodes of each graph stepwise, until all graphs contained 12 nodes. For all these graph sets we calculated the distance matrices and used them for hierarchical complete-linkage clustering (Supplementary Fig. 6). We then use the resulting clusters to compute the BHI and observe that it decreases for visibility graphs with a reduced number of nodes (Supplementary Fig. 7). The visibility graphs with 20 and 19 nodes per graph have a perfect score of 1.0 for all clusters, thus showing that the corresponding node density of 10–14 pixel/nodes is optimal for the detection of distinguishing global shape features. In addition, we observe that the BHI score is slightly below 0.9 for 17 and 18 nodes, demonstrating the robustness of our approach for small differences in node numbers. Changes in BHI for the number of nodes that differ by one is not larger than 36%.

Next, by comparing a set of selected pavement cells, which includes shapes along a gradient of complexity, we find that the Laplacian distance measure results in the most homogenous clusters, closely followed by the Fourier transform using the correlation distance (Supplementary Fig. 8, Supplementary Table 2). Therefore, we conclude that the proposed approaches for comparison of visibility graphs with the same or different number of nodes provide excellent ways for comparison of shapes and outperform classical solutions in the tested scenarios.

---

### Predicting phase behavior of grain boundaries with evolutionary search and machine learning [^18d166bc]. Nature Communications (2018). Medium credibility.

Clustering analysis

The three energy minima shown in Fig. 2a represent the lowest energy configurations of the three grain boundary phases. Other structures generated by the evolutionary search may correspond to variations of these three phases or belong to other grain boundary phases that have not been identified yet. For example, a Kite configuration with a single vacancy or an interstitial will have a different atomic density and energy from that of the perfect Kite structure. However, this defective grain boundary should still be identified with the Kite phase. In general, each grain boundary structure generated by the evolutionary search represents just one microstate. A grain boundary phase on the other hand is a macrostate: it is represented by an ensemble of similar microstates. To identify distinct macrostates, i.e. predict the number of grain boundary phases, we cluster the generated grain boundary structures based on the similarity in their properties. In a single component system a grain boundary is described by a set of excess properties such as excess volume per unit area [V] N, grain boundary stress tensorand the number of atoms [n] (See Supplementary Note 2 and Supplementary Fig. 6 for the definitions). First-order phase transitions manifest themselves by discontinuous changes in thermodynamic properties, which in turn suggests that these properties could be used to distinguish different macrostates. In addition to these thermodynamic properties which explicitly enter the equation of state or the adsorption equation –, we can formally introduce other excesses based on structural order parameters. In this work we use Steinhardt order parameters Q 4, Q 6, Q 8 and Q 12 designed to distinguish different bulk phases based on local environments. In our work Q-series were calculated for each atom in the system and the excess grain boundary amounts of [Q i] N per unit area were computed as described in the Supplementary Note 2. This new application of the Q-series was developed to capture differences in local environment present in different GB phases. We assign a vector f = ([n], [V] N, [Q 4] N, [Q 6] N, [Q 8] N, [Q 12] N) composed of four thermodynamic and four structural features to each grain boundary configuration. A distance between two grain boundary structures a and b is then calculated aswhere all the feature differences were renormalized, so that their values are in the range from 0 to 1. With the distance defined, the clustering was performed using the method of fast search and find of density peaks. In this method for each data point we calculate the number of neighbors ρ i within a cutoff distance d c and the minimum distance δ i from the point to the other point that has a higher number of neighbors. The centers of the clusters are then identified as points that have high number of neighbors and separated from each other by the largest distances. All other data points are then assigned to the closest cluster centers which completes the clustering procedure.

---

### Improved machine learning algorithm for predicting ground state properties [^d52497ff]. Nature Communications (2024). High credibility.

Finding the ground state of a quantum many-body system is a fundamental problem in quantum physics. In this work, we give a classical machine learning (ML) algorithm for predicting ground state properties with an inductive bias encoding geometric locality. The proposed ML model can efficiently predict ground state properties of an n -qubit gapped local Hamiltonian after learning from onlydata about other Hamiltonians in the same quantum phase of matter. This improves substantially upon previous results that requiredata for a large constant c. Furthermore, the training and prediction time of the proposed ML model scale asin the number of qubits n. Numerical experiments on physical systems with up to 45 qubits confirm the favorable scaling in predicting ground state properties using a small training dataset.

---

### A general patterning approach by manipulating the evolution of two-dimensional liquid foams [^7ec00d8c]. Nature Communications (2017). Medium credibility.

To prepare perfect hexagonal arrays of larger bubbles, increases in the side length of the hexagonal cells result in the increases in the pillar interval, which leads to the formation of defects. An alternative method is to increase the number of silicon pillars on each side (denoted as N) when increase the side length (denoted as l). As shown in Fig. 2c, the pillar interval a equals l /(N −1). For the same side length l, more pillars on each side N correspond to a smaller the pillar interval a, and an increased occurrence of the reverse Ostwald ripening phenomenon. This deduction agrees well with the experiments exhibited in Fig. 2d. The squares on the map mean that N is large and a /2 will be small and less than ρ; thus, only reverse Ostwald ripening happens, eventually forming perfect hexagonal bubble arrays. The crosses in the map suggest that N is small and a /2 will be much larger than ρ; thus, only the defect formation will happen. The triangles show that if N is not too large, a /2 will be slightly more than ρ, and some bubbles shows the reverse Ostwald ripening (Fig. 2a) and others exhibits the defect formation (Fig. 2b). The coexistence of reverse Ostwald ripening and defect formation leads to the formation of partly hexagonal bubble arrays (Supplementary Fig. 4b). The least number of silicon pillars on each side and the corresponding maximum pillar interval for preparing perfect bubble arrays of different side length were summarized in Fig. 2e. With the increase of side length, the N min increases significantly for rejecting the increase of pillar interval. Therefore the maximum pillar interval only increases slightly. The allowed maximum pillar interval, a max, can be considered 2 ρ because both are the allowed maximum pillar interval for forming perfect bubble arrays, that is, a /2 ≤ a max /2≈ ρ.

---

### Thermodynamics of structure-forming systems [^889f6e63]. Nature Communications (2021). High credibility.

Equilibrium thermodynamics of structure-forming systems

We now focus on the equilibrium thermodynamics obtained, for example, by considering the maximum entropy principle. Consider the internal energyUsing Lagrange multipliers to maximize the functionalleads to the following:and the resulting distribution isHere, we introduce the partial partition functions, and the quantity Λ = e − α. Λ is obtained fromwhich is a polynomial equation of order m in Λ. The connection with thermodynamics follows through Eq. (13). By multiplying withand summing over i, j, we get. Note thatis the number of clusters, divided by the number of particles in the system. The number of clusters per particle isThe Helmholz free energy is thus obtained asFinally, we can write the total partition function as

Comparison with the grand-canonical ensemble

To compare the presented exact approach with the grand-canonical ensemble, consider the simple chemical reaction, 2 X ⇌ X 2. Without loss of generality, assume that free particles carry some energy, ϵ. We calculate the Helmholtz free energy for both approaches in Supplementary Information. In Fig. 1, we show the corresponding specific heat. For large systems, the usual grand-canonical ensemble approach and the exact calculation with a strictly conserved number of particles converge. For small systems, however, there appear notable differences. This is visible in Fig. 1, where only for large n and low concentrations, the specific heat for the exact approach (squares) and the grand-canonical ensemble (triangles) become identical. The inset shows the ratio of the specific heat, c C / c GC − 1, vanishing for large n. For large systems, the exact approach and the the grand-canonical ensemble are equivalent.

Fig. 1
Specific heat, c (T), for the reaction 2 X ⇌ X 2 for the presented canonical approach with an exact number of particles in comparison to the grand-canonical ensemble.

The specific heat for the canonical ensemble (C) is drawn by squares, and the specific heat for the grand-canonical ensemble (GC) is drawn by triangles. n denotes the number of particles. For small systems the difference of the approaches becomes apparent. The inset shows the ratio of the specific heat calculated from the exact approach to the one obtained from the grand-canonical ensemble, c C / c GC − 1. For large n the quantity decays to zero for any temperature.

---

### Exponential suppression of bit or phase errors with cyclic error correction [^0c82f42b]. Nature (2021). Excellent credibility.

Logical errors in the repetition code

We decode detection events and determine logical error probabilities following the procedure in ref. Briefly, we use a minimum-weight perfect matching algorithm to determine which errors were most likely to have occurred given the observed detection events. Using the matched errors, we then correct the final measured state of the data qubits in post-processing. A logical error occurs if the corrected final state is not equal to the initial state. We repeat the experiment and analysis while varying the number of detection rounds from 1 to 50 with a fixed number of qubits, 21. We determine logical performance of smaller code sizes by analysing spatial subsets of the 21-qubit data (see Supplementary Section VII). These results are shown in Fig. 3a, where we observe a clear decrease in the logical error probability with increasing code size. The same data are plotted on a semilog scale in Fig. 3b, highlighting the exponential nature of the error reduction.

Fig. 3
Logical errors in the repetition code.

a, Logical error probability versus number of detection rounds and number of qubits for the phase-flip code. Smaller code sizes are subsampled from the 21-qubit code as shown in the inset; small dots are data from subsamples and large dots are averages. b, Semilog plot of the averages from a showing even spacing in log(error probability) between the code sizes. Error bars are estimated standard error from binomial sampling given the total number of statistics over all subsamples. The lines are exponential fits to data for rounds greater than 10. c, Logical error per round (ε L) versus number of qubits, showing exponential suppression of error rate for both bit-flip and phase-flip, with extracted Λ factors. The fits for Λ and uncertainties were obtained using a linear regression on the log of the logical error per round versus the code distance. The fit excludes n qubits = 5 to reduce the influence of spatial boundary effects (Supplementary Information section VII).

---

### Number of samples-hypothesis testing [^69e3a758]. Nuclear Medicine and Biology (2008). Low credibility.

This note reviews "further discussions on choosing the number of animals for an experiment". Particular emphasis will be given to the table therein.

---

### Using the number needed to treat in clinical practice [^646ae153]. Archives of Physical Medicine and Rehabilitation (2004). Low credibility.

The number needed to treat (NNT) is gaining attention as a method of reporting the results of clinical trails with dichotomous outcome measures. The NNT is defined as the number of patients who would need to be treated, on average, with a specific intervention to prevent 1 additional bad outcome or to achieve 1 desirable outcome in a given time period. Because it reports outcomes in terms of patient numbers, it is extremely useful to clinicians for making decisions about the effort expended with a particular intervention to achieve a single positive outcome. This special communication describes the NNT statistic and its utility for choosing clinical interventions.

---

### The complexity of NISQ [^7a2d31ee]. Nature Communications (2023). High credibility.

Finally, we consider a classical computer that can repeatedly run the noisy quantum device and analyze the output from the noisy quantum device.

These constraints are chosen to encapsulate the gap between the physical limitations of what we can achieve with existing quantum computers and general quantum computation. We note these considerations preclude the implementation of all known general fault-tolerant quantum computation schemes –, but that removing any one of these constraints would already allow for some form of nontrivial quantum fault tolerance. The obstruction to fault tolerance can be understood intuitively. The noisy quantum gates cause all qubits to accrue entropy, which cannot be pumped out until the measurement at the end. Since too much entropy would destroy all useful quantum correlations, it is not possible for the noisy quantum devices under the above constraints to perform an arbitrarily long quantum computation. Note that in contrast, if we consider more benign noise which does not increase entropy, such as dephasing or amplitude-damping noise, then there are already schemes for achieving fault-tolerant quantum computationwithin the above three constraints.

Motivated by the above considerations, in Section "Definition of NISQ" we formally define the NISQ complexity class to be the set of all problems that can be efficiently solved by a classical computer with access to a noisy quantum device that can (i) prepare a noisy poly(n)-qubit all-zero state, (ii) execute noisy quantum gates, and (iii) perform a noisy measurement on all of the poly(n) qubits. We subsequently show how the NISQ complexity class is situated relative to classical computers, and full-fledged quantum computers with quantum error correction.

---

### 120 GOPS photonic tensor core in thin-film lithium niobate for inference and in situ training [^03c64454]. Nature Communications (2024). High credibility.

The confusion matrix for 500 images (Fig. 4 d, e) shows an accuracy of 91.8% for the generated predictions, in contrast to 92% for the numerical results calculated on a CPU. Our IPTC achieved near theoretical accuracy, indicating that the in situ training scheme enables the system to inherently account for the hardware nonidealities, including fabrication variations and noise. Essentially, the nonidealities are "baked into" the training process. This has also been experimentally demonstrated in ref.

Images clustering

Supervised learning can successfully solve real-world challenges, but it has some drawbacks. One of the main limitations is that it requires a large number of accurately labeled data to train the model. Creating such a database is a time-consuming and resource-intensive task that may not always be feasible. In contrast, unsupervised learning can be operated on unlabeled data to discover its underlying structure, offering an alternative approach for extracting data features.

We demonstrate the potential of our device for unsupervised learning AI tasks by utilizing it to cluster the MNIST large-scale handwritten digits with principle component analysis — one of the most commonly used unsupervised learning models. Principle component analysis simplifies high-dimensional data by geometrically projecting them onto a limited number of principal components (PCs), i.e. unit vectors, to obtain the best summary of the data. Clustering handwritten digits with principle component analysis involves two main steps: (1) finding the PCs for the unlabeled database, i.e. training the model, and (2) projecting the data onto each PC. Here, we used the power method to find the PCs thatwhere A = XᵀX, Xᵀ means the transpose of X, X is a p × n data matrix with column-wise zero empirical mean, and p and n are the total number of handwritten digits and the pixels of each digit, respectively. b i is a n × 1 unit vector, obtained at the i t h iteration, and b 0 is a randomly generated unit vector. b i converges to the first PC (PC1) when the variance of the projected points, X b i, achieves the maximum value. The subsequent PCs can be obtained by a similar approach after subtracting all the previous PCs from X. More details regarding the power method can be found in Methods.

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^6517cbfd]. Annals of the American Thoracic Society (2023). High credibility.

Basic stereological measurements — first-order parameters and probes identify that "First-order parameters are volume (three-dimensional [3D]), surface (two-dimensional [2D]), length (one-dimensional [1D]), and number (zero-dimensional [0D])". Measurements "are performed using geometric test probes, such as points (0D), lines (1D), planes (2D), or volumes (3D)". These probes create countable events in the image, and "Raw counts provide ratios… that are multiplied by the reference space volume to obtain absolute measures for the lung or subcompartment".

---

### A model-free method to learn multiple skills in parallel on modular robots [^66aded13]. Nature Communications (2024). High credibility.

Real-world experimental setup

Our real-world experiments are the physical tests of algorithm 2 on the physical twins. We start by placing a real robot in the centre of a 7 × 4 m arena. We run a CPG-network with random weights and an initial state from a central laptop for 120 s. A custom built tracking system using ArUco markers tracks the robot's position and orientation every 0.1 s. At the end of the trial, the tracking data is then matched to the controller state at that time, and a rolling fitness is calculated for all the skills. This process is repeated until all the trials are complete.

A perfectly tracked run produces 600 samples, but due to noise and image merging artefacts, this number can sometimes be less. Although it rarely occurs in practice, missing robot data is filled in with the last previously captured state from the buffer. This operation only affects the fitness values when data is missing at the end of a trial because → fitness only considers the state at the end and ↺/↻ fitness performs a sum of the angular differences between subsequent samples, which is zero when samples are copied. Furthermore, filling in missing data with the last-seen values underestimates fitness values, insofar as it effectively shortens the evaluation period while simultaneously dividing by the same fixed evaluation time t e v a l = 60.

For ISO we test five random initial states per run (N t r i a l s = 5) with three repetitions (N r e p = 3). After all the trials are finished, we calculate the best overall initial state sample s 0* for each skill and re-test in order to obtain the final fitness for each skill in a single run. Including overhead and re-testing the acquired skill, the total time for the real robot experiments is t t o t a l ≤ 15 min.

For the sim-to-real experiments, we re-run the overall best WO* controller from the 30 runs completed in simulation for each skill on the physical robots. We re-test each WO* controller one time. We measure the performance in a similar fashion as we do for the ISO real setup. Ultimately, we compare the real-world performance of all three ISO runs and WO*.

---

### The snm procedure guideline for general imaging 6.0 [^638aaec4]. SNMMI (2010). Medium credibility.

SNM Procedure Guideline for General Imaging — matrix size and pixel depth in nuclear medicine acquisitions notes that matrix size is almost always a power of 2 with typical values 64 x 64, 128 x 128, 256 x 256 and 512 x 512, and that non-square matrix sizes also exist for whole-body studies. Each pixel can be represented with a single byte (pixel values ranging from 0 to 255 counts) or with 16 bit words (pixel values ranging up to a maximum of 32k or 64k). Overflow occurs when the number of counts recorded at some given position exceeds the maximum number of counts, and overflow is more likely to occur when using a byte matrix.

---

### Universal and uniquely human factors in spontaneous number perception [^bc660017]. Nature Communications (2017). Medium credibility.

We used mixed effects logistic regression equations of the general form:

where β {number, s }, β {area, s }, β {icpt, s } are by-subject adjustments to the overall number effect (β {number}), area effect (β {area}) and intercept (β {icpt}), respectively. The outcome P { s, i } is the probability of responding in agreement with the Number category on the s th subject's i th trial. This form of mixed effect/hierarchical regression is similar to fitting a regression within each subject and then averaging the resulting coefficients. However, it handles the statistics in a more robust and correct way, and also allows greater flexibility in testing hypotheses. Essentially, it provides the regression version of a paired (within-subjects) analysis. In the regression literature, this equation is often written as 'Category ∼ Number + Area + (1 + Number + Area | Subject)' where the first part, 'Number + Area' means that we are computing overall averages of coefficients and the second part '(1 + Number + Area | Subject)' means that we are adjusting these coefficients by ('|') subject. The following regressions were used:

Data availability

The data set used for this study is available from the corresponding author on request.

---

### Sample sizes based on three popular indices of risks [^4a61a58f]. General Psychiatry (2018). Low credibility.

Sample size justification is a very crucial part in the design of clinical trials. In this paper, the authors derive a new formula to calculate the sample size for a binary outcome given one of the three popular indices of risk difference. The sample size based on the absolute difference is the fundamental one, which can be easily used to derive sample size given the risk ratio or OR.

---

### The cost of large numbers of hypothesis tests on power, effect size and sample size [^9df69432]. Molecular Psychiatry (2012). Low credibility.

To compensate for a greater number of tests, a more realistic strategy may be to increase the sample size (equation 2). Table 1 and Figure 2c give sample sizes needed to maintain the original power at the original targeted effect size. For one million tests, the sample size multiplier is 5.06 for 80% power and 4.33 for 90% power, using equation (2). In the first example above, 506 subjects would be sufficient to reach 80% power to detect that the means differ by 2 in one million Bonferroni-adjusted tests. Although it might appear counterintuitive, the sample size multiplier is smaller for 90% power because the initial sample size is larger. In the same example, 132 subjects would be needed to reach 90% power for one test. For 90% power for one million tests, 4.33 × 132 = 572 subjects are needed. Noting the nearly linear relationship in Figure 2c, we also obtained an approximate rule-of-thumb for the sample size multiplier by fitting zero-intercept linear regression models to the results in Figure 2c. The estimated slopes show that m is approximately, where γ = 1.2 for 50% power, 0.68 for 80% power, 0.55 for 90% power and 0.38 for 99% power.

The rate at which the critical value and, consequently, the effect size and sample size multipliers increase becomes slower and slower as the number of tests becomes larger (Figure 2d), owing to the exponential decline in the tails of the Normal density. For example, effect size multipliers for one million vs ten million tests at 80% power are 2.25 and 2.39, respectively. Sample size multipliers are 5.06 and 5.71. At 80% power, ten million tests require only a 6% increase in the targeted effect size or a 13% increase in the sample size when compared to one million tests. In contrast, 10 tests require a 30% increase in the targeted effect size or a 70% increase in the sample size as compared with a single test. For 80% power and one billion or one trillion tests, the required sample sizes, respectively, are approximately 7 or 9 times that needed for a single test. See Table 1 for some numerical results and the provided Excel calculator (Supplementary Table 1) to explore unreported results and specific study designs.

---

### Genomic prediction of complex human traits: relatedness, trait architecture and predictive meta-models [^d8688c33]. Human Molecular Genetics (2015). Low credibility.

Finally, we note that the problem of over-fitting, and hence the need for regularization, is particularly prominent when we want to fit a complex model with relatively few data points (here, individuals). In the context of genomic predictions, this problem is relevant when we consider whole-genome regression methods. This is often referred to as the 'large p small n' situation, where the number of model parameters, β j 's, corresponds to the number of genomic markers, P, and is typically one or two orders of magnitude larger than the number of samples (individuals), n, in a data set. When P > > n, the solution to the least squares objective in Equation (4) will fit the data points in D perfectly. However, a lot of the β j 's will be describing the 'noise' in D, i.e. unique idiosyncrasies of the specific data set, which do not generalize in the overall population.

---

### Experimental realization of the quantum universal NOT gate [^1c1416bf]. Nature (2002). Excellent credibility.

In classical computation, a 'bit' of information can be flipped (that is, changed in value from zero to one and vice versa) using a logical NOT gate; but the quantum analogue of this process is much more complicated. A quantum bit (qubit) can exist simultaneously in a superposition of two logical states with complex amplitudes, and it is impossible to find a universal transformation that would flip the original superposed state into a perpendicular state for all values of the amplitudes. But although perfect flipping of a qubit prepared in an arbitrary state (a universal NOT operation) is prohibited by the rules of quantum mechanics, there exists an optimal approximation to this procedure. Here we report the experimental realization of a universal quantum machine that performs the best possible approximation to the universal NOT transformation. The system adopted was an optical parametric amplifier of entangled photon states, which also enabled us to investigate universal quantum cloning.

---

### The eighty five percent rule for optimal learning [^85cf0442]. Nature Communications (2019). High credibility.

Two-layer network with MNIST stimuli

As a more demanding test of the Eighty Five Percent Rule, we consider the case of a two-layer neural network applied to more realistic stimuli from the Modified National Institute of Standards and Technology (MNIST) dataset of handwritten digits. The MNIST dataset is a labeled dataset of 70,000 images of handwritten digits (0 through 9) that has been widely used as a test of image classification algorithms (see ref.for a list). The dataset is broken down into a training set consistent of 60,000 images and a test set of 10,000 images. To create binary classification tasks based on these images, we trained the network to classify the images according to either the parity (odd or even) or magnitude (less than 5 or not) of the number.

The network itself consisted of 1 input layer, with 400 units corresponding to the pixel values in the images, 1 hidden layer, with 50 neurons, and one output unit. Unlike the Perceptron, activity of the output unit was graded and was determined by a sigmoid function of the decision variable, h where the decision variable was given bywhere w 2 were the weights connecting the hidden layer to the output units and a was the activity in the hidden layer. This hidden-layer activity was also determined by a sigmoidal functionwhere the inputs, x, corresponds to the pixel values in the image and w 1 were the weights from the input layer to the hidden layer.

All weights were trained using the Backpropagation algorithmwhich takes the error, and propagates it backwards through the network, from output to input stage, as a teaching signal for the weights. This algorithm implements stochastic gradient descent and, if our assumptions are met, should optimize learning at a training accuracy of 85%.

---

### 111In-leukocyte scintigraphy for suspected infection / inflammation 3.0 [^7c3eefef]. SNMMI (2024). High credibility.

111In-leukocyte quality control includes determining labeling efficiency by recentrifugation (approximately 450 g for 5 min) of washed cells resuspended in 5 mL of buffered saline, with the leukocyte pellet resuspended in 5 mL of cell-free plasma; leukocyte clumping may be checked on a hemocytometer slide; and the average number of cells per 50-micron (small) square is then determined with No. cells/cm³ (mL) = average number of cells/small square × (2 × 10⁶).

---

### Sample size and power [^881444c9]. American Journal of Infection Control (2012). Low credibility.

A quick review of the basics of sample size and power is presented. Readers can participate in an online exercise that introduces them to a power calculator that can be used in their practice, and illustrates the concepts discussed in the article.

---

### Non-equilibrium critical scaling and universality in a quantum simulator [^a55212f1]. Nature Communications (2025). High credibility.

Fig. 4
Unscaled (a) and scaled (b) fluctuations after a double quench.

a We plot the unscaled fluctuations along y direction at the predicted critical points for system sizes of N = 10–50 ions. The second quench is applied when the fluctuations following the first quench reach their maxima and the time t 2 is counted after the second quench. b We apply the same scaling collapse technique as for the single quench to find the best scaling exponents (α 2, ζ 2) for the experimental data. See Supplementary Fig. 4 for numerical simulations of the data. We observe that the critical fluctuations do not monotonically grow for increasing system sizes, as would be expected from the scaling relations. This effect can be attributed to the imperfect switching time between the first and second quench; a nearly perfect collapse can be reproduced numerically using the precise switch times (SI Sec. VII.D). We also report exponents found by a power-law fit of the maximum fluctuations which agree more closely with the analytical prediction (Inset b.). While determining the experimental critical exponents, we have excluded 50 ion data (gray) [see main text for details]. The error bars of the experimental data are statistical fluctuation around the mean value.

Experimental decoherence causes the observed fluctuations to be damped for both single and double quenches. We see that the unscaled 50 ion fluctuations after the double quench are significantly damped (Fig. 3 a). The major sources of decoherence, which scale with the system size, remain within acceptable thresholds for system sizes N < 50, but these errors start to dominate for N ≥ 50 ("Methods"). This effect is more adverse for the double-quench sequence than the single-quench, since the former involves longer evolution under two quenches. For completeness, we have included all the 50 ion data in Fig. 4 a, c, but excluded it in determining the best collapse exponent. Fitting the maximum amplitudes of the fluctuations toyields exponent α 2 = 0.69(9), with tighter error bounds (Inset Fig. 4 c). Errors in identifying the peak fluctuation result in erroneous switch time between the two quenches, contributing to imperfect exponents. This effect can be reproduced in the simulation with exact experimental parameters, and correction for such errors in further simulations results in exponents that are well in agreement with the analytically predicted non-equilibrium values (SI Sec. VII.D).

---

### How I read an article that uses machine learning methods [^0f2f5676]. Blood Advances (2023). Medium credibility.

Step 1: Understand the problem being addressed. The first step in reading an ML paper is to understand the problem that the authors are trying to solve and, more importantly, understand the clinical or scientific impact of solving this problem. In other words, if the aim of the study is to solve a clinical problem, how does the answer or the recommendation provided by the algorithm help physicians or researchers in their day-to-day practice, and is this solution mature enough to be implemented in clinical workflows? Major clinical problems in health care can mainly affect either patient outcomes or operations (can I make the process easier and faster for the patient and the health care system?).

Step 2: Assess the quality of the data. The quality of the data used to build the ML model is crucial for the validity of the results. Following are some questions that can be used to evaluate the data:
1 Sample size: Is the size of the training, validation, and test sets enough to build a reproducible and generalizable ML model? Is this size of the data appropriate for the chosen methods (ie, some methods are "data-hungry" and understanding which methods require larger datasets is key)? However, different algorithms require different data types (image, tabular, text, or others) and sizes, and there are no rules of thumb or formulas that can estimate the perfect data.
2 Relevance: Are the data appropriate and relevant to the problem that the model is trying to solve?
3 Accuracy: How are the data collected and annotated (human vs natural language process). How are the data transformed to make it ready for ML use, etc.
4 Consistency: Are the data consistent? Do they have any missing values and how the authors dealt with this?
5 Representativeness: The data should be representative of the population being studied.
6 Balance: The data should be balanced, with roughly equal representation of all relevant classes or groups. However, most health care data are unbalanced. It is critical to understand how the authors dealt with unbalanced data.
7 Bias: To evaluate bias in data, it is important to look at the distribution of certain characteristics, such as race, gender, or socioeconomic status, among the samples in the data set, and how the data were collected. This will help to identify any disparities or overrepresentation of certain groups, which can indicate the presence of bias in the data. It is critical to evaluate bias at this stage because if this is not addressed properly, it could produce a biased model.

---

### Machine learning in spectral domain [^b6a6f35e]. Nature Communications (2021). High credibility.

In Fig. 6, we report the results of the tests performed when operating under the deep linear configuration. Symbols are analogous to those employed in Fig. 5. In all inspected cases, the entry layer is made of N 1 = 784 elements and the output one has N ℓ = 10 nodes. The first five points, from left to right, refer to a three layers (linear) neural network. Hence, ℓ = 3 and the size of the intermediate layer is progressively increased, N 2 = 20, 80, 100, 500, 800. The total number of trained eigenvalues is N 2 + N 3, and gets therefore larger as the size of the intermediate layer grows. The successive four points of the collections are obtained by setting ℓ = 4. Here, N 2 = 800 while N 3 is varied (= 100, 200, 400, 600). The training impacts on N 2 + N 3 + N 4 parameters. Finally, the last point in each displayed curve is obtained by working with a five layers deep neural network, ℓ = 5. In particular, N 2 = 800, N 3 = 600 and N 4 = 500, for a total of N 2 + N 3 + N 4 + N 5 tunable parameters. Also in this case, the spectral algorithm performs better than conventional learning schemes constrained to operate with an identical number of free parameters. Similarly, the distribution of the weights of an equivalent perceptron trained in reciprocal space matches that obtained when operating in the space of the nodes and resting on a considerably larger number of training parameters. To sum up, eigenvalues are parameters of key importance for neural networks training, way more strategic than any other set of equivalent cardinality in the space of the nodes. As such, they allow for a global approach to the learning, with significant reflexes of fundamental and applied interest. In all cases here considered, the learning can extend to the eigenvectors: an optimised indentation of the eigen-directions contribute to enhance the overall performance of the trained device.

---

### Topological solitonic macromolecules [^a6d20815]. Nature Communications (2023). High credibility.

Numerical modeling

For chiral nematic LCs, the energy cost of spatial deformations of n(r) can be expressed by the Frank-Oseen free energy functional:

Here the Frank elastic constant, anddetermine the energy cost of splay, twist and bend deformations, respectively. Further, the surface energy iswhere W is the surface anchoring strength andthe preferred orientation of which is perpendicular to the surface. When external electric fieldis applied, the dielectric properties of LCs induce an additional dielectric coupling term in the free energy, so that the free energy is supplemented by the following electric coupling term:whereis the vacuum permittivity, andis the dielectric anisotropy of the LC. The total free energy F is the sum of F elastic, F electric and F surface. Polyskyrmionomers of different orders emerge as local or global minima of F, and a relaxation routine based on the variational method is used to identify an energy-minimizing configuration n(r). Applying this method, at each iteration of the numerical simulation n (r) is updated based on a formula derived from the Euler-Lagrange equation, where subscript i denotes spatial coordinates, denotes the functional derivative of F with respect to n i, and MSTS is the maximum stable time step of the minimization routine, determined by the elastic constants and the spacing of the computational grid. To scale the time step in the real system, we assume that the director dynamics is governed by the balance equation:whereis the rotational viscosity. The end-of-the-relaxation condition is identified by monitoring the change in the spatially averaged functional derivatives in consecutive iterations. Approaches zero, it signifies proximity of the system to a steady state, and the relaxation routine comes to a halt. The 3D spatial discretization is performed on large 3D square-periodicgrids, and the spatial derivatives are calculated using finite-difference methods with the second-order accuracy, which allows us to minimize discretization-related artifacts in modeling polyskyrmionomers of different orders. To encode data in the 8-bit binary format, we first derive stable structures of nonamers, then we slightly move the initial position of the intrinsic defects and let them relax (Supplementary Fig. 1). As a result, the non-zero skyrmion number spontaneously emerges along the x direction. For all simulations, the following parameters were used: d / p 2, = , and.

---

### The use of plots in orthopaedic literature [^95fed5f5]. The Bone & Joint Journal (2015). Low credibility.

Plots are an elegant and effective way to represent data. At their best they encourage the reader and promote comprehension. A graphical representation can give a far more intuitive feel to the pattern of results in the study than a list of numerical data, or the result of a statistical calculation. The temptation to exaggerate differences or relationships between variables by using broken axes, overlaid axes, or inconsistent scaling between plots should be avoided. A plot should be self-explanatory and not complicated. It should make good use of the available space. The axes should be scaled appropriately and labelled with an appropriate dimension. Plots are recognised statistical methods of presenting data and usually require specialised statistical software to create them. The statistical analysis and methods to generate the plots are as important as the methodology of the study itself. The software, including dates and version numbers, as well as statistical tests should be appropriately referenced. Following some of the guidance provided in this article will enhance a manuscript.

---

### The problem with unadjusted multiple and sequential statistical testing [^02331c8d]. Nature Communications (2019). High credibility.

In research studies, the need for additional samples to obtain sufficient statistical power has often to be balanced with the experimental costs. One approach to this end is to sequentially collect data until you have sufficient measurements, e.g. when the p -value drops below 0.05. I outline that this approach is common, yet that unadjusted sequential sampling leads to severe statistical issues, such as an inflated rate of false positive findings. As a consequence, the results of such studies are untrustworthy. I identify the statistical methods that can be implemented in order to account for sequential sampling.

---

### Stochastic representation of many-body quantum States [^87cd3ffa]. Nature Communications (2023). High credibility.

Methods

Regression parameters

In the examples shown, 80% of samples at iteration j are generated from the probability distributionof the previous step (which is the initial guess when j = 1) by importance sampling using the Metropolis algorithm. Note that this does not require the wavefunction to be normalized. Another 20% are uniformly distributed in a large domain around the wavefunction, helping to ensure that the wavefunction is suppressed where it should be. The size of the domain in which the samples are distributed is chosen such that it covers all features of the wavefunction. In order to estimate how large that area needs to be, we can consider the (usually analytically tractable) asymptotic behavior and choose a cutoff where the wavefunction is sufficiently suppressed. For all harmonic oscillator cases, we chose a domain where all coordinates are between −5 and 5. The restriction of the domain affects the uniformly distributed samples as well as those we generate through importance sampling. Since we obtain the majority of the samples through importance sampling, they will mostly be concentrated in areas where the wavefunction has large absolute values.

All calculations use stochastic gradient decent in the optimization process and a simple network structure with 1–5 hidden layers, each layer consisting of up to 2048 neurons. This proves to be expressive enough even for more complicated wavefunctions. The learning rate is the hyperparameter we focused on most during optimization. To find its optimal value, we start with a high learning rate and gradually decrease its initial value until the learning curve declines within the first 10 epochs. As a criterion for an accurate fit, we use a mean squared error below 10 −4, which is relatively small compared to the maximal absolute value of the wavefunction (normalized to 1). Our data is divided into a training set and a validation set, and in order to avoid overfitting we always verify that the validation loss does not deviate from the training loss.

---

### Harnessing citizen science through mobile phone technology to screen for immunohistochemical biomarkers in bladder cancer [^6d081954]. British Journal of Cancer (2018). Low credibility.

Improving efficiency of crowdsourced scoring

In early 2015, there was concern that users were finding it difficult to score the outer squares of a TMA core, which often contained only a handful of cells and therefore lacked the tissue structure that often helps distinguish cancer from non-cancer tissue. An analysis was performed using an MRE11 test set to assess the effect of scoring only the central 16 squares compared to all 36 squares. This revealed that these mostly empty segments could be discarded from analysis without detrimental effect on accuracy. Additionally, an interim analysis was performed in August 2015 to calculate accuracy of scoring as a function of the number of ratings per image. This was to see whether the datasets could be processed more quickly by reducing the number of raters from 25 per segment (Fig. 3a). Based on this figure it was decided that obtaining more than 5 ratings per segment, i.e. 80 ratings across the 16 segments of a core, would yield minimal additional accuracy. For example, with 80 raters per core the p21 stain is scored with an accuracy of 0.85. Having 1000 raters per core would yield an accuracy of 0.89. The trade-off, then, is to sacrifice 0.04 in accuracy to be able to analyse 12 times more stain types. Hence, we decided to use these additional ratings to analyse additional stains.

Fig. 3
a Scatter plots for individual IHC stains ranked in order of H-score Spearman rho. X -axes represent the expert scores and y -axes the citizen score. Diagonal line represents a perfect score whereby the expert score is identical to the crowdsourced score; b The relationship between number of classifications and accuracy. The y -axis represents the H-score Spearman rho between expert and crowdsourced scores, and the x-axis represents the number of classifications used for a core. The accuracy is estimated through bootstrapping with 1000 samples. The error bars represent the bootstrapped 95% confidence interval (2.5 and 97.5 percentile of bootstrapped samples)

---

### Data-driven discovery of dimensionless numbers and governing laws from scarce measurements [^8fa447ff]. Nature Communications (2022). High credibility.

We design an iterative two-level optimization scheme to determine the two sets of unknown parameters in the regression problem, namely the basis coefficients γ and polynomial coefficients β. The optimization scheme includes multiple iterative steps. At each step, we adjust the first-level basis coefficients γ while holding the second-level polynomial coefficients β constant, and then optimize the second-level polynomial coefficients β while keeping the first-level basis coefficients γ constant. This process is repeated until the result is converged, that is, the values of γ and β remain unchanged. There are several advantages to the proposed two-level approach over a single-level approach that combines the two sets of unknowns together during optimization. We can use different optimization methods and parameters (such as the learning rate) for these two-level models to significantly improve the efficiency of the optimization. More importantly, we can utilize physical insights to inform the learning process. The first-level basis coefficients γ have a clear physical meaning, which is related to the powers that produce the dimensionless number. Thus, those values have to be rational numbers to maintain dimensional invariance. Moreover, their typical range is limited. It is worth noting that the absolute values of the coefficients in most of the dimensionless numbers and scaling laws are less than four. To leverage those physical insights or constraints, we design several methods for optimizing the first-level basis coefficients, including a simple grid search (used in this section) and a much more efficient pattern search (Section 4.2 of the SI). For the second-level coefficients, we conduct multiple standard representation learning methods, including the polynomial regression used in this section, tree-based extreme gradient boosting (XGBoost) used in the section "Porosity formation in 3D printing of metals", and general gradient descent method (Section 4.1 of the SI). Details on the two-level optimization framework are provided in Section 4 of the SI.

---

### The problem with unadjusted multiple and sequential statistical testing [^d8d1537a]. Nature Communications (2019). High credibility.

Fig. 2
False discovery rate for unadjusted sequential testing (blue curve) and uncorrected multiple independent testing (red curve)

As can be seen, the issue is very severe — although less severe than the case of uncorrected multiple independent tests. Even if you would apply some rule where you stop collecting new data once n exceeds, say, 25, your false discovery rate exceeds 25%. Rather than the one-in-twenty chance of labelling a null result significant, we have a one-in-four chance, five times higher than intended.

Note that this problem not only affects the p -values, but also the estimates themselves. With sequential sampling, with each step the distance between the means of both groups will sometimes increase, sometimes decrease — simply due to coincidence. If we continue sampling until the means of both groups are sufficiently far apart in order to call it significant, we overestimate the effects. Thus, not only is the significance biased, so is the effect size.

So, in an attempt to require as few measurements — whether it concerns animals, participants, or something else — as possible for the experiment, this strategy would actually invalidate a study. Even more worrisome, it does so in a way that cannot be corrected for in a later stage. Thus, the informational value of the study is diminished, such that a new study is needed. In the end, this leads to more test animals/participants/etc. being needed, rather than less.

I outlined why unadjusted sequential testing is problematic. (Note that I'm by far not the first to do this, see e.g. and the references therein.) This does not imply, however, that the concept of sequential analysis — increasing your sample size in small bits until you meet some threshold — is not a good idea. It actually is a good idea, provided the necessary corrections have been made, as it safeguard against taking a sample larger than necessary (ref. p.448,449). There are two classes of such sequential approaches: interim analyses (also known as group sequential analyses) and full sequential analyses.

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^7e0b2d1d]. Annals of the American Thoracic Society (2023). High credibility.

Basic stereological measurements for lung imaging — first-order parameters, probes, and reference space are specified. First-order parameters are volume (three-dimensional [3D]), surface (two-dimensional [2D]), length (one-dimensional [1D]), and number (zero-dimensional [0D]). Measurements are performed using geometric test probes, such as points (0D), lines (1D), planes (2D), or volumes (3D), with probe–voxel interactions generating countable events; raw counts yield ratios that are multiplied by the reference space volume to obtain absolute lung or subcompartment measures. It is crucial to define and measure a biologically meaningful reference space for analysis and reporting, because measurements expressed only as ratios are subject to "the reference trap", where changes can arise from the numerator, denominator, or both; an efficient volume measurement method, point counting, is used for acinar components by micro-CT and for extrapulmonary organ volume by CT.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^dee6cacc]. CDC (2011). Medium credibility.

Community viral load sample size calculations — sample size to detect differences in geometric mean (GM) viral load depends on power and standard deviation (S). The table specifies "α = 0.05 and W = 0.8", with S columns 1 to 1.5; for k = 3 the minimum sample sizes by S are 54, 66, 78, 92, 106, and 122, and "1.2 is the standard deviation of national VL data", so at S = 1.2 the needed sample size is 78.

---

### Recommendations for measuring pulmonary nodules at CT: a statement from the Fleischner society [^eb2981f8]. Radiology (2017). Medium credibility.

Pulmonary nodule CT measurements — rounding and recording: Results should be rounded to the nearest whole number, and recording both long- and short-axis diameters should also be rounded to the nearest millimeter; when estimating average diameter with manual or automated tools, fractional measurements may be considered, but the result should still be recorded as a whole number.

---

### Healthy housing reference manual [^10031d72]. CDC (2006). Medium credibility.

Portable fire extinguishers — ratings, placement, and operation are outlined for home use. Fire extinguishers have a number rating for Type A fires in gallons of water: 1 means 1¼ gallons of water; 2 means 2½ gallons of water; 3 means 3¾ gallons of water, and for Type B and Type C fires the number represents square feet, for example 2 equals 2 square feet and 5 equals 5 square feet. For multi-class units, an extinguisher labeled 2A5B is good for Type A fires with a 2½-gallon equivalence and for Type B fires with a 5-square-foot equivalency. A good extinguisher to have in each residential kitchen is a 2A10BC fire extinguisher. An extinguisher should be placed in the kitchen and in the garage or workshop, and each extinguisher should be installed in plain view near an escape route and away from potential fire hazards such as heating appliances. PASS is a simple acronym to remind you how to operate most fire extinguishers — pull, aim, squeeze, and sweep. Pull the pin at the top of the cylinder. Aim the nozzle at the base of the fire. Squeeze or press the handle. Sweep the contents from side to side at the base of the fire until it goes out. Shut off the extinguisher and then watch carefully for any rekindling of the fire.

---

### Genome-driven evolutionary game theory helps understand the rise of metabolic interdependencies in microbial communities [^ab9d7d94]. Nature Communications (2017). Medium credibility.

Automated identification of the Nash equilibria

Upon constructing the payoff matrix, as described above, one can identify the Nash equilibria of the game. We developed NashEq Finder, an optimization-based procedure to automate the identification of all pure strategy Nash equilibria of an n -player game. Here, for ease of presentation, we describe the NashEq Finder formulation for a two-player non-symmetric game. Let P and Q denote the set of all conceivable strategies for player 1 and player 2, respectively. A binary decision variable is defined as follows to capture whether or not each entry pq of the payoff matrix satisfies the conditions of a Nash equilibrium:Now, let the entry pq of the payoff matrix constitutewithandbeing the payoffs of players 1 and 2, respectively. NashEq Finder can be formulated as follows:where, LB 1 and LB 2 are non-zero lower bounds on the payoff values of players 1 and 2, respectively, The objective function of this optimization problem maximizes the values of binary variables corresponding to entries of the payoff matrix. The definition of binary variable w pq is mathematically imposed by Constraints (6) and (7). In particular, Constraint (6) mathematically describes the conditions of a pure strategy Nash equilibrium for player 1, i.e. if player 2's strategy is fixed at q ∈ Q, player 1 attains its maximum payoff by taking strategy p ∈ P. Constraint (7) imposes the same condition for player 2 (see Supplementary Methods for an example detailing how these constraints work). Notice that NashEq Finder is an integer linear program (ILP), which can be always solved to global optimality (if an optimal solution exists). Upon solving this optimization problem, any entry of the payoff matrix for which the corresponding binary variable is equal to one will be a Nash equilibrium. NashEq Finder can thus identify all pure strategy Nash equilibria of the game in one shot by solving this ILP problem. If this optimization problem is infeasible, it means that no pure strategy Nash equilibrium exists. This formulation can be easily generalized for an n -player game. A python script implementing NashEq Finder for an n -player game is available in Supplementary Software 1. A rudimentary assessment of the computational efficiency of the NashEq Finder for the case studies presented in this paper is also provided in Supplementary Methods.

---

### The problem with unadjusted multiple and sequential statistical testing [^d9b83228]. Nature Communications (2019). High credibility.

As most editors and reviewers are aware of the need for multiple testing, it rarely happens in published research that authors explicitly abstain from any correction for multiple testing. This does not imply that this practice is without problems. First, it is not straightforward to decide which tests within a single paper constitute the 'family' for which the familywise error rate needs to be capped at 5%. Consider, for instance, the common situation of a two-way ANOVA. Here, one performs three tests: a main effect of each of both 'ways' plus an interaction. Yet, researchers rarely correct for this.

Second, correcting for many tests has a deteriorating effect on the statistical power (too often not rejecting H 0 even though it is false). Third, one could present fewer comparisons than were actually performed, and thus employ a more lenient correction. For instance, when a study has been performed where three groups were mutually compared, the Bonferroni-adjusted α -level would be 0.05/3 = 0.0167. By omitting one group from the paper, the α -level for the comparison between the remaining groups could remain at 0.05. This research practice is clearly questionable, yet not uncommon.

Things are different, and much less well-known for sequential testing. Sequentially collecting data until some threshold is reached doesn't have to be problematic, as long as you employ an appropriate correction. Here, I outline the problem and indicate what can be done to deal with this. I will demonstrate this based on the well-known t -test as the simplicity of this test works for demonstrative purposes. The issue is not exclusive to the t -test, and holds for all significance testing procedures.

Suppose you want to perform an independent samples t -test. You begin with n = 2 measurements per group (with 1 measurement per group you cannot compute the within-group-variance, and thus cannot conduct a t -test). You perform the experiment, take your measurements and conduct your t -test. If p < 0.05, you stop collecting more data, else you collect one more measurement per group. Again, you conduct the analyses and conduct the t -test. This approach continues until you either have p < 0.05 or have run out of resources to collect more data or reached a pre-decided stopping point.

---

### Hydrodynamic cavitation in stokes flow of anisotropic fluids [^5eb0b0ef]. Nature Communications (2017). Medium credibility.

In our experiments, the corresponding values of Er ranged from ≃0.5 to 850: for Er < 1, the elastic effects outweighed the viscous effects, whereas at Er > 1, the viscous effects are dominant over their elastic counterpart. As Re and Er are both proportional to vl, we immediately find that Re = Kρ / η² Er. Inserting the typical numbers for the materials properties of LCs, we find a general conversion factor: Re≃10⁻⁴ Er, for K ≃10⁻¹¹, obtained as a ratio of interaction energy between molecules and the molecular distance, ρ ≃10³ and η ≃10⁻². Therefore, the specific conversion factor depends on the material properties of the particular LC and can vary for different LC compounds. For 5CB, Re≃10⁻⁶ Er. Nevertheless, the experiments and simulations remain in the laminar flow regime for all Er considered.

MD simulations

We consider a confined fluid with a cylindrical pillar filled with a fluid composed of N molecules (see Fig. 8c). The system is confined by atomically resolved solid substrates that induce friction on the flowing molecules to obtain a steady state during the course of a simulation.

In this work all quantities of interest are given in the usual dimensionless (that is, 'reduced') units, by taking the molecule's mass m, diameter σ and potential energy scale ɛ as the basic units of mass, length and energy, respectively. All simulations reported here have been carried out for a LC fluid containing N = 2.4 × 10⁴ molecules in a volume V = s x s y s z. We choose a Cartesian reference system oriented as in the experiments. To ensure that the LC fluid is sufficiently deep in the nematic phase we choose a temperature T = 0.90 and, under isothermal–isobaric conditions, a pressure P = 1.80. For this thermodynamic state point we obtain a mean number density ofin the bulk.

---

### A myoelectric digital twin for fast and realistic modelling in deep learning [^1a7ccdef]. Nature Communications (2023). High credibility.

This discretization process converts the continuous operator problem of Eq. (1) to a finite system of linear equations:where A is a symmetric and sparse n v × n v matrix, n v is the number of mesh vertices, is a vector of potential values at mesh nodes, andis a vector containing source information. Because the electric potential is defined up to a constant, the matrix A always has a one-dimensional null space. To obtain a unique solution to the system of Eq. (2), we constrain potentials v to have a zero sum.

In the context of EMG, we are not interested in finding electric potentials everywhere in the conductor, but only at the electrode locations. Let S be a selection matrix with a shape n e × n v which only selects the values at EMG electrode locations (n e is the number of electrodes). Each row of S can be designed to select a single point location or to integrate over an area (e.g. the electrode-skin interface) depending on the location and number of its non-zero elements. Also, let b (r) correspond to a point source at location r. The resulting EMG signal is thus given by:

Let us analyse in more detail the structure of A and b from Eq. (2). Let { w i (r), i = 1… n v } be a set of n v P 1 (piecewise linear) basis functions over the tetrahedral mesh Ω t. Note, that w i is 1 at the i -th vertex of the mesh, is 0 at all other vertices and is linear at all tetrahedra adjacent to the i -th vertex. In this case, A and b have the following structure:

First, let us notice that A is symmetric and, in general, a very large matrix which can be stored only because it is sparse. Indeed, the functions w i have a compact support and their pairwise scalar product is non-zero only for neighbour functions. Since the pseudo-inverse (or the inverse) of a sparse matrix is usually not a sparse matrix, it is impractical to compute it because of the amount of memory needed to store it. Thus, iterative methods are typically used to solve the system of Eq. (2) for every given b.

---

### Thermalization and criticality on an analogue-digital quantum simulator [^60f1ff91]. Nature (2025). Excellent credibility.

Figure 3b shows the ramp time dependence of the average energy density, averaged over n B = 110 bonds (N q = 65) and corrected for readout errors (Methods). As t r increases and the dynamics become more adiabatic, we observe a decrease in energy density towards the theoretically predicted ground state value of ε gs = −0.56, as well as the predicted Kosterlitz–Thouless (KT) transition energy density, ε KT = −0.53 ± 0.01 (grey and black lines, respectively). As demonstrated below, the final states are thermalized to a strong extent, so ε can be used to evaluate the final effective temperature. To correct for photon decay errors, we apply digital entangling gates at the end of the circuit to convert each pair of qubits to the Bell basis (Methods). This allows for postselecting with respect to photon number conservation (red squares), which yields an improved value of ε = −0.53 ± 0.01, roughly equal to the Kosterlitz–Thouless transition point. The remaining discrepancy from ε gs is attributed to dephasing effects, which are not corrected by this technique.

---

### Abstract representations emerge naturally in neural networks trained to perform multiple tasks [^55073c20]. Nature Communications (2023). High credibility.

Methods

Abstraction metrics

Both of our abstraction methods quantify how well a representation that is learned in one part of the latent variable space (e.g. a particular context) generalizes to another part of the latent variable space (e.g. a different context). To make this concrete, in both metrics, we train a decoding model on representations from only one — randomly chosen — half of the latent variable space and test that decoding model on representations from the non-overlapping half of the latent variable space.

The classifier generalization metric

First, we select a random balanced division of the latent variable space. One of these halves is used for training, the other is used for testing. Then, we select a second random balanced division of the latent variable space that is orthogonal to the first division. One of these halves is labeled category 1 and the other is labeled category 2. As described above, we train a linear classifier on this categorization using 1000 training stimuli from the training half of the space, and test the classifier's performance on 2000 stimuli from the testing half of the space. Thus, chance is set to 0.05 and perfect generalization performance is 1.

The regression generalization metric

As above, except we train a linear ridge regression model to read out all D latent variables using 4000 sample stimulus representations from the training half of the space. We then test the regression model on 1000 stimulus representations sampled from the testing half of the space. We quantify the performance of the linear regression with its r² value:where X is the true value of the latent variables andis the prediction from the linear regression. Because the MSE is unbounded, the r 2 value can be arbitrarily negative. However, chance performance is r² = 0, which would be the performance of the linear regression always predicted the mean of X, and r² = 1 indicates a perfect match between the true and predicted value.

---

### Game theoretical inference of human behavior in social networks [^0715385e]. Nature Communications (2019). High credibility.

Behavior estimation method

For the behavior estimation method, we introduce the NE-distance function, and the goal is to identify the set of parametersthat minimizes it. In order to approach the problem, we first verify that it is well-posed by proving convexity and smoothness of the distance function (the proof can be found in Supplementary Note 3). These two properties imply both tractability as well as scalability of algorithmic approaches to the Minimum NE-Distance problem.

Theorem. Let be a continuous function of and. Moreover, assume to be linear in, and let be a compact subset of. Consider the following function: Then is continuously differentiable, and its gradient is Moreover, F is a convex function.

Then, we distinguish two cases. In the first case there are no violations, and the minimum of the distance function is. In this case, the set of minimizers is a convex polyhedron that can be described by a finite number of inequalities:whereand, i.e. the inequalities need to be evaluated only at the vertices of the action space. We emphasize that, for all, the NE conditions corresponding to agentare full-filled.

In the second case, when NE violations occur, we consider a discrete version of the Minimum NE-Distance problem by approximating the integral over the action space with a finite sum over a regular grid. The discrete problem inherits smoothness and convexity, so it is possible to use a projected gradient method to find a solution. Moreover, the discrete problem can be viewed as an Ordinary Least Square regression problem, with the exception that error terms are non-negative, thus the bias of the estimates is non-zero and errors cannot be modeled as normally distributed. Nonetheless, we are able to build confidence intervals of the parameters. The detailed method is described in Supplementary Note 3.

Reporting summary

Further information on research design is available in the Nature Research Reporting Summary linked to this article.

---

### Coding schemes in neural networks learning classification tasks [^24e4fa20]. Nature Communications (2025). High credibility.

To study the properties of the network after learning, we employ a Bayesian framework where, following learning, the parameters Θ are drawn from a posterior distributionwhere P 0 (Θ) is a Gaussian prior, a mean squared error loss, and Z the normalization constant (see Methods for details). We denote expectations w.r.t. the weight posterior by 〈⋅〉 Θ. In the "zero temperature limit" β → ∞ any set of parameters Θ drawn from the posterior corresponds to a network, which perfectly solves the training task.

The geometrical intuition given above about the effect of non-lazy scaling of the output implicitly assumed that the individual components of both the readout vectors as well as the feature vectors are of O (1), hence their norms are of. However, learning could change the order of magnitude of these norms, for example, during learning the readout weights could grow by a factorwhich would trivially undo the non-lazy scaling and result in a network operating in the lazy regime, despite the O (1/ N) scaling of the output. To control the properties of the network after learning within the Bayesian framework, we adjust in particular the prior variance of the readout weightssuch that all preactivation norms as well as the norm of the readout weights are. In summary, we consider networks drawn from a weight posterior such that they are perfectly trained and such that the non-lazy scaling is not trivially undone by learning, allowing us to study the salient properties of learning in non-lazy networks.

To understand the properties of the emergent representations, we develop a mean-field theory of the weight posterior. To this end, we consider the limit N, N 0, P → ∞ while the number of readout units, as well as the number of layers remain finite, i.e. m, L = O (1). The relation between P and the size parameters will be discussed below.

---

### The minimal work cost of information processing [^927873c9]. Nature Communications (2015). Medium credibility.

Classical mappings and dependence on the logical process

Our result, which is applicable to arbitrary quantum processes, applies to all classical computations as a special case. Classically, logical processes correspond to stochastic maps, of which deterministic functions are a special case. As a simple example, consider the AND gate. This is one of the elementary operations computing devices can perform, from which more complex circuits can be designed. The gate takes two bits as input, and outputs a single bit that is set to 1 exactly when both input bits are 1, as illustrated in Fig. 2a.

The logical process is manifestly irreversible, as the output alone does not allow to infer the input uniquely. If one of the inputs is zero, then the logical process effectively has to reset a three-level system to zero, forgetting which of the three possible inputs 00, 01 or 10 was given; this information can be viewed as being discarded, and hence dumped into the environment. We can confirm this intuition with our main result, using the fact that a general classical mapping is given by the specification of the conditional probability p (x ′| x) of observing x ′ at the output if the input was x. Embedding the classical probability distributions into the diagonals of quantum states, the infinity norm in expression (2) becomes simply

---

### Why questionnaire scores are not measures: a question-raising article [^3d475008]. American Journal of Physical Medicine & Rehabilitation (2023). Medium credibility.

The Puzzle of Missing Values

Missing items are common in questionnaires, resulting from different reasons. Typical examples are made by subjects who may inadvertently skip over some questions or may not have enough time to complete the test. Subjects would prefer not to answer if higher penalties are assigned to wrong than missing answers. Sometimes, some items are reserved to some respondents, for example, those who attempted in the past a given activity or have other characteristics such as sex or language.

When missing answers are found, the easiest and most practiced solution would be to transform scores into the percentage of the total score given by the answered items. For example, if a subject misses two items and totals seven on a questionnaire of 10 dichotomous items, the subject's "total score" is 87.5% (7/8). Note that this procedure is equivalent to the common "mean substitution" procedure, assigning the missing items the average score observed in the other items. This method relies on the (strong and unlikely) assumption that questionnaires' items are all indicators of the same quantity of the latent variable of interest and thus exchangeable (i.e. they all share the same difficulty level).

Table 2 provides an example showing why this solution does not work. Suppose 10 items again, each scored 0 or 1, aligned from left to right for increasing difficulty. The "x" symbol flags a missing response. According to traditional psychometrics, subjects A and B are assigned the same score. Subject C, providing two missing answers, is assigned a lower score. However, it seems that subject A's missing response should be given a higher score estimate, given that the subject passed five more difficult items. In contrast, subject B failed three items easier than the missed one. It is doubtful that subject C deserves a lower score than subject B: he/she passed four items more difficult than the missed items.

TABLE 2
Estimation of missing values, based on average scores across answered items

Presumably, subjects A and C were careless in their answers or had not enough time to complete the test.

This example is intentionally simplistic. The treatment of missing data is a complex research field, faced by the whole field of statistics. As far as psychometric statistics is concerned, RA provides a satisfactory answer to this problem.

---

### Validation of monte carlo estimates of three-class ideal observer operating points for normal data [^26b6e5d9]. Academic Radiology (2013). Low credibility.

Rationale and Objectives

Traditional two-class receiver operating characteristic (ROC) analysis is inadequate for the complete evaluation of observer performance in tasks with more than two classes.

Materials and Methods

Here, a Monte Carlo estimation method for operating point coordinates on a three-class ROC surface is developed and compared with analytically calculated coordinates in two special cases: (1) univariate and (2) restricted bivariate trinormal underlying data.

Results

In both cases, the statistical estimates were found to be good in the sense that the analytical values lay within the 95% confidence interval of the estimated values about 95% of the time.

Conclusions

The statistical estimation method should be key in the development of a pragmatic performance metric for evaluation of observers in classification tasks with three or more classes.

---

### A molecular overlayer with the fibonacci square grid structure [^41928076]. Nature Communications (2018). Medium credibility.

We also compare autocorrelation functions that are calculated from the STM image and from a model Fibonacci square grid. Figure 2d is the autocorrelation function of the C 60 molecules in the STM image shown in Fig. 1c. Similar to the FFT of the C 60 molecules (Fig. 2b), the contribution from the substrate in STM was removed so that the autocorrelation function arises solely from the C 60 molecules. Figure 2e shows the autocorrelation function of a perfect grid, i.e. a point object is placed at every vertex of a Fibonacci square grid of S = 1.26 nm and L = 2.04 nm. The size of grid used to calculate the autocorrelation was 50 nm × 50 nm, but we have shown only a section for comparison. As expected, spots in the autocorrelation function of the model grid form a perfect Fibonacci square grid. In agreement with the model, the autocorrelation function of the STM image can also be mapped by a Fibonacci square grid. The tile lengths of the grid are also S and L. This means that C 60 molecules at the τ -deflated grid do not contribute to the autocorrelation pattern. This is expected, as only a fraction of the τ -deflated vertices are occupied by C 60. In a perfect Fibonacci square grid, the number of vertices in the τ -deflated grid is τ² times the number of vertices in the original grid. The relative density of vertices of the τ -deflated grid that do not overlap with the original grid is thus τ² − 1 = τ, i.e. there are ~162% more than the vertices of the original grid. However, only ~8% of these vertices are occupied by C 60 as observed by STM, and therefore these molecules do not produce additional features in the autocorrelation pattern.

---

### Guidelines for the appropriate use of bedside general and cardiac ultrasonography in the evaluation of critically ill patients-part I: general ultrasonography [^7247ea7c]. Critical Care Medicine (2015). Medium credibility.

Consensus categories for voting thresholds — used to grade agreement among respondents — are defined as: perfect consensus when all respondents agree on one number between 7 and 9; very good consensus when the median and middle 50% are at one integer or when 80% of respondents are within one integer of the median; good consensus when 50% of respondents are within one integer of the median or 80% are within two integers; some consensus when 50% are within two integers or 80% are within three integers; and no consensus for all other responses or any median with disagreement.

---

### Controllable gliders in a nanomagnetic metamaterial [^aab39645]. Nature Communications (2025). High credibility.

Methods

Evolutionary algorithm

The EA begins with a random population of individuals (solutions). As the algorithm progresses, the individuals are assessed through simulation. The better performing individuals are retained, with a chance of variation (mutation or recombination). Thus, through a process akin to natural evolution, the population undergoes iterative improvement. The EA was run with a population size of 100 for 100 generations.

Individuals in the EA specify two field strengths H⁺ and H⁻, and an initial state. The EA has control of the initial state of a roughly-square patch of 128 magnets at the centre of an ASI. The initial state is represented as a one dimensional list of real values, with each element corresponding to one of the magnets in the centre square region of the ASI. In close analogy to CA theory, we define the orange or pink spin state as on, and the blue or green as off. A value of greater than 0.5 in the initial state list, causes the corresponding magnet to be initialised in the on state (polarised rightwards), all other magnets begin in the off state.

To evaluate an individual we simulate its trajectory under the a A b B clock protocol using flatspin. The trajectory is the series of ASI states, assembled by sequentially applying the fields of the protocol and recording the resulting spin state of the full ensemble. We define the following fitness function on the resulting trajectory, to be minimised by the EA:where a t is the number of magnets in the on state at time t, k is the penalty factor, n u is the number of unique ASI states in the period t ∈ [t 0, T] and N is the length of the trajectory (T − t 0).

In minimising this function, the EA rewards individuals which maintain a close to constant number of magnets in the on state. Additionally we employ a penalty when the number of unique states in the trajectory is lower than the total length of the trajectory, e.g., the trajectory settles into looping or stationary behaviour. This penalty deters the EA from achieving a constant number of active magnets by utilising undesirable behaviour, such as frozen or oscillating dynamics. We use a penalty factor of k = 5100, equal to the number of nanomagnets in our simulated pinwheel ASI. A fitness score of 0 indicates perfect glider behaviour has been attained.

---

### Gap junctions desynchronize a neural circuit to stabilize insect flight [^e2acfa65]. Nature (2023). Excellent credibility.

The splayness index

When spike times of all N neurons in a network are available, splayness can be quantified by comparing the neuron's phase differences to those that would arise in a perfect splay state.

To this end, we first discretized the time from the first spike of the last neuron that started spiking until the last spike of the first neuron that stopped spiking with a time step of 1 ms. If there were breaks in the recording (where the fly stopped and started flying again), those times were omitted. Again, the breaks were defined from the last spike of the first neuron that stopped spiking until the first spike of the last neuron that started spiking. This way, all phases of all N neurons were sampled with a 1 ms time step at K time points τ k = k ms for k = 0, 1. K − 1.

For each neuron i, its phases were interpolated between its spikes t n, i as:

Then, for each point in time, the phases were ordered such that

Phase differences were computed asand

To calculate the splayness, these phase differences were compared to the phase differences of the most splayed state (splay state) and to those of the least splayed state (sync state):

From this we constructed the time-averaged splayness

With this definition, a perfectly splayed network yields s = 1 and a perfectly in-phase synchronized network yields s = 0.

---

### Primer on binary logistic regression [^8c5f1975]. Family Medicine and Community Health (2021). Medium credibility.

Family medicine has traditionally prioritised patient care over research. However, recent recommendations to strengthen family medicine include calls to focus more on research including improving research methods used in the field. Binary logistic regression is one method frequently used in family medicine research to classify, explain or predict the values of some characteristic, behaviour or outcome. The binary logistic regression model relies on assumptions including independent observations, no perfect multicollinearity and linearity. The model produces ORs, which suggest increased, decreased or no change in odds of being in one category of the outcome with an increase in the value of the predictor. Model significance quantifies whether the model is better than the baseline value (ie, the percentage of people with the outcome) at explaining or predicting whether the observed cases in the data set have the outcome. One model fit measure is the count- [Formula: see text], which is the percentage of observations where the model correctly predicted the outcome variable value. Related to the count- [Formula: see text] are model sensitivity-the percentage of those with the outcome who were correctly predicted to have the outcome-and specificity-the percentage of those without the outcome who were correctly predicted to not have the outcome. Complete model reporting for binary logistic regression includes descriptive statistics, a statement on whether assumptions were checked and met, ORs and CIs for each predictor, overall model significance and overall model fit.

---

### Neural-like computing with populations of superparamagnetic basis functions [^2062628a]. Nature Communications (2018). Medium credibility.

If the gripper strikes in the down zone, the opposite is implemented.

The key advantage of this learning rule is its simplicity: there is no need to perform a precise measurement of the error (here distance between the gripper and the object) as required by most learning methods in the literature. Note that the proposed system is independent of this learning rule and that different algorithms could be used to perform more complex tasks. Figure 3b shows that the distance between the object and the gripper is progressively decreased through repeated learning steps. After 3000 learning steps, the mean error is below 2.5% of the range: learning is successful. As can be seen in Fig. 3c the grasping error decreases as the number of junctions in the input population increases. The precision of the result indeed improves as the population grows, better approximating an ideal, infinite basis set. Figure 3c also demonstrates that transfer of information between populations of different sizes can be achieved, allowing changes of basis if needed.

The example of the gripper in Fig. 3 shows how we can transfer information without degradation from one population to a different one performing a basis change. Now we show that our system and our simple learning procedure can also transform information during the transfer between populations, in other words, realize more complex functions than the identity of Fig. 3. In Fig. 4a, we illustrate increasingly more complicated transformations: linear but not identity (double), square, inverse, and sine of the stimulus. Each can be learned with excellent precision, similar to the identity.

Fig. 4
Evaluation of stochastic population coding with superparamagnetic tunnel junctions. a Performance of several transformations, including non-linear. The 2 inputs label corresponds to transformation from polar to Cartesian coordinates. The series label corresponds to two transformations in series implementing the function sin 2 (x). b Distance to target versus variability of the energy barrier (bottom axis) and variability of the natural frequency (top axis). The experimental variability is indicated in red. c Distance to target for different times of observation during which switching rates are recorded, leading to different energy dissipated by the junctions (see Methods). Longer acquisition time allows better precision of the transformation, but leads to higher energy consumption. Each population is composed of 100 junctions and 3000 learning steps are used. Each data point corresponds to the average over 50 trials and the error bar to the associated standard deviation

---

### The eighty five percent rule for optimal learning [^0684215b]. Nature Communications (2019). High credibility.

Researchers and educators have long wrestled with the question of how best to teach their clients be they humans, non-human animals or machines. Here, we examine the role of a single variable, the difficulty of training, on the rate of learning. In many situations we find that there is a sweet spot in which training is neither too easy nor too hard, and where learning progresses most quickly. We derive conditions for this sweet spot for a broad class of learning algorithms in the context of binary classification tasks. For all of these stochastic gradient-descent based learning algorithms, we find that the optimal error rate for training is around 15.87% or, conversely, that the optimal training accuracy is about 85%. We demonstrate the efficacy of this 'Eighty Five Percent Rule' for artificial neural networks used in AI and biologically plausible neural networks thought to describe animal learning.

---

### A summary of the methods that the national clinical guideline centre uses to produce clinical guidelines for the national institute for health and clinical excellence [^17347588]. Annals of Internal Medicine (2011). Medium credibility.

NICE study search and retrieval workflow — Search results undergo a First sift: titles (done by information scientist or systematic reviewer) to exclude studies outside the topic; a Second sift: abstracts (done by systematic reviewer) to exclude studies that are not relevant to the review questions; and Assessment of full articles (done by systematic reviewer) to exclude studies on limits set by the GDG (for example, study design or outcomes), after which studies included proceed for data extraction; because of potential bias or error, a second reviewer performs sampling checks, and usually several thousand titles are sifted at the first stage.

---

### Work-relatedness [^700e045f]. Journal of Occupational and Environmental Medicine (2018). Medium credibility.

Work-relatedness — Table 1 steps for evaluating epidemiological evidence list procedural actions and criteria: "Collect all epidemiological literature reported on that disorder", "Identify the design of each study", and "Assess each study's methods" including "Exposure assessment methods and potential biases", "Disease ascertainment methods and potential biases", "Absence of significant uncontrolled confounders; consideration of residual confounding", "Addressing of other potential biases", "Adequacy of biostatistical methods and analytical techniques", and "Ascertainment of statistical significance — degree to which chance may have produced those results". The table then directs to "Assess the studies using the Updated Hill's Criteria" including "Temporality", "Strength of association", "Dose–response", "Consistency", "Coherence", "Specificity", "Plausibility", "Reversibility", "Prevention/Elimination", "Experiment", and "Predictive Performance", and to make a "Conclusion regarding the degree to which such a causal association is/is not met".

---

### Research techniques made simple: sample size Estimation and power calculation [^b2b6b836]. The Journal of Investigative Dermatology (2018). Low credibility.

Sample size and power calculations help determine if a study is feasible based on a priori assumptions about the study results and available resources. Trade-offs must be made between the probability of observing the true effect and the probability of type I errors (α, false positive) and type II errors (β, false negative). Calculations require specification of the null hypothesis, the alternative hypothesis, type of outcome measure and statistical test, α level, β, effect size, and variability (if applicable). Because the choice of these parameters may be quite arbitrary in some cases, one approach is to calculate the sample size or power over a range of plausible parameters before selecting the final sample size or power. Considerations that should be taken into account could include correction for nonadherence of the participants, adjustment for multiple comparisons, or innovative study designs.

---

### Understanding statistical terms: 2 [^351bd399]. Drug and Therapeutics Bulletin (2009). Low credibility.

An increasing number of statistical terms appear in journal articles and other medical information. A working knowledge of these is essential in assessing clinical evidence. With this in mind, we are producing a series of explanatory articles covering various statistical terms and their uses. This, the second article in the series, will focus on some of the most common terms used in reporting the results of randomised controlled trials.1.

---

### Graphs, tables, and figures in scientific publications: the good, the bad, and how not to be the latter [^a3a1cb03]. The Journal of Hand Surgery (2012). Low credibility.

Graphs, figures, and tables can save readers time and energy, aid their understanding of an article, and reduce the word count of the main text. However, many graphics submitted to and published in scientific journals fail to meet their potential and include mistakes that jeopardize their clarity. Many formats are available for presenting data, as well as a variety of techniques for enhancing interpretability. When the appropriate format is used to depict data, it conveys the greatest amount of information in the clearest fashion, complements the text, and deepens readers' understanding. The aims of this article are to draw attention to the necessity of well-constructed graphs, tables, and figures in scientific publications, and to show how to create them.

---

### How many days was that? We' re still not sure, but we' re asking the question better! [^b41c7375]. Medicine and Science in Sports and Exercise (2008). Low credibility.

Unreliable measures limit the ability to detect relationships with other variables. Day-to-day variability in measurement is a source of unreliability. Studies vary substantially in numbers of days needed to reliably assess physical activity. The required numbers of days has probably been underestimated due to violations of the assumption of compound symmetry in using the intraclass correlation. Collecting many days of data become unfeasible in real-world situations. The current dilemma could be solved by adopting distribution correction techniques from nutrition or gaining more information on the measurement model with generalizability studies. This would partition the variance into sources of error that could be minimized. More precise estimates of numbers of days to reliably assess physical activity will likely vary by purpose of the study, type of instrument, and characteristics of the sample. This work remains to be done.

---

### A practical guide for understanding confidence intervals and P values [^c0a7d4a0]. Otolaryngology — Head and Neck Surgery (2009). Low credibility.

The 95 percent confidence interval about the mean demarcates the range of values in which the mean would fall if many samples from the universal parent population were taken. In other words, if the same observation, experiment, or trial were done over and over with a different sample of subjects, but with the same characteristics as the original sample, 95 percent of the means from those repeated measures would fall within this range. This gives a measure of how confident we are in the original mean. It tells us not only whether the results are statistically significant because the CI falls totally on one side or the other of the no difference marker (0 if continuous variables; 1 if proportions), but also the actual values so that we might determine if the data seem clinically important. In contrast, the P value tells us only whether the results are statistically significant, without translating that information into values relative to the variable that was measured. Consequently, the CI is a better choice to describe the results of observations, experiments, or trials.

---

### Photon-efficient imaging with a single-photon camera [^e1e3b62a]. Nature Communications (2016). Medium credibility.

The implementation of our reconstruction algorithm can be divided into the following three steps (Fig. 2; Supplementary Note 1).

Step 1: natural scenes have reflectivities that are spatially correlated — the reflectivity at a given pixel tends to be similar to the values at its nearest neighbours — with abrupt transitions at the boundaries between objects. We exploit these correlations by imposing a transverse-smoothness constraint using the total-variation (TV) normon our reflectivity image. In this process, we ignore data from the hot-pixel set. The final reflectivity imageis thus obtained by solving a regularized optimization problem.

Step 2: natural scenes have a finite number of reflectors that are clustered in depth. It follows that in an acquisition without background-light or dark-count detections, the set of detection times collected over the entire scene would have a histogram with N z bins that possesses non-zero entries in only a small number of small subintervals. This longitudinal sparsity constraint is enforced in our algorithm by solving a sparse deconvolution problem from the coarsely time-binned photon-detection data, which is specific to the array imaging set-up, to obtain a small number of representative scene depths. Raw photon-detection events at times corresponding to depths differing by more than cT p /2 from the representative scene depths are censored. As step 2 has identified coarse depth clusters of the scene objects, the next step of the algorithm uses the filtered set of photon detections to determine a high-resolution depth image within all identified clusters.

Step 3: similar to what was done in step 1 for reflectivity estimation, we impose a TV-norm spatial smoothness constraint on our depth image, where data from the hot-pixel setand censored detections at the remaining pixels are ignored. Thus, we obtainby solving a regularized optimization problem.

---

### Common errors in statistics and methods [^f5075482]. BMJ Paediatrics Open (2024). High credibility.

Significant in one group, but not significant in the other: interaction?

Researchers are often interested in whether relationships among variables are the same for different subsets of the data. One common way that they look at this is to analyse each subset separately and then compare p values, often basing conclusions on whether the relationship is significant in one subset, both or neither. Andrew Gelman wrote an important article The Difference Between 'Significant' and 'Not Significant' is not Itself Statistically Significant, which makes the point that you can get 'significant' and 'not significant' results even when the actual difference is tiny, or get both significant or not significant results even when it is large. This question is better analysed by using interactions. To test for an interaction, include an interaction term in the model. To show an interaction, use graphics.

Coefficient (Cronbach's) alpha > 0.7 = 'reliable'

The use of 0.7 is often cited as being from Nunnally (1972) or from the second edition of the same book (Nunnally and Bernstein, 1994). We suspect that most authors who cite this have not read the original texts, because what this text says is more nuanced, and it does not say that ' > 0.7 = good'. In addition, coefficient alpha has been described as 'riddled with problems stemming from unrealistic assumptions'.

Not checking model assumptions or, at least, not reporting them

This error is very common. Most (maybe all) statistical methods make assumptions. The researcher needs to be aware of these, check them and report the results. For instance, multiple regression with ordinary least squares has several assumptions, including: (1) linear parameters, (2) correct model specification, (3) errors are independent and identically distributed and (4) errors are normally distributed.

---

### Understanding statistical power [^f422ca63]. The Journal of Orthopaedic and Sports Physical Therapy (2001). Low credibility.

This article provides an introduction to power analysis so that readers have a basis for understanding the importance of statistical power when planning research and interpreting the results. A simple hypothetical study is used as the context for discussion. The concepts of false findings and missed findings are introduced as a way of thinking about type I and type II errors. The primary factors that affect power are described and examples are provided. Finally, examples are presented to demonstrate 2 uses of power analysis, 1 for prospectively estimating the sample size needed to insure finding effects of a known magnitude in a study and 1 for retrospectively estimating power to gauge the likelihood that an effect was missed.

---

### How do you design randomised trials for smaller populations? A framework [^45132483]. BMC Medicine (2016). Low credibility.

How should we approach trial design when we can get some, but not all, of the way to the numbers required for a randomised phase III trial?We present an ordered framework for designing randomised trials to address the problem when the ideal sample size is considered larger than the number of participants that can be recruited in a reasonable time frame. Staying with the frequentist approach that is well accepted and understood in large trials, we propose a framework that includes small alterations to the design parameters. These aim to increase the numbers achievable and also potentially reduce the sample size target. The first step should always be to attempt to extend collaborations, consider broadening eligibility criteria and increase the accrual time or follow-up time. The second set of ordered considerations are the choice of research arm, outcome measures, power and target effect. If the revised design is still not feasible, in the third step we propose moving from two- to one-sided significance tests, changing the type I error rate, using covariate information at the design stage, re-randomising patients and borrowing external information. We discuss the benefits of some of these possible changes and warn against others. We illustrate, with a worked example based on the Euramos-1 trial, the application of this framework in designing a trial that is feasible, while still providing a good evidence base to evaluate a research treatment. This framework would allow appropriate evaluation of treatments when large-scale phase III trials are not possible, but where the need for high-quality randomised data is as pressing as it is for common diseases.

---

### Dextrose (dextrose 10%) [^a9a7d390]. FDA (2025). Medium credibility.

CHARACTER

This product is colorless or almost colorless clear liquid.

---

### Common errors in statistics and methods [^a384755a]. BMJ Paediatrics Open (2024). High credibility.

Over-reliance on ratio effects instead of difference effects to interpret dichotomous outcomes

For dichotomous outcomes, although relative risks or ORs are most common, they are more difficult to translate in common sense, absolute terms to the population. Risk difference has poorer mathematical properties but better intelligibility. It is best, therefore, to provide both ratio and difference measures to improve data interpretation. Assuming, for instance, that the risk (probability) of improving in the experimental arm is 0.5 but only 0.3 in the control arm, the relative risk would be 1.67 meaning a 67% 'relative improvement' in the experimental arm versus the control arm. However, the risk difference of 0.2 means that 20 'more people' over 100 treated, or 20%, will improve, an absolute improvement that is more understandable than the relative risk. In meta-analytical uses, it has now become practically compulsory to use both metrics to complement the interpretation of intervention effects, and we believe it is time for this reporting standard to be applied also to all primary reports on the efficacy of interventions.

Lack of detail on how missing data are handled

Missing data are ubiquitous. Research subjects refuse to answer questions, or they drop out, or data are lost, or whatever. Do not neglect to tell readers how you dealt with this. Although the proper procedure depends on the details and should ideally be prespecified within a statistical analysis plan, one common set of procedures is multiple imputation, which is underutilised and increasingly straightforward with modern statistical software.

---

### Digit preference in office blood pressure measurements, United States 2015–2019 [^2d63793d]. American Journal of Hypertension (2021). Medium credibility.

Background

Blood pressure (BP) measurement error may lead to under- or overtreatment of hypertension. One common source of error is terminal digit preference, most often a terminal digit of "0". The objective was to evaluate national trends in terminal digit preference in office BP measurements among adults with treated hypertension.

Methods

Data were from IQVIA's National Disease and Therapeutic Index, a nationally representative, serial cross-sectional survey of office-based physicians. The analysis included office visits from 2015 to 2019 among adults aged ≥ 18 years receiving antihypertensive treatment. Annual trends were examined in the percent of systolic and diastolic BP measurements ending in zero by patient sex, age, and race/ethnicity, physician specialty, and first or subsequent hypertension treatment visit.

Results

From 2015 to 2019, there were ~60 million hypertension treatment visits annually (unweighted N: 5,585–9,085). There was a decrease in the percent of visits with systolic (41.7%-37.7%) or diastolic (42.7%-37.8%) BP recordings ending in zero. Trends were similar by patient characteristics. However, a greater proportion of measurements ended in zero among patients aged ≥ 80 (vs. 15–59 or 60–79) years, first (vs. subsequent) treatment visits, visits to cardiologists (vs. primary care physicians), and visits with systolic BP ≥ 140 or diastolic BP ≥ 90 (vs. < 140/90) mm Hg.

Conclusions

Despite modest improvement, terminal digit preference remains a common problem in office BP measurement in the United States. Without bias, 10%-20% of measurements are expected to end in zero. Reducing digit preference is a priority for improving BP measurement accuracy and hypertension management.

---

### Dextrose (dextrose 10%) [^eeaa183c]. FDA (2025). Medium credibility.

SPECIFICATION

（1）50ml:2.5g（2）50ml:5g（3）100ml：5g（4）100ml:10g（5）250ml:12.5g（6）250ml:25g（7）500ml:25g（8）500ml:50g（9）1000ml:50g（10）1000ml:100g

---

### FDA raw data-HHS.gov… [^2697bb6e]. HHS (2022). Medium credibility.

FDA, 2020–7893, ,4-Nov-20, 4-Nov-20, Y, 656, Complex, 22-Aug-22, Closed Denial, "5 U. S. C. 552, 5 U. S. C. 552", 11/4/2020 8: 52, NA,… FDA, 2021–2185, ,2-Apr-21, 2-Apr-21, Y, 398, Complex, ,7-May-21, 5-May-22, Closed, 5 U. S. C. 552, ,5 U. S. C. 552, 4/2/2021 8: 40, D, 4/2/2021 8: 40, NA,… FDA, 2021–4461, ,9-Jul-21, 9-Jul-21, Y, 448, Complex, 30-Sep-22, Closed Denial, "5 U. S. C. 552 O, 5 U. S. C. 552", 7/9/2021 8: 15, NA,… FDA, 2021–5957, ,9-Sep-21, 9-Sep-21, Y, 53, Complex, ,16-Sep-21, 1-Nov-21, Appeal Withdrawn, 5 U. S. C. 552, ,5 U. S. C. 552, 9/9/2021 7: 46, NA, 9/9/2021 7: 46, NA,… FDA, 2021–8224, ,1-Dec-21, 1-Dec-21, Y, 295, Complex, ,4-Jan-22, 22-Sep-22, Closed Denial, Not Applicable, ",5 U. S. C. 552 O, 5 U. S. C. 552", 12/1/2021 8: 15, NA,… FDA, 2022–2047, ,16-Mar-22, 16-Mar-22, Y, 12, Complex, 28-Mar-22, Closed Denial, "5 U. S. C. 552, 5 U. S. C. 552"

---

### Best (but oft-forgotten) practices: checking assumptions concerning regression residuals [^f2ebb771]. The American Journal of Clinical Nutrition (2015). Low credibility.

The residuals of a least squares regression model are defined as the observations minus the modeled values. For least squares regression to produce valid CIs and P values, the residuals must be independent, be normally distributed, and have a constant variance. If these assumptions are not satisfied, estimates can be biased and power can be reduced. However, there are ways to assess these assumptions and steps one can take if the assumptions are violated. Here, we discuss both assessment and appropriate responses to violation of assumptions.

---

### Understanding statistical terms: 1 [^50f69b79]. Drug and Therapeutics Bulletin (2009). Low credibility.

An increasing number of statistical terms appear in journal articles and other medical information. A working knowledge of these is essential in assessing clinical evidence. With this in mind, we are producing a series of explanatory articles covering various statistical terms and their uses. This, the first article in the series, will focus on some of the most common terms used in randomised controlled trials.

---

### The blight of the type II error: when No difference does not mean No difference [^43447622]. Arthroscopy (2021). Medium credibility.

Much focus in research has been given to minimizing type I errors, where we incorrectly conclude that there is a difference between 2 treatments or populations. In contrast, our standard scientific method and power analysis allows for a much greater rate of type II errors, in which we fail to show a difference when, in fact, one exists (≥ 20% rate of type II errors vs ≤ 5% rate of type I errors). Additional factors that can cause type II errors may propel their prevalence to well in excess of 20%. Failure to reject the null hypothesis may be a tolerable outcome in a certain proportion of studies. However, type II errors may become dangerous when the conclusions of a study overreach, incorrectly stating that there is no difference, when, in fact, a difference exists. Type II errors resulting in overreaching conclusions may impede incremental advances in our field, as the advantages of small improvements may go undetected. To avert this danger in studies that fail to meet statistical significance, we as researchers (20% or more, vs 5% for type I errors) be precise in our conclusions stating simply that the null hypothesis could not be rejected.

---

### An optical neural network using less than 1 photon per multiplication [^7d2531cc]. Nature Communications (2022). High credibility.

Evaluation of dot-product accuracy

The numerical accuracy of dot products was characterized with pairs of vectors consisting of non-negative elements; since there is a straightforward procedural modification to handle vectors whose elements are signed numbers, the results obtained are general (Supplementary Note 11). The dot-product answers were normalized such that the answers for all the vector pairs used to fall between 0 and 1; this normalization was performed such that the difference between true and measured answers could be interpreted as the achievable accuracy in comparison to the full dynamic range of all possible answers (for the equations used for the error calculation, see Supplementary Note 12).

Before the accuracy-characterization experiments were performed, the setup was calibrated by recording the output of the detector for many different pairs of input vectors and fitting the linear relationship between the ground truth of the dot-product answer and the detector's output (Supplementary Note 12).

The vector pairs used for accuracy characterization were generated from randomly chosen grayscale natural-scene images (STL-10 dataset). The error of each computed dot product was defined as the difference between the measured dot-product result and the ground truth calculated by a digital computer (Fig. 2a). The number of photons detected for each dot product was tuned by controlling the integration time window of the detector (Supplementary Note 12). The measurements were repeated many times to capture the error distribution resulting from noise. For each vector size displayed in Fig. 2c, the dot products for 100 vector pairs were computed. The root-mean-square (RMS) error was calculated based on data collected for different vector pairs and multiple measurement trials. Therefore, the RMS error includes contributions from both the systematic error and trial-to-trial error resulting from noise. The RMS error can be interpreted as the "expected" error from a single-shot computation of a dot product with the setup. The noise equivalent bits were calculated using the formulaNEB = −log₂(RMS Error).

---

### Scale-free networks are rare [^6039ef07]. Nature Communications (2019). High credibility.

Fig. 7
Moment ratio scaling. For 3662 degree sequences, the empirical ratio of the second to first momentsas a function of network size n, showing substantial variation across networks and domains, little evidence of the divergence pattern expected for scale-free distributions, and perhaps a roughly sublinear scaling relationship (smoothed mean via exponential kernel, with smoothed standard deviations)

Overall, the results of these tests corroborate our primary findings of relatively little empirical evidence for the ubiquity of scale-free networks, and suggest that empirical degree distributions exhibit a richer variety of patterns, many of which are lower variance, than predicted by the scale-free hypothesis.

---

### Multiple choice answers: what to do when you have too many questions [^622557ce]. The Journal of Foot and Ankle Surgery (2015). Low credibility.

Carrying out too many statistical tests in a single study throws results into doubt, for reasons statistical and ethical. I discuss why this is the case and briefly mention ways to handle the problem.

---

### Scale-free networks are rare [^fc0b79f3]. Nature Communications (2019). High credibility.

Parameters for defining scale-free network

Threshold parameters for the primary evaluation criteria were selected to balance false positive and false negative rates, and to provide a consistent evaluation of evidence independent of the associated graph properties or source of data. For the Super-Weak and Weakest categories, a threshold of 50% ensures that the given property is present in a majority of simple graphs associated with a network data set. For the Weak category, a threshold of at least 50 nodes covered by the best-fitting power law in the upper tail follows standard practicesto reduce the likelihood of false positive errors due to low statistical power. For the Strong category, covers the full parameter range for which scale-free distributions have an infinite second moment but a finite first moment. For the Strongest category, the thresholds of 90% for the goodness-of-fit test and 95% for likelihood ratio tests against alternatives match the expected error rates for both tests under the null hypothesis. If every graph associated with a network data set is scale free, the goodness-of-fit test is expected to incorrectly reject the power-law model 0.1 of the time, and the likelihood ratio test will falsely favor the alternative 0.05 of the time. In the "most permissive" parameterization of the scheme (see Supplementary Note 5), we relax the threshold requirements so that if at least one graph meets the given criteria, the network is placed in this category. In this permissive parameterization, a directed network with a power-law distribution in the in-degrees should be and is classified as Strongest.

For specific networks, domain knowledge may suggest that some degree sequences are potentially scale free while others are likely not. A non-uniform weighting scheme on the set of associated degree sequences would allow such prior knowledge to be incorporated in a Bayesian fashion. However, no fixed non-uniform scheme can apply universally correctly to networks as different as, for example, directed trade networks, directed social networks, and directed biological networks. To provide a consistent treatment across all networks, regardless of their properties or source, we employ an uninformative (uniform) prior, which assigns equal weight to each associated degree sequence. In future work on specific subgroups of networks, a domain-specific weight scheme could be used with the evaluation criteria described here.

---

### Evaluating and defining the sharpness of intraocular lenses: microedge structure of commercially available square-edged hydrophilic intraocular lenses [^ce5ba7c4]. Journal of Cataract and Refractive Surgery (2009). Low credibility.

Purpose

To evaluate the microstructure of the edges of currently available hydrophilic acrylic intraocular lenses (IOLs) in terms of their deviation from an "ideal" square as a follow-up of preliminary in vitro studies of experimental poly(methyl methacrylate) IOLs and commercially available foldable hydrophobic IOLs.

Setting

Berlin Eye Research Institute, Berlin, Germany.

Methods

Twenty-four designs of hydrophilic acrylic IOLs were used in this study. For each design, a +20.0 diopter (D) IOL and a +0.0 D IOL (or the lowest available plus dioptric power) were evaluated. The IOL edge was imaged under low-vacuum (0.7 torr), high-magnification scanning electron microscopy (SEM) using an environmental microscope and standardized technique. The photographs were imported to a digital computer program, and the area above the posterior-lateral edge, representing the deviation from a perfect square, was measured in square microns.

Results

Currently available hydrophilic acrylic IOLs labeled as square edged had an area of deviation from a perfect square ranging from 60.84 to 871.51 microm(2) for the +20.0 D IOLs and from 35.52 to 826.55 microm(2) for the low-diopter IOLs. Although some differences in edge finishing between the IOLs analyzed were observed, edge surfaces of hydrophilic acrylic IOLs appeared overall smooth under environmental SEM.

Conclusions

Analysis of the microstructure of the optic edge of currently available square-edged hydrophilic acrylic IOLs showed a large variation of the deviation area from a perfect square.

---

### Universal scaling in real dimension [^329e2ba7]. Nature Communications (2024). High credibility.

Based on the previous arguments and in agreement with ref.we assume in our subsequent analysis that the spectral dimension of the LRDG follows Eq. (6) with η = 0. Since our analysis will mostly focus in the neighbourhood of d s = 3, which falls below σ = 1.5, the quantitative analysis of the possible correction η ≲ 0 will be left to future work.

In order to validate the universality of scaling phenomena on the LRDG graph, we computed the correlation length exponent of the SARW on the graph. The results of this analysis are reported in Fig. 3 and compared with the theoretical expectation obtained by the Flory theory, replacing the integer dimension d with the spectral dimension d s in Eq. (5). The MC data fall neatly on the theoretical curve in the whole range d s ∈ [2, 3.5] providing a very strong indication of universality in the LRDG.

Fig. 3
Inverse correlation length exponent 1/ ν as a function of the spectral dimension d s.

The value of 1/ ν, obtained from Monte Carlo simulations (red diamonds), is compared with analytical and numerical results in integer dimensions (black squares) and with the Flory theory prediction Eq. (5) with d = d s (grey dashed line). Uncertainties on the regular lattice are not shown as the values in d = 2, 4 are exact and the one in d = 3 has a precision better than 0.01%. The symbol contains vertical and horizontal lines representing the uncertainty. Each diamond symbol is obtained by fitting the gyration ratio of the SARW for the three largest sizes of the LRDG and taking the average. The uncertainty is obtained as the largest deviation between the aforementioned values, see the Supplementary Note II for more information. The MC uncertainty of the square boxes is smaller than the size of the symbol.

---

### Standards of care in diabetes – 2025 [^ef149bc3]. Diabetes Care (2025). High credibility.

Regarding specific circumstances for diabetic nephropathy, more specifically with respect to pediatric patients, ADA 2025 guidelines recommend to determine the eGFR at the time of diagnosis and annually thereafter.

---

### Simple solution to a common statistical problem: interpreting multiple tests [^3ce57fa0]. Clinical Therapeutics (2004). Low credibility.

Background

The misinterpretation of the results of multiple statistical tests is an error commonly made in scientific literature. When testing several outcome variables simultaneously, many researchers declare a statistically significant result for each test having a P value of < 0.05, for example. This approach ignores the fact that, based on a probability result called the Bonferroni inequality, the risk of incorrectly declaring as significant ≥ 1 test result increases with the number of tests conducted. The implication of this practice is that many scientific results are presented as statistically significant when the underlying data do not adequately support such a claim (sometimes referred to as false-positive results). Although the sequentially rejective Bonferroni test is well known among statisticians, it is not used routinely in scientific literature.

Objective

The intent of this article was to increase the awareness and understanding of the sequentially rejective Bonferroni test, thereby expanding its use.

Methods

This article describes the statistical problem and demonstrates how the use of the sequentially rejective Bonferroni test ensures that incorrect declarations of statistical significance for ≥ 1 test result are bounded by 0.05, for example.

Conclusion

The sequentially rejective Bonferroni test is an easily applied, versatile statistical tool that enables researchers to make simultaneous inferences from their data without risking an unacceptably high overall type I error rate.

---

### Rigorous location of phase transitions in hard optimization problems [^d98ebea5]. Nature (2005). Excellent credibility.

It is widely believed that for many optimization problems, no algorithm is substantially more efficient than exhaustive search. This means that finding optimal solutions for many practical problems is completely beyond any current or projected computational capacity. To understand the origin of this extreme 'hardness', computer scientists, mathematicians and physicists have been investigating for two decades a connection between computational complexity and phase transitions in random instances of constraint satisfaction problems. Here we present a mathematically rigorous method for locating such phase transitions. Our method works by analysing the distribution of distances between pairs of solutions as constraints are added. By identifying critical behaviour in the evolution of this distribution, we can pinpoint the threshold location for a number of problems, including the two most-studied ones: random k-SAT and random graph colouring. Our results prove that the heuristic predictions of statistical physics in this context are essentially correct. Moreover, we establish that random instances of constraint satisfaction problems have solutions well beyond the reach of any analysed algorithm.

---

### Summary benchmarks-full set – 2024 [^76ffec10]. AAO (2024). High credibility.

Preferred Practice Pattern (PPP) guidelines — scope and principles emphasize that Preferred Practice Patterns provide guidance for the pattern of practice, not for the care of a particular individual, and that the Preferred Practice Pattern guidelines are not medical standards to be adhered to in all individual situations. The series is based on three principles: each Preferred Practice Pattern should be clinically relevant and specific enough to provide useful information to practitioners; each recommendation that is made should be given an explicit rating that shows its importance to the care process; and each recommendation should also be given an explicit rating that shows the strength of evidence that supports the recommendation and reflects the best evidence available. Adherence to these Preferred Practice Patterns will not ensure a successful outcome in every situation, and these practice patterns should not be deemed inclusive of all proper methods of care or exclusive of other methods of care reasonably directed at obtaining the best results.