# Medical Question & Answer

**Sample ID**: 2ae96005-6c4f-5fbb-b8bf-1d65383970fb
**Dataset Index**: 268048

---

## Question

3. **Let the function $f**: \mathbf{R} \rightarrow \mathbf{R}$, for any $x, y \in \mathbf{R}$, we have $f(x+y) = f(x)+f(y)$, and when $x < 0$, $f(x) < 0$. If the graph of $y = f(x)$ intersects or is tangent to the line $2007 x + 2008 y = 2009$, then the number of intersection or tangent points is:
(A) 1
(B) 2
(C) 3
(D) Uncertain, depends on the specific form of $y = f(x)$

---

## Answer

> Let's see… What do we have here? The user is asking how many intersection or tangent points there are between the graph of a function (f) satisfying Cauchy's functional equation and a given line, under the condition that (f(x) < 0) for (x < 0). Let's break this down step-by-step. First, I need to think about what the functional equation (f(x+y) = f(x) + f(y)) implies about the form of (f). Then, I should verify how the sign condition (f(x) < 0) for (x < 0) constrains that form. Next, I will set up the intersection equation with the line and analyze the number of solutions. Finally, I will consider edge cases and whether any exceptions change the count, and then select the correct option.

> Let me first confirm the implication of the functional equation. The condition (f(x+y) = f(x) + f(y)) for all real (x, y) is Cauchy's functional equation, and under minimal regularity (for example, continuity at a point or monotonicity), the only solutions are linear functions of the form (f(x) = kx) for some constant (k). Wait, let me verify the regularity point: without any regularity, there are pathological solutions, but the sign condition (f(x) < 0) for (x < 0) implies monotonicity, which in turn forces linearity, so (f(x) = kx) is the unique form here [^notfound].

> I need to check how the sign condition constrains (k). If (x < 0), then (f(x) = kx < 0). Since (x) is negative, this requires (k > 0). So (f) is a strictly increasing linear function through the origin with positive slope, and (f(0) = 0) by additivity, which I should confirm: yes, (f(0) = f(0+0) = f(0) + f(0)) implies (f(0) = 0).

> Next, I should review the line equation and set up the intersection. The line is (2007x + 2008y = 2009). Substituting (y = f(x) = kx), I get (2007x + 2008(kx) = 2009), which simplifies to ((2007 + 2008k)x = 2009). Since (k > 0), the coefficient (2007 + 2008k) is strictly positive, so this linear equation in (x) has exactly one solution, meaning there is exactly one intersection point. Hold on, I should verify that this is indeed a transverse intersection rather than tangency; because the slopes differ (the line has slope (-2007/2008) and (f) has slope (k > 0)), they are not parallel, so the intersection is transverse and unique.

> But wait, what if the line were parallel to (f)? Could there be infinitely many solutions or none? I should double-check that possibility. Parallelism would require (k = -2007/2008), but (k > 0) while (-2007/2008 < 0), so parallelism is impossible here, which guarantees exactly one intersection point regardless of the specific positive value of (k).

> I will now examine whether any exception could change the count. The functional form is fixed to (f(x) = kx) with (k > 0), and the line is fixed, so the intersection equation is always a linear equation with a unique solution; there are no parameter choices that yield zero or multiple solutions in this setup. Therefore, the number of intersection or tangent points is necessarily 1.

> Conclusion: The correct answer is (A) 1, because (f) must be a strictly increasing linear function through the origin with positive slope, and the given line is not parallel to such a function, ensuring exactly one intersection point.

---

The function (f) satisfies Cauchy's functional equation (f(x+y) = f(x) + f(y)) and the condition (f(x) < 0) for (x < 0), which together imply (f(x) = kx) with (k > 0). The line (2007x + 2008y = 2009) has slope (-\\frac{2007}{2008}), which is negative and thus different from (k). Therefore, the line and the graph of (f) intersect at exactly one point. The answer is **1**.

---

## Step 1: Determine the form of the function (f(x))

The functional equation (f(x+y) = f(x) + f(y)) is Cauchy's equation, whose general solution under mild regularity (e.g. continuity or monotonicity) is (f(x) = kx) for some constant (k). The condition (f(x) < 0) for (x < 0) implies (k > 0), so (f(x) = kx) with (k > 0).

---

## Step 2: Find the intersection points

Substitute (y = f(x) = kx) into the line equation:

[
2007x + 2008(kx) = 2009
]

[
(2007 + 2008k)x = 2009
]

Since (k > 0), (2007 + 2008k \\neq 0), so there is a unique solution:

[
x = \\frac{2009}{2007 + 2008k}
]

Thus, there is exactly one intersection point.

---

## Step 3: Conclusion

The number of intersection or tangent points is **1**.

---

The answer is **A**.

---

## References

### Bateman gradients from first principles [^388ef9cd]. Nature Communications (2022). High credibility.

In a monogamous resident pair, we have simply N x = n x and N y = n y. But if a mutant individual of type x is able to attractfertilisation partners of type y, then for that individual, and the corresponding Bateman function iswhere the fertilisation function f is as described above. Because of symmetry, the corresponding function for y is found simply by swapping x and y. This function can reproduce the characteristic Bateman gradient asymmetry as gamete numbers diverge (progressing from isogamy to anisogamy in Fig. 1), showing how Bateman's assertion follows from biophysical effects that arise from unequal numbers of fusing particles: the fertilisation function f is derived solely from such biophysical effects, not from any sex-specific assumptions. Equation (1) makes no reference to sexes, and they only become specified when values are assigned to n x and n y. For example, if n x = 10 and n y = 10,000, the female Bateman function isand the male Bateman function, where for the latter all x s in Eq. (1) are replaced with y s and vice versa. The labels x and y are truly just labels. While there are inevitably assumptions built into the equations, crucially we can be certain there are no sex-specific assumptions. Yet the typical shapes reminiscent of Bateman gradients arise from the model when different values are specified for n x and n y (Fig. 1).

---

### Biophysical ambiguities prevent accurate genetic prediction [^ff8edf3c]. Nature Communications (2020). High credibility.

Phenotypes to free energy for non-pleiotropic mutations

Mutations in the CI protein can affect protein-folding energy (Δ G F), dimerization energy (Δ G D), binding energy to the operator sites (Δ G OR1–OR3) and tetramerization energy (Δ G T) at the biophysical level. We assume that mutations in CI that alter the free energy of DNA binding do so by the same magnitude for all three operators (ΔΔ G B = ΔΔ G OR1 = ΔΔ G OR2 = ΔΔ G OR3). To calculate only one biophysical change that can lead to the phenotype, we reversed the Forward Function described in the previous section. The Reverse Function for both P PR and P PRM is composed of two sub-functions. The first sub-function is the above-mentioned Forward Function, which calculates phenotypes from biophysical changes. This function is written in the form of y = f(x), where y is the phenotype and x is a set of biophysical parameters including the total expression level of CI. The second sub-function is an Inverse Function that finds all roots for an equation in the form of y – f(x) = 0. A root-finding process is performed using the uniroot.all function in the R package rootSolve. Specifically, for each perturbation of biophysical parameter (∆∆ G), we looked for all the roots within a range of −2–10 kcal per mol, and returned the ∆∆ G values that produce the phenotypes while the other biophysical parameters are not perturbed.

Mutational effects are modelled at a fixed expression level of CIthat corresponds to ~99% repression of the P R promoter and the CI concentration in a lysogen. To calculate changes in the biophysical parameters for single mutants with known effects on expression from P R or P RM, we first generated 136 evenly spaced phenotypes (with an interval of 0.1 in log(2) scale from −13.5 to 0). Then, for a given phenotype, we calculated corresponding changes in any of the four free-energy terms (biophysical parameters), each time allowing only one biophysical parameter to change using the Reverse Function explained in in the previous paragraph.

---

### A space-time tradeoff for implementing a function with master equation dynamics [^57fa10b0]. Nature Communications (2019). High credibility.

Master equations are commonly used to model the dynamics of physical systems, including systems that implement single-valued functions like a computer's update step. However, many such functions cannot be implemented by any master equation, even approximately, which raises the question of how they can occur in the real world. Here we show how any function over some "visible" states can be implemented with master equation dynamics-if the dynamics exploits additional, "hidden" states at intermediate times. We also show that any master equation implementing a function can be decomposed into a sequence of "hidden" timesteps, demarcated by changes in what state-to-state transitions have nonzero probability. In many real-world situations there is a cost both for more hidden states and for more hidden timesteps. Accordingly, we derive a "space-time" tradeoff between the number of hidden states and the number of hidden timesteps needed to implement any given function.

---

### Recovery coupling in multilayer networks [^c0323da7]. Nature Communications (2022). High credibility.

Recovery coupling simulations and phase space

To understand the implications of recovery coupling for multilayer network resilience, we consider the symmetric case in which the network structure, damage, and recovery parameters are the same in both systems. Since the two systems support each other, we let the repair rate of Y be influenced by the state of X in the same manner as Eq. (5): In the symmetric case f x = f y = f, leading to a single equation that governs the state of the system. If the failures are uniformly distributed, we can use percolation theory, to analytically derive the equation that governs the expected fraction of primary failures in the coupled system, where u (x) is the probability that a link does not lead to the largest connected component when a random fraction 1 − x of the nodes are removed, and is determined by the network topology. Equation (6) has one or two stable solutions depending on the value of the control parameter. The non-symmetric case has similar results, as we shown in supplementary note 1 and Supplementary Fig. 1. In contrast, the uncoupled case (2), which we recover from (6) for α = 0, has a single stable solution. The new solution describes a stable fixed point at f = 1 (all nodes failed), which persists even for high recovery rates(see Fig. 4 a). The existence of two stable solutions for f for the same recovery rateindicates that for a wide range of conditions, recovery coupled networks are resilient: they display functionality comparable to the uncoupled case and return to full functionality following small perturbations. However, a sufficiently large perturbation can force the system to cross the unstable branch, pushing it into a dynamically stable non-functional state (Fig. 4 a). This is more likely with correlated perturbations across layers, as we show in Supplementary Fig. 2. The existence of this behavior analytically predicts a "catch 22" phase that follows a sufficiently large disaster: infrastructure system X cannot be repaired because it requires resources from Y, and Y cannot be repaired because it requires resources from X. The fact that the collapsed state persists even for high repair rates and low damage rates predicts that it is harder to bootstrap a broken system than it is to maintain the functionality of one that is damaged but still working. Synthesizing elastic residual curves (Fig. 4 c) like the observations in Fig. 2 d, we find that the full coupling α = 1 reproduces the shape of the curve, while lower values of α do not, providing further evidence that the general deviation from elasticity is consistent with recovery coupling.

---

### Inferring time derivatives including cell growth rates using gaussian processes [^d99eaf3f]. Nature Communications (2016). Medium credibility.

Inferring the first and second time derivatives

To determine the time derivative of the data, we use that the derivative of a Gaussian process is another Gaussian process. We can therefore adapt standard techniques for Gaussian process to allow time derivatives to be sampled too.

Building on the work of Boyle, we let g (x) and h (x) be the first and second derivatives with respect to x of the latent function f (x). If f (x) is a Gaussian process then so are both g (x) and h (x). Writing ∂ 1 and ∂ 2 for the partial derivatives with respect to the first and second arguments of a bivariate function, we have

and that

as well as

following ref.

Consequently, the joint probability distribution for y and f *, g * and h * evaluated at points X * is again Gaussian (cf. equation (7)):

where we write K = K (X, X) and K ✱ = K (X *, X *) for clarity.

The covariance function is by definition symmetric: k (x i, x j) = k (x j, x i) from equation (1). Therefore, and so

for all positive integers k and. Consequently, the covariance matrix in equation (13) is also symmetric.

Conditioning on y now gives that the distribution P (f *, g *, h *| X, y, θ, X *) is Gaussian with mean

and covariance matrix

Equation (16) includes equation (9) and shows that

which gives the error in the estimate of the first derivative. Similarly,

is the error in estimating the second derivative.

Using an empirically estimated measurement noise

Although our derivation is given for a Gaussian process where the measurement errors in the data are independent and identically distributed with a Gaussian distribution of mean zero, the derivations are unchanged if the measurement noise has a different s.d. for each time point.

When the magnitude of the measurement noise appears to change with time, we first empirically estimate the relative magnitude of the measurement noise by the variance across all replicates at each time point. We then smooth this estimate over time (with a Gaussian filter with a width of 10% of the total time of the experiment, but the exact choice is not important) and replace the identity matrix, I, in equations (6), (15) and (16) by a diagonal matrix with the relative measurement noise on the diagonal in order to make predictions.

---

### Recovery after stroke: not so proportional after all? [^d3259e7a]. Brain (2019). Medium credibility.

Spurious r(X,Δ) are likely when σ Y /σ X is small

For any X and Y, it can be shown that:

A formal proof of Equation 1 is provided in the Supplementary material, Appendix A [proposition 4 and theorem 1; also see]; its consequence is that r(X,Δ) is a function of r(X, Y) and σ Y /σ X. To illustrate that function, we performed a series of simulations (Supplementary material, Appendix B) in which r(X, Y) and σ Y /σ X were varied independently. Figure 2 illustrates the results: a surface relating r(X,Δ) to r(X, Y) and σ Y /σ X. Figure 3 shows example recovery data at six points of interest on that surface.

Figure 2
The relationship between r(X, Y), r(X,Δ) and σ Y /σ X. Note that the x -axis is log-transformed to ensure symmetry around 1; when X and Y are equally variable, log(σ Y /σ X) = 0. Supplementary material, proposition 7 in Appendix A, provides a justification for unambiguously using a ratio of standard deviations in this figure, rather than σ Y and σ X as separate axes. The two major regimes of Equation 1 are also marked in red. In Regime 1, Y is more variable than X, so contributes more variance to Δ, and r(X,Δ) ≈ r(X, Y). In Regime 2, X is more variable than Y, so X contributes more variance to Δ, and r(X,Δ) ≈ r(X,−X) (i.e. −1). The transition between the two regimes, when the variability ratio is not dramatically skewed either way, also allows for spurious r(X,Δ). For the purposes of illustration, the figure also highlights six points of interest on the surface, marked A–F; examples of simulated recovery data corresponding to these points are provided in Fig. 3.

---

### Inferring time derivatives including cell growth rates using gaussian processes [^b5444dae]. Nature Communications (2016). Medium credibility.

Methods

Using a Gaussian process to fit time-series data

In the following, we will denote a Gaussian distribution with mean μ and covariance matrix Σ as(μ, Σ) and use the notation of Rasmussen and Williamsas much as possible.

Prior probability

For n data points y i at inputs x i (each x i is a time for a growth curve), we denote the underlying latent function as f (x). We define a covariance matrix k (x, x ′), which has an explicit dependence on hyperparameters θ, and obeys

where the expectations are taken over the distribution of latent functions (samples of f (x)).

We interpret equation (1) as giving the prior probability distribution of the latent functions f (X), where were we use X to denote the inputs x i, such that

where K (X, X) is the n × n matrix with components k (x i, x j). With f denoting [f (x 1). f (x n)], this prior probability can be written as

noting the dependence of k (x, x ′; θ) on the hyperparameters θ.

---

### Cox process representation and inference for stochastic reaction-diffusion processes [^7eaae477]. Nature Communications (2016). Medium credibility.

where u = (u 1,…, u N) and the sum over r ′ runs over all reactions with two product particles of species X i. The propensities g r (u) are obtained by replacing the n i variables with u i variables and Ω with h M in the expressions for the f r propensities of the approximated reactions. The factor of two in the square root in equation (28) comes from the fact that two identical molecules become produced in these reactions. Reintroducing the label l denoting the compartment number in equation (28), and the species label i in equation (27), we can add the two contributions to obtain

whereand u i l is the PR variable of species X i in the l th compartment. If we again define discretized density fields, where x l is the centre of the l th compartment, and, we can take the continuum limit of equation (29) that leads to the real-valued SPDE for the intensity fields given in equation (4). The g r (u (x, t)) therein are not functions of single PR variables anymore, but rather functionals of the space-dependent intensity field vector u (x, t) = (u 1 (x, t),…, u N (x, t)). They are obtained by taking the corresponding propensity functions f r (n) of the approximate reactions in real space, replacing n i → u i (x, t) and 〈 n i 〉→〈 u i (x, t)〉, and omitting Ω factors. The latter can be identified with h M here, and hence get absorbed in the definition of the intensity fields given below equation (29).

---

### Quantifying economic resilience from input-output susceptibility to improve predictions of economic growth and recovery [^85076be7]. Nature Communications (2019). High credibility.

Methods

Linear response theory of input−output economics

Consider an economy with N sectors, each sector producing Y i units of a single homogeneous good. Assume that sector j requires A ij units from sector i as input to produce one unit itself, which gives the so-called technical coefficients A ij. Each sector sells some of its output to consumers, the demand D i. The open Leontief IO model, the standard model in economics to depict and analyze inter-sectoral relationships, assumes linear production functions given by Y = AY + D (matrix notation). The stationary (equilibrium) state of this economy is given by(being the N -dimensional identity matrix). For the time evolution of an economy in its stationary state, assuming that differences in dynamic demand AY + D and dynamic production Y are compensated by production changes, this model gives the differential equation

We assume that each sector i experiences a time-dependent demand shock, X i (t), and the presence of multivariate white noise, i.e. a stochastic force, F i (t). In the picture of the example of the house of cards given above, the noise F i (t) represents the tiny nudge that we apply to the serving tray to understand if the house would survive a much larger shock, X i (t). More formally, the nudge consists of noise with mean value 〈 F i (t)〉 0 = 0, and covariance 〈 F i (t) F j (s)〉 0 = ν ij δ (t − s). Here, δ (x) denotes the Dirac-delta function, ν is a matrix of constants, andis the equilibrium expectation value of the function x (t), evaluated in the absence of an external force (X (t) = 0), with f 0 (Y) being the probability distribution to find a given value of Y under noise F i (t). This leads to the stochastic differential equation

---

### Size limits the sensitivity of kinetic schemes [^71d1065f]. Nature Communications (2023). High credibility.

A different measure of sensitivity — the amplification of a fold-change in the input — provides a solution to this problem. Suppose that for some value x 0 of the input parameter x, scaling by a factor a scales the output by b, so f (a x 0) = b f (x 0). Then the quotientcan be thought of as a discrete approximation of the derivative defining the logarithmic sensitivity. And if f (x) is differentiable everywhere, then by the mean value theorem, there must be a value x * of x for whichThis means that careful measurement of any two points on the input-output curve (x versus f (x)) witnesses the (local, infinitesimal) logarithmic sensitivity somewhere. Importantly — unlike in the case of fitting to a Hill function — if error in the measurements is very low, then they are also telling us the derivative for some value of x very accurately.

Equation (25) leads us to another common definition of the effective Hill coefficient:where S 0.9 and S 0.1 are the values of the input variable (in our case, x) required to get 90% and 10% (respectively) of the maximum value of the output variable (in our case, f (x)). Note that (26) is like (25) with a = S 0.9 / S 0.1 and b = 9. It implies that somewhere between S 0.1 and S 0.9 there is a logarithmic sensitivity of H eff /2.

There is yet another common definition, specific to models of binding. Suppose x is the concentration of a ligand and 〈 n b 〉(x) is the expected number of sites bound by a ligand out of a total of n possible binding sites. It is common then, to takeor to report, as the Hill coefficient, the slope of a line fitted to x versusdata on a log-log plot.

---

### The burden of proof studies: assessing the evidence of risk [^ae0fefba]. Nature Medicine (2022). Excellent credibility.

Basis splines, measurement mechanism and shape constraints

We used a Bayesian regularized spline to obtain the general shape of the nonlinear relationship. Basis splines represent nonlinear curves as linear combination of recursively generated basis elements. The basis elements were recursively generated using piecewise smooth polynomials, and were roughly localized to certain regions of the exposure variable in the data. Most of the time, quadratic or cubic polynomials were used, often with linear tails in the presence of sparse data. This approach allowed the common restricted cubic spline and constraints on the shape of the relationship (including nondecreasing and nonincreasing).

Given basis functions f 1,…, f k and coefficient vector β = (β 1. β k), the final curve is obtained as a β -linear combination

Specifically, for any given exposure (x), the prediction using the spline model is given bywhere X is a vector containing (f 1 (x),…, f k (x)). Derivatives and integrals of splines can likewise be expressed as linear combinations of spline coefficient β. For additional details about B -splines see Zheng et al.

Many studies of dose–response relationships report relative risks between categories defined by intervals of consumption. In mathematical notation, these observations are given bywhere y ij is the reported relative risk corresponding to measurement j in study i, [a ij, b ij] delineates the reference group exposure interval, and [c ij, d ij] delineates the alternative group exposure interval.

When f (x) is represented using a spline, each integral is a linear function of β similar to equation (1). The model (equation (2)) is then a ratio of linear functions, with the log relative risk given by

Equation (4) is a nonlinear function of the spline coefficients β.

When studying dose–response relationships, we allow for shape constraints of the inferred mean response. For example, for some harmful risks, such as smoking and air pollution, we assume the relative risk is monotonically increasing with exposure. To regularize the splines, and capture biologically plausible limits, we also allow a maximal derivative constraint, which is similar to penalizing total variation or limiting the spline degree. To introduce each of these constraints, we used the fact that derivatives of splines are linear functions of spline coefficients, similar to equation (1).

---

### Bielectron vortices in two-dimensional dirac semimetals [^87a64ef6]. Nature Communications (2017). Medium credibility.

Let us now consider a model interaction given by U (r) = U 0 /(1 + (r / d) 2), with an on-site energy U 0 and the long-range cutoff parameter d, which may be related to the separation between the 2D semimetal and the back-gate or to the screening length. This model potential provides a reasonable approximation to the more realistic potential decaying at large distances as 1/ r 3, for details see the Supplemental Information. Notably this functional form is well known in optics as the spatially inhomogeneous Maxwell's fish-eye lens, and remarkably is the simplest exactly solvable model, as the square well does not admit a nontrivial solution.

The system of equations (2) can be reduced to a second order differential equation for ϕ 2 only, which admits an analytical solution for the chosen interaction. This solution is square-integrable only at the Dirac point (E = 0). The same is true for any potential decaying faster than the Coulomb potential, so from now on we consider zero-energy states only. Now, when r ~ 0, one finds the usual short-range behavior. Meanwhile the asymptotic behavior as r → ∞ is given by the decay, where. Thus, we are motivated to seek a solution of Eq. (2) with the ansatzwhere f (r) is a polynomial in r that does not affect the short- and long-range behavior. Upon substituting Eq. (3) into Eq. (2), eliminating ϕ 1,3 (r), and using the new variable ξ = (r / d) 2, we arrive at the following equation for f (ξ):which is a form of the Gauss hypergeometric equation. Its solution, regular at ξ = 0, is given bywhere we have terminated the power series in the Gauss hypergeometric function 2 F 1 (a, b; c; x) to ensure decaying solutions at infinity. This termination leads to the following quantization condition for the formation of bound bielectron pairs

---

### Beyond the unknown: a broad framing for preparedness for emerging infectious threats [^82e0589e]. The American Journal of Tropical Medicine and Hygiene (2022). Medium credibility.

There have been multiple instances of novel pathogen emergence that have affected the health and security of the global community. To highlight that these novel pathogens presented a clear danger to public health, the WHO included "Disease X" on their list of priority pathogens in 2018. Indeed, since the emergence of SARS-CoV-2, Disease X has been pointed to as the looming threat of "the next big thing". However, developing surveillance and preparedness plans with Disease X as the linchpin is too narrow and ignores a large swath of potential threats from already identified, often neglected diseases. We propose instead the idea of "Disease f(x)" as a preferred call to arms with which to prioritize research and programmatic development. The common mathematical notation f(x) represents the knowledge that outbreaks are a function of many variables that define the transmission trajectory of that pathogen. Disease f(x) exploits commonalities across pathogen groupings while recognizing that emergences and outbreaks are fluid and that responses need to be agile and progressively tailored to specific pathogens with cultural and regional context. Adoption of this mindset across sectors, including biotechnology, disaster management, and epidemiology, will allow us to develop more efficient and effective responses to address the next major infectious threat.

---

### Parametric and nonparametric statistical methods for genomic selection of traits with additive and epistatic genetic architectures [^f632b26b]. G3 (2014). Low credibility.

There are many loss functions used for SVM regression. Some of the popular loss function choices include the squared loss, absolute loss, and the ε -insensitive loss. Here, we present these loss function formulations.
1 The squared loss function has the form L (y − f (x)) = (y − f (x)) 2. It scales the loss quadratically by the size of the error. Using this loss function indicates that outliers are also weighted quadratically, which requires the user to deal with the outliers before the regression analysis.
2 The absolute loss function has the form L (y − f (x)) = | y − f (x)|. The absolute loss function scales the loss linearly by the size of the error eliminating the difficulty of using data sets with outliers.
3 The ε -insensitive loss function has a form:

where ε determines the number of support vectors used in the regression function. By definition, a support vector is a vector x i that satisfies the equation y i (w x i + b) = 1. Increasing ε implies that fewer support vectors are used in the fitting. The ε -insensitive loss function ignores the errors in the regression that have size less than ε. When the error is greater than ε, the loss is | y − f (x)| − ε.

Figure 2 illustrates the absolute loss, squared loss, and ε -insensitive loss functions as a function of the error y − f (x).

Figure 2
Loss functions used for SVM regression. The first panel shows the absolute loss function. The second panel is the square loss function, and the last panel is the ε -insensitive loss function.

---

### Warped linear mixed models for the genetic analysis of transformed phenotypes [^8dd6d2fb]. Nature Communications (2014). Medium credibility.

Results

Summary of the method

Both, when specifying a phenotype transformation or when inferring it from the data (for example, using WarpedLMM), the implicit assumption is that the quantitative trait under genetic control is unobserved or latent, with the observed phenotype being determined by a nonlinear mapping g that links the latent phenotype to the observed measurements (Supplementary Fig. 1). Thus, to recover the true genetic model, an estimate of the ideal phenotype transformation f (where f = g −1) is needed. If we denote the observed phenotype for individual n as y n, an estimate of the latent phenotype z n can be obtained by applying the function f, optionally parameterized by ψ:

In WarpedLMM, these functions are constrained to be invertible and are termed 'warping functions'. The functional form of f is determined by parameters ψ, which are inferred jointly with the remaining model parameters of the LMM. The most probable transformation can then be inferred by maximizing the sum of the standard log likelihood and a Jacobian term that accounts for the complexity of the fitted warping function. Several functional forms of the warping functions can be chosen (see Methods), differing in number of free parameters and in the complexity of the functions they can represent. In the following, we consider a particular family of functions initially proposed by Snelson et al. which can be expressed as linear combination of a linear scaling term and multiple nonlinear step functions. If the observed phenotype y n does not require a transformation, only the linear term will be used. Otherwise, the function will consist of both the linear term and one or more step functions.

---

### Proof-of-concept of a novel structural equation modelling approach for the analysis of functional magnetic resonance imaging data applied to investigate individual differences in human pain responses [^5a5be8c0]. Human Brain Mapping (2023). Medium credibility.

FIGURE 3
Graphical depiction of the SEM method with physiological modeling in a more general form than depicted in Figure 2.

The resulting matrix equation describing the output signaling is thus an eigenvalue and eigenvector problem with the eigenvalues equal to one: S output = M output S output. The matrix representation of the network as shown in Figure 3 will always have eigenvalues equal to one corresponding to each latent input as described above. The eigenvector corresponding to a latent input shows the output value of each region relative to the latent input value. In other words, an eigenvector, x, of the matrix M output, is a solution to the equation x = M outpu t x, and x can be thought of as the values of S output at a single time point in the time series. Similarly, x can be scaled by any amount, a, and a x = M output a x. For a given value of a latent input at some time point, the corresponding values in all other regions are given by the proportionate values in x. In the case of two latent variables, the effects can simply be summed using the eigenvectors corresponding to each latent variable, x1 and x2 respectively. Since x1 = M output x1, and x2 = M output x2, therefore (x1 + x2) = M output (x1 + x2). As a result, for any values of the latent inputs the values for all other regions in the network can be determined that fulfill the conditions of the matrix equations. For example, for a model with a single latent input, Latent1, the matrix M output has an eigenvector, x1, corresponding to an eigenvalue of 1. The values of x1 can be scaled so that the value corresponding to the latent input is equal to one, and called x1s, and then S output can be represented as x1s multiplied by Latent1 at each time point. The effects of the latent inputs can thus be propagated through the network. Now, since S input = M input S output, and S output = x1s Latent1 then S input = M input x1s Latent1. This can be solved for Latent1 as:in this equation the subscript T represents the matrix transpose and the subscript −1 represents the matrix inverse. This equation applies for multiple latent inputs as well, with the term x1s extended as a matrix that includes all of the scaled eigenvectors. Again, in the perfect solution to this set of equations with no residual, S input is equal to the matrix of observed BOLD responses in every region, S BOLD.

---

### Impact of sampling rate on statistical significance for single subject fMRI connectivity analysis [^f7ae0c31]. Human Brain Mapping (2019). Medium credibility.

Let x (n) and y (n) denote the filtered time series which are related to the unfiltered time series v (n) and w (n) via the convolution sum:

Let r x (k) and r y (k) represent the autocorrelation sequences of the filtered time series. Our aim was to find the variance of the SPCC r xy between the filtered time series. Using the classical result from the multivariate time series analysis (Roy, 1989) and the linear system theory (Hayes, 1996), it may be verified that the PDF of the SPCC r xy in Figure 1, is asymptotically normal with the mean ρ xy and the variancewhere c is a 3 × 1 vector given byand D is a 3 × 3 matrix

In the above equations, r x (p) = r v (p) * C h (p), r y (p) = r w (p) * C h (p), γ xy (p) = γ vw (p) * C h (− p), and γ yx (p) = γ xy (− p). The steps leading to the variance in Equation (9) are given in Section S2. We noted that the variance characterizes the SPCC when the autocorrelation in the filtered time series is partly due to the inherent autocorrelation in the unfiltered time series and in part due to the autocorrelation induced by the filter through its impulse response h (n). We considered a practical band pass filter implemented in the Analysis of Functional NeuroImages (AFNI) Cox (1996) with the impulse responsewhere f h is the high cutoff frequency (HCF) and f l the low cutoff frequency (LCF) of the filter. Since the h (n) is a function of the parameters such as the TR and the f h, the variance in Equation (9) is a function of these parameters as well. Thus, we related the variance of the SPCC to the sampling rate and the HCF. We also considered various other choice of band pass filters such as the Gaussian filters in FMRIB Software Library (FSL; Smith et al. 2004) and the finite impulse response (FIR) filters in MATLAB used in many neuroimaging analyses.

---

### An alternative to the breeder's and lande's equations [^1313df68]. G3 (2014). Low credibility.

Selection on multiple traits

The results of the aforementioned sections are naturally generalized to selection on multiple traits. Consider the vectors of parental breeding values y 0 = (y 1, y 2,…, y N), environmental effects e = (e 1,…, e N) and their phenotype z 0 = y 0 + e, to which a selection function W (z) is applied. Using the same notations as in the previous sections, we find without difficulty thatAs before, using FT, these relations transform intowhere ∇ is the gradient operator: ∇ f = (∂ f /∂ x 1,… ∂ f /∂ x N). We see again thatandare linearly related ifwhere A is a constant matrix. The linear relation is automatically satisfied if both p 0 and f follow a Gaussian distributionwhere G and E are the covariance matrices for the genotype and environmental effects. Defining P = G + E as the phenotype covariance matrix, it is straightforward to show that in this case A = EG −1 and therefore which is the usual breeder's equation for multiple traits. We stress that the limitation of this relation is the same as that of the scalar version: it relies on the normal distribution of the genotype. On the other hand, if the selection function W (z) is Gaussianthe arguments of the previous section 2 can be repeated and lead to the generalization of the alternative vectorial breeder's equation (21)which, in analogy with equation (21) we write as

---

### Linkage analysis and map construction in genetic populations of clonal F1 and double cross [^238cf0af]. G3 (2015). Low credibility.

Recombination frequency estimation in scenarios 5 and 7 in clonal F 1 progenies

In scenario 5, locus 1 has two genotypes X 1 C 1 and X 1 D 1, and locus 2 has two genotypes X 2 C 2 and X 2 D 2. In scenario 6, locus 1 has two genotypes A 1 X 1 and B 1 X 1, and locus 2 has two genotypes A 2 X 2 and B 2 X 2. Table 5 shows theoretical frequencies of the four identifiable genotypes at the two loci. Obviously, theoretical frequencies do not contain the female recombination frequency in scenario 5 and do not contain the male recombination frequency in scenario 7. Thus, r F cannot be estimated in scenario 5; r M cannot be estimated in scenario 7. MLE of r M in scenario 5 can be calculated from its likelihood functions, given in Equation (9).

Table 5
Theoretical frequencies of the four identifiable genotypes in the clonal F 1 population

where n i is the observed sample size of the i th genotype (Table 5), n i: j is the sum of n i to n j, and n is the total sample size (i.e. n = n 1:4). Define the estimate of r in Equation (10).

MLE of r F in scenario 7 can be calculated from its likelihood functions, given in Equation (11). Define the estimate of r in Equation (12). Similar to Equation (5) and Equation (7), the estimates defined in Equation (10) and Equation (12) are less than 0.5, and are MLE of r for scenarios 5 and 7, respectively.

---

### Automated design of synthetic microbial communities [^63413175]. Nature Communications (2021). High credibility.

System equations

State variables in each system are rescaled to improve speed of obtaining numerical approximations:

Each model is represented as sets defining the system:

The system is represented as differential equations:

Growth is modelled by Monod's equation for nutrient limited growth:

Killing by bacteriocin is modelled via a Hill function, whereif strain is insensitive:

Induction or repression of bacteriocin expression by QS, A y :Simulations were conducted for 1000 h, the final 100 h were used to calculate the summary statistics and were stopped early if the population of any strain fell below 1e−10 (extinction event). Simulations with an extinction event have distances set to maximum in order to prevent excessive time spent simulating collapsed populations.

Bayesian inference

Let θ ∈ Θ be a sampled parameter vector with a prior π (θ). Given an objective of x 0, where x 0 exists in the solution space. We define the likelihood function for the objective behaviour as f (x 0 ∣ θ). Bayes' theorem gives us the posterior distribution of θ that exists for the objective x 0:

We can rewrite π (x 0) where a and b represent the lower and upper bounds of the parameter value:

The posterior distribution informs us of the parameter distribution that gives rise to the objective:

Let m be a model from a vector of competing models, M, such that m ∈ M = { m 0, m 2,.m n }. Each model has its own parameter space, allowing us to define a joint space, (m, θ) ∈ M × Θ.

We can write Bayes' theorem in the context of a model space:

Since the M is discrete, we can rewrite this as:

The marginal likelihood of the model, f (x 0 ∣ m), is the expectation of the likelihood function taken over the model parameter prior distribution. It measures a model's fit:

---

### Is the physical decay correction of the (18) F-FDG input function in dynamic PET imaging justified? [^59895589]. Journal of Nuclear Medicine Technology (2009). Low credibility.

Unlabelled

In this theoretic note, the rationale for the physical decay correction of the (18)F-FDG input function in dynamic PET is investigated, using the Patlak equation as an example.

Methods

The Patlak equation conventionally obtained when correcting the (18)F-FDG input function and correcting the tissue activity measurement for (18)F physical decay can also be derived from a 2-compartment analysis that does not conceptually involve any physical decay correction of the (18)F-FDG input function but accounts only for the physical decay of the trapped tracer.

Results

We demonstrate that exactly the same equation can be derived from the 2 conceptual approaches, and hence each approach yields the correct uptake rate of the tracer.

Conclusion

No advantage in (18)F-FDG dynamic PET can be expected from using the concept of uncorrected data rather than that of decay-corrected data. Nevertheless, conceptually, we show that correcting the (18)F-FDG input function for radioactive decay cannot be justified and that this correction is not compatible with the calculation of patient radiation dose.

---

### Advancing mathematics by guiding human intuition with AI [^0bfbd7a0]. Nature (2021). Excellent credibility.

Methods

Framework

Supervised learning

In the supervised learning stage, the mathematician proposes a hypothesis that there exists a relationship between X (z) and Y (z). In this work we assume that there is no known function mapping from X (z) to Y (z), which in turn implies that X is not invertible (otherwise there would exist a known function Y ° X −1). While there may still be value to this process when the function is known, we leave this for future work. To test the hypothesis that X and Y are related, we generate a dataset of X (z), Y (z) pairs, where z is sampled from a distribution P Z. The results of the subsequent stages will hold true only for the distribution P Z, and not the whole space Z. Initially, sensible choices for P Z would be, for example, uniformly over the first N items for Z with a notion of ordering, or uniformly at random where possible. In subsequent iterations, P Z may be chosen to understand the behaviour on different parts of the space Z (for example, regions of Z that may be more likely to provide counterexamples to a particular hypothesis). To first test whether a relation between X (z) and Y (z) can be found, we use supervised learning to train a functionthat approximately maps X (z) to Y (z). In this work we use neural networks as the supervised learning method, in part because they can be easily adapted to many different types of X and Y and knowledge of any inherent geometry (in terms of invariances and symmetries) of the input domain X can be incorporated into the architecture of the network. We consider a relationship between X (z) and Y (z) to be found if the accuracy of the learned functionis statistically above chance on further samples from P Z on which the model was not trained. The converse is not true; namely, if the model cannot predict the relationship better than chance, it may mean that a pattern exists, but is sufficiently complicated that it cannot be captured by the given model and training procedure. If it does indeed exist, this can give a mathematician confidence to pursue a particular line of enquiry in a problem that may otherwise be only speculative.

---

### Genetic association mapping leveraging gaussian processes [^987fa797]. Journal of Human Genetics (2024). Medium credibility.

GP regression and its use in genetic association mapping

Because the GP yielding f (x) has various useful properties inherited from the normal distribution, GP can be used to estimate a nonlinear function f (X) from output dataalong continuous factor X. The extended linear model y = f (X) + ε is referred to as the GP regression and widely used in the machine learning framework. This model can be used to map dynamic genetic associations for normalized gene expression or other common complex quantitative traits (e.g. human height) along the continuous factor x (e.g. cellular states or donor's age). Let us denote the genotype vectorand the kinship matrix R among N individuals, the mapping model, as proposed by us or others can be expressed as follows:whereare all GPs with similar covariance matrices, where ⊙ denotes element wise product between two vectors or matrices with the same dimensions, K = k (X, X) denotes the covariance matrix with a kernel function, and ε denotes the residuals. Intuitively, α models the average baseline change of y in relation to x, while β represents the dynamic genetic effect along x. The effect size is multiplied by the genotype vector g, indicating that the output y i varies between different genotype groups (g i ∈ {0, 1, 2}). In fact, the effect size β (x i) is additive to the baseline α (x i) at each x i, which is the same as the standard association mapping. Here statistical hypothesis testing is performed under the null hypothesis of δ g = 0, as the strength of genetic association is determined by δ g.

---

### Optical neural network via loose neuron array and functional learning [^1031e905]. Nature Communications (2023). High credibility.

Methods

Functional learning

The optimization of the parameters of a model-free system is challenging. In order to train the hardware parameters of the LFNN, we propose a deep-learning paradigm, functional learning. Specifically, we generalize the learning problem aswhere x is the input, y is the target output, p is the target trainable parameters, ∥ ⋅ ∥ denotes a general distance metric, andis an unknown function in the function space mapping the continuous input x to the target y. Because f (⋅; p) has no explicit formulation, we cannot mathematically solve the original problem of Equation (3). Therefore, we assume f (⋅; p) is an implicit function that can be represented by a functional neural network (FNN) as an instance with a learnable functional space. We then approximate the original problem with two sub-problems, where g N (⋅)(⋅; ⋅) is a FNN (Functional Neural Network), z is the functional space coefficients, and p is the primary parameter, i.e. the hardware parameter in our case. However, both sub-problems are under-determined if we simultaneously optimize the functional space and the parameters that alter the mapping between input and output. Therefore we reformulate the equation as the following, by decoupling the variables to make each sub-problem solvable:where p 0 and z 0 are auxiliary variables. In this formulation, the problem can be solved by alternating minimization, which has shown convergence for non-convex problems, of each sub-problem. Specifically, at each epoch k, we update p 0 ← p k and z 0 ← z k +1. The update rules can be then simplified as follows:

We propose different deep learning schemes to minimize sub-problems of Equation (6) and Equation (7), denoted as z-learning and p-learning, respectively.

---

### Optimality and sub-optimality in a bacterial growth law [^277aa2e8]. Nature Communications (2017). Medium credibility.

with carbon import rate β that depends on the carbon source, which is reduced by allosteric inhibition of pumpswith half-way point k 1. The expression of the enzymes for uptake of the carbon source is a function P(C) of the carbon sector C, where C is defined as the fraction of the proteome regulated by CRP. The level of most uptake enzymes is proportional to C (refs,), such that P(C) = C. Finally, since total protein concentration is approximately constant, we can normalize out all other proteins whose concentration does not change under the present conditions, and set R+C = 1 (Supplementary Note 1).

The model allows calculation of O-curves as the steady-state growth rate μ as a function of C (Supplementary Note 2). The modelled O-curves fit the measured O-curves well (R 2 = 0.94; Fig. 1g; Supplementary Fig. 4a).

Cells determine the size of the C-sector using a feedback loop in which intracellular carbon x inhibits cAMP. We describe this by a Michaelis–Menten repression function between 0 and 1, where C = 1 means that all cellular resources go to carbon uptake:

The regulatory constraint given by f(x) defines the modelled C-line (Fig. 3b; Supplementary Note 4). We find that the modelled C-line, which resembles a straight line with some curvature at high C, fits the experimental C-line data well (R 2 = 0.97; Fig. 1c; Supplementary Fig. 4b).

The model allows us to ask when the growth law (C-line) maximizes the growth rate. We call CRP control robustly optimal if for all environmental conditions (represented by β values) the C-line intersects the O-curve at its maximum (Fig. 3b), as is the case for lactose (Fig. 1).

---

### In vivo disentanglement of diffusion frequency-dependence, tensor shape, and relaxation using multidimensional MRI [^963d85e0]. Human Brain Mapping (2024). Medium credibility.

Diffusion MRI with free gradient waveforms, combined with simultaneous relaxation encoding, referred to as multidimensional MRI (MD-MRI), offers microstructural specificity in complex biological tissue. This approach delivers intravoxel information about the microstructure, local chemical composition, and importantly, how these properties are coupled within heterogeneous tissue containing multiple microenvironments. Recent theoretical advances incorporated diffusion time dependency and integrated MD-MRI with concepts from oscillating gradients. This framework probes the diffusion frequency, < mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:semantics > < mml:mrow > < mml:mi > ω < /mml:mi > < /mml:mrow > < mml:annotation > $$ \omega $$ < /mml:annotation > < /mml:semantics > < /mml:math >, in addition to the diffusion tensor, < mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:semantics > < mml:mrow > < mml:mi > D < /mml:mi > < /mml:mrow > < mml:annotation > $$ \mathbf{D} $$ < /mml:annotation > < /mml:semantics > < /mml:math >, and relaxation, < mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:semantics > < mml:mrow > < mml:msub > < mml:mi > R < /mml:mi > < mml:mn > 1 < /mml:mn > < /mml:msub > < /mml:mrow > < mml:annotation > $$ {R}_1 $$ < /mml:annotation > < /mml:semantics > < /mml:math >, < mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:semantics > < mml:mrow > < mml:msub > < mml:mi > R < /mml:mi > < mml:mn > 2 < /mml:mn > < /mml:msub > < /mml:mrow > < mml:annotation > $$ {R}_2 $$ < /mml:annotation > < /mml:semantics > < /mml:math >, correlations. A < mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:semantics > < mml:mrow > < mml:mi > D < /mml:mi > < mml:mfenced > < mml:mi > ω < /mml:mi > < /mml:mfenced > < mml:mo > - < /mml:mo > < mml:msub > < mml:mi > R < /mml:mi > < mml:mn > 1 < /mml:mn > < /mml:msub > < mml:mo > - < /mml:mo > < mml:msub > < mml:mi > R < /mml:mi > < mml:mn > 2 < /mml:mn > < /mml:msub > < /mml:mrow > < mml:annotation > $$ \mathbf{D}\left(\omega \right)-{R}_1-{R}_2 $$ < /mml:annotation > < /mml:semantics > < /mml:math > clinical imaging protocol was then introduced, with limited brain coverage and 3mm 3 voxel size, which hinder brain segmentation and future cohort studies. In this study, we introduce an efficient, sparse in vivo MD-MRI acquisition protocol providing whole brain coverage at 2mm 3 voxel size. We demonstrate its feasibility and robustness using a well-defined phantom and repeated scans of five healthy individuals. Additionally, we test different denoising strategies to address the sparse nature of this protocol, and show that efficient MD-MRI encoding design demands a nuanced denoising approach. The MD-MRI framework provides rich information that allows resolving the diffusion frequency dependence into intravoxel components based on their < mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:semantics > < mml:mrow > < mml:mi > D < /mml:mi > < mml:mfenced > < mml:mi > ω < /mml:mi > < /mml:mfenced > < mml:mo > - < /mml:mo > < mml:msub > < mml:mi > R < /mml:mi > < mml:mn > 1 < /mml:mn > < /mml:msub > < mml:mo > - < /mml:mo > < mml:msub > < mml:mi > R < /mml:mi > < mml:mn > 2 < /mml:mn > < /mml:msub > < /mml:mrow > < mml:annotation > $$ \mathbf{D}\left(\omega \right)-{R}_1-{R}_2 $$ < /mml:annotation > < /mml:semantics > < /mml:math > distribution, enabling the creation of microstructure-specific maps in the human brain. Our results encourage the broader adoption and use of this new imaging approach for characterizing healthy and pathological tissues.

---

### The dynamics and geometry of choice in the premotor cortex [^85a979c1]. Nature (2025). Excellent credibility.

The discretized latent trajectoryis obtained by marginalizing the continuous trajectoryover all latent paths connectingandduring each ISI. These marginalizations are implicit in the transition probability densitiesin equation (5). The transition probability densityaccounts for the drift and diffusion in the latent space and also for the absence of spikes during each interval between adjacent spike observations. This probability density satisfies a modified Fokker–Planck equation, which we derived previously:

Hereis the deterministic potential force, and the termleads to the probability decay due to spikes emitted by any neurons in the population, such thatincludes only trajectories consistent with no spikes emitted between t j −1 and t j. The solution of this equationpropagates the latent probability density forwards in time during each ISI. To model the reaction time task, we solved equation (6) with absorbing boundary conditions, which ensure that trajectories reaching a boundary before the trial end do not contribute to the likelihood. In addition, the absorption operatorin equation (5) enforces that the likelihood includes only trajectories terminating on the boundaries at the trial end time t E (Supplementary Methods 2.1). Together, these two conditions ensure that the likelihood includes only trajectories that reach one of the boundaries for the first time at the trial end time.

To fit the model to data, we derived analytical expressions for the gradients of the model likelihood with respect to each of the model components (Supplementary Methods 2.2). We computed functional derivatives of the likelihood with respect to latent dynamics as previously described. In this study, we computed functional derivatives of the likelihood with respect to tuning functions (Supplementary Methods 2.2). Instead of directly updating the functions Φ (x), p 0 (x) and f i (x) we, respectively, update the forceand auxiliary functions

The potential Φ (x) is obtained from F (x) viawhere we fixed the integration constant C to satisfy. As C is an arbitrary integration constant, a specific choice of C was not important as long as it was the same for all models. The initial state distribution p 0 (x) was obtained from the auxiliary function F 0 (x) in equation (7) via

---

### Universal DNA methylation age across mammalian tissues [^315ce959]. Nature Aging (2023). High credibility.

Transformation based on log–linear age for clock 3

Our measure of log–linear age leverages. The transformation has the following properties: it takes the logarithmic form when the chronological age is young, and it takes the linear form otherwise. It is continuously differentiable at the change.

First, we define a ratio of the age relative to ASM, termed, as the following:where the addition ofensures that theis always positive. To model a faster rate of change during development, we used a log–linear transformation onbased on a function that generalizes the original transformation proposed for the human pan-tissue clock:In the function f (x; m), x denotes, m represents a parameter and f represents the log-linear transformation. The output, y, is the results of applying the function f to x and m. This transformation is designed to reflect a higher rate of change for younger RelativeAdultAges when x ≤ m. This transformation ensures continuity and smoothness at the change point at.

In the following, we describe the estimation of the parameter m. To ensure that the maximum value ofis the same across all species, the parametershould be proportional to the maximum offor each species, that is, the best value for m would be the oracle value

(Extended Data Fig. 1d).

The proportionality constantcontrols the distribution of. We chose the value ofso thatfollows approximately a normal distribution with mean zero. Because we wanted to define clock 3 without using, we opted to use the ratioas a surrogate for the oracle value. We achieved this approximation by fitting the following regression model with all mammalian species available in our AnAge database,

The two log variables in equation (7) have moderate correlation (r = 0.5). Subsequently, we definedas follows:whereWe choseso that log–linear age termed y in equation (5) follows approximately a normal distribution with mean zero (median = 9.0 × 10 −4, skewness = −0.02; Extended Data Fig. 1f). Setting x = RelativeAdultAge in equation (5) results in

Universal clock 3 predicts(denoted as). To arrive at an age estimate, we employ both equations (4) and (6):

---

### Linkage analysis and map construction in genetic populations of clonal F1 and double cross [^34184c9e]. G3 (2015). Low credibility.

Recombination frequency estimation in scenarios 2 and 3 in clonal F 1 progenies

In scenario 2, locus 1 has four genotypes A 1 C 1, A 1 D 1, B 1 C 1, and B 1 D 1, and locus 2 has two genotypes X 2 C 2 and X 2 D 2. In scenario 3, locus 1 has four genotypes A 1 C 1, A 1 D 1, B 1 C 1, and B 1 D 1, and locus 2 has two genotypes A 2 X 2 and B 2 X 2. Table 3 shows theoretical frequencies of the eight identifiable genotypes at the two loci. The theoretical frequencies do not contain the female recombination frequency in scenario 2, and they do not contain the male recombination frequency in scenario 3. Therefore, r F cannot be estimated in scenario 2; r M cannot be estimated in scenario 3. MLE of r M in scenario 2 can be calculated from its likelihood functions, given in Equation (4).

Table 3
Theoretical frequencies of the eight identifiable genotypes in the clonal F 1 population

where n i is the observed sample size for the i th genotype (Table 3), n i: j is the sum of n i to n j, and n is the total sample size (i.e. n = n 1:8). Define the estimate of r in Equation (5). It can be easily seen that the estimate thus defined is less than 0.5. In addition, the estimate in Equation (5) is MLE of r, when directly calculated from its likelihood function.

MLE of r F in scenario 3 can be calculated from its likelihood function, given in Equation (6).where n i is the observed sample size of the i th genotype (Table 3), n i: j is the sum of n i to n j, and n is the total sample size (i.e. n = n 1:8). Define the estimate of r in Equation (7). Similar to Equation (5), the estimate thus defined is less than 0.5, and is MLE of r.

---

### Principle of structural equation modeling for exploring functional interactivity within a putative network of interconnected brain areas [^b6a7115e]. Magnetic Resonance Imaging (2009). Low credibility.

Functional neuroimaging first allowed researchers to describe the functional segregation of regionally activated areas during a variety of experimental tasks. More recently, functional integration studies have described how these functionally specialized areas, interact within a highly distributed neural network. When applied to the field of neurosciences, structural equation modeling (SEM) uses theoretical and/or empirical hypotheses to estimate the effects of an experimental task within a putative network. SEM represents a linear technique for multivariate analysis of neuroimaging data and has been developed to simultaneously examine ratios of multiple causality in an experimental design; the method attempts to explain a covariance structure within an anatomical constrained model. This method, when combined with the concept of effective connectivity, can provide information on the strength and direction of the functional interactions that take place between identified brain regions of a putative network.

---

### What quantile regression does and doesn' T Do: a commentary on petscher and logan (2014) [^220ddc10]. Child Development (2019). Medium credibility.

Petscher and Logan's (2014) description of quantile regression (QR) might mislead readers to believe it would estimate the relation between an outcome, y, and one or more predictors, x, at different quantiles of the unconditional distribution of y. However, QR models the conditional quantile function of y given x just as linear regression models the conditional mean function. This article's contribution is twofold: First, it discusses potential consequences of methodological misconceptions and formulations of Petscher and Logan's (2014) presentation by contrasting features of QR and linear regression. Second, it reinforces the importance of correct understanding of QR in empirical research by illustrating similarities and differences in various QR estimators and linear regression using simulated data.

---

### Protein sequence modelling with Bayesian flow networks [^330a3aff]. Nature Communications (2025). High credibility.

Sampling

We explored a variety of alternative sampling methods that reduce the overall temperature of sample generation from ProtBFN. In the conventional discrete-data sampling method described in, restricting ourselves to the logit space, the discrete sample generation process moves from y (t) at time t to y (s) at time s according to the equation, where, is isotropic noise and α s = β (s) − β (t) is the change in accuracy. By the additivity of normally distributed variables and with y (0) = 0, the distribution of y (s) given preceding steps at times i = 0,… t is, or equivalently, where z ~ N (0, I) is a fixed isotropic noise. Sampling according to Equation (15) would yield an ODE similar to that described in. However, we observed that simply by replacing the summation of previous predictions with the most recent prediction, e.g.substantially reduced the perplexity of generated samples. Effectively, this method is equivalent to taking our most recent predictionand supposing that we had predicted it at every step 0… s. This method is further motivated under the assumption that the most recent prediction from the model is the 'best guess' for the centre of the input distribution; by reconstructing the input distribution at that point, we more closely match the flow distribution seen during sampling. We note the similarity of our method to the sampling method proposed in ref. where at each step, the input distribution is reconstructed from the most recent prediction. The main distinction between the two methods is that the sampler proposed in ref.resampled the underlying isotropic noise at each step (SDE-like), whereas our method uses a fixed isotropic noise throughout the process (ODE-like). Our overall sampling method is described in Box 2, which we refer to as Reconstructed ODE (R-ODE). As the choice of sampling method can dramatically change performance, we include an empirical comparison of various sampling methods in Supplementary Note 4.

Box 1 Continuous-time loss

1: Input: Φ, x

2: Output: L

3: t ~ U [0, 1]

4: β (t) ← β 1 t 2

5: α (t) ← 2 β 1 t

6:

7:

8:

9: return L ∞

---

### Linkage analysis and map construction in genetic populations of clonal F1 and double cross [^4052a946]. G3 (2015). Low credibility.

Recombination frequency estimation in scenarios 6 and 8 in clonal F 1 progenies

In scenario 6, locus 1 has two genotypes X 1 C 1 and X 1 D 1, and locus 2 has three genotypes A 2 A 2, A 2 B 2, and B 2 B 2. In scenario 8, locus 1 has two genotypes A 1 X 1 and B 1 X 1, and locus 2 has three genotypes A 2 A 2, A 2 B 2, and B 2 B 2. Table 6 shows theoretical frequencies of the six identifiable genotypes at the two linked loci. The theoretical frequencies do not contain the female recombination frequency in scenario 6 and do not contain the male recombination frequency in scenario 8. Thus, r F cannot be estimated in scenario 6, and r M cannot be estimated in scenario 8. MLE of r M in scenario 6 can be calculated from its likelihood function, given in Equation (13).

Table 6
Theoretical frequencies of the six identifiable genotypes in the clonal F 1 population

where n i is the observed sample size of the i th genotype (Table 6) and n i: j is the sum of n i to n j. Define the estimate of r in Equation (14).

Maximum likelihood estimates of r F in scenario 8 can be calculated from its likelihood function, given in Equation (15). Define the estimate of r in Equation (16). Similar to Equation (5), Equation (7), Equation (10), and Equation (12), the estimates defined in Equation (14), Equation (15), and Equation (16) are less than 0.5 and are MLE of r for scenarios 6 and 8, respectively.

---

### An additive gaussian process regression model for interpretable non-parametric analysis of longitudinal data [^03f8e4da]. Nature Communications (2019). High credibility.

Methods

Notation

We model target variables (gene/protein/bacteria/etc;) one at a time. Let us assume that there are P individuals and there are n i time-series measurements from the i th individual. The total number of data points is thus. We denote the target variable by a column vector y = (y 1, y 2. y N) T and the covariates by X = (x 1, x 2. x N), where x i = (x i 1, x i 2. x id) T is a d -dimensional column vector and d is the number of covariates. We denote the domain of the j th variable byand the joint domain of all covariates is. In general, we use a bold font letter to denote a vector, an uppercase letter to denote a matrix, and a lowercase letter to denote a scale value.

Gaussian process

GP can be seen as a distribution of nonlinear functions. For inputs, GP is defined aswhere μ (x) is the mean and k (x, x ′) is a positive-semidefinite kernel function that defines the covariance between any two realizations of f (x) and f (x ′) bywhich is called kernel for short. The mean is often assumed to be zero, i.e. and the kernel has parameters θ, i.e. k (x, x ′| θ). For any finite collection of inputs X = (x 1, x 2. x N), the function values f (X) = (f (x 1), f (x 2). f (x N)) T have joint multivariate Gaussian distributionwhere elements of the N -by- N covariance matrix are defined by the kernel [K X, X (θ)] i, j = k (x i, x j | θ).

We use the following hierarchical GP modelwhere π (ϕ) defines a prior for the kernel parameters (including), is the noise variance, and I is the N -by- N identity matrix. For a Gaussian noise model, we can marginalise f analytically

---

### A mapping framework of competition-cooperation QTLs that drive community dynamics [^13568f26]. Nature Communications (2018). Medium credibility.

Results

Fitting bacterial growth curves

The behavior of microorganisms under a particular condition follows some biological rule, for example, microbial growth is characterized by different phases, lag, exponential, and stationary. In the lag phase, the specific growth rate starts from zero and then accelerates to a maximal value in the exponential phase. When growth reaches the stationary phase, the rate decreases and finally approaches zero. The three phases are each specified by a distinct parameter: the exponential phase by the maximum-specific growth rate (r, the tangent in the inflection point), the lag phase by the lag time (λ, the x -axis intercept of this tangent), and the stationary phase by the asymptote (A, the maximal value reached). A number of growth equations have been derived to capture these phases, among which three representatives are Gompertz, logistic, and Richards, expressed aswhere N (t) is the abundance of a microbe at time t and the Richards equation includes an additional parameter (v) that describes the shape of a curve.

By assuming the four-parameter Richards equation as one that can exactly predict microbial growth in monoculture, we implemented a statistical procedure to test and validate whether any of the three-parameter Gompertz and logistic equations can sufficiently describe the data. Results from the F test for model comparison suggest that the Richards equation provides an optimal fitness to time-dependent abundance data for both E. coli and S. aureus in monoculture (Fig. 1). Supplementary Table 1 gives the estimates of growth parameters by each growth equation for each species. Using Sun et al.'s heterochronic formula, we divided the Richard curve of microbial growth into lag, exponential, and stationary phases, which span 0–0.54, 0.54–9.14, and 9.14–36 h for E. coli and 0–1.21, 1.21–13.49, and 13.49–36 h for S. aureus, respectively, showing that these two species have different forms of growth.

---

### Sources of path integration error in young and aging humans [^8533f4ab]. Nature Communications (2020). High credibility.

To derive the non-linear observation function we need to find a coordinate transformation such that in the transformed frame the noise is added linearly. According to Eq. (2), the noise is added linearly in log-polar coordinates. The observation functiontherefore corresponds to the transformation from cartesian to log-polar coordinates:and the observation z s +1 is related to the reportsandby

The EKF framework permits the calculation of two important distributions using Gaussian approximations: the posterior distribution of the hidden variablegiven the observations z 1 to z s (predictive distribution), and the posterior distribution ofgiven z 1 to z s +1 (updated distribution). We denote the mean and covariance of these posterior distributions as

Mean and covariance of both distributions can be calculated recursively over stopping points using the standard EKF update equations:where the matrices F s +1 and H s +1 are the Jacobian matrices of transition and observation function evaluated at the previous updated mean μ s | s and predictive mean μ s +1| s, respectively:

At the starting point (s = 0, we initialize μ s = 0| s = 0 = x start = 0 and P s = 0| s = 0 = 0. Next, we calculate the predicted distribution of the next measurement z s +1 given the previous measurements z 1 to z s by integrating out the internal estimate:where we have used the linearization approximation of the EKF at the third line. This allows us to express the full log-likelihood aswhere the dependency on the parameters Θ is introduced through f, its Jacobian F s +1, Σ x and Σ z. In analogy to Eq. (8), we find the maximum likelihood (ML) estimate for Θ by numerically maximizing the log-likelihood:

Numerical parameter optimization was performed using the fminunc-function of MATLAB's optimization toolbox.

---

### An additive gaussian process regression model for interpretable non-parametric analysis of longitudinal data [^56514e83]. Nature Communications (2019). High credibility.

Results

Additive GP

Linear models and their mixed effect variants have become a standard tool for longitudinal data analysis. However, a number of challenges still persist in longitudinal analysis, e.g. when data contains nonlinear and ns effects.

GP are a flexible class of models that have become popular in machine learning and statistics. Realizations from a GP correspond to random functions and, consequently, GPs naturally provide a prior for an unknown regression function that is to be estimated from data. Thus, GPs differ from standard regression models in that they define priors for entire nonlinear functions, instead of their parameters. While nonlinear effects can be incorporated into standard linear models by extending the basis functions e.g. with higher order polynomials, GPs can automatically detect any nonlinear as well as ns effects without the need of explicitly defining basis functions. By definition, the prior probability density of GP function values f (X) = (f (x 1), f (x 2), ⋯, f (x N)) T for any finite number of fixed input covariates X = (x 1, x 2. x N) (where) is defined to have a joint multivariate Gaussian distributionwhere elements of the N -by- N covariance matrix are defined by the GP kernel function [K X, X (θ)] i, j = k (x i, x j | θ) with parameters θ. Mean in Eq. (1) can in general depend on X but zero mean is often assumed in practice. Covariance (also called the kernel function) of the normal distribution defines the smoothness of the function f, i.e. how fast the regression function can vary. Intuitively speaking, although GP is defined such that any finite-dimensional marginal has a Gaussian distribution, GP regression is a non-parametric method in the sense that the regression function f has no explicit parametric form. More formally, GP contains countably infinite many parameters that define the regression function, which are the function values f at all possible inputs. For a comprehensive introduction to GPs we refer the reader to the bookand the Methods section.

---

### An alternative to the breeder's and lande's equations [^2ccecec9]. G3 (2014). Low credibility.

The breeder's equation is a cornerstone of quantitative genetics, widely used in evolutionary modeling. Noting the mean phenotype in parental, selected parents, and the progeny by E(Z0), E(ZW), and E(Z1), this equation relates response to selection R = E(Z1) - E(Z0) to the selection differential S = E(ZW) - E(Z0) through a simple proportionality relation R = h(2)S, where the heritability coefficient h(2) is a simple function of genotype and environment factors variance. The validity of this relation relies strongly on the normal (Gaussian) distribution of the parent genotype, which is an unobservable quantity and cannot be ascertained. In contrast, we show here that if the fitness (or selection) function is Gaussian with mean μ, an alternative, exact linear equation of the form R' = j(2)S' can be derived, regardless of the parental genotype distribution. Here R' = E(Z1) - μ and S' = E(ZW) - μ stand for the mean phenotypic lag with respect to the mean of the fitness function in the offspring and selected populations. The proportionality coefficient j(2) is a simple function of selection function and environment factors variance, but does not contain the genotype variance. To demonstrate this, we derive the exact functional relation between the mean phenotype in the selected and the offspring population and deduce all cases that lead to a linear relation between them. These results generalize naturally to the concept of G matrix and the multivariate Lande's equation Δ(z) = GP(-1)S. The linearity coefficient of the alternative equation are not changed by Gaussian selection.

---

### Modern sea-level rise breaks 4, 000-year stability in southeastern China [^c093fbbf]. Nature (2025). Excellent credibility.

To further account for short-term and long-term nonlinear sea-level variability, we separate covariance functions (that is, kernels) (1), (3) and (4) into a fast (< 300 years temporal length scale) and a slow (> 300 years temporal length scale) component, so that equation (13) becomes:Therefore, the final process function contains nine components, represented by seven covariance functions, including fast and slow spatially uniform terms g f (t) and g s (t); a temporally linear term m (x)(t − t 0); fast and slow temporally nonlinear and spatially varying terms r f (x, t) and r s (x, t); fast and slow temporally nonlinear and locally varying terms l f (x, t) and l s (x, t); and two GIA components mentioned above. Because g (t) and GIA g (t) both reflect a globally uniform signal, we interpret them together as the final GMSL change signal. Hyperparameter optimization results can be found in Extended Data Table 3.

---

### Homoscedasticity: an overlooked critical assumption for linear regression [^a43524e6]. General Psychiatry (2019). Medium credibility.

Simulation study

To evaluate the performance of the F -test when either normality or homoscedasticity is violated, we use MC simulated data to create multiple samples from an ANOVA. By repeatedly testing the null of no group mean difference using each simulated sample and comparing the per cent of times when the null is rejected with a prespecified type I error, we can see how each assumption impacts the performance of the F -test. We start with specifying an ANOVA from which samples will be drawn using MC simulation.

For brevity and without loss of generality, we consider three groups with a common mean μ and group size n, in which case the ANOVA in equation (3) simplifies to:

We set, and consider testing the null of no group difference,

Thus, the null hypothesis H 0 is true for the ANOVA in equation (6). If we set type I error level to, say, α = 0.05, and repeatedly test the null hypothesis using data Y ki sampled from the ANOVA in equation (6) using MC simulation, we expect to reject the null H 0 5% of the times. We will refer to per cent of times when the null is rejected through repeated testing from multiple samples (MC simulated samples in the current setting) as empirical type I errors.

For evaluating the performance of the F -test under non-normal distribution, we simulate the error termsin equation (6) from (A) a centred and rescaled χ 2 with 1 df and (B) a centred and rescaled Weibull distribution with both shape and scale equal to 1, so the resulting distributions in both cases have mean 0 and variance 1. In this case, the simulated Y ki have the same mean (μ = 0) and variance (σ = 1) across the three groups. However, as the Y ki no longer follow the normal distribution, the F -test in equation (5) may not follow the F distribution, in which case we may not reject the null H 0 5% of the times if we set the nominal level as α = 0.05.

---

### Multilayer block copolymer meshes by orthogonal self-assembly [^a9b06266]. Nature Communications (2016). Medium credibility.

The single chain partition function can be evaluated as follows

where q (r, s) is a restricted chain partition function (propagator) that could be calculated by solving a modified diffusion equation

subjected to the initial condition q (r, 0) = 1. As the two ends of the polymer are distinct, a complementary partition function q * (r, s) is defined similarly and satisfies the same modified diffusion equation with an initial condition q *(r, 1) = 1. Here, we utilized s as a chain contour variable in units of N. All lengths were expressed in units of the unperturbed radius-of-gyration of a polymer, R g = (Nb 2 /6) 1/2, where b is the statistical segment length. The solution to the modified diffusion equation was conducted by implementing the pseudo-spectral method. An iterative relaxation of the fields towards their saddle-point values was implemented following ref.

By evaluating q (r, s) and its complementary function, the segment volume fractions can be determined as follows

Here we considered a 2D rectangular domain of size (L x × L y = 24 × 14R g 2). The SCFT equations were solved on a grid that had an equal lattice spacing of 0.2R g in the x and y directions. The volume fraction of the polymer f A was set to 0.5, whereas the degree of incompatibility χN was 17. This resulted in a structure that had the form of stripes (2D projection of in-plane cylinders or out-of-plane lamellae). The sides of the computational cell had periodic boundary conditions simulating an unconfined system in the x direction. Confinement was applied to the top and bottom surfaces where both have equal affinity to block A.

---

### Laser chimeras as a paradigm for multistable patterns in complex systems [^9ac9460e]. Nature Communications (2015). Medium credibility.

which shows the mapping dynamics from x n −1 to x n, via a spatial non-linear and non-local coupling between the position σ and a ξ shift around it. When comparing equation (5) with the standard models of equations (1) or (2), originally used to explore chimera states, one can identify f as the sine non-linear function coupling the spatially distributed oscillators. The impulse response h is also revealed as a distance-dependent coupling weight, appearing as the analogue of the function G in equation (1). The shape of the impulse response h can be used to identify the typical important parameters in chimera states: the coupling strength is the height of h (s), and the coupling distance corresponds to the width of this pulse-shaped function (see Supplementary Note 2 for details).

Laser wavelength delay dynamics

The photonic setup is depicted in Fig. 1. It is designed according to the above described requirements allowing for the observation of chimera in a delay dynamics. The physics and photonic concepts are inspired by a wavelength chaos generator designed previously for optical chaos communication. Two essential modifications have been necessary compared with the original wavelength chaos setup to allow for the emergence of chimera: a band-pass filter instead of low-pass one is providing the left-hand side in equation (3), which is responsible for a long-term stabilization of the chimera pattern; and a FP interferometer is involved instead of a birefringent one, providing an asymmetric non-linear Airy function:

The latter asymmetric function provides the conditions for the coexistence of whether chaotic amplitude oscillations (related to positive amplitudes associated to the sharp maximum of f [x]), or a constant plateau (at negative amplitudes, related to the broad minimum of f [x]). The asymmetry is thus at the origin of the different coherent and incoherent clusters of the chimera states.

---

### Fractional response analysis reveals logarithmic cytokine responses in cellular populations [^0d1364aa]. Nature Communications (2021). High credibility.

Calculation of typical fractions

The fractions of cells stimulated with dose i that have responses typical to dose j, v ij, can be easily calculated from data regardless of the number of doses and the type of experimental measurements. We have thatCalculation of typical fractions, v ij, with the above formula requires the possibility to examine the condition P (y | x j) > P (y | x k) for any experimentally observed response, y. The distributions P (y | x j) can be reconstructed from data using a variety of probability density estimators. The use of the available estimators, however, might be problematic for multivariate responses. We, therefore, propose a more convenient strategy. We replace the condition P (y | x j) > P (y | x k) with an equivalent condition that is computationally much simpler to evaluate. Precisely, we propose to use the Bayes formulaIf we set the equiprobable prior distribution, i.e. P (x j) = 1 /m, we have that P (y | x j) is proportional to P (x j | y) and the condition P (y | x j) > P (y | x k) is equivalent toThe above strategy allows avoiding estimation of the response distributions, P (y | x j), from data. For continuous and multivariate variable y the estimation of P (x j | y) is generally simpler than estimation of P (y | x j). Precisely, an estimatorof the distribution P (x j | y) can be built using a variety of Bayesian statistical learning methods. For simplicity and efficiency, here we propose to use logistic regression, which is known to work well in a range of applications. In principle, however, other classifiers could also be considered. The logistic regression estimators of P (x j | Y = y) arise from a simplifying assumption that log-ratio of probabilities, P (x j | Y = y) and P (x m | Y = y) is linear. Precisely, The above formulation allows fitting the logistic regression equations to experimental data, i.e. finding values of the parameters, α j and β j that best represent the data. The fitted logistic regression model allows assigning cellular responses to typical doses based on conditions given by Eq. 8. Formally, the fractions v ij defined by Eq. 6 are calculated aswhere n i is the number of cells measured for the dose x i, y l i denotes response of the l- th cell, andis equal 1 iffor anyand 0 otherwise.

---

### Forecasting the prevalence of diabetes mellitus using econometric models [^1104c166]. Diabetes Therapy (2019). Medium credibility.

Relationship of Population Growth and GDP to the Growth in the Number of Patients with DM

A regression analysis was performed to determine the relationship of population growth and GDP to the growth in the number of diabetic patients in Kazakhstan. The model used took the following form:More precisely, it was found to bewhere y is the number of patients with DM, and R 2 = 0.98 is the coefficient of determination. This is the proportion of variance of the dependent variable, explained by the model of dependence under consideration, i.e. the explanatory variables. The determination coefficient (R 2; 0 ≤ R 2 ≤ 1) is a measure of the quality of the regression model; i.e. how well it describes the relationship between the dependent and independent variables of the model. The closer the value of the coefficient of determination is to 1, the better the model. If R 2 = 1, then the empirical points (x i; y i) lie exactly on the regression line and there is a linear functional relationship between variables Y and X. If R 2 = 0, then all of the variation of the dependent variable is due to factors not taken into account in the model.

In the present research, the model shows the relationship between growth in GDP and the DM population. x 1 is the population. We used the F test to determine the statistical significance of all the coefficients. F was calculated to be 496.4881, meaning that all of the coefficients were statistically significant. x 2 is the GDP, and the constant a = − 128,438. There can be functions where one variable depends on the values of two or more other variables, where x 1 and x 2 together determine the value of y. The value of a shows that if x 1 and x 2 are equal to zero then y will equal to zero too. In the present research, a has a negative value, which means that if there is no population growth and the economy expands, the number of patients with DM will decrease. b 1 = 14.47913, which shows that iif the economy does not expand, the number of DM patients will increase by approximately 14,000 people as compared with the number in the previous year. b 2 = 0.003268, which shows that if the population continues to grow at the same rate the following year and the rate of GDP increases by 1 million tenge, the number of DM patients will be increasing by this ratio.

---

### Discovery of long-range inhibitory signaling to ensure single axon formation [^048b3c0b]. Nature Communications (2017). Medium credibility.

Reaction-diffusion simulation with a growing neurite

To validate our approximation of R i in equation (2), we simulated reaction-diffusion of the Rho-kinase with a moving boundary caused by neurite growth, i.e. ∂ C /∂ x| x = L (t) = 0. Suppose a spatial scaling of y = x / L (t), whereand. Then, equation (2) was transformed towith boundary conditions of C (y = 0, t) = C s (t) and ∂ C /∂ y| y = 1 = 0. Based on equation (9), we performed an exact simulation of the reaction-diffusion process coupled with neurite growth. We then confirmed that the steady state approximation was valid (Supplementary Fig. 5d–f).

Non-linear regression method

We performed a regression in which the parameters in equations (1 and 2), denoted in total by θ, were estimated by non-linear regression to minimize the quadratic error function as follows:

Where f (z n; θ) is the simulated retraction length depending on the initial length of the neurite, as shown in Fig. 4f, which we aimed to fit to the experimental data; z n and t n are the observed length (i.e. minor neurites and axon) at the onset of photoactivation and the observed retraction length after photoactivation, respectively, of the n th neurite; and M indicates the total number of neurites among all neurons. We used a simplex Nelder-Mead algorithm to minimize the error function (10).

---

### The efficiency of synchronization dynamics and the role of network syncreactivity [^a2bc3bec]. Nature Communications (2024). High credibility.

Fig. 1
The reactivity of the synchronization dynamics of networks.

(a) Illustrates the effects of reactivity on the transient dynamics towards synchronization of a simple 2-node network. For the set of initial conditions [x 1 (0), x 2 (0)] close to a point x s (black dot) belonging to the synchronous solution s (t) (black lines), a non-reactive coupling (r (x s) < 0) results in a direct convergence to the synchronous state (blue lines), while a reactive coupling (r (x s) > 0) results in an initial increase in the separation between trajectories before an eventual settling down to the synchronous state in the long term (orange lines). (b) Presents two unweighted networks of y-coupled Lorenz oscillators (see Methods "Example details") with two different topologies (Network I and II). Reactive (non-reactive) points x s on the attractorare depicted in lavender (green); therefore, the reactivity of green points is lower than the reactivity computed in lavender points. Based on the density of reactive points in the attractor, we can say that Network II is more '' than Network I. Indeed, we see the occurrence of jumps in the distance from the synchronous state(and therefore in the transverse motion to the synchronization manifold) for the more reactive network II (evolution in red), but not for network I (evolution in blue).

We now provide as a novel contribution of this paper a precise definition of the transverse reactivity of a point on the attractor. The transverse reactivity of the perturbations about x s on the synchronous solution is given bywhere operator e 1 (⋅) computes the largest eigenvalue, D F (x s) is the Jacobian of F at x s and the quantity, is often referred to as the algebraic connectivity of directed graphs, as a generalization of the classical concept of algebraic connectivity for undirected graphs. The matrixis an orthonormal basis for the null subspace of, i.e. the matrix V is any matrix with normal columns that are orthogonal toand to each other. See Methods "Isolating the dynamics transverse to the synchronoussolution" for detailed derivation and discussion of the transverse reactivity r (x s).

---

### Different population dynamics in the supplementary motor area and motor cortex during reaching [^5386a3e2]. Nature Communications (2018). Medium credibility.

We consider the data matrix, A, where each column contains the firing rate of one neuron across times and conditions. We estimate the latent variables as a projection, X = AW T, where each column of X contains the values of one latent variable across times and conditions. The rows of the orthogonal matrix W are the "neural dimensions", found by minimizing a cost function f (W). Because the above hypothesis contains three components, we employ a tripartite cost function:The first term, f rec (W), is identical to the PCA cost function, and is small if the latent variables capture considerable variance (i.e. if the firing rates in A are accurately reconstructed by A rec = XW). The second two terms relate to the hypothesis that there exists condition-invariant structure in some dimensions and dynamical structure in other, orthogonal dimensions. If so, an appropriate W will result in latent variables X = [X invar, X dyn]. X invar are latent variables that vary with time but not condition. X dyn are latent variables whose evolution obeys linear dynamics. f invar (W) is small if X invar varies strongly with time but not condition. f dyn (W) is small if the fitis good for some choice of D. Equations for f rec (W), f invar (W), and f dyn (W) are provided in the Methods. By minimizing f (W) we ask the question: does there exist a projection of the population response that captures considerable variance and has the hypothesized condition-invariant and dynamical structure?

---

### Sizing nanomaterials in bio-fluids by cFRAP enables protein aggregation measurements and diagnosis of bio-barrier permeability [^e306862d]. Nature Communications (2016). Medium credibility.

Methods

Theory of cFRAP

We start from the rectangle FRAP (rFRAP) model developed before for measuring a single average diffusion coefficient according to Deschout et al.which makes use of both time and spatial information in the recovery images:

where t is the time after photobleaching, K 0 the photobleaching parameter (which determines the extent of bleaching), D is the isotropic diffusion coefficient of diffusing species, l x and l y are the width and height of the rectangular photobleaching area, and r 2 is the mean square resolution of the bleaching and imaging point-spread function. In case of N independent diffusing components, we can simply make a superposition of the individual fluorescence recovery profiles:

where α i is the relative fraction of the i th component and ɛ i is the corresponding relative fluorescence brightness. Evidently.

Defining:

the multicomponent rFRAP model becomes:

The multicomponent rFRAP model of equation (4) can be generalized to describe a continuous distribution of diffusion coefficients α (D):

where f(x, y, t, D, K 0 (D), ɛ(D), r(D)) describes the fluorescence recovery of a component with diffusion coefficient D. Inserting equations (1) and (2) into equation (5) yields:

where f ′(x, y, t, D, r(D)) is defined as:

For numerical computation according to the MEM we now make the transition to the semi-continuous case. Let D be discretized in n components (for example, with equal interval in log (D) space) in the range of D min to D max, equation (7) becomes:

---

### Quantitative MRI by nonlinear inversion of the Bloch equations [^db8b3d9b]. Magnetic Resonance in Medicine (2023). Medium credibility.

Purpose

Development of a generic model-based reconstruction framework for multiparametric quantitative MRI that can be used with data from different pulse sequences.

Methods

Generic nonlinear model-based reconstruction for quantitative MRI estimates parametric maps directly from the acquired k-space by numerical optimization. This requires numerically accurate and efficient methods to solve the Bloch equations and their partial derivatives. In this work, we combine direct sensitivity analysis and pre-computed state-transition matrices into a generic framework for calibrationless model-based reconstruction that can be applied to different pulse sequences. As a proof-of-concept, the method is implemented and validated for quantitative < mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:semantics > < mml:mrow > < mml:msub > < mml:mrow > < mml:mi > T < /mml:mi > < /mml:mrow > < mml:mrow > < mml:mn > 1 < /mml:mn > < /mml:mrow > < /mml:msub > < /mml:mrow > < mml:annotation > $$ {\mathrm{T}}_1 $$ < /mml:annotation > < /mml:semantics > < /mml:math > and < mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:semantics > < mml:mrow > < mml:msub > < mml:mrow > < mml:mi > T < /mml:mi > < /mml:mrow > < mml:mrow > < mml:mn > 2 < /mml:mn > < /mml:mrow > < /mml:msub > < /mml:mrow > < mml:annotation > $$ {\mathrm{T}}_2 $$ < /mml:annotation > < /mml:semantics > < /mml:math > mapping with single-shot inversion-recovery (IR) FLASH and IR bSSFP sequences in simulations, phantoms, and the human brain.

Results

The direct sensitivity analysis enables a highly accurate and numerically stable calculation of the derivatives. The state-transition matrices efficiently exploit repeating patterns in pulse sequences, speeding up the calculation by a factor of 10 for the examples considered in this work, while preserving the accuracy of native ordinary differential equations solvers. The generic model-based method reproduces quantitative results of previous model-based reconstructions based on the known analytical solutions for radial IR FLASH. For IR bSFFP it produces accurate < mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:semantics > < mml:mrow > < mml:msub > < mml:mrow > < mml:mi > T < /mml:mi > < /mml:mrow > < mml:mrow > < mml:mn > 1 < /mml:mn > < /mml:mrow > < /mml:msub > < /mml:mrow > < mml:annotation > $$ {\mathrm{T}}_1 $$ < /mml:annotation > < /mml:semantics > < /mml:math > and < mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:semantics > < mml:mrow > < mml:msub > < mml:mrow > < mml:mi > T < /mml:mi > < /mml:mrow > < mml:mrow > < mml:mn > 2 < /mml:mn > < /mml:mrow > < /mml:msub > < /mml:mrow > < mml:annotation > $$ {\mathrm{T}}_2 $$ < /mml:annotation > < /mml:semantics > < /mml:math > maps for the National Insitute of Standards and Technology (NIST) phantom in numerical simulations and experiments. Feasibility is also shown for human brain, although results are affected by magnetization transfer effects.

Conclusion

By developing efficient tools for numerical optimizations using the Bloch equations as forward model, this work enables generic model-based reconstruction for quantitative MRI.

---

### Recent changes in breast cancer incidence in Spain, 1980–2004 [^57cab10a]. Journal of the National Cancer Institute (2009). Low credibility.

Statistical Analysis

Age-adjusted breast cancer incidence rates were calculated for each registry and 5-year period (1980–1984, 1985–1989, 1990–1994, 1995–1999, and 2000–2004) by using the direct method and the European standard population as the reference population. We computed the age-adjusted incidence rate ratios and 95% confidence intervals (CIs) for each individual registry with respect to all registries combined in each 5-year period by assuming a Poisson distribution for the number of cases.

Age- and registry-adjusted changes in incidence rates over the study period were evaluated by using log-linear Poisson models. More specifically, the number of invasive breast cancer cases, d apr, diagnosed at age a in year p and recorded in registry r was assumed to follow a Poisson distribution with mean λ apr n apr and free dispersion parameter φ, where λ apr is the underlying incidence rate and n apr is the number of woman-years at risk. The effects of age at diagnosis, calendar year of diagnosis, and registry on the log rate were assumed to be additive, where ρ r and α a are the parameters associated with registry r and age group a and whose respective averages (weighted by the corresponding marginal number of woman-years) are constrained to be 0, and f (p) is a predetermined parametric function of the year of diagnosis that includes an intercept term. Two alternative parameterizations were used for the function f (p). First, to describe the observed temporal trend without imposing any particular functional form, this function consisted of the usual indicator variables for each single year of diagnosis. Second, to formally detect and estimate changes in incidence rates over the study period, the function f (p) consisted of two intersecting linear trends with a smooth transition at an unknown change point τ, where β 0 is the intercept, β 1 − β 2 and β 1 + β 2 represent the period slopes below and above the change point τ, respectively, and γ is a transition parameter that controls the sharpness of the transition between the two linear trends at the change point τ, allowing not only for abrupt changes but also for more gradual transitions. As γ approaches 0, the function f (p) converges to the usual two intersecting straight lines with a sharp bend at τ; that is, f (p) = β 0 + (β 1 − β 2)(p − τ) if p ≤ τ and β 0 + (β 1 + β 2)(p − τ) if p > τ. This change-point model was fitted by using a grid search over τ and γ, and the asymptotic P value of the test for the existence of a change point was corrected for the search made over τ and γ by applying the improved Bonferroni inequality. The model provided: 1) the corrected P value of the test for the change point, 2) the estimate and 95% confidence interval for the location of the change point, and 3) the estimates and 95% confidence intervals for the annual percent change in incidence rate before and after the estimated change point. Further details on inference and testing procedures are provided elsewhere. A program using R statistical software (R Foundation for Statistical Computing, Vienna, Austria) to fit the above change-point models is available in the(available online).

---

### Uniform spatial pooling explains topographic organization and deviation from receptive-field scale invariance in primate V1 [^509b3ddb]. Nature Communications (2020). High credibility.

Formulating predictions of F 1/ F 0 in the pooling model

We measured phase selectivity at the preferred SF (f o) as F 1/ F 0. Specifically, we obtained the peak-to-peak amplitude from the Fourier transform of the phase tuning curve, and then divided by the mean over phase (i.e. F 1/ F 0). To model F 1/ F 0 as a function of f o in pooled scale invariance, we start with the assumption that the pooled population of scale invariant RFs are simple cells with a half-wave rectifying nonlinearity. Next, we inherit the same Gaussian pooling window that was fit in Eq. (5): σ h (x) = 0.24°. As formalized below, the input from each pooled RF is phase-shifted and Gaussian-weighted according to σ h (x), then they are all summed to yield the output cell's phase tuning (Fig. 4). As the simplest case, we start with a constant relative phase, whereby the absolute phase shift is coupled to the retinotopic shift of the envelope; i.e. the RFs look identical, but are shifted by the retinotopy (Fig. 4c, left). In this case, absolute phase shift (°) equals f o times the retinotopic shift of the RF (" x "deg). To formulate F 1/ F 0 as a function of f o, we start with convolution between the spatial pooling function and a scale invariant Gabor.where G x (0, σ h (x)) is the spatial pooling function — it is a Gaussian in the spatial domain (x) with peak = 1, mean = 0 deg, and SD = σ h (x) deg. The right side of the convolution is a scale invariant Gabor with carrier frequency f o. Importantly, the Gabor is normalized by the square root of its L2 norm, which ensures that the Fourier amplitude at f o (i.e. the F 1) is constant prior to the convolution. We are interested in the Fourier amplitude following the above convolution, which is proportional to the product of two Gaussians in the frequency (f) domain:The left side of the product is the Fourier amplitude of the pooling function and the right side is the Fourier amplitude of the Gabor. Both are unity amplitude Gaussians with the mean and SD in parentheses, in units of cyc/°. Evaluating at f o gives the following, which is proportional to the " F 1":For a half-wave rectifying nonlinearity, the maximum F 1/ F 0 is π, so we scale accordingly to getwhich is a Gaussian centered at 0 cyc/°, with an SD of (σ h (x) 2 π) −1 = 0.66 cyc/°. This function is shown in Fig. 4a (bottom dashed green), which underestimates the population's phase selectivity at each f o. The above equations assume that all the scale invariant inputs have the same relative phase (e.g. they all have even symmetry), which means that any change in the absolute phase is coupled to the RF envelope. For a more accurate model, we reduced the rate of the absolute phase progression with a coefficient, denoted D, in Eq. (6) below. D was fit by minimizing the mean-squared error.

---

### An L0-norm-based method for passive shimming design in MRI: a simulation-based study [^be93cb94]. NMR in Biomedicine (2025). Medium credibility.

4.2 The Working Mechanism of the L0‐Norm Approximation

The new method overcomes the drawbacks of conventional LP approaches by leveraging the L0‐norm property in the new optimization model, which promotes the sparsity of the PS solution.

As a "sparse norm," the L0‐norm plays a crucial role in compressed sensing, which aims to reconstruct signals from a limited measurement. The L0‐norm has also recently been used in machine learning for tasks such as feature selection and signal/image recovery.12,13 However, the mathematical properties of the L0‐norm have never been employed in shimming designs. The L0‐norm–based objective function is non‐convex, which is challenging to be used directly in optimization. Various methods are available for constructing approximation functions for the L0‐norm that are compatible with gradient operations in nonlinear optimization algorithms. As illustrated in Table 1, the approximation function described in Equation (5) effectively represents the mathematical properties of the L0‐norm. As confirmed by the case studies, the approximated L0‐norm operation generates more sparse shim patterns than the L1‐norm approach.

TABLE 1
Illustration of the L 0 ‐norm approximation using Equation (5).

The parameter μ is determined based on an investigation of its relationship with the function value of f (X, μ). As shown in Figure 6, the value of μ needs to be necessarily small to ensure the approximation function is close to the L0‐norm of the vector X. In the example shown in Table 1, we choose μ = 1e −7, resulting in the approximated function value of 8.008, representing well for counting the number of non‐zeros (8) in the given vector X = (−12, −1, −0.1, −0.005, 0, 0.005, 0.1, 1, 12). In this study, the same μ value is used throughout the analyses.

FIGURE 6
The relationship between the parameter μ and the function value of f (X, μ) for approximating the L0‐norm of the vector X.

---

### Evaluation of the evenness score in next-generation sequencing [^1f5f1936]. Journal of Human Genetics (2016). Low credibility.

The evenness score (E) in next-generation sequencing (NGS) quantifies the homogeneity in coverage of the NGS targets. Here I clarify the mathematical description of E, which is 1 minus the integral from 0 to 1 over the cumulative distribution function F(x) of the normalized coverage x, where normalization means division by the mean, and derive a computationally more efficient formula; that is, 1 minus the integral from 0 to 1 over the probability density distribution f(x) times 1-x. An analogous formula for empirical coverage data is provided as well as fast R command line scripts. This new formula allows for a general comparison of E with the coefficient of variation (= standard deviation σ of normalized data) which is the conventional measure of the relative width of a distribution. For symmetrical distributions, including the Gaussian, E can be predicted closely as 1-σ(2)/2⩾E⩾1-σ/2 with σ ≤ 1 owing to normalization and symmetry. In case of the log-normal distribution as a typical representative of positively skewed biological data, the analysis yields E≈exp(-σ*/2) with σ*(2) = ln(σ(2)+1) up to large σ (≤ 3), and E≈1-F(exp(-1)) for very large σ (⩾2.5). In the latter kind of rather uneven coverage, E can provide direct information on the fraction of well-covered targets that is not immediately delivered by the normalized σ. Otherwise, E does not appear to have major advantages over σ or over a simple score exp(-σ) based on it. Actually, exp(-σ) exploits a much larger part of its range for the evaluation of realistic NGS outputs.

---

### Tracking COVID-19 using online search [^ea474a36]. NPJ Digital Medicine (2021). Medium credibility.

We develop forecasting models using Gaussian Processes (GP), training a different model for each D days ahead forecasting task. The choice of GPs is justified by previous work on modelling infectious diseases. GPs are defined as random variables any finite number of which have a multivariate Gaussian distribution. GP methods aim to learn a functiondrawn from a GP prior. They are specified through a mean and a covariance (or kernel) function, i.e. where x and(both) denote rows of the input matrix X for the SAR-F model. By setting μ (x) = 0, a common practice in GP modelling, we focus only on the kernel function. The specific kernel function used in SAR-F is given bywhere k SE (⋅, ⋅) denotes a squared exponential (SE) covariance function (Supplementary Eq. 5), δ (⋅, ⋅) denotes a Kronecker delta function used for an independent noise component, ℓ 's and σ 's are lengthscale and scaling (variance) parameters, respectively. This composite covariance function uses separate kernels for online search and deaths data, as well as a kernel where they are modelled jointly. This provides the GP with more flexibility in adapting its decisions to one class of observations over the other. The covariance function of the AR-F model is a simplified version of Eq. (9), where search data is not used, i.e. The hyperparameters of both covariance functions (σ 's, ℓ 's) are optimised using Gaussian likelihood and variational inference.

In our analysis, we compare the aforementioned forecasting models with a basic persistence model (PER-F). For a D days ahead forecasting task, PER-F uses the most recently observed value of the ground truth, y t, as the forecasting estimate for the time instance t + D. Given the limited time span and the irregularities in reporting, this is a competitive baseline to improve upon.

---

### Spin dephasing under nonlinear gradients: implications for imaging and field mapping [^eaf2803c]. Magnetic Resonance in Medicine (2012). Low credibility.

This work examines the prototypical MR echo that would be expected for a voxel of spins evolving in a strong nonlinear field, specifically focusing on the quadratic z(2) - ½(x(2) + y(2)) field. Dephasing under nonlinear gradients is increasingly relevant given the growing interest in nonlinear imaging, and here, we report several notable differences from the linear case. Most notably, in addition to signal loss, intravoxel dephasing under gradients creating a wide and asymmetric frequency distribution across the voxel can cause skewed and nonlinear phase evolution. After presenting the qualitative and analytical origins of this difference, we experimentally demonstrate that neglecting these dynamics can lead to significant errors in sequences that assume phase evolution is proportional to voxel frequency, such as those used for field mapping. Finally, simplifying approximations to the signal equations are presented, which not only provide more intuitive forms of the exact expression but also result in simple rules to predict key features of the nonlinear evolution.

---

### Elacestrant (Orserdu) [^25e5bc7d]. FDA (2024). Medium credibility.

The dosage of elacestrant PO for treatment of breast cancer in postmenopausal female adults with ESR1 mutation (advanced or metastatic, ER-positive, HER2-negative, progression after ≥ 1 line of endocrine therapy) is 345 mg PO daily until disease progression or unacceptable toxicity

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^bb9b7f12]. Nature Communications (2025). High credibility.

After n refinement steps, we thus obtain a functiondefined at discrete points:which we extend to the entire interval [0, 1) by linear interpolation; see Fig. 5 d for an illustration. In practice we found that computations (specifically the risk computed by integrating) converge after about eight refinement steps.

This procedure has the important property that once a point is sampled, it does not change on further refinements:which follows from equation (67). Recall now that, as stated above, a process is self-consistent if "for small enough Δ Φ, the probability distribution at a point Φ [does] not depend on the level of refinement". Since equation (70) clearly satisfies that requirement, we see that the process obtained after infinitely many refinement steps is indeed self-consistent. We thus define the hierarchical beta (HB) process as

To complete the definition of, we need to specify how we choose the initial end pointsand. In our implementation, they are drawn from normal distributionswith Φ ∈ {0, 1}, where again c is determined via our proposed calibration procedure; this is simple and convenient, but otherwise arbitrary. We also need to explain how we choose the beta parameters α and β, which is the topic of the next subsection.

Choosing beta distribution parameters

All HB processes are monotone, continuous and self-consistent, but within this class there is still a lot of flexibility: since α and β are chosen independently for each subinterval, we can mouldinto a wide variety of statistical shapes. We use this flexibility to satisfy the two remaining desiderata: a) that realisationstrackover Φ ∈ [0, 1]; and b) that the variability ofbe proportional to. It is the goal of this subsection to give a precise mathematical meaning to those requirements.

Let x 1 ~ Beta(α, β) and x 2 = 1 − x 1. (The density function of a beta distribution is given in (24).) The mean and variance of x 1 areFor a given Φ, it may seem natural to select α and β by matchingtoandto. However both equations are tightly coupled, and we found that numerical solutions were unstable and unsatisfactory; in particular, it is not possible to make the variance large whenapproaches either 0 or 1 (otherwise the distribution of x 1 would exceed [0, 1]).

---

### MRzero-automated discovery of MRI sequences using supervised learning [^7cddf599]. Magnetic Resonance in Medicine (2021). Medium credibility.

Purpose

A supervised learning framework is proposed to automatically generate MR sequences and corresponding reconstruction based on the target contrast of interest. Combined with a flexible, task-driven cost function this allows for an efficient exploration of novel MR sequence strategies.

Methods

The scanning and reconstruction process is simulated end-to-end in terms of RF events, gradient moment events in x and y, and delay times, acting on the input model spin system given in terms of proton density, < mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:msub > < mml:mi > T < /mml:mi > < mml:mn > 1 < /mml:mn > < /mml:msub > < /mml:math > and < mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:msub > < mml:mi > T < /mml:mi > < mml:mn > 2 < /mml:mn > < /mml:msub > < /mml:math >, and < mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:mrow > < mml:mi > Δ < /mml:mi > < mml:msub > < mml:mi > B < /mml:mi > < mml:mn > 0 < /mml:mn > < /mml:msub > < /mml:mrow > < /mml:math >. As a proof of concept, we use both conventional MR images and < mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:msub > < mml:mi > T < /mml:mi > < mml:mn > 1 < /mml:mn > < /mml:msub > < /mml:math > maps as targets and optimize from scratch using the loss defined by data fidelity, SAR penalty, and scan time.

Results

In a first attempt, MRzero learns gradient and RF events from zero, and is able to generate a target image produced by a conventional gradient echo sequence. Using a neural network within the reconstruction module allows arbitrary targets to be learned successfully. Experiments could be translated to image acquisition at the real system (3T Siemens, PRISMA) and could be verified in the measurements of phantoms and a human brain in vivo.

Conclusions

Automated MR sequence generation is possible based on differentiable Bloch equation simulations and a supervised learning approach.

---

### Estimating and testing direct genetic effects in directed acyclic graphs using estimating equations [^947f28fb]. Genetic Epidemiology (2018). Low credibility.

2.1 Analysis of a quantitative primary trait with CIEE

First, we focus on the analysis of a (completely observed) normally distributed primary phenotype Y withindependent observations. In CIEE, unbiased estimating functions are constructed considering the two linear regression models fitted sequentially in the G‐estimation method (Vansteelandt et al. 2009), which are as follows. In the first stage, the effect of K on Y, α 1, is estimated, adjusting for other factors, by using the LS estimation method under the modelThen, to block all indirect paths of X on the primary phenotype Y, the adjusted phenotypeis obtained by removing the effect of K on Y withwhereand.

In the second stage, the direct effect of X on Y, is tested under the model

In CIEE, we formulate unbiased estimating equationsfor a consistent estimation of the unknown parameter vectorwith, whereand φ(.) is the probability density function of the standard normal distribution. To give an intuition on how these estimating equations are obtained, is the log‐likelihood function under the model in (1) andis the log‐likelihood function under the model in (3) given that α 1 is known. By solving the first five estimating equations based onin (5), we are hence fitting the model in (1) to obtain an estimate of θ 1, that is obtaining the maximum likelihood (ML) estimates under the model in (1). Analogously, solving the last three estimating equations based onyields an estimate of θ 2. Hence, we obtain the estimate of θ, denoted by, by solving. As a difference to the two‐stage sequential G‐estimation method, we estimate all parameters in θ simultaneously and consider the additional variability obtained in the phenotype adjustment in (2) by using the robust Huber–White sandwich estimator of the standard error of.

Under mild regularity conditions (White, 1982), is asymptotically normally distributed with mean 0 and covariance matrixthat can be consistently estimated with, wherewithbeing the j ‐th element in equation (4) and. The robust Huber–White sandwich estimate of the standard error ofcan then be obtained as. Having obtained the estimates of α XY and its standard error, we use the large‐sample Wald‐type test statisticfor testingvs. Under H 0, W has an asymptotic standard normal distribution.

---

### Resolving discrepancies between chimeric and multiplicative measures of higher-order epistasis [^4a9f2911]. Nature Communications (2025). High credibility.

Multiplicative and additive epistasis measures

We start by relating the multiplicative epistasis formula (18) to the multivariate Bernoulli distribution. A careful reader may observe that the natural parameterin the bivariate Bernoulli distribution (21) bears close resemblance to the multiplicative epistasis measure in equation (10). Specifically, if the fitnessof each genotype (x 1, x 2) ∈ {0, 1} 2 is proportional to the probabilityof that genotype in the multivariate Bernoulli, then the natural parameter β 12 is equal to the logarithmof the multiplicative epistasis measure. Thus, for L = 2 loci, epistasis is measured by the natural parameters β of a bivariate Bernoulli distribution.

We prove that this observation is not specific to the bivariate Bernoulli distribution with L = 2 loci, and in fact generalizes to any number L of loci. Specifically, we prove that if the fitnessof genotype (x 1,…, x L) is proportional to the probabilityof observing the genotype, then for each subset S ⊆ [L] of loci, the natural parameter β S equals the logarithmof the corresponding multiplicative epistasis measure as defined in Equation (18).

Theorem 1

Letbe fitness values for genotypes x = (x 1,…, x L) ∈ {0, 1} L such that f x = c ⋅ p x for some constant c > 0 and for some multivariate Bernoulli random variable (X 1,…, X L) with general parameters. Then for all subsets S ⊆ {1,…, L } of loci, the log-multiplicative epistasis measureis equal to the interaction parameter β S of the random variable (X 1,…, X L).

By using the equivalence between multiplicative fitness values and additive log-fitness values, we also derive a similar probabilistic interpretation of the additive epistasis formula. Specifically, if fitnessis proportional to the log -probabilityof observing the genotype (x 1,…, x L), then for each subset S = { i 1,…, i K } ⊆ [L] of loci, the natural parameter β S equals the logarithmof the corresponding additive epistasis measure as defined in Equation (16). We formalize this observation as the following Corollary of Theorem 1.

---

### One-shot learning for solution operators of partial differential equations [^d6bcc900]. Nature Communications (2025). High credibility.

Learning the solution operator for a linear PDE

We first demonstrate the capability of our method with a pedagogical example of a one-dimensional Poisson equationwith the zero Dirichlet boundary condition, and the solution operator is.

As described in the one-shot learning method based on the principle of locality, we first need to select a "canonical" local domain, which is a line segment in 1D case. We choose the simplest local solution operator using 3 nodes (Fig. 2 b)with Δ x = 0.01 from. The training datasetonly has one data point(Fig. 3 a), whereis generated based on. Using the training dataset, we placeat different locations to generate data points and train the local solution operator. For a new PDE condition f, we apply our trainedand enforce the zero Dirichlet boundary condition at x = 0 and x = 1 to find the solution. In Fig. 3 c, we show examples of testing cases f = f 0 + Δ f with different σ test. When σ test is larger, there is a more significant discrepancy between f 0 and f.

Fig. 3
1D Poisson equation.

a The training data includes a randomgenerated from GRF and the corresponding solution. b The convergence of L 2 relative errors of different approaches for various test cases. The shaded regions represent one standard deviation of the L 2 relative errors from 100 test cases. c Testing examples of random f = f 0 + Δ f with Δ f sampled from GRF of σ test = 0.02, 0.05, 0.10, 0.15 and l test = 0.1. d Prediction example of different approaches for various test cases. e Prediction errors (the geometric mean and standard deviation) when training by f random with amplitude σ train and testing on functions with amplitude σ test. The length scales l train = 0.01 and l test = 0.10 are fixed. f Prediction errors (the geometric mean and standard deviation) when training by f random with different length scale l train and amplitude σ train = 0.10, and testing on functions with amplitude σ test = 0.10 and l test = 0.10.

---

### How dieting might make some fatter: modeling weight cycling toward obesity from a perspective of body composition autoregulation [^32b99e3d]. International Journal of Obesity (2020). Medium credibility.

Two exponential fits are shown: a generalized linear model (GLM) with 95% confidence intervals (solid line), and a linear model (LM), with R 2 values (dotted line). The four figures correspond to the four methods presented in the main text for computing the value FAT END at the final time END at which the subjects would have completely recovered their initial FFM. The two red squares correspond to the US Army Ranger data points presented in the section on 'Applications of the Model'.

Their exact values depend on the method used for computing the values FAT END when the FFM has been completely recovered and on the type of statistical regression used. As explained previously, we consider four methods for computing FAT END. In Fig. 4, we show the fits obtained from a generalized linear model (GLM) with 95% confidence intervals (solid line) and from a linear model (LM), with R 2 values (dotted line). Following the performance of diagnostics tests to analyze the residuals, it is found that the GLM satisfies the major assumptions of regression analysis better than the LM, especially concerning the normality assumption of the error terms. Nevertheless, as shown in Fig. 4, both models give curves that are close to each other. In simple terms, the main difference between the above LM and GLM approaches to estimate the best parameter values for the constants a and b is the way the error term is handled. In the LM method, we write y = a × e bx × ε, where ε is a random error variable that follows a log-normal distribution with parameters μ = 0 and σ 2. Taking the logarithm on both sides of the equality gives, which corresponds to the linear model, where, x ′ = x, β 1 = b andis a random error variable that follows a normal distribution with mean μ = 0 and variance σ 2. Note that in the LM method, the error term ε is multiplied with the exponential function y = a × e bx. In the GLM method, the error term is instead added to the exponential function. Formally, we write y = a ×e bx + ε, where ε is a random error variable that follows a normal distribution with mean 0 and variance σ 2. As a consequence, the GLM admits the possibility of the value y = 0 (which is excluded in LM) and assumes that the variance V (y) is constant, while the LM method assumes that V (y) varies with x. Finally, note that the LM method admits exact analytical solutions for the model parameters a and b, while the GLM method requires numerical optimization algorithms to find the best values (the maximum likelihood estimates) for a and b.

---

### Scalable spatiotemporal prediction with Bayesian neural fields [^c7f97742]. Nature Communications (2024). High credibility.

Methods

Posterior inference

Let P (Θ f, Θ y, Y) denote the joint probability distribution over the parameters and observable field in Box 1. The posterior distribution is given by Eq. (13) in the main text. We describe two approximate posterior inference algorithms for BNF. In these sections, we define Θ = (Θ f, Θ y), θ = (θ f, θ y) and r = (s, t).

Stochastic MAP ensembles

A simple approach to uncertainty quantification is based on the "maximum a-posteriori" estimate:We find an approximate solution to the optimization problem (15) using stochastic gradient ascent on the joint log probability, according to the following procedure, where B ≤ N is a mini-batch size and (ϵ 1, ϵ 2,…) is a sequence of learning rates:Repeat until convergence

We construct an overall "deep ensemble"containing M ≥ 1 MAP estimates by repeating the above procedure M times, each with a different initialization of θ 0 and random seed.

Stochastic variational inference

A more uncertainty-aware alternative to MAP ensembles is mean-field variational inference, which uses a surrogate posteriorover Θ to approximate the true posterior(13) given the data. Optimal values for the variational parametersare obtained by maximizing the "evidence lower bound":where Eq. (22) follows from the independence of the priors. Finding the maximum of Eq. (22) is a challenging optimization problem. Our implementation leverages a Gaussian variational posterior q ϕ with KL reweighting, as described in Blundell et al. (Sections 3.2 and 3.4 of ref.).

Mean-field variational inference is known to underestimate posterior variance and can also get stuck in local optima of Eq. (21). To alleviate these problems, we use a variational ensemble that is analogous to the MAP ensemble described above. More specifically, we first perform M ≥ 1 runs of stochastic variational inference with different initializations and random seeds, which gives us an ensemble { ϕ i, i = 1,…, M } of variational parameters. We then approximate the posteriorwith an equal-weighted mixture of the resulting variational distributions.

---

### Hydrocodone bitartrate (hysingla ER) [^0c03a0bc]. FDA (2023). Medium credibility.

Warnings and precautions regarding the use of hydrocodone bitartrate ER PO (also known as Hysingla ER):
- **Adrenal insufficiency**: use caution in patients taking the drug for a prolonged period (> 1 month).
- **Central sleep apnea**: use caution in patients taking higher doses.
- **Decreased serum hydrocodone level**: use caution in patients taking CYP3A4 inducers (such as rifampin, carbamazepine, or phenytoin) or discontinuing CYP3A4 inhibitors (such as macrolide antibiotics, azole antifungals, or protease inhibitors).
- **Erectile dysfunction, infertility**: use caution in patients taking the drug for a prolonged period.
- **Exacerbation of increased ICP**: use caution in patients with increased ICP, brain tumor, or head injury.
- **Growth suppression**: use caution in patients with chronic corticosteroid therapy. Monitor growth regularly in pediatric patients receiving chronic corticosteroid therapy. Reassess the need for hydrocodone regularly and adjust the corticosteroid therapy as appropriate.
- **Hypotension, syncope**: use caution in patients with reduced blood volume or taking other CNS depressants. Monitor BP after initiation and dose titration. Avoid use in patients with circulatory shock.
- **Mask symptoms of head injury**: use caution in patients with head injury. Avoid use in patients with impaired consciousness or coma.
- **Opioid overdose**: use caution in patients taking CNS depressants or with a history of opioid use disorder or prior opioid overdose. Consider prescribing naloxone based on the patient's risk factors for overdose.
- **Opioid withdrawal syndrome**: do not discontinue abruptly in patients physically dependent on opioids.
- **Opioid withdrawal syndrome**: use caution in patients taking CYP3A4 inducers (such as rifampin, carbamazepine, or phenytoin) or discontinuing CYP3A4 inhibitors (such as macrolide antibiotics, azole antifungals, or protease inhibitors).
- **Opioid withdrawal syndrome**: use extreme caution in patients taking mixed agonist/antagonist analgesics (such as pentazocine, nalbuphine, or butorphanol) or partial agonist analgesics (such as buprenorphine).
- **Prolonged QT interval**: use caution in patients with congestive HF, bradyarrhythmia, electrolyte abnormalities, or taking drugs prolonging QT interval. Avoid use in patients with congenital long QT syndrome. Do not exceed 90 mg BID in patients developing QT prolongation.
- **Seizure**: use caution in patients with seizure disorder.
- **Serotonin syndrome**: use caution in patients taking serotonergic drugs.
- **Serotonin syndrome**: use extreme caution in patients taking MAOIs or within 14 days of stopping treatment.
- **Somnolence**: use extreme caution in patients performing activities requiring mental alertness, such as driving or operating machinery.
- **Sphincter of Oddi dysfunction**: use caution in patients with biliary tract disease and acute pancreatitis.

---

### A novel oppositional binary crow search algorithm with optimal machine learning based postpartum hemorrhage prediction model [^f6de23e1]. BMC Pregnancy and Childbirth (2022). Medium credibility.

Whereasrepresent the opposite number and x ∈ R denotes a real number determined on range of x ∈[a, b]. While a = 0 and b = 1 Eq. (3) becomes

While there is a point P (x 1, x 2,… x n) in n dimension coordinate and x 1, x 2,…, x n ∈ R later, the opposite pointis determined as its coordinates:

In such cases, have 2 values, x represent initial arbitrary value in [a, b] anddenotes the opposite values of x. They calculate f (x)&in all the iterations of OBCSA, later, employ on the evaluation function g ifselect x or else selectConsequently, the f l would be in range: f l ∈[f l min, f l max]. The opposite numbercan be determined by:

Later, evaluate the fitness for the first f l value and the fitness forin all the iterations. When, they select f l, or elsewould be selected. The stages of presented method can be given in the following.

Step1: The count of crows is n c = 25, f l min = 0.1, f l max = 1.8, A P = 0.3, and the maximal number of iterations is t max = 100.

Step2: The position that represent the features are made by U (0, 1).

Step3: The fitness function (FF) can be determined by

Whereas C represent the classification performance, W represent the weighted factors in the range of zero and one, F all represent the overall amount of features and F sub signifies the length of elected feature.

Step4: The position of the crows are upgraded as Eq. (2)

Step5: Steps 3 & 4 are repetitive till a t max is attained.

---

### Control of finite critical behaviour in a small-scale social system [^c81152c0]. Nature Communications (2017). Medium credibility.

Independent model inference

The independent model consists of individuals participating in conflict randomly, with the frequencies of individual appearance equal to their empirically measured, heterogeneous values f i = 〈 x i 〉. Naively, this can be written as a relative negative log likelihood, and is equivalent to a maximum entropy model that matches only the frequencies f i. (The relative negative log likelihood L (x) of state x is related to the likelihood p (x) by p (x) = exp(− L (x))/ Z, where Z is a normalization constant set by the constraint that the sum of likelihoods over all states is one. In statistical physics, Z is the partition function and L (x) is proportional to the free energy of state x.)

However, as detailed in 'Operational definitions' in the Methods section, a fight was operationalized for these analyses as involving two or more socially mature individuals. (Observed fights that involved only juveniles and 0 or 1 mature individuals are therefore excluded.) Correspondingly, we forbid our models from producing fights of size smaller than two. We treat this as an additional constraint on the model. The resulting maximum entropy model is then the one in which the likelihood of states with fewer than two individuals present is taken to zero. This corresponds to a relative negative log likelihood

where Θ(z) is 0 when z ≤ 0 and 1 when z > 0.

In the unconstrained case (α = 0), we can easily solve for h i:

We must now solve numerically for h i to match the empirically measured f i = 〈 x i 〉 = 〈 x i 〉 α →∞. To accomplish this, note that the unconstrained model will have modified statistics:

where f 0 is the frequency of size zero fights, f i 1 is the frequency of size one fights consisting solely of individual i, andis the overall frequency of size one fights (all measured in the unconstrained model). In terms of unconstrained individual frequencies, these are

We use an iterative procedure to solve equation (7) for 〈 x i 〉 α = 0, which are then used in equation (6) to find the fields. Finally, samples from the independent model (equation (4)) are produced by sampling using equation (5) and simply discarding samples in which fewer than two individuals appear. For our data, this results in discarding about 17% of samples produced with equation (5).

---

### Dirichlet latent modelling enables effective learning and sampling of the functional protein design space [^cdd552d6]. Nature Communications (2024). High credibility.

Variational inference of model parameters

In a Variational Autoencoder framework, NNs are used to compute the variational parameters ϕ for a fixed family of probability distributions q ϕ (z ∣ x) and model parameters θ for conditional likelihood p θ (x ∣ z). Here, we use SVI to find an approximate solution to the problem of maximising the marginal likelihood ∫ p θ (x ∣ z) p θ (z) d z by maximising the Evidence Lower Bound (ELBO) w.r.t both model parameters θ and variational parameters ϕ as follows:where KL is the Kullback-Leibler divergence, and the expected conditional likelihoodis optimised with respect to maximum log-likelihood, as the probability distribution p θ (x ∣ z) over the amino acid space is categorical.

Depending on the choice of parametric families for q ϕ (z ∣ x) and p θ (x ∣ z), computing the expected conditional likelihood can be challenging, is often intractable and requires numerical approximations, whereas KL can be computed analytically. Ultimately, we need to compute a gradient of the expected conditional likelihood w.r.t parameters ϕ: However, in this case, the gradient computation cannot be moved under the expectation operator, since the expectation is done w.r.t q ϕ (z ∣ x). Nonetheless, for many parametric distributions, a number of low variance gradient estimators have been proposed, such as those based on pathwise derivatives, alternatively known as reparametrization trick, but they cannot be applied to the Dirichlet distribution unless Gaussian-based approximations are used at the cost of losing the characteristic properties of the Dirichlet distribution. The general idea of the generalised reparametrization trick that applies to the majority of continuous distributions is based on implicit differentiation that results in ∇ ϕ z term that can be computed using only Probability Density Function (PDF) q ϕ (z) and derivatives of Cumulative Density Function (CDF) ∇ ϕ F (z ∣ ϕ) or its numerical approximation as follows:

The exact derivation of ∇ ϕ z for Dirichlet distribution is provided in refs. with the latter being an official implementation of the PyTorch library.

---

### Recommendations for a standardized pulmonary function report. An official American Thoracic Society technical statement [^2f5b3095]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

American Thoracic Society pulmonary function report — reference equations, calibration, and quality grading specify that for diffusing capacity the report is consistent with the 2017 European Respiratory Society (ERS)/ATS Technical Standard for this test and that barometric pressure should be measured and reported and the measured value corrected to the standard pressure of 760 mm Hg. Newer collated reference equations for spirometry and diffusing capacity have been developed since prior ATS documents and warrant wide implementation; the Global Lung Function Initiative (GLI)-2012 multiethnic spirometry reference values are recommended for use in North America and elsewhere for the ethnic groups represented, and the National Health and Nutrition Examination Survey (NHANES) III reference values remain appropriate where maintaining continuity is important. Regardless of the reference source or lower limit of normal (LLN) chosen, interpreters should be aware of uncertainty when interpreting values near any dichotomous boundary. For lung volumes and diffusing capacity, no prior ATS recommendation has been made because of the wide divergence of available reference values, with a large compilation completed for the diffusing capacity of the lung for carbon monoxide (DlCO) and underway for lung volumes, and the resulting reference equations should be widely adopted when published. Pulmonary function tests that fail to meet optimal standards may still provide useful information and a grading system for test quality can allow for this use, while providing an indication of the uncertainty imposed, and is most helpful if widely standardized. For spirometry, FVC and FEV1 are graded separately on an A–F scale either manually or by software, with evidence that grades A–C are clinically useful, grades D and E may have limited value, and grade F should not be used, and the same scale, with different criteria values, is used for children; for diffusing capacity a similar grading scale is presented on the basis of 2017 ERS/ATS standards.

---

### Identifying domains of applicability of machine learning models for materials science [^72c4ba76]. Nature Communications (2020). High credibility.

An illustrative example

Before describing the details of DA identification and its integration into the ML process, let us illustrate the concept and its utility via a synthetic example (see Fig. 1). We consider a simple two-dimensional representation consisting of independent features x 1 and x 2 that are each distributed according to a normal distribution with mean 0 and variance 2 (N (0, 2)) and a target property y that is a third-degree polynomial in x 1 with an additive noise component that scales exponentially in x 2 :That is, the y values are almost determined by the third-degree polynomial for low x 2 values but are almost completely random for high x 2 values. Discovering applicable domains reveals how different models cope differently with this setting even if they have a comparable average error. To show this, let us examine the error distributions obtained from three different kernelized regression models of the formwith parameter vector ν that are fitted around a training, or fitting (F), setwith three different choices for the kernel function k. We observe:
When using the linear (lin) kernel, the resulting linear model is globally incapable to trace the variation of the third-order polynomial except for a small stripe on the x 1 -axis where it can be approximated well by a linear function. Consequently, there is a very high error globally that is substantially reduced in the DA described by σ lin (x 1, x 2) ≡ −0.3 ≤ x 1 ≤ 0.3.
When using the Gaussian kernel), the resulting radial basis function (rbf) model is able to represent the target property well locally unless (a) the noise component is too large and (b) the variation of the target property is too high relative to the number of training points. The second restriction is because the rbfs have non-negligible values only within a small region around the training examples. Consequently, the discovered DA is not only restricted in x 2 -direction but also excludes high absolute x 1 -values: σ rbf ≡ −3.3 ≤ x 11 ≤ 3.1 ∧ x 2 ≤ 0.1.
In contrast, when using the non-local third-degree polynomial (poly) kernel, data sparsity does not prevent an accurate modeling of the target property along the x 1 -axis. However, this non-locality is counterproductive along the x 2 -axis where overfitting of the noise component has a global influence that results in higher prediction errors for the almost deterministic data points with low x 2 -values. This is reflected in the identified DA σ poly (x 1, x 2) ≡ −3.5 ≤ x 2 ≤ 0.1, which contains no restriction in x 1 -direction, but excludes both high and low x 2 -values. This highlights an important structural difference between the rbf and the polynomial model that is not reflected in their similar average errors.

---

### Bacterial motility can govern the dynamics of antibiotic resistance evolution [^c771c187]. Nature Communications (2023). High credibility.

Analytical and numerical techniques

In addition to computer simulations, we developed a combination of analytical and numerical techniques, which are used to compute the evolutionary time, ecological time and the adaptation rate. In contrast to simulations, these techniques are based on closed-form solutions of master equations, numerical methods for solving algebraic equations (such as Newton-Raphson method) and solving ODEs (such as Runge-Kutta method). To illustrate these techniques, we show how to compute the expected waiting times at a resistance state R and the adaptation rate a R in the original staircase model.
Find the wild-type profile N x at a resistance state R. Use Newton-Raphson method to solve the algebraic equations for the non-trivial fixed point of the mean-field theorywhereis the indicator function.
Compute the evolutionary time. A closed-form solution exists and follows from a modification of the theory in Hermsen et al. We consider three independent adaptation paths along which wild-type can produce a first mutant in the overlap region x = R + 1. On the position-genotype lattice (x, g), they are the division path D: (R, R) → (R, R + 1) → (R + 1, R + 1) (mutation at x = R followed by migration to the right); the overlap path O: (R + 1, R) → (R + 1, R + 1) (mutation at x = R + 1); and the no-division path N: (R + 2, R) → (R + 2, R + 1) → (R + 1, R + 1) (mutation at x = R + 2 followed by migration to the left). As the original theoryassumes low motility, it neglects the no-division path since there are no wild-type cells to mutate at position x = R + 2 in this regime (SI Theorem 3). Moreover, we assume that the flux of mutants between neighbouring compartments in the division region x ≤ R and the no-divison region x ≥ R + 2 is equilibrated, so that we can ignore the net probability flux of mutants that enter or exit the compartments x = R − 1, R, R + 1 and also ignore longer paths such as (R − 1, R) → (R − 1, R + 1) → (R, R + 1) → (R + 1, R + 1) (mutation at x = R − 1 followed by double migration to the right). This is a reasonable assumption as the flux of mutant-producing wild-type is equilibrated far from the population front. The computation ofhas the following structure. The waiting times till a mutant is produced in the overlap region along path i ∈ { D, O, N } are denoted by T i (Fig. 2 a). The probability that the waiting will be longer than t isand the probability density function of T i is given by F i (t) = −d S i /d t. Notice thatand that T i are independent. Thus. The probability density function ofis The expected timeis found by numerical integration. Therefore, the problem is solved by finding the functions S i (t) and differentiating them to produce F i (t). According to Hermsen et al. these functions are: For path D: whereandwhere 〈 n (t)〉 D is the average mutant population in compartment x = R at time t For path O: For path N: whereandwhere 〈 n (t)〉 N is the average mutant population in compartment x = R + 2 at time t Finally, if there is a nonnegligible chance that the first mutant in the overlap region dies before division, the evolutionary timemust be corrected to, where q is the probability that the first mutant in the overlap region divides before its death (Supplementary Note 8).
Compute the ecological time. The competition between wild-type and mutants can be described well by the mean-field equations:with initial conditions at time t = 0:In these equations w x and m x stand for the number of wild-type and mutant cells in spatial compartment x, respectively. The system is simulated from the end state of the previous stochastic process when a first mutant appears in the overlap region. This system can be simulated efficiently by a Runge-Kutta (RK4) method. The timeis reached when w R +1 < m R +1 for the first time, i.e. the mutants outcompete the wild-type in the overlap region.
Find the adaptation rate. The adaptation rate is found from

---

### The future is 2D: spectral-temporal fitting of dynamic MRS data provides exponential gains in precision over conventional approaches [^fe589ed4]. Magnetic Resonance in Medicine (2023). Medium credibility.

Purpose

Many MRS paradigms produce 2D spectral-temporal datasets, including diffusion-weighted, functional, and hyperpolarized and enriched (carbon-13, deuterium) experiments. Conventionally, temporal parameters-such as T 2, T 1, or diffusion constants-are assessed by first fitting each spectrum independently and subsequently fitting a temporal model (1D fitting). We investigated whether simultaneously fitting the entire dataset using a single spectral-temporal model (2D fitting) would improve the precision of the relevant temporal parameter.

Methods

We derived a Cramer Rao lower bound for the temporal parameters for both 1D and 2D approaches for 2 experiments: a multi-echo experiment designed to estimate metabolite T 2 s, and a functional MRS experiment designed to estimate fractional change (< mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:semantics > < mml:mrow > < mml:mi > δ < /mml:mi > < /mml:mrow > < mml:annotation > $$ \delta $$ < /mml:annotation > < /mml:semantics > < /mml:math >) in metabolite concentrations. We investigated the dependence of the relative standard deviation (SD) of T 2 in multi-echo and < mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:semantics > < mml:mrow > < mml:mi > δ < /mml:mi > < /mml:mrow > < mml:annotation > $$ \delta $$ < /mml:annotation > < /mml:semantics > < /mml:math > in functional MRS.

Results

When peaks were spectrally distant, 2D fitting improved precision by approximately 20% relative to 1D fitting, regardless of the experiment and other parameter values. These gains increased exponentially as peaks drew closer. Dependence on temporal model parameters was weak to negligible.

Conclusion

Our results strongly support a 2D approach to MRS fitting where applicable, and particularly in nuclei such as hydrogen and deuterium, which exhibit substantial spectral overlap.

---

### Practical guide for interpreting and reporting cardiac PET measurements of myocardial blood flow: an information statement from the American Society of Nuclear Cardiology, and the Society of Nuclear Medicine and Molecular Imaging [^461b4a68]. Journal of Nuclear Cardiology (2021). High credibility.

Know your software (retention and compartment models) — compartment and retention model approaches for myocardial blood flow (MBF) are described as follows: The compartment model approach for calculating MBF requires a series of dynamic images that record tracer kinetics into and washout from the myocardium and uses a non-linear fit of the model to the acquired data to solve for the regional MBF, tracer washout, and partial volume artifacts. The retention model approach assumes tracer retention is determined by the blood pool concentration and irreversible extraction into the myocardium; the amount of radiotracer retained in the myocardium is corrected for or normalized to the amount of radiotracer delivered via arterial blood ("the arterial input function") and thus the integral of the arterial radiotracer concentration (assumed to be derived from the initial two-minute images), is corrected for the flow-dependent or K1-specific extraction fraction, and by assuming the tracer does not significantly washout determines blood flow based on tracer uptake; this model does not consider washout, assumes a partial volume correction factor, and its shortcoming can be somewhat mitigated by using a short dynamic scan acquisition time.

---

### Thinking outside the curve, part I: modeling birthweight distribution [^49eb3a50]. BMC Pregnancy and Childbirth (2010). Low credibility.

Results

1. Pragmatics for mixture modeling

a. Finite normal mixture models

Many phenomena cannot be accurately described via a normal distribution. When no other commonly used probability distribution seems appropriate, a finite normal mixture model is often reasonable. We now briefly describe the model. Readers interested in theoretical developments may consult references and works cited therein.

Let f (x;μ,σ) denote the probability density for the normal distribution with mean μ and standard deviation σ. A finite normal mixture model with k components has probability density

A common way to interpret Equation (1) is to imagine that the full population consists of k subpopulations. The proportion of individuals in the full population belonging to subpopulation j is p j. In subpopulation j, measurements are normally distributed with mean μ j and standard deviation σ j.

The mixture components may or may not represent subpopulations with obvious biological definitions outside the statistical model. For example, in a 2-component normal mixture describing birthweights for white singletons in the United States, there is not an obvious biological characterization for the two components: we may say that the component with the smaller mean reflects compromised pregnancies, but we cannot immediately attribute the compromised pregnancies to a specific biological mechanism.

Ideally, modeling with finite normal mixtures may lead to discoveries of subpopulations with biological definitions that were not immediately obvious, although the mixture components themselves may still only be approximations to such subpopulations.

---

### A cochlear-bone wave can yield a hearing sensation as well as otoacoustic emission [^4a2d305b]. Nature Communications (2014). Medium credibility.

The coefficients G 1 (k) and G 2 (k) are defined as follows:

Here we have used the abbreviation L (k) = [2 i ω ρ (F 1 + F 2) w bm +3 F 1 F 2 Z bm (x)]/[3 A 1 A 2 Z bm (x)] with F 1/2 = A 1/2 k 2 − cω 2 ρ. The dispersion relation L(k) = 0 has been derived from the eigenvalues of the matrix equation (10).

The Green's functions for bone stimulation can be derived analogously. Assume that both cochlear chambers, at a certain longitudinal location x 0, are sinusoidally compressed and expanded:

We make the following ansatz for the Greens functions:

which yields the amplitude equations

The solutions are

with L (k) as given above. In the symmetric case of equal chamber areas, A 1 = A 2, we obtain W 1 (k) = W 2 (k). No basilar-membrane displacement then arises, because the pressures in both chambers are equal.

When attempting to compute the integral in the ansatz for the Green's functions (equations (25) and (29)), we encounter a problem: the integrand has a singularity at the wave vectors k for which L (k) = 0, that is, at those wave vectors that obey the dispersion relation. However, we can employ the residue theorem of complex analysis to compute the integrals. Indeed, for propagation apical of the generation site, that is at a location x < x 0, we can close the contour in the upper-half plane because the integrand there is exponentially suppressed. The integral then only involves a contribution from the poles in the upper-half plane. In the case of basilar-membrane stimulation, we obtain a contribution proportional to. The pressures p 1/2 (− k bm, ω, x 0) represent the pressures of the basilar membrane mode in the two chambers

Analogous results can be obtained for the cochlear-bone wave with k cb (x).

In the opposite case, for a cochlear location basal to the generation site, x > x 0, the integration path can be closed in the lower-half plane.

---

### Estimation in medical imaging without a gold standard [^82dda7fe]. Academic Radiology (2002). Low credibility.

Rationale and Objectives

In medical imaging, physicians often estimate a parameter of interest (eg, cardiac ejection fraction) for a patient to assist in establishing a diagnosis. Many different estimation methods may exist, but rarely can one be considered a gold standard. Therefore, evaluation and comparison of different estimation methods are difficult. The purpose of this study was to examine a method of evaluating different estimation methods without use of a gold standard.

Materials and Methods

This method is equivalent to fitting regression lines without the x axis. To use this method, multiple estimates of the clinical parameter of interest for each patient of a given population were needed. The authors assumed the statistical distribution for the true values of the clinical parameter of interest was a member of a given family of parameterized distributions. Furthermore, they assumed a statistical model relating the clinical parameter to the estimates of its value. Using these assumptions and observed data, they estimated the model parameters and the parameters characterizing the distribution of the clinical parameter.

Results

The authors applied the method to simulated cardiac ejection fraction data with varying numbers of patients, numbers of modalities, and levels of noise. They also tested the method on both linear and nonlinear models and characterized the performance of this method compared to that of conventional regression analysis by using x-axis information. Results indicate that the method follows trends similar to that of conventional regression analysis as patients and noise vary, although conventional regression analysis outperforms the method presented because it uses the gold standard which the authors assume is unavailable.

Conclusion

The method accurately estimates model parameters. These estimates can be used to rank the systems for a given estimation task.

---

### The linear-quadratic model is an appropriate methodology for determining isoeffective doses at large doses per fraction [^9164b9e6]. Seminars in Radiation Oncology (2008). Low credibility.

The tool most commonly used for quantitative predictions of dose/fractionation dependencies in radiotherapy is the mechanistically based linear-quadratic (LQ) model. The LQ formalism is now almost universally used for calculating radiotherapeutic isoeffect doses for different fractionation/protraction schemes. In summary, the LQ model has the following useful properties for predicting isoeffect doses: (1) it is a mechanistic, biologically based model; (2) it has sufficiently few parameters to be practical; (3) most other mechanistic models of cell killing predict the same fractionation dependencies as does the LQ model; (4) it has well-documented predictive properties for fractionation/dose-rate effects in the laboratory; and (5) it is reasonably well validated, experimentally and theoretically, up to about 10 Gy/fraction and would be reasonable for use up to about 18 Gy per fraction. To date, there is no evidence of problems when the LQ model has been applied in the clinic.

---

### Topological regression as an interpretable and efficient tool for quantitative structure-activity relationship modeling [^0c5de2be]. Nature Communications (2024). High credibility.

Neighborhood training model

Customarily, the weights are expressed as a deterministic function of the distances in the predictor space. In standard KNN regression, we assume that distance in the predictor space is proportional to the distance in the response space. In metric learning, a transformation of the predictor space is learned such that there is an approximate isometry between the transformed predictor space and the response space. In TR, we instead write a formal statistical model to connectwith the squared Euclidean distances in the predictor space in the following fashion:

We define the weightsand since we have I * and I to be disjoint and the responses could be assumed to be absolutely continuous, we can definewith the entries in, i.e.being real quantities. Define the squared Euclidean distance matrix in the predictor space as

We define a simple multivariate linear regression model connectingwith D X. Consider the m th row of. Observe that, this row consists of the weights used to express the m th response in I using all the responses in I *. We envision this row to be a set of repeated measurements taken on the m th point in I from the vantage points in I *. Thus, denoting the K elements in the m th row ofby, the corresponding row of predictors in D X by, and the matrix of regression coefficients bywe arrive at the following regression modelwith. Now assuming mutual independence across the N rows ofand since N > K (by construction), we can obtain the MLEs of B and Σ. Letanddenote their respective estimates. Then, for a new query point, we can computeand, using, obtain the predictions. However, observe that (1) requires (W 1, q u e r y, W 2, q u e r y, ⋯, W K, q u e r y) to generate a prediction for the query point, and simply exponentiating the output, of (6) will yield a biased estimate of W becausedue to Jensen's inequality. Therefore, we use the properties of the multivariate log-normal distribution to improve the estimate of W in the following way:

---

### Recommendations for a standardized pulmonary function report. An official American Thoracic Society technical statement [^345386c9]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Selecting and reporting reference values — procedural guidance states that PFT laboratories must select appropriate reference values generated from high-quality data from a large sample of healthy asymptomatic individuals who have never smoked or had other respiratory illness or significant exposures, reports should identify the source of reference values, any change in selected equations should be noted and prior percent predicted values recalculated if possible, pediatric–adult equation discontinuity is discouraged, and extrapolation beyond the age range should not be done during growth, increases uncertainty in the elderly, and must be noted in technician comments.

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^aeeaae45]. Nature Communications (2025). High credibility.

Fitting models to data is an important part of the practice of science. Advances in machine learning have made it possible to fit more-and more complex-models, but have also exacerbated a problem: when multiple models fit the data equally well, which one(s) should we pick? The answer depends entirely on the modelling goal. In the scientific context, the essential goal is replicability: if a model works well to describe one experiment, it should continue to do so when that experiment is replicated tomorrow, or in another laboratory. The selection criterion must therefore be robust to the variations inherent to the replication process. In this work we develop a nonparametric method for estimating uncertainty on a model's empirical risk when replications are non-stationary, thus ensuring that a model is only rejected when another is reproducibly better. We illustrate the method with two examples: one a more classical setting, where the models are structurally distinct, and a machine learning-inspired setting, where they differ only in the value of their parameters. We show how, in this context of replicability or "epistemic uncertainty", it compares favourably to existing model selection criteria, and has more satisfactory behaviour with large experimental datasets.

---

### CurveCurator: a recalibrated F-statistic to assess, classify, and explore significance of dose-response curves [^cf66bf7c]. Nature Communications (2023). High credibility.

Dose-response curves are key metrics in pharmacology and biology to assess phenotypic or molecular actions of bioactive compounds in a quantitative fashion. Yet, it is often unclear whether or not a measured response significantly differs from a curve without regulation, particularly in high-throughput applications or unstable assays. Treating potency and effect size estimates from random and true curves with the same level of confidence can lead to incorrect hypotheses and issues in training machine learning models. Here, we present CurveCurator, an open-source software that provides reliable dose-response characteristics by computing p-values and false discovery rates based on a recalibrated F-statistic and a target-decoy procedure that considers dataset-specific effect size distributions. The application of CurveCurator to three large-scale datasets enables a systematic drug mode of action analysis and demonstrates its scalable utility across several application areas, facilitated by a performant, interactive dashboard for fast data exploration.

---

### Scalable spatiotemporal prediction with Bayesian neural fields [^fea8687d]. Nature Communications (2024). High credibility.

Prediction queries

We can approximate the posterior (13) using a set of samples, which may be obtained from either MAP ensemble estimation or stochastic variational inference (by sampling from the ensemble of M variational distributions). We can then approximate the posterior-predictive distribution(which marginalizes out the parameters Θ) of Y (r *) at a novel field index r ✱ = (s *, t *) by a mixture model with M equally weighted components:Equipped with Eq. (23), we can directly compute predictive probabilities of events { Y (r *) ≤ y }, predictive probability densities { Y (r *) = y }, or conditional expectationsfor a probe function. Prediction intervals around Y (r *) are estimated by computing the α -quantile y α (r *), which satisfiesFor example, the median estimate is y 0.50 (s *, t *) and 95% prediction interval is [y 0.025 (s *, t *), y 0.975 (s *, t *)]. The quantiles (24) are estimated numerically using Chandrupatla's root finding algorithmon the cumulative distribution function of the mixture (23).

Temporal seasonal features

Including seasonal features (c.f. Eq. (8)), where possible, is often essential for accurate prediction. Example periodic multiples p for datasets with a variety of time units and seasonal components are listed below (Y = Yearly; Q = Quarterly; Mo = Monthly; W = Weekly; D = Daily; H; Hourly; Mi = Minutely; S = Secondly):
Q: Y = 4
Mo: Q = 3, Y = 12
W: Mo = 4.35, Q = 13.045, Y = 52.18
D: W = 7, Mo = 30.44, Q = 91.32, Y = 365.25
H: D = 24, W = 168, Mo = 730.5, Q = 2191.5, Y = 8766
Mi: H = 60, D = 1440, W = 10080, Mo = 43830, Q = 131490, Y = 525960
S: Mi = 60, H = 3600, D = 86400, W = 604800, Mo = 2629800, Q = 7889400, Y = 31557600

---

### Allometric cascade as a unifying principle of body mass effects on metabolism [^0bbbcb77]. Nature (2002). Excellent credibility.

The power function of basal metabolic rate scaling is expressed as aM(b), where a corresponds to a scaling constant (intercept), M is body mass, and b is the scaling exponent. The 3/4 power law (the best-fit b value for mammals) was developed from Kleiber's original analysis and, since then, most workers have searched for a single cause to explain the observed allometry. Here we present a multiple-causes model of allometry, where the exponent b is the sum of the influences of multiple contributors to metabolism and control. The relative strength of each contributor, with its own characteristic exponent value, is determined by the control contribution. To illustrate its use, we apply this model to maximum versus basal metabolic rates to explain the differing scaling behaviour of these two biological states in mammals. The main difference in scaling is that, for the basal metabolic rate, the O(2) delivery steps contribute almost nothing to the global b scaling exponent, whereas for the maximum metabolic rate, the O(2) delivery steps significantly increase the global b value.

---

### KDIGO 2024 clinical practice guideline for the evaluation and management of chronic kidney disease [^b40fce01]. Kidney International (2024). High credibility.

Selection of GFR estimating equations — use of validated eGFR: We recommend using a validated GFR estimating equation to derive GFR from serum filtration markers (eGFR) rather than relying on the serum filtration markers alone (1D). Practice Point 1.2.4.1 advises to use the same equation within geographical regions (as defined locally [e.g., continent, country, and region] and as large as possible), noting that within such regions, equations may differ for adults and children. The recommendation places a high value on using an estimating equation validated in the population of interest, and the key points are to use an equation validated and most suited to the population of interest; there is potential for harm if people get different eGFR values when receiving care in different settings, and there is benefit to clinical care, research, and public health with the use of validated equations such that decisions, research findings, and public policy are informed by accurate estimates of CKD.

---

### A probabilistic Bayesian approach to recovermap and phase images for quantitative susceptibility mapping [^b146d367]. Magnetic Resonance in Medicine (2022). Medium credibility.

Purpose

Undersampling is used to reduce the scan time for high-resolution three-dimensional magnetic resonance imaging. In order to achieve better image quality and avoid manual parameter tuning, we propose a probabilistic Bayesian approach to recover < mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:semantics > < mml:mrow > < mml:msubsup > < mml:mrow > < mml:mi > R < /mml:mi > < /mml:mrow > < mml:mrow > < mml:mn > 2 < /mml:mn > < /mml:mrow > < mml:mrow > < mml:mo > ∗ < /mml:mo > < /mml:mrow > < /mml:msubsup > < /mml:mrow > < mml:annotation > $$ {R}_2^{\ast } $$ < /mml:annotation > < /mml:semantics > < /mml:math > map and phase images for quantitative susceptibility mapping (QSM), while allowing automatic parameter estimation from undersampled data.

Theory

Sparse prior on the wavelet coefficients of images is interpreted from a Bayesian perspective as sparsity-promoting distribution. A novel nonlinear approximate message passing (AMP) framework that incorporates a mono-exponential decay model is proposed. The parameters are treated as unknown variables and jointly estimated with image wavelet coefficients.

Methods

Undersampling takes place in the y-z plane of k-space according to the Poisson-disk pattern. Retrospective undersampling is performed to evaluate the performances of different reconstruction approaches, prospective undersampling is performed to demonstrate the feasibility of undersampling in practice.

Results

The proposed AMP with parameter estimation (AMP-PE) approach successfully recovers < mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:semantics > < mml:mrow > < mml:msubsup > < mml:mrow > < mml:mi > R < /mml:mi > < /mml:mrow > < mml:mrow > < mml:mn > 2 < /mml:mn > < /mml:mrow > < mml:mrow > < mml:mo > ∗ < /mml:mo > < /mml:mrow > < /mml:msubsup > < /mml:mrow > < mml:annotation > $$ {R}_2^{\ast } $$ < /mml:annotation > < /mml:semantics > < /mml:math > maps and phase images for QSM across various undersampling rates. It is more computationally efficient, and performs better than the state-of-the-art < mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:semantics > < mml:mrow > < mml:msub > < mml:mrow > < mml:mi > l < /mml:mi > < /mml:mrow > < mml:mrow > < mml:mn > 1 < /mml:mn > < /mml:mrow > < /mml:msub > < /mml:mrow > < mml:annotation > $$ {l}_1 $$ < /mml:annotation > < /mml:semantics > < /mml:math > -norm regularization (L1) approach in general, except a few cases where the L1 approach performs as well as AMP-PE.

Conclusion

AMP-PE achieves better performance by drawing information from both the sparse prior and the mono-exponential decay model. It does not require parameter tuning, and works with a clinical, prospective undersampling scheme where parameter tuning is often impossible or difficult due to the lack of ground-truth image.

---

### Estimating and testing direct genetic effects in directed acyclic graphs using estimating equations [^36f8fd9e]. Genetic Epidemiology (2018). Low credibility.

Figure 1
Overview of the directed acyclic graph considered in this study. Y is the primary outcome measure of interest; K is a secondary phenotype; X is the genetic marker of interest and α XY is the direct effect of interest. It is assumed thatso that L is a measured predictive factor of K, however, CIEE is also valid if L is a measured confounder of(i.e.and). U represents unmeasured factors and confounders potentially influencing L and Y.

Two traditional methods for the aim to estimateare (i) to include the intermediate phenotypes and factors as covariates in a multiple regression (MR) model of the primary phenotype on the genetic marker, or (ii) to first regress the primary phenotype on the intermediate phenotypes and factors, and then regress the extracted residuals on the genetic marker (regression of residuals, RR). These approaches are frequently used for the analysis of continuous primary phenotypes using a linear regression model, and MR is also a frequently used approach for the analysis of binary or categorical primary phenotypes (using generalized linear regression models), or potentially censored time‐to‐event primary phenotypes (using, for example, proportional hazards (PH) or accelerated failure time (AFT) regression models). However, both traditional approaches can lead to biased point estimates and invalid testing of direct genetic effects on the primary phenotype in some situations, by removing part of the true association or by failing to remove the effect of the intermediate phenotype (i.e. the indirect genetic effect) or unmeasured confounders (Cole & Hernán, 2002; Goetgeluk, Vansteelandt, & Goetghebeur, 2008; Rosenbaum, 1984; Vansteelandt et al. 2009). More elaborate approaches have been proposed to overcome these limitations. The structural equation modeling method (SEM; Bollen, 1989) is a popular approach for modeling DAGs, and has been applied to genetic association studies under similar DAGs as in this study (for example, see Hancock et al. 2015). Further approaches have been developed in studies on causal inference using structural nested models and G‐estimation methods (Goetgeluk et al. 2008; Robins, 1986, 1992; Robins & Greenland, 1994), or the inverse probability weighting method (Robins, Hernán, & Brumback, 2000). A more detailed overview of these approaches can be found in Vansteelandt and Joffe (2014).

---

### Predicting outcomes of patients in Japan after first acute stroke using a simple model [^761d69e7]. American Journal of Physical Medicine & Rehabilitation (2001). Low credibility.

Objective

Prediction of patient outcome can be useful as an aid to clinical decision making. Many studies, including my own, have constructed predictive multivariate models for outcome following stroke rehabilitation therapy, but these have often required several minutes work with a pocket calculator. The aim is to develop a simple, easy-to-use model that has strong predictive power.

Methods

Four hundred sixty-four consecutive patients with first stroke who were admitted to a rehabilitation hospital during a period of 19 mo were enrolled in the study. Sex, age, the stroke type, Functional Independence Measure total score on admission (X), onset to admission interval (number of days from stroke onset to rehabilitation admission), and length of rehabilitation hospital stay (number of days from hospital admission to discharge) were the independent variables. Functional Independence Measure total score at discharge (Y) was the dependent variable.

Results

Stepwise multiple regression analysis resulted in the model containing age (P < 0.0001), X (P < 0.0001), and onset to admission interval (P < 0.0001). The equation was: Y = 68.6–0.32 (age) + 0.80X - 0.13 (onset to admission interval), a multiple correlation coefficient (R) = 0.82, and a multiple correlation coefficient squared (R2) = 0.68. Simple regression analysis revealed the relation between Xand Y: Y = 0.85X + 37.36, and R = 0.80 R2 = 0.64. In fact, plots of X vs. Ywere nonlinear, but seemed to be able to be linearized by some form of equation. It was found that there is a linear relation between logX and Y. The equation is Y = 106.88x - 95.35, where x = logX, R = 0.84, and R2 = 0.70. The correlation is improved by a regression analysis of a natural logarithmic transformation of X (R = 0.84 vs. R = 0.82).

Conclusion

The results in this study confirm that the simple regression model using a logarithmic transformation of X (R = 0.84) has predictive power over the simple regression model (R = 0.80). This model is well validated and clinically useful.

---

### Mixed models for repetitive measurements: the basics [^d40754d3]. Fertility and Sterility (2024). Medium credibility.

Linear mixed models are regularly used within the field of reproductive medicine. This manuscript explains the basics of mixed models, when they could be used, and how they could be applied.

---

### LinkedSV for detection of mosaic structural variants from linked-read exome and genome sequencing data [^f423756f]. Nature Communications (2019). High credibility.

If we regard (R (F i 1), L (F i 2)) as a point in a two-dimensional plane, according to Eq. (2), for almost all (98.01%) of the fragment pairs (F i 1, F i 2), ((R (F i 1), L (F i 2)) is restricted in a G × G square region with the point (b 1, b 2) being a vertex (Supplementary Fig. 14b).

We used a graph-based method to fast group the points into clusters and find square regions where the numbers of points were more than expected. First, every possible pair of endpoints (R (F 1), L (F 2)) meeting B (F 1) = B (F 2) formed a point in the two-dimensional plane. Each point indicated a pair of fragments that share the same barcode. For example, if ten fragments share the same barcode, pairs of endpoints will be generated. A point/pair of endpoints may or may not support an SV because there are two possible reasons for observing two fragments sharing the same barcode: (1) the two fragments originated from two different HMW DNA molecules but were dispersed into the same droplet partition and received the same barcode and (2) the two fragments originated from the same HMW DNA molecule but the reads were reconstructed into two fragments due to an SV. The points are sparsely distributed in the two-dimensional plane and it is highly unlikely to observe multiple points in a specific region. Next, a k – d tree (k = 2) was constructed, of which each node stores the (X, Y) coordinates of one point. A k – d tree is a binary tree that enable fast query of nearby nodes. Therefore, we could quickly find all pairs of points within a certain distance. Any two points (x 1, y 1) and (x 2, y 2) were grouped into one cluster if | x 1 − x 2 | < G and | y 1 − y 2 | < G. For each cluster, if the number of points in the cluster was more than a user-defined threshold (default: 5), it was considered as a potential region of enriched fragment endpoints. If the points in the cluster were not within a G × G square region, we used a G × G moving square to find a square region where the points are best enriched. Theoretically, the best enriched square region should contain 98.01% (0.99 × 0.99) of the points, according to Eq. (2). The predicted breakpoints were the X and Y coordinates of the right-bottom vertex of the square. The points in the square region were subjected to a statistical test describe below.

---

### The snm procedure guideline for general imaging 6.0 [^773d53ba]. SNMMI (2010). Medium credibility.

Gated imaging — electrocardiogram (ECG) gating is used to synchronize image acquisition with the patient's heart rate, and the number of frames per R-R interval should be no less than 16 for ejection fraction measurements and 32 for time-based measurements. For gated cardiac blood pool imaging, electronic zoom generally should be used to magnify the field of view to approximately 25 cm, a matrix size of 64 x 64 is sufficient, and typically a total of at least 5 million counts in the entire study will provide sufficient statistics for quantitative and functional image processing.

---

### 2013 ACC / AHA guideline on the assessment of cardiovascular risk: a report of the American college of cardiology / American Heart Association task force on practice guidelines [^58242b99]. Journal of the American College of Cardiology (2014). Medium credibility.

Pooled Cohort Equations — equation parameters for estimating 10-year risk of hard atherosclerotic cardiovascular disease (ASCVD) are provided for each race and sex group, with an example profile of 55 years of age, total cholesterol 213 mg/dL, HDL-C 50 mg/dL, untreated systolic BP 120 mm Hg, nonsmoker, and without diabetes. For women, baseline survival is 0.9665 for White and 0.9533 for African American, and the estimated 10-y risk for the example is 2.1% and 3.0%, respectively. Representative women's coefficients include Ln Age (y) −29.799 (White) and 17.114 (African American) and Ln HDL-C (mg/dL) −13.578 and −18.920. For men, baseline survival is 0.9144 for White and 0.8954 for African American, and the estimated 10-y risks for the example are 5.3% and 6.1%. Representative men's coefficients include Ln Age (y) 12.344 (White) and 2.469 (African American) and Ln Total Cholesterol (mg/dL) 11.853 and 0.302. Hard ASCVD is defined as the first occurrence of nonfatal myocardial infarction or CHD death, or fatal or nonfatal stroke.

---

### Lipid measurements in the management of cardiovascular diseases: practical recommendations a scientific statement from the National Lipid Association writing group [^7c8d5ed1]. Journal of Clinical Lipidology (2021). High credibility.

Sampson equation considerations describe potential utility and current evidence gaps: A bivariate quadratic equation published by Sampson and colleagues calculates LDL-C from TG and non–HDL-C and yields accurate estimates for samples with a TG level up to 800 mg/dL, although it may underestimate low LDL-C levels. In samples with normal triglyceride levels, it yields similar results to Martin/Hopkins at low LDL-C, but additional data in individuals with low LDL-C are needed before it can be more generally recommended for LDL-C calculation.

---

### Lipid measurements in the management of cardiovascular diseases: practical recommendations a scientific statement from the National Lipid Association writing group [^73dfb73a]. Journal of Clinical Lipidology (2021). High credibility.

Martin/Hopkins estimation method — mechanism and scope are specified: The equation replaces the fixed divisor of five for very-low-density lipoprotein cholesterol (VLDL-C) from TG in the Friedewald method with one of 174 adaptable divisors based on a patient's individual non–HDL-C and TG values; the ratios range from 3.1 to 9.5 and the formula is intended for samples with TG < 400 mg/dL.

---

### Recommendations for a standardized pulmonary function report. An official American Thoracic Society technical statement [^d6d8fcbc]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Spirometry reporting specifies that numerical values are given for the FEV1, the FVC, and the FEV1/FVC ratio; the latter should be reported as a decimal fraction and the space for percent predicted value left blank, and if bronchodilators are given the LLN column need not be repeated with absolute and percent change given only for FEV1 and FVC. Other numerical values such as the forced inspiratory flow at 75% of FVC (FEF75%) and FEF25–75% are not recommended for routine use. Graph requirements include that for the volume–time curve the volume scale should be at least 10 mm/L, the time scale at least 20 mm/s, and 1 second prior to the start of expiration should be displayed; on the flow–volume plot the flow display should be at least 5 l/min/L/s, and the ratio of flow to volume should be 2 L/s to 1 L, and linear and log scales where values are plotted as z-scores relative to the predicted value (z = 0) give an intuitive sense of severity.

---

### Reconfigurable metamaterial processing units that solve arbitrary linear calculus equations [^ecc1706e]. Nature Communications (2024). High credibility.

Discussion

Solving calculus equations has significant applications in mechanics, electromagnetics, and other fields. Here, we present two examples of solving practical calculus equations with the proposed MPU. The first one is about earthquake-induced structural vibrations. As shown in Fig. 4a, structural vibrations occur in buildings when subjected to external forces such as earthquakes. Considering or predicting this structural vibration is an important aspect of building design. To simplify the problem, the structural vibration is modeled as a one-dimensional damped-forced vibration, as shown in Fig. 4 b, c. Based on the principle of mechanics, the trajectory of the object x (t) and the external force F (t) are subjected to the following linear differential equation:

Fig. 4
Diagrams of practical examples for solving calculus equations.

a A schematic diagram of the vibration of a building during an earthquake, where the motion trajectory of the building is affected by external forces and damping. b Simplify the motion of the building to one dimension. c It can be modeled by a forced damped vibration model, where the applied external force F(t) and the trajectory x (t) of the object satisfy the following equation: mx "(t) + μx '(t) + kx = F (t). d Assume that the external force varies as a function of time as F (t) = cos(2π t / T) + 0.3*sin(3π t / T), where T = 200/9 ns. e It is presumed to have m = 2, μ = 0.2, and k = −1.414. The blue line is the result of numerical simulation, while the orange line is the result of MPU, which has good consistency. f In the RLC series circuit, a voltage u (t) is applied over time, and the charge q (t) at the terminals of the capacitor satisfies the equation: Lq "(t) + Rq '(t) + q (t)/ C = u (t). g Assume that the voltage varies as a function of time as u (t) = cos(2π t / T) + 0.5*sin(3π t / T) + sin(6π t / T), where T = 200/9 ns. h It is presumed to have L = 0.2, R = 2, and C = 0.707. The blue line is the result of numerical simulation, while the orange line is the result of MPU, which has good consistency.

---

### Online legal driving behavior monitoring for self-driving vehicles [^52c5b579]. Nature Communications (2024). High credibility.

The trigger condition for Article 38.1 is that the ego vehicle overlaps with StopLine and the traffic light is yellow or red.

The ego vehicle should not enter the intersection under a red light. When the yellow light is on and the ego vehicle has not yet entered the intersection, it should not enter the intersection.where t in represents the moment when the ego vehicle starts to overlap with the stop line and t y represents the initial moment when the yellow light turns on.

In this study, we analyzed the right-of-way using the example of a left-turn and straight-moving conflict scenario. The corresponding trigger condition is when a left-turning vehicle intrudes into the virtual lane line of the oncoming straight-moving traffic.

Where VirtualLane_O represents the virtual lane of oncoming straight-moving traffic.

When the ego vehicle intrudes into the virtual lane line of the oncoming straight-moving traffic, it should ensure that TTIdiff exceeds. The specific expressions are as follows:where Tgt os represents the oncoming straight-moving vehicles.

Indicators for compliance threshold determination on highway

During the threshold selection process, in addition to excluding abnormal data, we calculated certain indicators to obtain the final thresholds. The removal of abnormal data is described in the Supplementary Methods. The selection of the high-speed thresholdsandis relatively straightforward. However, for determining thethreshold, an optimization algorithm is used to determine the optimal threshold line.

The longitudinal and lateral RSS distances are calculated using parameter values from previous studies. The longitudinal and lateral RSS distances can be calculated as follows:where v r and v f represent the velocities of the rear and forward vehicles, respectively, and v 1 and v 2 represent the lateral velocities of the rear and forward vehicles, respectively.

The fitness function of the threshold line optimization is defined as follows:where f represents the threshold fitting curve ofand f = a ⋅ x + b, with a, and b being undetermined coefficients. P FP represents the false-positive rate, and r FP represents the maximum allowable false-positive rate. The cost function is defined as the weighted sum of N TP and N TN. N TP represents the number of true positives, N TN represents the number of true negatives, Q is the weighting coefficients.

---