# Medical Question & Answer

**Sample ID**: 4581626f-406f-d151-d148-a8039102a471
**Dataset Index**: 188842

---

## Question

Péter, Rezső, Sándor, and Tamás together have 6 drawing triangles. In how many ways is this possible? (It is not certain that each of them brought a drawing triangle, for example, P-, R-, S-, T- could have 2,0,3,1 respectively.)

---

## Answer

> Let's see… What do we have here? The user is asking how many ways 6 identical drawing triangles can be distributed among 4 people (Péter, Rezső, Sándor, Tamás), where each person can receive zero or more triangles. Let's break this down step-by-step. First, I need to think about the combinatorial model that fits this problem. Then, I should verify the formula and compute the exact count. Next, I will consider a quick sanity check and, finally, I will confirm the answer and state it clearly with the reasoning behind it.

> Let me first confirm the model. This is a classic "stars and bars" problem: distributing k identical items into n distinct bins, allowing empty bins, which corresponds to counting non-negative integer solutions to x1 + x2 + x3 + x4 = 6, where xi is the number of triangles for each person. I should double-check that this framing is correct, and yes, it matches the example given (2, 0, 3, 1) as a valid distribution [^d608ab1f].

> Now, I need to recall the formula. The number of non-negative integer solutions to x1 +… + xn = k is C(k + n − 1, n − 1), which is also written as C(k + n − 1, k). Here, k = 6 triangles and n = 4 people, so the count should be C(6 + 4 − 1, 4 − 1) = C(9, 3). Wait, let me verify the arithmetic: 6 + 4 − 1 is indeed 9, and 4 − 1 is 3, so C(9, 3) is correct [^d608ab1f].

> Next, I should compute C(9, 3). I will calculate it step-by-step to avoid mistakes: C(9, 3) = 9! / (3! × 6!) = (9 × 8 × 7) / (3 × 2 × 1) = 504 / 6 = 84. Hold on, I should double-check that division: 504 divided by 6 is indeed 84, so the exact count is 84 distinct distributions [^d608ab1f].

> Let me consider a quick sanity check. If I mistakenly thought "each person must get at least one", I would have used C(6 − 1, 4 − 1) = C(5, 3) = 10, which is clearly too small and contradicts the example (2, 0, 3, 1) where someone gets zero. That confirms I should not subtract 1 from k in this case, reinforcing that the correct model allows zeros and uses C(k + n − 1, n − 1) [^d608ab1f].

> Final answer: There are 84 ways to distribute 6 identical drawing triangles among Péter, Rezső, Sándor, and Tamás, allowing some to receive zero triangles. I should confirm the notation one more time: this is C(9, 3) = 84, which aligns with the stars-and-bars derivation and the example distribution provided [^d608ab1f].

---

The number of ways to distribute 6 identical drawing triangles among 4 people is **84**, calculated using the stars and bars method [^d608ab1f] as C(6 + 4 − 1, 6) = C(9, 6) = 84. This counts all non-negative integer solutions to p + r + s + t = 6, where p, r, s, and t are the triangles held by Péter, Rezső, Sándor, and Tamás, respectively.

---

## Mathematical formulation

We need to find the number of non-negative integer solutions to the equation:

p + r + s + t = 6

where:

- (p) = number of triangles Péter has
- (r) = number of triangles Rezső has
- (s) = number of triangles Sándor has
- (t) = number of triangles Tamás has

This is a classic **stars and bars** problem [^d608ab1f], where we distribute 6 identical items (triangles) into 4 distinct bins (people).

---

## Stars and bars method

The stars and bars theorem states that the number of ways to distribute (n) identical items into (k) distinct bins is given by the binomial coefficient:

C(n + k − 1, k − 1)

In our case, (n = 6) (triangles) and (k = 4) (people), so the number of ways is:

C(6 + 4 − 1, 4 − 1) = C(9, 3)

---

## Calculation

Now, we calculate the binomial coefficient:

C(9, 3) = [9!/(3!(9 − 3)!)] = [9!/(3!6!)] = [362880/(6 × 720)] = [362880/4320] = 84

---

## Verification with examples

To ensure the correctness of the result, let's consider a few examples of valid distributions:

- **Péter**: 2, Rezső: 0, Sándor: 3, Tamás: 1 (sum = 6)
- **Péter**: 1, Rezső: 1, Sándor: 1, Tamás: 3 (sum = 6)
- **Péter**: 0, Rezső: 0, Sándor: 0, Tamás: 6 (sum = 6)

These examples illustrate that the distributions can include cases where one or more individuals have zero triangles, which is consistent with the problem statement.

---

## Conclusion

The number of ways to distribute 6 identical drawing triangles among Péter, Rezső, Sándor, and Tamás is **84**.

---

## References

### The backtracking survey propagation algorithm for solving random K-SAT problems [^f6ee7b94]. Nature Communications (2016). Medium credibility.

Discrete combinatorial optimization has a central role in many scientific disciplines, however, for hard problems we lack linear time algorithms that would allow us to solve very large instances. Moreover, it is still unclear what are the key features that make a discrete combinatorial optimization problem hard to solve. Here we study random K-satisfiability problems with K = 3,4, which are known to be very hard close to the SAT-UNSAT threshold, where problems stop having solutions. We show that the backtracking survey propagation algorithm, in a time practically linear in the problem size, is able to find solutions very close to the threshold, in a region unreachable by any other algorithm. All solutions found have no frozen variables, thus supporting the conjecture that only unfrozen solutions can be found in linear time, and that a problem becomes impossible to solve in linear time when all solutions contain frozen variables.

---

### Data splitting to avoid information leakage with dataSAIL [^f801c5d2]. Nature Communications (2025). High credibility.

Implementation details

We here provide details on the workflow of the heuristic implemented in the DataSAIL Python package and visualized in Fig. 2. In the first step (Fig. 2 a), the users can choose between several algorithms to compute similarities or distances for different data types or provide a custom matrix (following Table 2). All distances or similarities are scaled to [0, 1].

Table 2
Algorithms to compute similarities and distances

This table lists algorithms per type of dataset to compute similarities or distances for biochemical data available in DataSAIL.

Subsequently (Fig. 2 b), the input datasetis clustered into K clusters, where K is a constant that the user can adjust. This is done separately for the two data types in two-dimensional datasets, leading to 2 K clusters in total. To cluster similarities, DataSAIL uses spectral clustering; for distances, agglomerative clustering is used(as implemented in scikit-learn).

Using the resulting set of clusters, DataSAIL then constructs a problem instance of size K (or 2 K for two-dimensional datasets) as follows (Fig. 2 c): The clustersact as data elements, inter-cluster similarities or distancesare computed using average-, single-, or complete-linkage depending on the choice of the user (if distances are provided, the cluster distances are transformed to cluster similarities as). Similarities between entities of different types are currently not supported, i.e. DataSAIL assumesif A andare clusters of data elements of different types. Cardinalities are defined as. The remaining parameters (number of folds k, type and class assignments t and σ, desired relative fold sizes s i, error margins ϵ and δ) are inherited from the input provided by the user. This constant-size instance is then solved by feeding its ILP formulation (Section "An integer linear programming formulation of the (r, R, C)-DataSAIL problem") into CVXPY – — a Python package for convex optimization that provides a unified interface for multiple solvers such as GUROBI, MOSEK, or SCIP.

The ILP solvers return a partitionof the set of clusters. In the last step (Fig. 2 d), this cluster partition is unpacked into a partitionof the original data points by settingfor eachand each x ∈ A. Note that, by definition of, the fact thatrespects the constraints specified in Eqs. (3) and (4) implies that the same constraints are also respected by π.

---

### Clustering knowledge and dispersing abilities enhances collective problem solving in a network [^60e1f0f9]. Nature Communications (2019). High credibility.

The NK problem space has been a standard formalization used to model complex problems – (We explain many of the design and analytic decisions in the "Methods" section, but offer a conceptual overview here.) An NK space is an N -dimensional space, which are each associated with one of several potential scores. What determines the exact score for any particular point in this space depends on a fixed tuning parameter K that sets up the NK space as either more rugged (higher values of K) or smoother (lower values of K). In an NK space with a low value of K, agents with different search strategies (i.e. say some search strategy A and B) can readily find the best solution (global optimum) with ease. However, in an NK space with a high value of K, agents meander about local optima that are less profitable in terms of score than the easily achievable low- K global optimum space. So much so, agents with narrow search capabilities can get stuck in local valleys. Figure 1 offers a stylized characterization of a comparison of two NK spaces, one with a low K, and one with a high K.

Fig. 1
NK space. Hypothetical plot of two NK problem spaces with varying levels of K

Every point in an NK space is defined by an address, represented by a sequence of length N of 1s and 0s. Agents in a simulation are given this address and manipulate it in various ways iteratively and myopically (e.g. changing one of the random 1s in the address to a 0 or vice-versa, for instance) to jump to a new point in the space that might have higher score. NK spaces that are very simple allow agents to quickly jump to points with higher scores, and those that are very complex are typically very difficult for agents for find paths that lead to the global optimum. Hence, tuning K allows us to create a challenging but searchable space to explore. The scores for each NK space are normalized such that the global optimum is assigned a score of 1 and the worst possible solution is assigned a score of 0.

---

### Input-output maps are strongly biased towards simple outputs [^50fca0dd]. Nature Communications (2018). Medium credibility.

Many systems in nature can be described using discrete input-output maps. Without knowing details about a map, there may seem to be no a priori reason to expect that a randomly chosen input would be more likely to generate one output over another. Here, by extending fundamental results from algorithmic information theory, we show instead that for many real-world maps, the a priori probability P(x) that randomly sampled inputs generate a particular output x decays exponentially with the approximate Kolmogorov complexity [Formula: see text] of that output. These input-output maps are biased towards simplicity. We derive an upper bound P(x)≲[Formula: see text], which is tight for most inputs. The constants a and b, as well as many properties of P(x), can be predicted with minimal knowledge of the map. We explore this strong bias towards simple outputs in systems ranging from the folding of RNA secondary structures to systems of coupled ordinary differential equations to a stochastic financial trading model.

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^bb9b7f12]. Nature Communications (2025). High credibility.

After n refinement steps, we thus obtain a functiondefined at discrete points:which we extend to the entire interval [0, 1) by linear interpolation; see Fig. 5 d for an illustration. In practice we found that computations (specifically the risk computed by integrating) converge after about eight refinement steps.

This procedure has the important property that once a point is sampled, it does not change on further refinements:which follows from equation (67). Recall now that, as stated above, a process is self-consistent if "for small enough Δ Φ, the probability distribution at a point Φ [does] not depend on the level of refinement". Since equation (70) clearly satisfies that requirement, we see that the process obtained after infinitely many refinement steps is indeed self-consistent. We thus define the hierarchical beta (HB) process as

To complete the definition of, we need to specify how we choose the initial end pointsand. In our implementation, they are drawn from normal distributionswith Φ ∈ {0, 1}, where again c is determined via our proposed calibration procedure; this is simple and convenient, but otherwise arbitrary. We also need to explain how we choose the beta parameters α and β, which is the topic of the next subsection.

Choosing beta distribution parameters

All HB processes are monotone, continuous and self-consistent, but within this class there is still a lot of flexibility: since α and β are chosen independently for each subinterval, we can mouldinto a wide variety of statistical shapes. We use this flexibility to satisfy the two remaining desiderata: a) that realisationstrackover Φ ∈ [0, 1]; and b) that the variability ofbe proportional to. It is the goal of this subsection to give a precise mathematical meaning to those requirements.

Let x 1 ~ Beta(α, β) and x 2 = 1 − x 1. (The density function of a beta distribution is given in (24).) The mean and variance of x 1 areFor a given Φ, it may seem natural to select α and β by matchingtoandto. However both equations are tightly coupled, and we found that numerical solutions were unstable and unsatisfactory; in particular, it is not possible to make the variance large whenapproaches either 0 or 1 (otherwise the distribution of x 1 would exceed [0, 1]).

---

### Analog optical computer for AI inference and combinatorial optimization [^d9ce3cb7]. Nature (2025). Excellent credibility.

Inference and export to the AOC

Once training has completed, the weight matrix W is quantized to signed 9-bit integers usingwith the rounded and clamped matrix on the right-hand side being the quantized weight matrix W Q. Whenever we report AOC-DT results, we report results obtained with the quantized matrix.

Exporting trained models to the AOC requires several further steps. First, the model inputs x and the bias term b need to be condensed into a single vector b AOC = b + x followed by clamp to ensure the values fit into the dynamic range of the AOC device (Supplementary Information section D). Second, as the optical matrix multiplication is implemented using SLMs, elements of the weight matrix are bounded by one such that all quantization-related factors disappear. However, the original maximum element of the matrix max(W) needs to be re-injected, which we achieve via the β gain in equation (2), approximately restoring the original matrix W.

The quantized matrix is split into positive and negative parts, and each part is displayed on its respective SLM.

AOC sampling and workflow

Each classification instance (that is, MNIST or Fashion-MNIST test image) is run once on the AOC, and the fixed point is sampled at the point marked in Extended Data Fig. 3 after a short 2.5 - μs cooldown window after the switch is closed, as shown in Extended Data Fig. 5a, b. The sampling window extends over 40 samples at 6.25 MHz, corresponding to 6.4 μs. This ensures that the search of fixed points for the equilibrium models happens entirely in the analog domain. Once sampled, we digitally project the vector into the output space. For classification, the input is projected from 784 to 16 dimensions, the output is projected from 16 to 10 classes. The label is then determined by the index of the largest element in the output vector (argument-max). For regression tasks, the IP and OP layers transform a scalar to 16 dimensions and back, respectively. The MSE results in Fig. 2c were obtained by averaging over 11 repeats for each input. This means that we restart the solution process 11 times, including the sampling window, and average the resulting latent fixed-point vectors. Importantly, the solve-to-solve variability appears to be centred close to the curve produced by the AOC-DT, enabling us to average this variability out (Supplementary Fig. 6).

---

### Multiple tipping points and optimal repairing in interacting networks [^bf70cdce]. Nature Communications (2016). Medium credibility.

Geometry of the Manhattan distance minimization problem

The optimal strategies shown in different colours in Fig. 5 are derived from the geometrical reasoning shown in Fig. 9. Figure 9a shows a plot of a series of curves consisting of points at identical Manhattan distances from point S 1 (equidistant curves). They produce a 'diamond' shape, and the minimal Manhattan distance between point S 1 and the green region translates into the task of 'fitting' the diamond so that it just touches the green region and its centre is at S 1. The diamond in Fig. 9a touches the green region at two points — triple points, which are the solution to the minimization problem. Figure 9b shows the solution for point S 6 in the light blue region. Here the solution suggests a different strategy — decreasing only.

---

### Rigorous location of phase transitions in hard optimization problems [^d98ebea5]. Nature (2005). Excellent credibility.

It is widely believed that for many optimization problems, no algorithm is substantially more efficient than exhaustive search. This means that finding optimal solutions for many practical problems is completely beyond any current or projected computational capacity. To understand the origin of this extreme 'hardness', computer scientists, mathematicians and physicists have been investigating for two decades a connection between computational complexity and phase transitions in random instances of constraint satisfaction problems. Here we present a mathematically rigorous method for locating such phase transitions. Our method works by analysing the distribution of distances between pairs of solutions as constraints are added. By identifying critical behaviour in the evolution of this distribution, we can pinpoint the threshold location for a number of problems, including the two most-studied ones: random k-SAT and random graph colouring. Our results prove that the heuristic predictions of statistical physics in this context are essentially correct. Moreover, we establish that random instances of constraint satisfaction problems have solutions well beyond the reach of any analysed algorithm.

---

### A continuous-time maxSAT solver with high analog performance [^2b617fbe]. Nature Communications (2018). Medium credibility.

Methods

SAT/MaxSAT definitions

Boolean satisfiability in conjunctive normal form (CNF) is a constraint satisfaction problem formulated on N Boolean variables x i ∈ {0, 1}, i = 1, …, N and M clauses C 1, …, C M. A clause is the disjunction (OR operation) of a set of literals, a literal being either the normal (x i) or the negated (NOT) formof a variable, an example clause being: C 4 = . The task is to find an assignment for the variables such that all clauses are satisfied, or alternatively, the conjunctive formula = C 1 ∧… ∧ C M evaluates to 1 (TRUE). If all clauses contain exactly k literals, the problem is k -SAT. For k ≥ 3 this is an NP-complete decision problem, meaning that a candidate solution is easily (poly-time) checked for satisfiability, but finding a solution can be hard (exp-time). Oftentimes, when studying the performance of algorithms over sets of randomly chosen problems the constraint density α = M / N is used as a statistical guide to problem hardness. MaxSAT has the same formulation as SAT, but the task is to maximize the number of satisfied clauses. For both SAT and MaxSAT, all known algorithms require exponentially many computational steps (in N) in the worst case, to find a solution. However, guaranteeing optimality of solution for MaxSAT is as hard as finding the solution itself, and thus MaxSAT is harder than SAT (NP-hard). Max 2-SAT (i.e. k = 2) is already NP-hard.

---

### Thinking outside the curve, part I: modeling birthweight distribution [^b60d1510]. BMC Pregnancy and Childbirth (2010). Low credibility.

b. Simulation study on calibrating confidence intervals

For our second simulation study we generated 25 overlapping data sets of size 50,000 from design C in Table 3, the degree of overlap consistent with a population of 200,000. For each of various C between 2.0 and 5.0, we used Equation (7) to form confidence intervals for the mixture parameters p 1, p 2, p 3, p 4, μ 1, μ 2, μ 3, μ 4, σ 1, σ 2, σ 3, σ 4. We recorded how many of the mixture parameters were contained in their respective confidence intervals. This was repeated nine more times, and we tabulated how many of the 120 = 12 × 10 confidence intervals contained their targets. Confidence intervals were also formed using Equation (6) for comparative purposes. The above steps were repeated with overlapping data sets consistent with a population of 1,000,000 and with nonoverlapping data sets consistent with an effectively infinite population.

The results are summarized in Table 5. With an effectively infinite population, only 81.7% of the confidence intervals formed using Equation (6) contained their targets at C = 5.0. The confidence intervals formed using Equation (7) contained their targets 95.0% of the time at C = 2.5. The adjustment suggested by Equation (8) appears reasonable: φ = .05 = 50,000/1,000,000 and N rep = 25 yield C φ = 1.315 C 0, which accords with the 95.8% capture of mixture parameters at C = 3.5 ≈ 1.315 × 2.5 with a population of 1,000,000.

Table 5
Confidence Interval Coverage Probabilities in Simulation Studies

The row with " C " = 2 and "Population size" = 200,000 identifies the numbers and percentages of confidence intervals containing their targets of mixture parameters, based on 10 repetitions in each of which 25 samples of size 50000 were simulated from a 4-component normal mixture with 12 parameters; results under the heading of "Bias adjustment included" are based on Equation (7) with C = 2, results under the heading of "Bias adjustment omitted" are based on Equation (6) with C = 2, and the 25 samples of size 50000 had overlap consistent with a population size of 200,000. Other rows correspond to different choices of C and/or population sizes.

---

### An event-based architecture for solving constraint satisfaction problems [^698edc12]. Nature Communications (2015). Medium credibility.

Graph colouring problems

A k -colouring for an undirected graph G with vertices V (G) and a set of edges E (G) is a map φ: V (G)→{1, 2, …, k }. In the graph colouring problem, the goal is to find a proper k-colouring φ 0 of G where φ 0 (x)≠ φ 0 (y) for all { x, y }∈ E (G).

To solve a k -colouring problem, we map each vertex in the graph to a network node with k input ports and k output ports as shown in Fig. 5. Whenever the internal oscillator in a node/vertex generates an event, the node advertises its colour by generating an event on one of the k output ports. Events from a node/vertex are routed to all its neighbours in the graph. Each node maintains k counters that count how many of its neighbours have a particular colour. These counters are incremented when a node receives events from its neighbours. At an internal oscillator event, if the counter corresponding to the current node colour is non-zero (one of the neighbours has the same colour), the node chooses a different colour. If the internal Boolean variable, 'heuristic', is true, the node chooses the colour with the fewest conflicts (smallest neighbour count). If 'heuristic' is false, the node chooses the next colour in a fixed arbitrary ordering of colours. The node then resets the k counters, flips the 'heuristic' binary variable and generates an event to advertise its colour. A min conflict heuristic thus alternates with a heuristic-free scheme to update a conflicting node each cycle.

We assessed the performance of this algorithm on several k -colouring problems of intermediate difficulty (Table 1) taken from ref.in which a different massively parallel colouring algorithm (gravitational swarm intelligence) was assessed. As in the previous section on Boolean satisfiability, we cannot attempt state-of-the-art-sized problems since the software simulation of a large network takes an infeasibly long time. In terms of average numbers of oscillation cycles to solution, the network compares favourably with gravitational swarm intelligence.

---

### A space-time tradeoff for implementing a function with master equation dynamics [^57fa10b0]. Nature Communications (2019). High credibility.

Master equations are commonly used to model the dynamics of physical systems, including systems that implement single-valued functions like a computer's update step. However, many such functions cannot be implemented by any master equation, even approximately, which raises the question of how they can occur in the real world. Here we show how any function over some "visible" states can be implemented with master equation dynamics-if the dynamics exploits additional, "hidden" states at intermediate times. We also show that any master equation implementing a function can be decomposed into a sequence of "hidden" timesteps, demarcated by changes in what state-to-state transitions have nonzero probability. In many real-world situations there is a cost both for more hidden states and for more hidden timesteps. Accordingly, we derive a "space-time" tradeoff between the number of hidden states and the number of hidden timesteps needed to implement any given function.

---

### Practical guidance for the evaluation and management of drug hypersensitivity: specific drugs [^45c8244a]. The Journal of Allergy and Clinical Immunology: In Practice (2020). High credibility.

Valproic acid oral desensitization protocol — over 2-day intervals, escalating daily doses are scheduled across Interval (d) steps 1–2, 3–4, 5–6, 7–8, 9–10, 11–12, 13–14, 15–16, 17–18, 19–20, 21–22, 23–24, 25–26, 27–28, and 29–30 with corresponding Daily dose (mg) values of 0.05, 0.1, 0.25, 0.5, 0.75, 1, 2.5, 5, 10, 25, 50, 100, 200, 400, and 800; formulation transitions at later steps specify 1 tablet, 1 tablet twice a day, and 2 tablets twice a day, and solutions are prepared to concentrations of 0.1 mg/mL (Solution A), 5 mg/mL (Solution B), and 200 mg/mL oral solution with 200 mg tablets (Solution C/tablet details).

---

### Stability of synchronization in simplicial complexes [^14660083]. Nature Communications (2021). High credibility.

Fig. 1
Synchronization in simplicial complexes of Rössler oscillators.

Contour plots of the time averaged (over an observation time T = 500) synchronization error E (see "Methods" for definition and the vertical bars of each panel for the color code) in the plane (σ 1, σ 2) for some examples of simplicial complexes (whose sketches are reported in the top left of each panel). Simulations refer to coupled Rössler oscillators (x = (x, y, z) T and f = (− y − z, x + a y, b + z (x − c)) T) with parameters fixed in the chaotic regime (a = b = 0.2, c = 9). In a – d, while in (e). As for the other coupling function, one hasin (d) andin all other panels. The blue continuous lines are the theoretical predictions of the synchronization thresholds obtained from Eq. (3). a, b, and c are examples of class III problems, whereas panels d and e are examples of class II problems.

Far from being limited to the case of D = 2, our approach can be extended straightforwardly to simplicial complexes of any order D. Each term on the right hand side of Eq. (1) can, indeed, be manipulated following exactly the same three conceptual steps described in the "Methods". Once again, one is entitled to select the eigenvector set which diagonalizes, to introduce the new variables η = (V −1 ⊗ I m) δ x. Following the very same steps which led us to write Eq. (3), one then obtainswhere JG (d) = J 1 g (d) (x s, …, x s) + J 2 g (d) (x s, …, x s) +… + J d g (d) (x s, …, x s) and the coefficientsresult from transformingwith the matrix that diagonalizes. As a result, one has conceptually the same reduction of the problem to a single, uncoupled, nonlinear system, plus a system of N − 1 coupled linear equations, from which the maximum Lyapunov exponent Λ max = can be extracted and monitored (for each simplicial complex) in the D -dimensional hyper-space of the coupling strength parameters.

---

### The decision path not taken [^5ea0a156]. Neuron (2015). Low credibility.

Real-life decisions often involve multiple intermediate choices among competing, interdependent options. Lorteije et al. (2015) introduce a new paradigm for dissecting the neural strategies underlying such decisions.

---

### Graphs [^c6c6d5a7]. Chest (2006). Low credibility.

Two rules of good graphs are presented and explicated.

---

### A continuous-time maxSAT solver with high analog performance [^3d122302]. Nature Communications (2018). Medium credibility.

Many real-life optimization problems can be formulated in Boolean logic as MaxSAT, a class of problems where the task is finding Boolean assignments to variables satisfying the maximum number of logical constraints. Since MaxSAT is NP-hard, no algorithm is known to efficiently solve these problems. Here we present a continuous-time analog solver for MaxSAT and show that the scaling of the escape rate, an invariant of the solver's dynamics, can predict the maximum number of satisfiable constraints, often well before finding the optimal assignment. Simulating the solver, we illustrate its performance on MaxSAT competition problems, then apply it to two-color Ramsey number R(m, m) problems. Although it finds colorings without monochromatic 5-cliques of complete graphs on N ≤ 42 vertices, the best coloring for N = 43 has two monochromatic 5-cliques, supporting the conjecture that R(5, 5) = 43. This approach shows the potential of continuous-time analog dynamical systems as algorithms for discrete optimization.

---

### Response to "Frameworks are pretty on paper but often don't fit reality" [^88e96851]. Journal of Perinatology (2017). Low credibility.

This corrects the article DOI: 10.1038/jp.2016.180.

---

### The backtracking survey propagation algorithm for solving random K-SAT problems [^dcfc43b2]. Nature Communications (2016). Medium credibility.

Results

Probability of finding a SAT assignment

The standard way to study the performance of a solving algorithm is to measure the fraction of instances it can solve as a function of α. We show in Fig. 1 such a fraction for BSP run with three values of the r parameter (r = 0,0.5 and 0.9) on random 4-SAT problems of two different sizes (N = 5,000 and N = 50,000). The probability of finding a solution increases both with r and N, but an extrapolation to the large N limit of these data is unlikely to provide a reliable estimation of the algorithmic threshold α a BSP.

In each plot having α on the abscissa, the right end of the plot coincides with the best estimate of α s, in order to provide an immediate indication of how close to the SAT-UNSAT threshold the algorithm can work.

Order parameter and algorithmic threshold

In order to obtain a reliable estimate of α a BSP we look for an order parameter vanishing at α a BSP and having very little finite size effects. We identify this order parameter with the quantity Σ res / N res, where Σ res and N res are respectively the complexity (for example, log of number of clusters) and the number of unassigned variables in the residual formula. As explained in Methods, BSP assigns and re-assigns variables, thus modifying the formula, until the formula simplifies enough that the SP fixed point has only null messages: the residual formula is defined as the last formula with non-null SP fixed point messages. We have experimentally observed that the BSP algorithm (as the SID one) can simplify the formula enough to reach the trivial SP fixed point only if the complexity Σ remains strictly positive during the whole decimation process. In other words, on every run where Σ becomes very close to zero or negative, SP stops converging or a contradiction is found. This may happen either because the original problem was unsatisfiable or because the algorithm made some wrong assignments incompatible with the few available solutions. Thanks to the above observation we have that Σ res ≥ 0 and thus a null value for the mean residual complexity signals that the BSP algorithm is not able to find any solution, and thus provides a valid estimate for the algorithmic threshold α a BSP. From the statistical physics solution to random K -SAT problems we expect Σ res to vanish linearly in α.

---

### Optimal free descriptions of many-body theories [^f77fa88f]. Nature Communications (2017). Medium credibility.

Methods

Optimization

The optimization to find σ andin (3) is performed by a Monte Carlo basin-hopping strategyusing the Nelder–Mead simplex algorithm for local minimization within basins of the cost function. This global strategy was selected to counteract an observed tendency for local methods to get trapped in local minima. The initial guess for this search is found as follows. The normalization constant E 0 is the lowest entanglement energy of the input entanglement spectrum { E }. We iteratively construct an approximate set of single-particle entanglement energies starting from an empty set. First, we take the lowest remaining level in the spectrum and subtract E 0 to produce a new single-particle level k. Then we remove the many-body levels, which are closest to the new combinatorial levels generated according to (2) by the additional single-particle level. This process is repeated until the input spectrum is exhausted. We can also introduce a truncation of the entanglement spectrum cutting off high entanglement energies, making the construction of the initial guess terminate faster. The minimization of D (σ, σ f) to identify the optimal free model for the Ising Hamiltonian (5) is calculated using a local Nelder–Mead method.

Finite-size scaling

We perform the finite-size scaling according to an ansatz (4). The parameters of the collapse were estimated using the method of ref. From the scaling ansatz (4) and for a trial set of scaling parameters g c, ν and ζ, the scaled values x L = (g − g c) L 1/ ν andare calculated from each unscaled data point. From this collection of scaled data points (x L, y L) across all L, we implicitly define a so-called master curve that best represents them. This curve y (x) is defined around a point x as the linear regression calculated by taking the scaled data points immediately left and right of x for each system size L. We characterize the deviation of the scaled points (x L, y L) from the master curve y (x L) using the χ² statistic. This measure is used as the cost function for an optimization problem over the scaling parameters g c, ν, ζ and θ, which can be solved using the same techniques as the previous problems.

---

### A computational toolbox for the assembly yield of complex and heterogeneous structures [^827ee6b2]. Nature Communications (2023). High credibility.

Discussion

Many structural and functional properties of biological systems rely on heterogeneous self assembly. Increasingly, building blocks with highly tuned geometries and interactions can be made experimentally, presenting an opportunity to design complex self-assembling systems with properties of living matter –. However, for increasing numbers of individual components, the yield of the desired product is diluted by an exponential number of off-target assemblies. Even if each of the off-target assemblies forms with a lower probability than the desired product, the multiplicity of these off-target assemblies can cause a yield catastrophe in which the desired product forms with negligible yield. An efficient method for predicting assembly yield would enable practitioners to design self-assembling systems of increasing complexity while mitigating this yield catastrophe.

In this work, we have developed a combined analytical/computational approach to calculate the equilibrium assembly yields of complexes comprised of heterogeneous building blocks with arbitrary geometries. Our approach involves two novel calculations: (i) computing the partition function of each (predefined) candidate complex (see Eq. (9)) and (ii) given these partition functions, calculating equilibrium assembly yields (see Eqs. (10) and (11)). The classical statistical mechanics methodology we put forth can be realized by modern automatic differentiation techniques enabled by advances in machine learning. While previously, the efficient calculation of the entropic parts of the partition function has been intractable except for simple cases, these advances make this calculation possible even for structures with complicated building blocks or with a large number of different components.

Our theoretical framework provides significant conceptual and practical improvements over simulation-based methods, the only other general-purpose method for yield prediction. Calculations via our approach require a drastically reduced computational cost compared to simulations (i.e. seconds or minutes vs. hours, weeks, or months). Moreover, simulation-based methods can be fraught with additional difficulties such as issues in simulating finite concentrations or efficiently sampling from equilibrium distributions. Lastly, since our method involves the direct calculation of the partition function, our approach can also be used to compute other statistical properties of an equilibrium thermodynamic system (e.g. heat capacity, energy fluctuations) without complicated modifications to the calculation. Indeed, since the gradient of a solution to Eqs. (10) and (11) can be computed implicitly, our method could enable inverse design with respect to related thermodynamic equilibrium properties.

---

### The computational neurology of movement under active inference [^dd0d84f5]. Brain (2021). Medium credibility.

Figure 3
Discrete generative model for movement planning. This schematic illustrates the hierarchical discrete state-space generative model that sits above the continuous model shown in Fig. 1. This model generates the hidden causes (v) that are the (imaginary) attracting points and the target locations from the perspective of the continuous model, which effectively induce movement. The discrete (categorical) causes that generate these come in two forms: the alternative attracting points (red spheres) that act as equilibrium points, and which of the three possible target locations is currently specified. These causes are themselves generated by states at a higher level. At the highest level (upper left) we have a set of alternative combinations of trajectories. Each of these is defined in terms of which vertex of a triangle (i.e. target location) is at the start and end of that trajectory. There are three configurations not shown that represent a single vertex of the triangle being the start and end of a trajectory (i.e. a static 'trajectory'). In addition, the higher level includes a replica of the three possible target states (upper right). However, while these are considered static at the timescale of the lower level, the slower dynamics of the higher level allow this to change over time. The key distinction here is the absence of arrows between alternative target configurations at the first level. The C -vector represents the statistics of a prior belief that policies will lead to correct outcomes (i.e. hand and target location match). This ensures sequences of actions that lead to the realization of this goal are more plausible than those that do not. The arrows within a level indicate the allowed transitions (encoded by B) between these configurations. The arrows between levels show the generation of lower level variables by higher level variables. This rests upon generation of a discrete outcome via A (2), which is then used to generate policies [via E (1)] or initial states [via D (1)]. The role of D (2) is to provide a prior belief about the initial states at the higher level. Note that, if we were to extend this model to include further levels, this would also become an empirical prior, recapitulating the role of D (1). However, given that Level 2 is the highest level considered here, D (2) is simply a vector of prior probabilities. This says that the target states may be in any initial configuration with equal probability and that the initial state probability is equally distributed among any of the trajectories that start at the lower-right target.

---

### Practical guidance for the evaluation and management of drug hypersensitivity: specific drugs [^a787a643]. The Journal of Allergy and Clinical Immunology: In Practice (2020). High credibility.

Rifampin oral desensitization protocol — A stepwise oral regimen uses solutions A–E with concentrations 0.001 mg/mL, 0.01 mg/mL, 0.1 mg/mL, 6 mg/mL, and 60 mg/mL and volumes 10 mL (A–D) and 15 mL (E), with each step listing Time (min) 30; example doses include step 1 0.00002 mg (solution A, 0.2 mL), step 5 2 mg (solution D, 0.3 mL), step 9 30 mg (solution E, 0.5 mL), and step 13 250 mg (solution E, 4.2 mL) for a cumulative 610.2 mg. The protocol specifies Full therapeutic dose administered 12 h after initiation of step 13.

---

### Evolutionary dynamics of any multiplayer game on regular graphs [^b42774e4]. Nature Communications (2024). High credibility.

Multiplayer games on graphs are at the heart of theoretical descriptions of key evolutionary processes that govern vital social and natural systems. However, a comprehensive theoretical framework for solving multiplayer games with an arbitrary number of strategies on graphs is still missing. Here, we solve this by drawing an analogy with the Balls-and-Boxes problem, based on which we show that the local configuration of multiplayer games on graphs is equivalent to distributing k identical co-players among n distinct strategies. We use this to derive the replicator equation for any n-strategy multiplayer game under weak selection, which can be solved in polynomial time. As an example, we revisit the second-order free-riding problem, where costly punishment cannot truly resolve social dilemmas in a well-mixed population. Yet, in structured populations, we derive an accurate threshold for the punishment strength, beyond which punishment can either lead to the extinction of defection or transform the system into a rock-paper-scissors-like cycle. The analytical solution also qualitatively agrees with the phase diagrams that were previously obtained for non-marginal selection strengths. Our framework thus allows an exploration of any multi-strategy multiplayer game on regular graphs.

---

### Towards large-scale quantum optimization solvers with few qubits [^13944394]. Nature Communications (2025). High credibility.

Quantum computers hold the promise of more efficient combinatorial optimization solvers, which could be game-changing for a broad range of applications. However, a bottleneck for materializing such advantages is that, in order to challenge classical algorithms in practice, mainstream approaches require a number of qubits prohibitively large for near-term hardware. Here we introduce a variational solver for MaxCut problems over m ≥ O(nᵏ) binary variables using only n qubits, with tunable k > 1. The number of parameters and circuit depth display mild linear and sublinear scalings in m, respectively. Moreover, we analytically prove that the specific qubit-efficient encoding brings in a super-polynomial mitigation of barren plateaus as a built-in feature. Altogether, this leads to high quantum-solver performances. For instance, for m = 7000, numerical simulations produce solutions competitive in quality with state-of-the-art classical solvers. In turn, for m = 2000, experiments with n = 17 trapped-ion qubits feature MaxCut approximation ratios estimated to be beyond the hardness threshold 0.941. Our findings offer an interesting heuristics for quantum-inspired solvers as well as a promising route towards solving commercially-relevant problems on near-term quantum devices.

---

### Principled approach to the selection of the embedding dimension of networks [^e899ba67]. Nature Communications (2021). High credibility.

If we repeat the same analysis using the LINE algorithm, although the rate of decay of the function L (d) is not identical for the two embedding algorithms, we find analogous behavior with L (d) smoothly decaying toward an asymptotic value as d increases (Fig. 1 a, b). However, a faster decay doesn't necessarily imply that the corresponding algorithm is able to represent more efficiently the network than the other algorithm. The reference embeddings used in the definition of L (d) are in fact algorithm dependent, and the quantitative comparison between the L (d) functions of two different algorithms may be not informative.

Similar observations can be made when we perform the geometric embedding of synthetic networks. In Fig. 2, we display L (d) for networks generated according to the SB model. We still observe a saturation of the function L (d). Graph clustering reaches maximal performance at slightly smaller d values than those necessary for the saturation of L (d), and slowly decreases afterwards. The drop of performance for values of the embedding dimension that are too large has been already observed in other papers, see for example refs. Results based on LE embedding provide similar insights (Supplementary Figs. 3–6): L (d) saturates quickly as d increases; the performance in the graph clustering task is optimal before the saturation point of L (d); further, the saturation point is compatible with the presence of a significant gap in the sorted Laplacian eigenvalues.

Fig. 2
Geometric embedding of synthetic networks.

a Normalized loss as a function of the embedding dimension in SB networks. We generate graph instances composed of N = 256 nodes and C = 16 groups with p i n = 0.2 and p o u t = 0.02. Embedding is performed using node2vec. The blue curve in panel a is the best fit curve of Eq. (2) (, and R 2 = 5.93 × 10 −3) with the data points. b Test of performance in recovering the ground-truth community structure in SB graphs as a function of the dimension of the embedding. We measure the recovery accuracy with normalized mutual information (NMI). Symbols refer to average values of NMI computed over 10 instances of the SB model; the shaded region identifies the range of values corresponding to one standard deviation away from the mean.

---

### General conditions for predictivity in learning theory [^e1e7f28e]. Nature (2004). Excellent credibility.

Developing theoretical foundations for learning is a key step towards understanding intelligence. 'Learning from examples' is a paradigm in which systems (natural or artificial) learn a functional relationship from a training set of examples. Within this paradigm, a learning algorithm is a map from the space of training sets to the hypothesis space of possible functional solutions. A central question for the theory is to determine conditions under which a learning algorithm will generalize from its finite training set to novel examples. A milestone in learning theory was a characterization of conditions on the hypothesis space that ensure generalization for the natural class of empirical risk minimization (ERM) learning algorithms that are based on minimizing the error on the training set. Here we provide conditions for generalization in terms of a precise stability property of the learning process: when the training set is perturbed by deleting one example, the learned hypothesis does not change much. This stability property stipulates conditions on the learning map rather than on the hypothesis space, subsumes the classical theory for ERM algorithms, and is applicable to more general algorithms. The surprising connection between stability and predictivity has implications for the foundations of learning theory and for the design of novel algorithms, and provides insights into problems as diverse as language learning and inverse problems in physics and engineering.

---

### Biased expectations about future choice options predict sequential economic decisions [^9b0d2bac]. Communications Psychology (2024). Medium credibility.

Next, the model works backwards through the sequence, iteratively using the aforementioned formula forwhen computing each respective action value Q for taking the option and declining the option for each t. Whenever the reward value of taking the current option is considered, the reward function R assigns reward values to options based on their ranks. h represents the relative rank of the current option.

In contrast, the reward value of sampling again is simply the cost to sample C.

This customisable R function allowed us to examine how the Ideal Observer changes its sampling strategy under the different reward payoff schemes used in our studies. Pilot full, Study 1 full, Study 2 and both conditions in Study 3 all involved instructing participants to try to choose the best price possible. In study conditions using these instructions, we implemented a continuous payoff function (resembling that of the classic Gilbert & Mosteller formulation), in which the relative rank of each choice would be rewarded commensurate with the value of its associated option. In Pilot baseline and the baseline, squares, timing, and prior conditions of Study 1, we adapted the payoff scheme to match participants' instructions that they would be paid £0.12 for the best rank, £0.08 for the second-best rank, £0.04 for the third best rank and £0 for any other ranks. Lastly, in the payoff condition of Study 1, we programmed the reward payoff function to match participants' reward of 5 stars for the best rank, 3 stars for the second-best rank, one star for the third-best rank and zero stars for any other ranks.

Another feature added to our implementation of the Ideal Observer, compared to the Gilbert & Mosteller base model, is the ability to update the model's generating distribution from its experience with new samples in a Bayesian fashion, instead of this generating distribution being specified in advance and then fixed throughout the paradigm. This Bayesian version of the optimality model treats option values as samples from a Gaussian distribution with a normal-inverse- χ² prior. The prior distribution is initialised before experiencing any options with four parameters: the prior mean μ₀, the degrees of freedom of the prior mean κ, the prior variance σ²₀ and the degrees of freedom of the prior variance ν. The μ 0 and σ 2 0 parameters of this prior distribution are then updated by the model following presentation of each newly sampled option value as each sequence progresses.

---

### The eighty five percent rule for optimal learning [^70a73b15]. Nature Communications (2019). High credibility.

The predicted reward E [r] was computed according to Eq. (20). In line with Law and Gold (Supplementary Fig. 2 in ref.), the proportionality constant B was computed using logistic regression on the accuracy and absolute value of the decision variable, | h |, from last L trials, where L = min(300, t).

In addition to the weight update rule (Eq. (21)), weights were normalized after each update to keep the sum of the squared weights, a constant (= 0.02). While this normalization has only a small overall effect (see Supplementary Material in ref.), we replicate this weight normalization here for consistency with the original model.

To initialize the network, the first 50 trials of the simulation had a fixed coherence COH = 0.9. After this initialization period, the coherence was adjusted according to the difference between the target accuracy, A target, and actual accuracy in the last L trials, A L, where L = min(300, t). Specifically, the coherence on trial t was set aswhere Γ t was adjusted according toand dΓ was 0.1.

To estimate the post-training precision parameter, β, we simulated behavior of the trained network on a set of 20 logarithmically spaced coherences between 10⁻³ and 1. Behavior at each coherence was simulated 100 times and learning was disabled during this testing phase. The precision parameter, β, was estimated using logistic regression between accuracy on each trial (0 or 1) and coherence; i.e.

---

### Statistical analysis and optimality of neural systems [^bf829ef6]. Neuron (2021). Medium credibility.

Normative theories and statistical inference provide complementary approaches for the study of biological systems. A normative theory postulates that organisms have adapted to efficiently solve essential tasks and proceeds to mathematically work out testable consequences of such optimality; parameters that maximize the hypothesized organismal function can be derived ab initio, without reference to experimental data. In contrast, statistical inference focuses on the efficient utilization of data to learn model parameters, without reference to any a priori notion of biological function. Traditionally, these two approaches were developed independently and applied separately. Here, we unify them in a coherent Bayesian framework that embeds a normative theory into a family of maximum-entropy "optimization priors". This family defines a smooth interpolation between a data-rich inference regime and a data-limited prediction regime. Using three neuroscience datasets, we demonstrate that our framework allows one to address fundamental challenges relating to inference in high-dimensional, biological problems.

---

### Using an algorithmic approach to shape human decision-making through attraction to patterns [^8488e2e2]. Nature Communications (2025). High credibility.

Fig. 4
Cost of bias for participants in the context of RaCaS and all other competing algorithms.

DS dynamic schedule, SS static schedule a Left: overlaid scatter plots depicting total number of obtained rewards (Y -axis) over the strength of Bias+ (X -axis). Participants under RaCaS are depicted in yellow and those under all other competing algorithms are depicted in gray. both scatters show that the participants that are the least biased towards either option earn more (comparison of the points within the red rectangle to all other data points (RaCaS; 28 vs. 24.6 rewards, t (255) = 6.74, p < 0.001, 95% CI: 2.37–4.331. Others; 30.6 vs. 28.6, t (3257) = 7.99, p < 0.001, 95% CI: 1.48–2.44. one-sample t -tests). Right: the X -axis reflects absolute (unsigned) bias — the amount of deviation from indifference. The regression line for RaCaS (r (260) = −0.346, p < 0.001, Spearman correlation) is overlaid on the individual observations; RaCaS participants are depicted in yellow. b Two overlaid histograms showing the distribution of the sum of rewards obtained by participants allocated to RaCaS (yellow) and all other algorithms (gray). The dashed vertical brown line depicts the expected sum of rewards given random choice. c Box plots depicting the sum of rewards obtained by participants under RaCaS (yellow) and under all other algorithms (gray). Dots represent individual participants. Algorithms are ranked by the magnitude of Bias+ they produced. The dashed brown line depicts the expected number of rewards under purely random choice. d The dynamics of the proportion of rewards obtained ("exploited") from those allocated to option Bias+ (red) and Bias− (blue) under RaCaS (solid line) and the pool of all other algorithms (dashed line), across all 100 trials. To ease comparison, Bias+ is again presented in gray for both RaCaS (solid line) and as the average of all other algorithms (dashed line). The rolling average was calculated with a temporal window of size 10. Error bands reflect SEM. Box bounds show interquartile range (IQR), line represent the median, and whiskers extend to points that lie within 1.5 IQRs.

---

### Bias free multiobjective active learning for materials design and discovery [^7aec4096]. Nature Communications (2021). High credibility.

Results

The polymers in our study are representative of dispersants that are typically used in solid suspension systems to prevent the flocculation of suspended particles, e.g. as means to ensure the color strength of pigments in coatings applications. Finding the optimal polymer for a dispersant-based application is a typical example of a multiobjective search. One would like to obtain a polymer that has optimal adhesion strength to the surface of the particles that need to be suspended. Once on the surface, the polymers need to repel the other particles and finally, one needs to ensure that the viscosity of the solution ensures kinetic stability. Interestingly, some of these criteria are in competition with each other. For instance, we can imagine that certain monomer types will enhance both the binding to the surface and the attraction between the polymers. In this case, there is no unique solution and we have to trade binding with the surface with the repulsion between the polymers. This is a general observation in many multiobjective problems. We will often find natural, completely unavoidable competing goals such as the strength-ductility trade-off.

In this work, we mimic the design of our dispersant using a coarse-grained model (see Fig. 1). Our model represents a typical linear copolymer that is often used as a dispersant. In this coarse-grained model, we map monomers with different interactions with the surface and the solvent to different beads, which translates, in our case, to a design space containing more than 53 million possible sequences of polymer beads (see Supplementary Note 2). For a given hypothetical dispersant, we use molecular simulation techniques to evaluate our three ("experimental") key performance indicators. Although we carry out the synthesis and experiments in silico, the number of possible dispersants and the required computational time to evaluate the performance is too large for a brute-force screening of all 53 million dispersants of our coarse-grained polymer genome. Therefore, also for this in silico example, we are limited by our resources and we aim to obtain our set of Pareto-optimal materials as efficiently as possible.

---

### The legacy of "just the facts" [^fbe407dc]. Schizophrenia Research (2011). Low credibility.

This commentary to the capstone paper by Keshavan and colleagues reflects on the history of this project and the challenging nature of curating and establishing facts. While a number of approaches exist, none of them in-and-of-themselves are without drawbacks. A combination of techniques may provide the most satisfying results. We emphasize the importance of determining what facts are specific to the disorder - a particularly challenging endeavor - and the usefulness of various ways of quantifying these facts.

---

### A machine learning automated recommendation tool for synthetic biology [^5caa5c39]. Nature Communications (2020). High credibility.

Optimization-suggesting next steps

The optimization phase leverages the predictive model described in the previous section to find inputs that are predicted to bring us closer to our objective (i.e. maximize or minimize response, or achieve a desired response level). In mathematical terms, we are looking for a set of N r suggested inputs, that optimize the response with respect to the desired objective. Specifically, we want a process that:
i. optimizes the predicted levels of the response variable;
ii. can explore the regions of input phase space (in Eq. (1)) associated with high uncertainty in predicting response, if desired;
iii. provides a set of different recommendations, rather than only one.

We are interested in exploring regions of input phase space associated with high uncertainty, so as to obtain more data from that region and improve the model's predictive accuracy. Several recommendations are desirable because several attempts increase the chances of success, and most experiments are done in parallel for several conditions/strains.

In order to meet these three requirements, we define the optimization problem formally aswhere the surrogate function G (x) is defined as:depending on which mode ART is operating in (see the "Key capabilities" section). Here, y * is the target value for the response variable, y = y (x), and Var(y) denote the expected value and variance, respectively (see "Expected value and variance for ensemble model" in Supplementary Information), denotes Euclidean distance, and the parameter α ∈ [0, 1] represents the exploitation-exploration trade-off (see below). The constraintcharacterizes the lower and upper bounds for each input feature (e.g. protein levels cannot increase beyond a given, physical, limit). These bounds can be provided by the user (see details in the "Implementation" section in Supplementary Information); otherwise, default values are computed from the input data as described in the "Input space set" section in Supplementary Information.

---

### Number representations drive number-line estimates [^6bf63201]. Child Development (2020). Medium credibility.

The number-line task has been extensively used to study the mental representation of numbers in children. However, studies suggest that proportional reasoning provides a better account of children's performance. Ninety 4- to 6-year-olds were given a number-line task with symbolic numbers, with clustered dot arrays that resembled a perceptual scaling task, or with spread-out dot arrays that involved numerical estimation. Children performed well with clustered dot arrays, but poorly with symbolic numbers and spread-out dot arrays. Performances with symbolic numbers and spread-out dot arrays were highly correlated and were related to counting skill; neither was true for clustered dot arrays. Overall, results provide evidence for the role of mental representation of numbers in the symbolic number-line task.

---

### Optimal design of k-space trajectories using a multi-objective genetic algorithm [^9fca0db2]. Magnetic Resonance in Medicine (2004). Low credibility.

Spiral, radial, and other nonrectilinear k-space trajectories are an area of active research in MRI due largely to their typically rapid acquisition times and benign artifact patterns. Trajectory design has commonly proceeded from a description of a simple shape to an investigation of its properties, because there is no general theory for the derivation of new trajectories with specific properties. Here such a generalized methodology is described. Specifically, a multi-objective genetic algorithm (GA) is used to design trajectories with beneficial flow and off-resonance properties. The algorithm converges to a well-defined optimal set with standard spiral trajectories on the rapid but low-quality end, and a new class of trajectories on the slower but high-quality end. The new trajectories all begin with nonzero gradient amplitude at the k-space origin, and curve gently outward relative to standard spirals. Improvements predicted in simulated imaging experiments were found to correlate well with improvements in actual experimental measures of image quality. The impact of deviations from the desired k-space trajectory is described, as is the impact of using different phantoms.

---

### How do you design randomised trials for smaller populations? A framework [^b88bf2ba]. BMC Medicine (2016). Low credibility.

Methods: The framework

The first step of the framework (Fig. 1) is to work out an ideal sample size and consider whether it is feasible. If it is not, the following sections describe how to apply different elements of the framework and how to approach evaluating them.

Reviewers, funders, collaborators, patients and regulators at the beginning, and regulators, journal editors, readers and patients at the end, will all need to be persuaded that the chosen design was the best approach and any unusual features were necessary. All these parties need to be persuaded of the key problem that the condition is not common enough to deliver the trial that would traditionally be expected, and that all efforts were made to maximise the number of patients available and the information obtained from them. Trials often take many years to design and deliver. Putting in many months to get the design right from the outset should not be considered disproportionate in terms of time or effort.

An important premise is to understand what we term a societal perspective: the view from the society affected by a specific disease. The aim of RCTs should be to find a way to improve outcomes for future patients.

---

### A network property necessary for concentration robustness [^723bca59]. Nature Communications (2016). Medium credibility.

Structural deficiency and absolute concentration robustness

Our main result is based on establishing whether or not the structural deficiency changes upon removing a single component from the network. To this end, we rely on the network obtained by eliminating a given component from each complex containing the component. Removal of a component may drastically alter the network, in terms of number of nodes, linkage classes and the rank of the stoichiometric matrix. For instance, the network in Fig. 1a upon removal of component C is illustrated in Fig. 1c; it has n = 7 nodes, l = 1 linkage class, and the rank of the stoichiometric matrix is r (N) = 5; the structural deficiency is, thus, of value δ s = 7−1−5 = 1. The number of complexes is reduced in the new system since the complex B+C upon removal of C coincide with the complex B, present in the original network. This is the reason why the stitching of the reactions is changed, leading to a single linkage class. In addition, removal of a component may lead to the introduction of the so-called zero complex. This is the case upon removal of components B from the network in Fig. 1a (Fig. 1d).

The idea of removing a component from biochemical reaction network has been previously employed to make statements about the possibility of the network to exhibit multistationarity. Namely, for a given set of rate constants, it has been shown that if a reaction system obtained upon removal of a component admits multiple non-degenerated positive steady states, so does the original system. Therefore, this result may be used to identify subnetworks conferring multistationarity to the entire network. Here, we establish a connection between a structural deficiency, as a key network invariant, and ACR for a particular component. It is this connection that allows us to apply the results to large-scale networks, typically arising in the study of metabolism.

We now have the concepts required for stating our main result, proved in Supplementary Methods.

---

### Symmetry breaking in optimal transport networks [^7c40204d]. Nature Communications (2024). High credibility.

Real-world cities

In this section, we study the properties of the subway systems in Atlanta, Boston, and Toronto under the lens of our framework. We choose monocentric cities, fairly isolated from other major urban centers, with a tree-like subway structure. We identify the intersection point of the real subway lines in all three cases as the city center. We see that this point corresponds to the downtown area in the three cities.

First, we incorporate real population data in our model. We rely on a two-dimensional triangular lattice multiplex model; we use the population data and the appropriate coordinates reference systems to impose the triangular lattice structure onto the city landscape. Details on the data and modeling of the city population distribution can be found in the Methods section. We denote all quantities relative to the real physical system using the same notation as for the multiplex model, but we add a tilde on top of the corresponding symbol. For example, R indicates the radius of the lattice model, anddenotes the radius of the city. Overlaying a city on top of the triangular lattice allows us to associate a weightto each node n in the slow layer that reflects the real population density within the city. We use those weights in the objective function of Eq. (6), and then take advantage of the greedy algorithm to obtain approximate solutions to the optimization problem of Eq. (4). Similar to the previous sections, we obtain two classes of optimal fast-layer configurations for all the considered parameters. Results for the city of Toronto are displayed in Fig. 4, where we see that optimal configurations comprise k ✱ = 1 (Fig. 4 a) or k ✱ = 2 (Fig. 4 b) branches. Similar results are valid for Atlanta and Boston, where we observe optimal configurations with k * ≤ 3 branches (see Supplementary Information Figure 11). For k * > 1, we note that the branches have no identical length; this is caused by the fact that the weight associated with the various nodes of the system is not constant. A typical phase diagram for Toronto is displayed in Fig. 4 c, where we fix η = 0.5, but vary L and c. The diagram is qualitatively similar to the one of Fig. 3 e. For fixed c, k * increases as L grows; however, for fixed L, k * decreases as c grows. The values of the parameters where the transitions between the various phases emerge differ from those of Fig. 3 e; this is due to the non-homogeneous density of the population used in the model of the city. Similar results for Atlanta and Boston can be found in the Supplementary Information Figure 11.

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^aeeaae45]. Nature Communications (2025). High credibility.

Fitting models to data is an important part of the practice of science. Advances in machine learning have made it possible to fit more-and more complex-models, but have also exacerbated a problem: when multiple models fit the data equally well, which one(s) should we pick? The answer depends entirely on the modelling goal. In the scientific context, the essential goal is replicability: if a model works well to describe one experiment, it should continue to do so when that experiment is replicated tomorrow, or in another laboratory. The selection criterion must therefore be robust to the variations inherent to the replication process. In this work we develop a nonparametric method for estimating uncertainty on a model's empirical risk when replications are non-stationary, thus ensuring that a model is only rejected when another is reproducibly better. We illustrate the method with two examples: one a more classical setting, where the models are structurally distinct, and a machine learning-inspired setting, where they differ only in the value of their parameters. We show how, in this context of replicability or "epistemic uncertainty", it compares favourably to existing model selection criteria, and has more satisfactory behaviour with large experimental datasets.

---

### Spontaneous perception of numerosity in humans [^9f9e3142]. Nature Communications (2016). Medium credibility.

Experimental procedures

In the first experiment, designed to measure unbiased thresholds in the area–density space, subjects chose the odd stimulus of three dot clouds, all presented simultaneously at 12° eccentricity at the vertices of a virtual equilateral triangle for 250 ms. Two of the patches (chosen at random) had the same area and density, and comprised the standard. The other (the odd-one-out) differed in either area or density, chosen from the constant stimuli described above. Subjects performed on average 5 sessions of 170 trials each, for a total of about 850 trials per numerosity. In the second experiment, subjects were asked to make an explicit judgment: which of two stimuli had the higher area, density or number (in separate sessions). Stimuli were presented 12° left and right of screen centre simultaneously for 250 ms.

Each subject completed on average 6 sessions of at least 80 trials for a total of 480 trials for each condition yielding a total of approximately 10,000 trials.

---

### When to keep it simple-adaptive designs are not always useful [^bc2b8212]. BMC Medicine (2019). Medium credibility.

Background

Adaptive designs are a wide class of methods focused on improving the power, efficiency and participant benefit of clinical trials. They do this through allowing information gathered during the trial to be used to make changes in a statistically robust manner - the changes could include which treatment arms patients are enrolled to (e.g. dropping non-promising treatment arms), the allocation ratios, the target sample size or the enrolment criteria of the trial. Generally, we are enthusiastic about adaptive designs and advocate their use in many clinical situations. However, they are not always advantageous. In some situations, they provide little efficiency advantage or are even detrimental to the quality of information provided by the trial. In our experience, factors that reduce the efficiency of adaptive designs are routinely downplayed or ignored in methodological papers, which may lead researchers into believing they are more beneficial than they actually are.

Main Text

In this paper, we discuss situations where adaptive designs may not be as useful, including situations when the outcomes take a long time to observe, when dropping arms early may cause issues and when increased practical complexity eliminates theoretical efficiency gains.

Conclusion

Adaptive designs often provide notable efficiency benefits. However, it is important for investigators to be aware that they do not always provide an advantage. There should always be careful consideration of the potential benefits and disadvantages of an adaptive design.

---

### Linear reinforcement learning in planning, grid fields, and cognitive control [^1bda7376]. Nature Communications (2021). High credibility.

Efficient solution is possible because, substituting the penalized rewards into the Bellman equation, the optimal value function is now given by a non-recursive, linear equation, :such as can be computed by a single layer of a simple, linear neural network. Here, v * is a vector of the optimal values (now defined as maximizing cumulative reward minus control cost) for each state; r is a vector of rewards at a set of "terminal" states (i.e. various possible goals); P is a matrix containing the probability of reaching each goal state from each other, nonterminal, state; and the key matrix M, which we call the DR, measures the closeness of each nonterminal state to each other nonterminal state (in terms of expected aggregate cost to all future visits) under the default policy. This is similar to the SR (S π, Eq. (3)), except that it is for the optimal values v * (not the on-policy values v π), and v * is systematically related to optimal values as defined in the original problem (, Eq. 1), with the difference being the additional penalties for deviation from the default policy. But these exert only a soft bias in π * toward π d, which furthermore vanishes altogether in an appropriate limit (see "Methods"). Thus, while M does depend on the default policy π d, it is stable over changes in goals and independent from π * in the sense that it can usefully find optimized policies π * (solving the interdependent optimization problem) even when these are far from π d. In comparison, v π (computed from the SR: S π) is only a useful approximation to v * (and thus only helpful in finding a new π *) when the SR's learned policy π is near the target policy π *. Effectively, linear RL works by introducing a smooth approximation of the "max" in Eq. (2), since the log-average-exp (with the average here taken with respect to the default distribution, π d) of a set of values approximates the maximum. The control costs, then, simply capture the difference between the original solution and the smooth approximate one. Note that distinguishing between terminal and nonterminal states is necessary, as only for this type of finite decision problem are the optimal values linearly computable; however, this places few limits on the flexibility of the model (see "Discussion").

---

### Quantifying randomness in real networks [^2db2deda]. Nature Communications (2015). Medium credibility.

Unfortunately, to the best of our knowledge, there currently exists no 3 k -construction algorithm that can be successfully used in practice to generate large 3 k -graphs with 3 k -distributions of real networks. The 3 k -distribution is quite constraining and non-local, so that the dk -construction methods described above for d = 1, 2 cannot be readily extended to d = 3. There is yet another option — 3 k -targeting rewiring, Fig. 2. It is 2 k -preserving rewiring in which each 2 k -swap is accepted not with probability 1, but with probability equal to min(1, exp(− β Δ H)), where β is the inverse temperature of this simulated-annealing-like process, and Δ H is the change in the L 1 distance between the 3 k -distribution in the current graph and the target 3 k -distribution before and after the swap. This probability favours and, respectively, suppresses 2 k -swaps that move the graph closer or farther from the target 3 k -distribution. Unfortunately, we report that in agreement withthis 2 k -preserving 3 k -targeting process never converged for any considered real network — regardless of how long we let the rewiring code run, after the initial rapid decrease, the 3 k -distance, while continuing to slowly decrease, remained substantially large. The reason why this process never converges is that the 3 k -distribution is extremely constraining, so that the number of 3 k -graphsis infinitesimally small compared with the number of 2 k -graphs,(refs). Therefore, it is extremely difficult for the 3 k -targeting Markov chain to find a rare path to the target 3 k -distribution, and the process gets hopelessly trapped in abundant local minima in distance H.

---

### Adaptive resetting for informed search strategies and the design of non-equilibrium steady-states [^d30abff2]. Nature Communications (2025). High credibility.

A naive approach would be to simply apply a constant resetting rate within the desired shape and zero resetting rate outside (Equation (S15) in the Supplementary Information). However, using our approach to predict the resulting steady-state, we find that this naive guess leads to a very fuzzy distribution, in which the arrow is barely discernible (see Fig. 5 a). This problem cannot be solved by increasing the resetting rate. The reason is that, for every reasonably complicated NESS, there will be areas that are very hard to reach without crossing regions with a high resetting rate. In our case, the area engulfed by the arrow can only be approached from the right, which significantly lowers its occupation probability, leading to the fuzzy arrow.

Fig. 5
Non-equilibrium steady-state design.

Two-dimensional arrow-shaped non-equilibrium steady-state (NESS). a Predicted from simulations without resetting with a naive r (x, y). b Predicted from simulations without resetting with an improved r (x, y). c Obtained from brute-force simulations with the improved resetting protocol.

Previously, testing different resetting protocols, in a trial-and-error fashion, would have required running thousands of trajectories with resetting for every protocol until the desired steady-state would have been obtained. Instead, using our approach, we can design an improved adaptive resetting protocol (Equation (S16) in the Supplementary Information) that would lead to the desired shape (Fig. 5 b), using the same set of trajectories without resetting that is already available. In the improved resetting strategy, the resetting rate was increased quadratically in the distance from the center of the box, and to the right of the arrow, to prevent the accumulation of density in the box's corners and to the right of the arrow, respectively. Results from simulations with this resetting protocol are given in Fig. 5 c for comparison, showing excellent agreement with our prediction.

---

### Mechanism of potassium ion uptake by the Na (+) / K (+)-ATPase [^1ad6cff1]. Nature Communications (2015). Medium credibility.

Kinetic modelling

The probabilities of occupancies for each state of the model shown in Fig. 4d are denoted as S 1 = P−E 2, S 2 = P−E 2 ·K 1, S 3 = P−E 2 ·K 2 and S 4 = E 2 (K 2), the equations to solve are:

and

Where the voltage dependence of the dissociation constants are given by:

and

The solution for the charge Q is given by:

and the rates are given by:

---

### Why is a small sample size not enough? [^e438e4d0]. The Oncologist (2024). Medium credibility.

What happens with a small sample size and why is it not ideal?

To illustrate some points, we use simulated data representing 2 different theoretical populations (group 1 and group 2)with a normal distribution for each of the populations (N = 1000 for each). Group 1 population has an asymptotic mean = 0 and SD = 1, while the group 2 population has an asymptotic mean = 0.5 and SD = 0.5. This is the entire population and therefore represents the "truth". Now we randomly select 10 values (10 data points) from each of the normal distributions and perform a (nonparametric) Wilcoxon rank-sum test (also known as Mann-Whitney U test) to examine whether both groups come from the same population or have the same shape. We repeat this exercise multiple times (Figure 1).

Figure 1.
(A) Two random samples of n = 10 each were drawn from 2 normally distributed populations each with N = 1000. Population 1 has means 0 and SD 1, and population 2 has mean 0.5 and SD 0.5. (B-D) Images illustrate new random samples using the same methodology as in panel (A).

These 4 results do not support a firm conclusion as to whether the 2 population distributions are either statistically the same or different. Why? Because in 2 of the random samples drawn, as shown in Figure 1a, 1b, the median values differ between the 2 groups, suggesting the population distributions are significantly (P value < 0.05) different; still, in the other 2 random samples, as shown in Figure 1c, 1d, the medians are close together, which suggests the population distributions are similar (the P values are much larger than.05).

---

### Bias free multiobjective active learning for materials design and discovery [^1a00810d]. Nature Communications (2021). High credibility.

The design rules for materials are clear for applications with a single objective. For most applications, however, there are often multiple, sometimes competing objectives where there is no single best material and the design rules change to finding the set of Pareto optimal materials. In this work, we leverage an active learning algorithm that directly uses the Pareto dominance relation to compute the set of Pareto optimal materials with desirable accuracy. We apply our algorithm to de novo polymer design with a prohibitively large search space. Using molecular simulations, we compute key descriptors for dispersant applications and drastically reduce the number of materials that need to be evaluated to reconstruct the Pareto front with a desired confidence. This work showcases how simulation and machine learning techniques can be coupled to discover materials within a design space that would be intractable using conventional screening approaches.

---

### Oligodendrocyte K4.1 channels clear out congested K [^b07cf43a]. Epilepsy Currents (2019). Medium credibility.

[Box: see text].

---

### Pushing the boundary of quantum advantage in hard combinatorial optimization with probabilistic computers [^4d8361ad]. Nature Communications (2025). High credibility.

Recent demonstrations on specialized benchmarks have reignited excitement for quantum computers, yet their advantage for real-world problems remains an open question. Here, we show that probabilistic computers, co-designed with hardware to implement Monte Carlo algorithms, provide a scalable classical pathway for solving hard optimization problems. We focus on two algorithms applied to three-dimensional spin glasses: discrete-time simulated quantum annealing and adaptive parallel tempering. We benchmark these methods against a leading quantum annealer. For simulated quantum annealing, increasing replicas improves residual energy scaling, consistent with extreme value theory. Adaptive parallel tempering, supported by non-local isoenergetic cluster moves, scales more favorably and outperforms simulated quantum annealing. Field Programmable Gate Arrays or specialized chips can implement these algorithms in modern hardware, leveraging massive parallelism to accelerate them while improving energy efficiency. Our results establish a rigorous classical baseline for assessing practical quantum advantage and present probabilistic computers as a scalable platform for real-world optimization challenges.

---

### A computational toolbox for the assembly yield of complex and heterogeneous structures [^b5890348]. Nature Communications (2023). High credibility.

Robust strategies for solving these problems in the laboratory are unknown. Biology shows that robust solutions exist, but has had millions of years of evolution to find the best designs. For synthetic systems, the time and monetary costs of experimentally testing various self-assembly mixtures to determine those with highest yields can be significant.

A typical route towards addressing these challenges is to use molecular simulations. Simulations could allow rapid screening of the design space of heterogeneous assembly in particular experimental systems, including how the shape, binding characteristics and relative concentrations of the building blocks contribute to the desired emergent property. Significant advances in computational software have made it possible to simulate assemblies of components with nontrivial shapes and interactions. However, such simulations, especially for large heterogeneous structures, are often prohibitively expensive. The use of simulations for exploring heterogeneous assembly has therefore been limited.

Another approach to predict the equilibrium self-assembly yield is to calculate it analytically. While for spherical particles with isotropic interactions the partition functions for small clusters can be calculated analytically, no such analytical calculation exists for anisotropic interactions. We hypothesized that automatic differentiation could be leveraged to perform this otherwise intractable calculation –. In automatic differentiation, the execution of a computer program is accompanied by the construction of a computation graph of primitive operations whose derivatives are known and can therefore be recombined (via the chain rule) to compute the gradient of the larger program. This procedure can be applied recursively, allowing us to efficiently evaluate higher-order derivatives of nearly any computer function with machine accuracy.

The goal of this paper is to develop a combined analytical/computational approach for calculating the concentration- and temperature-dependencies of equilibrium assembly yield for heterogeneous building blocks with complex geometries. We show that this approach enables us to calculate the relevant entropic factors — vibrational, rotational and translational — and estimate the equilibrium assembly yield of the structures.

---

### Humans rationally balance detailed and temporally abstract world models [^f4c01426]. Communications Psychology (2025). Medium credibility.

Linear RL choice model

To estimate the expected value of choosing the left versus right island using linear RL, values were estimated from a transition matrix T and rewards r (both learned over the course of the experiment):whereis a vector of optimal values at non-terminal states, M is the default representation (DM) matrix from all non-terminal to all other non-terminal states, T N T is the transition matrix from all non-terminal to terminal states, r T is the set of rewards (or negative step costs) at all terminal states, and λ arbitrates between exact and policy-guided estimates. Value estimates of the boats were given by taking the logarithm of the left-hand side and multiplying each value by λ.

The DR matrix is in turn given bywhere T N N is the transition matrix from all non-terminal to all other non-terminal states and r N is the set of rewards at all non-terminal states.

r was assumed to be 0 at islands, and for boats equal to the estimated V for that boat.

T was updated after each choice using a Hebbian learning rule, which was chosen to provide equivalent updates to those of the SR's M matrix:Choice between islands was modeled as a probabilistic decision between the two state values estimated via linear RL, again using a softmax distribution but with a separate inverse temperature β i s l a n d, as well as persistence estimate β p e r s i s t e n c e I in human model fitting:

Parameter estimation

We optimized the free parameters of the learning algorithms by embedding each of them within a hierarchical model to allow parameters to vary between subjects or simulated sessions. Subject-level parameters were modeled as arising from a population-level Gaussian distribution over subjects. We estimated the model to obtain best fitting subject- and group-level parameters to minimize the negative log likelihood of the data using an expectation-maximization algorithm with a Laplace approximation to the session-level marginal likelihoods in the M-step. For hypothesis testing on population-level parameters, we computed an estimate of the information matrix over the population-level parameters, taking account of the so-called "missing information" due to optimization in the E-step, itself approximated using the Hessian of a single Newton-Raphson step.

---

### Hierarchical representations of relative numerical magnitudes in the human frontoparietal cortex [^5f98206b]. Nature Communications (2025). High credibility.

Since previous studies suggested that different mechanisms may be involved depending on the range of numerical magnitude –, our study focused on the approximate number system (ANS) range. We created the three sets of numerosity, each containing four values that were approximately equidistant on a logarithmic scale. These sets were categorized as small (8, 10, 12, or 15), medium (12, 15, 18, or 22), and large (18, 22, 26, or 32) as shown in Fig. 1b. In each session, only one set was used, with the order of sets being counterbalanced across participants. Participants were not informed about the number of numerical magnitudes in each set, the number of sets, and the order in which they would be presented.

By referencing a preceding study, other irrelevant stimulus parameters, such as item surface area (the dot size), and total field area (the size of the virtual circle) of the dot arrays, were manipulated independently from numerical magnitudes. The range of numerosities varied across sessions, but the item surface area remained constant at either 0.2²π deg or 0.237²π deg², and the total field area was fixed at either 5.92²π deg² or 7²π deg², regardless of the session. Thus, in each session, the dot arrays were defined by 4 (numerical magnitude) × 2 (item surface area) × 2 (total field area) combinations, resulting in 16 unique combinations. These combinations were presented in a random order in each run. Dot positions were also randomized across trials. To prevent the texture-density mechanism from becoming predominant, we ensure that minimum edge-to-edge distance between elements, including the fixation cross, was no less than 0.219 deg. This constraint yielded an average edge-to-edge distance of at least 0.665 deg (mean = 1.173, max = 1.955) and an average center-to-center distance of at least 1.108 deg (mean = 1.610, max = 2.373), facilitating the processing of numerosity primarily through the ANS. Furthermore, to prevent the luminance of the dot array from being a proxy to estimate numerosity, half of the dots within an array were presented in black, and the rest were presented in white: for arrays with an odd number of dots (2 N + 1), half the trials featured N white dots and N + 1 black dots, while the other half had N + 1 white dots and N black dots.

---

### Practical guidance for the evaluation and management of drug hypersensitivity: specific drugs [^aed5a183]. The Journal of Allergy and Clinical Immunology: In Practice (2020). High credibility.

Paclitaxel desensitization — four-bag/16-step protocol: For paclitaxel (135–175 mg/m²) infused every 3 wk over 3 h (example = 294 mg), an example four-bag protocol lists Solution 1 as 250 mL at 0.00118 mg/mL with 9.38 mL infused (dose 0.011 mg), Solution 2 as 250 mL at 0.0118 mg/mL with 9.38 mL infused (0.111 mg), Solution 3 as 250 mL at 0.118 mg/mL with 18.75 mL infused (2.213 mg), and Solution 4 as 250 mL at 1.167 mg/mL with 250 mL infused (291.665 mg). The step table shows, for example, step 1 using solution 1 at 2.5 mL/h for 15 min with 0.625 mL infused (0.001 mg; cumulative 0.001 mg) and the final step 16 using solution 4 at 80 mL/h for 174.4 min with 232.5 mL infused (271.262 mg; cumulative 294.0 mg). Total time (h) = 6.67.

---

### Evolutionary dynamics of any multiplayer game on regular graphs [^d608ab1f]. Nature Communications (2024). High credibility.

Results

Model overview

We consider an infinite population on a regular graph, where each individual has k neighbors. An individual can adopt one of n strategies, labeled by the numbers 1, 2, …, n. On a regular graph, the number of co-players in every multiplayer game is equivalent to the constant number k of neighbors. For a given individual, suppose that there are k 1 co-players employing strategy 1, k 2 co-players employing strategy 2, and so on, up to k n co-players employing strategy n. In this context, the co-player configuration of an individual can be represented by k = (k 1, k 2, …, k n), which satisfies the condition. As illustrated in Fig. 2 a, counting the number of possible configurations of k is analogous to the classic Balls-and-Boxes problem, distributing k identical balls (i.e. co-players) into n distinct boxes (i.e. strategies), allowing for the possibility of empty boxes (e.g. k 1 = 0). Hence, there arepossible configurations of co-player strategy configurations k.

Fig. 2
The Balls-and-Boxes problem, payoff calculation, and strategy updates.

a To determine a co-player configuration, we distribute k identical co-players (balls) into n distinct strategies (boxes), corresponding to the classic Balls-and-Boxes problem. For instance, with k = 4 and n = 3, there are 15 possible co-player configurations. b An individual accumulates payoffs from 1 + k multiplayer games, organized by itself and its k neighbors. Each multiplayer game involves k + 1 players, including the organizer and its neighbors. c Strategy updates in an individual are governed by the pairwise comparison rule. A random neighbor is selected, and the focal individual either adopts the neighbor's strategy or maintains its original strategy, based on a probability proportional to the fitness (transformed from payoffs) in the pair.

---

### Using an algorithmic approach to shape human decision-making through attraction to patterns [^a7d388f5]. Nature Communications (2025). High credibility.

RaCaS implemented the above premises in the following manner (see "Methods" for a more formal description of the algorithm). First, rewards were transiently allocated, in a repeating sequence, only to the Bias + option. With successive choices of the Bias + option, rewards were allocated to that option in predictable and incrementally increasing intervals (see Fig. 1d; green-shaded trials). This regularity continued to unfold as long as the participant continued to choose the Bias + option (premise 1, above), but if the participant explored the alternative (Bias −) option, regularity broke down and reappeared only after the participant re-committed to the Bias + option (premise 2; see Fig. 1c; yellow shading). Multiple rewards that were necessarily allocated to the Bias − option to comply with the CEC rules, were allocated exactly when persevering with the Bias + option was assumed to confirm the participants' hypotheses about the existence and nature of unfolding regularity (premise 3). Any residual rewards allocated to the Bias − option were delayed until the final trials of the experiment (see Fig. 1c; orange shading). The implementation differed slightly across participants since it was dictated by the specific history of choices and rewards, as well as the current trial within a session (for the complete description of the algorithm, including all edge cases, see the publicly available online).

---

### 2025 AHA / ACC statement on cost / value methodology in clinical practice guidelines (update from 2014 statement): a report of the American college of cardiology / American Heart Association joint committee on clinical practice guidelines [^c734ee06]. Journal of the American College of Cardiology (2025). High credibility.

ACC/AHA economic value statement templates — three formats and threshold — are specified as follows: The examples define key terms and use a cost-effectiveness threshold of $120 000 per QALY gained; Format 1 asks "What is the cost-effectiveness of the intervention at its current cost?" and may include "an ICER of $i per QALY gained (< $120 000 per QALY gained in p% of probabilistic simulations)"; Format 2 asks "What would the cost of the intervention have to be in order for the intervention to meet the cost-effectiveness threshold?" and states the strategy is cost-effective "at a threshold of $120 000 per QALY gained if the cost of the (intervention) is less than $t"; Format 3 combines both questions; ICER (incremental cost-effectiveness ratio) and QALY (quality-adjusted life year) abbreviations are defined on-page.

---

### Physics-informed learning of governing equations from scarce data [^d1208a9b]. Nature Communications (2021). High credibility.

Harnessing data to discover the underlying governing laws or equations that describe the behavior of complex physical systems can significantly advance our modeling, simulation and understanding of such systems in various science and engineering disciplines. This work introduces a novel approach called physics-informed neural network with sparse regression to discover governing partial differential equations from scarce and noisy data for nonlinear spatiotemporal systems. In particular, this discovery approach seamlessly integrates the strengths of deep neural networks for rich representation learning, physics embedding, automatic differentiation and sparse regression to approximate the solution of system variables, compute essential derivatives, as well as identify the key derivative terms and parameters that form the structure and explicit expression of the equations. The efficacy and robustness of this method are demonstrated, both numerically and experimentally, on discovering a variety of partial differential equation systems with different levels of data scarcity and noise accounting for different initial/boundary conditions. The resulting computational framework shows the potential for closed-form model discovery in practical applications where large and accurate datasets are intractable to capture.

---

### Abstract representations of events arise from mental errors in learning and memory [^9c8f0a05]. Nature Communications (2020). High credibility.

Network structure guides reactions to novel transitions

Given a model of human behavior, it is ultimately interesting to make testable predictions. Thus far, in keeping with the majority of existing research. we have focused on static transition graphs, wherein the probability A ij of transitioning from state i to state j remains constant over time. However, the statistical structures governing human life are continually shifting, and people are often forced to respond to rare or novel transitions. Here we show that, when confronted with a novel transition — or a violation of the preexisting transition network — not only are people surprised, but the magnitude of their surprise depends critically on the topology of the underlying network.

We consider a ring graph where each node is connected to its nearest and next-nearest neighbors (Fig. 6 a). We asked subjects to respond to sequences of 1500 nodes drawn as random walks on the ring graph, but with 50 violations randomly interspersed. These violations were divided into two categories: short violations of topological distance two and long violations of topological distances three and four (Fig. 6 a). Using maximum likelihood estimation (Eq. (1)) as a guide, one would naively expect people to be equally surprised by all violations — indeed, each violation has never been seen before. In contrast, our model predicts that that surprise should depend crucially on the topological distance of a violation in the underlying graph, with topologically longer violations inducing increased surprise over short violations (Fig. 6 b).

---

### Time efficient design of multi dimensional RF pulses: application of a multi shift CGLS algorithm [^c93b3fd1]. Magnetic Resonance in Medicine (2011). Low credibility.

Designing multi dimensional ratio frequency excitation pulses in the small flip angle regime commonly reduces to the solution of a least squares problem, which requires regularization to be solved numerically. Usually, regularization is carried out by the introduction of a penalty, λ, on the solution norm. In most cases, the optimal regularization parameter is not known a priori and the problem needs to be solved for several values of λ. The optimal value can be selected, typically by plotting the L-curve. In this article, a conjugate gradients-based algorithm is applied to design ratio frequency pulses in a time-efficient way without a priori knowledge of the optimal regularization parameter. The computation time is reduced considerably (by a factor 10 in a typical set up) with respect to the standard conjugate gradients for least square since just one run of the algorithm is required. Simulations are shown and the performance is compared to that of conjugate gradients for least square.

---

### The geometry of masking in neural populations [^a6b955c3]. Nature Communications (2019). High credibility.

Can the 2D model be extended to account for our results? One possible way to fix the model is to allow population activity to be embedded in dimensions higher than two. This will enable responses in the masked condition to move out of the subspace defined by the unmasked responses (Fig. 6d). To see if such an approach could work in principle, consider a toy example where responses in the unmasked condition lie on the unit circle whereas the one in the masked condition to be a the result of a transformation, where A is an orthogonal matrix (representing a rotation), α is a scaling factor, and t a translation. It is then possible to find parameters of the transformation that reproduce the ratio between the lengths of the curves, as well as the dependence of discriminability and bias on orientation (Fig. 6d, middle and right panels). A more general affine transformation can be represented in homogenous coordinates aswhere the population vector now has an extra dimension to allow for translation. We can then write the transformation of the normalized population responses as, which corresponds to a projective linear transformation(Fig. 6e).

---

### Current sample size conventions: flaws, harms, and alternatives [^2ab4a8e1]. BMC Medicine (2010). Low credibility.

Sensitivity analysis

Sample size planning involves considerable uncertainty, and a simple and familiar way of assessing uncertainty is with sensitivity analyses: examining how results change under different assumptions. I propose a framework, illustrated in Table 1, for presenting a useful array of possibilities for a study's most important products, the estimated effect and its confidence interval. This is based on varying 1) assumptions that determine the precision of estimates and 2) the observed effect size. Together with discussion of the potential value of each resulting outcome, this provides an informal assessment of the value of the information that may result. This is less systematic than the value of information methods mentioned above, but it covers a range of likely scenarios, avoids technical difficulties of the mathematically formalized methods, and focuses on particular concrete results, which allows reviewers to easily assess the claimed potential value. Because the table entries show the information that would be used in a systematic review, the potential value can be discussed in terms of how it would modify a recent review or contribute to future ones, if those are deemed to be the most important considerations.

Table 1
Sample layout of sensitivity analysis.

Shown are possible study results with a given sample size (935 per group, based on the vitamin study discussed above), for a yes or no outcome. Rows have differing assumptions concerning precision of the estimates, ranging from high precision (top row) to low precision (bottom row). For a continuous outcome, the rows would instead be for optimistic (small), expected, and pessimistic (large) standard deviations.

The entries in the table are exactly the key results that interpretation should focus on when the study is completed, so this properly aligns planning with eventual use. The middle row can be a best guess such as would be used for conventional calculations; the other rows should reflect a reasonable range of uncertainty, which will depend on what is already known about the topic being studied. For the columns, inclusion of the intermediate case is important, because this will often include the most problematic or disappointing potential results. The vitamin study paired a safe and inexpensive intervention with a severe outcome, so even results in the middle column would be regarded as encouraging; the actual completed study landed essentially in Box 7, which should have been interpreted as very encouraging even though not definitive. Boxes 8 and 9 will usually be the least useful, but as noted above (False assurance), the risk of disappointing results is always present and should not be considered a flaw in study design.

---

### Solving perfect matchings by frequency-grouped multi-photon events using a silicon chip [^559ac16a]. Nature Communications (2025). High credibility.

In addition, multi-photon processors can enhance classical stochastic algorithms for solving NP problems, such as SAT, max-clique –, and dense subgraph –. The enhancement comes from the positive correlation between perfect matchings counts and the density of the graph. Thus, we can use biased randomness from perfect matchings sampler to enhance stochastic algorithms for these problems. The SAT is the problem of finding an assignment that satisfy the given Boolean formula. A clausal formula can be converted into a graph, and one SAT solution will correspond to a clique of the graph. We set a clausal formula with four clauses. It can be reduced to a 4-Clique problem of an eight-vertex graph shown in Fig. 4 a. The four-photon distribution in 2 h is shown in Fig. 4 b, with a fidelity of 98.25%. The four-vertex subgraph with the most samples is a clique, plotted with the red line in Fig. 4 a. It means that the formula F is satisfiable, and the corresponding assignment is x 1 = True, x 2 = False, and x 3 = True. On the other hand, the densest subgraph problem is defined to find the densest k -vertex subgraph with the largest density from a given n -vertex graph(k < n). No polynomial-time approximation scheme exists for the problem. To avoid being fooled by special graph structures, stochastic algorithms that employ uniform randomness for exploration are always preferable in solving the densest subgraph of a general graph. We verify the enhancement of random search by perfect matchings samples. For example, to find the densest subgraph of a 16-vertex graph with weighted edges as shown in Fig. 4 c, we experimentally obtained the four-photon distribution in 2 h shown in Fig. 4 d, with a fidelity of 92.49%. Note that the perfect matching number of an edge-weighted graph is a more general definition, that the weight of every perfect matching contains a coefficient of the multiplication of the weights of related edges. Then, we use the samples to identify the densest four-vertex subgraph via random search. In the random search, we draw several samples, each containing four vertices, compare the density of the samples, and select the one with the maximum density. We vary the number of samples drawn from 1 to 300 and repeat 400 times for each value. Figure 4 e shows the mean density obtained with a different number of samples. The ideal curve corresponds to the loss-free case. We can see that the protocol whose samples are drawn from the perfect matching solver performs markedly better than the uniform sampling.

---

### Fair algorithms for selecting citizens' assemblies [^f71d459f]. Nature (2021). Excellent credibility.

Formulating the optimization task

To inform our approach, we first analysed algorithms that pre-dated our own. Those algorithms that we have seen in use all have the same high-level structure: they select individuals for the panel one-by-one, and in each step randomly choose whom to add next from among those who — according to a myopic heuristic — seem unlikely to produce a quota violation later. As finding a quota-compliant panel is an algorithmically hard problem (Supplementary Information section 6), it is already an achievement that these simple algorithms find any panel in most practical instances. However, owing to their focus on finding any panel at all, these algorithms do not tightly control which panel they output or, more precisely, their output distribution (the probabilities with which they output different panels). Because the output distribution of an algorithm directly determines its probability allocation, the probability allocations of existing algorithms are also uncontrolled, which leaves room for them to be highly unfair.

In contrast to these existing algorithms, which have output distributions that arise implicitly from a sequence of myopic steps, the algorithms in our framework (1) explicitly compute a maximally fair output distribution and then (2) sample from that distribution to select the final panel (Fig. 1). Crucially, the maximal fairness of the output distribution found in the first step makes our algorithms optimal. To see why, note that the behaviour of any selection algorithm on a given instance is described by some output distribution; thus, as our algorithm finds the fairest possible output distribution, it is always at least as fair as any other algorithm.

Fig. 1
Algorithm optimizing a fairness measure F.

Step (1): construct a maximally fair output distribution 𝒟 over an optimal portfolio 𝒫 of quota-compliant panels (denoted by coloured boxes), which is done by iteratively building an optimal portfolio of panels and computing the fairest distribution over that portfolio. Step (2): sample the distribution to select a final panel.

As step (2) of our selection algorithm is simply a random draw, we have reduced the problem of finding an optimal selection algorithm to the optimization problem in step (1) — finding a maximally fair distribution over panels. To fully specify our algorithm, it remains only to solve this optimization problem.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^dee6cacc]. CDC (2011). Medium credibility.

Community viral load sample size calculations — sample size to detect differences in geometric mean (GM) viral load depends on power and standard deviation (S). The table specifies "α = 0.05 and W = 0.8", with S columns 1 to 1.5; for k = 3 the minimum sample sizes by S are 54, 66, 78, 92, 106, and 122, and "1.2 is the standard deviation of national VL data", so at S = 1.2 the needed sample size is 78.

---

### Reveal, don't conceal: transforming data visualization to improve transparency [^e70712b9]. Circulation (2019). Medium credibility.

Reports highlighting the problems with the standard practice of using bar graphs to show continuous data have prompted many journals to adopt new visualization policies. These policies encourage authors to avoid bar graphs and use graphics that show the data distribution; however, they provide little guidance on how to effectively display data. We conducted a systematic review of studies published in top peripheral vascular disease journals to determine what types of figures are used, and to assess the prevalence of suboptimal data visualization practices. Among papers with data figures, 47.7% of papers used bar graphs to present continuous data. This primer provides a detailed overview of strategies for addressing this issue by (1) outlining strategies for selecting the correct type of figure depending on the study design, sample size, and the type of variable; (2) examining techniques for making effective dot plots, box plots, and violin plots; and (3) illustrating how to avoid sending mixed messages by aligning the figure structure with the study design and statistical analysis. We also present solutions to other common problems identified in the systematic review. Resources include a list of free tools and templates that authors can use to create more informative figures and an online simulator that illustrates why summary statistics are meaningful only when there are enough data to summarize. Last, we consider steps that investigators can take to improve figures in the scientific literature.

---

### Counterfactual reasoning underlies the learning of priors in decision making [^d9f154d7]. Neuron (2018). Low credibility.

The second report was the belief that the base rate of the block favored right or left. This "belief" evolved gradually during the course of the block (Figure 2 D). Note that the belief is not an estimate of the base rate, as one can be fully confident in a weak bias, but its evolution was more rapid on average when the base rate was more informative (Figure 2 D, blue curves). We will attempt to explain the evolution of this bias by developing a theory of the two-way interaction between bias and choice — that is, the effect of bias on each decision and the effect of each decision on the estimate of the base rate of the block.

Hierarchical Bayesian Model

We developed a hierarchical Bayesian model in which subjects maintain a probability distribution over the base rate, within a block and use this knowledge to influence both their choice and their confidence within a trial. As we will see, the optimal way to updateis to use a counterfactual form of confidence — the probability that a choice rendered on the evidence would be correct if the base rate were unbiased (i.e.). We develop this idea in Figure 3 and provide a mathematical derivation in the STAR Methods.

Figure 3
Bayesian Model

(A) Graphical representation of the hierarchy of causes that give rise to a sample of momentary motion evidence. The shaded areas are observed variables, the unshaded areas are unobserved variables, and the filled squares are fixed hyperparameters. The block bias is sampled from a uniform distribution over six possible values (parameterized by). B is the block's base rate, which sets the prior probability over motion direction (d) for the entire block. The motion strengthis sampled on each trial from a uniform distribution over six possible values (parameterized by), independent of the block's base rate. In the experiment, and d set the probability that a dot plotted at time t will be replotted at time t + 40 ms toward the right or left target. The duration of motion (T) was sampled from a truncated exponential distribution. In the model, it is assumed that the momentary evidence follows a Gaussian distribution with a fixed varianceand a mean that depends onand d.

---

### Reproducibility: let's get it right from the start [^692b7962]. Nature Communications (2018). Medium credibility.

"… the trio of published checklists, source data and reviewer reports together will represent powerful proof of the strength and rigor of the published work and the peer review process that helped to improve it".

To further promote transparency, we will also be asking our authors to supply for publication the source data underlying any graphs and charts, and uncropped, unprocessed scans of any blots or gels. In order to avoid delaying the first round of peer review, we will initially only request that source data be supplied with the revised versions of the manuscript, unless we feel that it would be particularly important for the reviewers to have access to these data at an earlier stage. However, we strongly encourage authors to lead the way in promoting transparency by making sure that their work fulfils or exceeds our criteria right from the start. Here are four simple steps you can take to give your paper the best possible chance of flying through peer review:
Define your n numbers: just stating ' n = 5' is no longer sufficient — it must be clear what your n number refers to for each experiment. For example, did you do the experiment on five plants, on five tissue samples from the same plant, or did you run the same sample for analysis five times over? You should be able to justify your choice of sample size and statistical tests in the reporting summary.
Show us the spread: bar charts with error bars are commonly used to depict numerical data, but they can be misleading when sample sizes are small. Wherever possible, use graph types that include individual data points to directly show the spread of the data.
Make it available: before submitting your paper, take a close look at ouron the availability of materials, data and code, and craft clear availability statements in your paper that explain to reviewers and future readers how they can access everything they might need to reproduce your findings. For some types of data, deposition in an approved publicly accessible database is mandatory for publication, but don't stop there: include source data with your manuscript, and make use of repositories such asto make other data available to your reviewers and readers. For datasets with high re-use value, consider also submitting a data descriptor to our sister journal, and making the dataset publicly available through one of their.
Let us be your guide: the reporting summary and solar cells and lasing reporting summaries detail a lot of important information that can be used not just to help you describe your experimental design, figures of merit, the materials used and the analysis employed, but also to plan the experiment in the first place. Taking care to consider these requirements before you conduct your experiments will improve your chances of successful publication in any journal.

---

### Counterfactual reasoning underlies the learning of priors in decision making [^01318ebe]. Neuron (2018). Low credibility.

Figure 1
Motion Discrimination Task

Each block of 15 to 42 trials was assigned one of six possible base rates: the prior probability that the motion is rightward. In all blocks, the subject discriminated the direction of random dot motion. No feedback about individual decisions or the base rate was provided until the end of the block.

(A) Sequence of events within a trial. After fixation and a random delay, random dot motion was displayed for 0.1–0.9 s (truncated exponential). Subjects then positioned a cursor on the left or right arcs to indicate both choice (left versus right) and confidence (100% certainty [top] to guessing [bottom]). After the motion decision, the subjects reported whether they believed that the block had a rightward or leftward bias (placing the cursor in the left or right half of the line), together with the confidence in this belief (from unsure at the center to certainty at the extremes).

---

### Learning properties of quantum States without the IID assumption [^d59de50a]. Nature Communications (2024). High credibility.

We develop a framework for learning properties of quantum states beyond the assumption of independent and identically distributed (i.i.d.) input states. We prove that, given any learning problem (under reasonable assumptions), an algorithm designed for i.i.d. input states can be adapted to handle input states of any nature, albeit at the expense of a polynomial increase in training data size (aka sample complexity). Importantly, this polynomial increase in sample complexity can be substantially improved to polylogarithmic if the learning algorithm in question only requires non-adaptive, single-copy measurements. Among other applications, this allows us to generalize the classical shadow framework to the non-i.i.d. setting while only incurring a comparatively small loss in sample efficiency. We leverage permutation invariance and randomized single-copy measurements to derive a new quantum de Finetti theorem that mainly addresses measurement outcome statistics and, in turn, scales much more favorably in Hilbert space dimension.

---

### Counterfactual reasoning underlies the learning of priors in decision making [^c3049448]. Neuron (2018). Low credibility.

In models of bounded evidence accumulation, the prior is often incorporated as a static shift in the starting point of the accumulation (e.g.). In our task, however, this would be suboptimal, because as time elapses, the same level of accumulated evidence is a less reliable marker of decision accuracy, and thus the bias should be dynamic. Indeed, Hanks et al. showed that monkeys and humans are sensitive to the time dependence of the influence of the prior when making decisions similar to ours. In their implementation, a dynamic bias signal gradually displaces the DV toward a decision bound (the one representing the most likely choice) and away from the other. This approximation was motivated by observations in the neural recordings and reaction times. Here we pursued a normative approach and thus replaced this approximation with a representation of likelihood and its mapping, induced by the prior, to the dynamic posterior probability that motion is to the right or left (e.g. Figure 3 B).

---

### A simplified formula for T1 contrast optimization for short-TR steady-state incoherent (spoiled) gradient echo sequences [^c91a4f57]. Magnetic Resonance Imaging (2007). Low credibility.

There are certain instances in practical magnetic resonance imaging where T1 changes by a small amount relative to a neighboring pixel or between scans for a given pixel. The source of these small changes in T1 can be caused either by changes in tissue water content or by the uptake of a contrast agent. For short repetition time (TR) spoiled gradient echo imaging, we show that a robust and a simple, easy to use back-of-the-envelope expression for the flip angle that optimizes contrast under these conditions is given by √3 θₑ in radians or (180/π) √(6TR/T₁) in degrees. We show that for a TR/T1 ratio of up to 0.3 and for a T1 change of up to ± 50%, this approximation to the optimal flip angle produces a contrast to within 6% of the theoretical maximum value and that these predictions are in good agreement with experiment.

---

### Event generation and statistical sampling for physics with deep generative models and a density information buffer [^47552c65]. Nature Communications (2021). High credibility.

Simulating nature and in particular processes in particle physics require expensive computations and sometimes would take much longer than scientists can afford. Here, we explore ways to a solution for this problem by investigating recent advances in generative modeling and present a study for the generation of events from a physical process with deep generative models. The simulation of physical processes requires not only the production of physical events, but to also ensure that these events occur with the correct frequencies. We investigate the feasibility of learning the event generation and the frequency of occurrence with several generative machine learning models to produce events like Monte Carlo generators. We study three processes: a simple two-body decay, the processes e + e - →Z→l + l - and [Formula: see text] including the decay of the top quarks and a simulation of the detector response. By buffering density information of encoded Monte Carlo events given the encoder of a Variational Autoencoder we are able to construct a prior for the sampling of new events from the decoder that yields distributions that are in very good agreement with real Monte Carlo events and are generated several orders of magnitude faster. Applications of this work include generic density estimation and sampling, targeted event generation via a principal component analysis of encoded ground truth data, anomaly detection and more efficient importance sampling, e.g., for the phase space integration of matrix elements in quantum field theories.

---

### Setting the record straight [^d8743343]. Nature Neuroscience (2007). Medium credibility.

The discovery of serious errors in two recent papers in the journal leads to lessons for authors, referees and editors.

---

### Three learning stages and accuracy-efficiency tradeoff of restricted boltzmann machines [^87550a89]. Nature Communications (2022). High credibility.

In practical applications, one does not have access to p (x), but only to a collection of samples(training and/or test data). The empirical counterpart of Δ θ for such a dataset S issee also below Eq. (4). The critical part is again the partition function Z θ. Due to the aforementioned factorization (cf. Eq. (12)), evaluating (13) remains feasible as long as the number of hidden units N is sufficiently small, even if M is large. Similarly, for small N, we can draw independent samples from, without reverting to Markov chains and Gibbs sampling: We first generate independent samplesof the hidden units, using the fact thatremains accessible for small N. Subsequently, we sample configurations of the visible units using. This scheme was utilized to obtain the model test samplesfor the N ≤ 32 examples in Fig. 4. For the examples with N > 32, the samples inwere instead generated via Gibbs sampling according to (5), using 10 parallel chains and storing every τ θ -th sample after 2 × 10⁶ burn-in steps.

The accuracy measuresandinvolve empirical distributions of coarse-grained visible-unit samples. These reduced samples are obtained by using a weighted majority rule: For a partition { L 1, …, L L } of the visible-unit indices {1, …, M } and a threshold r ∈ [0, 1], we defineFor every samplein a given multiset S, the associated coarse-grained sample iswith.

---

### 3D particle averaging and detection of macromolecular symmetry in localization microscopy [^e1e3ea8d]. Nature Communications (2021). High credibility.

Symmetry detection

Registration errors are not the only reason that might lead to a deviation of S ij from identity. In fact, trace(S ij) can no longer be assumed to be close to 3 in case of a rotational symmetry. Any foundthat adheres to the symmetry constraint is a valid solution to the registration problem and therefore the S ij are expected to be close to any of the transformationsin the symmetry group (labeled with index k). The numerically found solution S k appears to be random, which enables us to experimentally assess the symmetry. If, for example, the symmetry group contains an n -fold rotational symmetry, then thecould be an n -fold rotation matrix. The trace of the rotation matrices can be expressed as:where ψ ij are the rotation angles. If we plot the histogram of the found trace(S ij), we expect peaks at values 1 + 2cos(2 πk / n), for k = 0, 1, …, n −1. It should be mentioned that these peaks are typically spread due to the aforementioned registration error. For example, for a 2-fold rotational symmetry we have peaks at {3,−1}, for a 3-fold rotational symmetry at {3,0} and for an 8-fold rotational symmetry at.

Further, we can also compute the axes of rotation from S ij using the following formula:in whichare the elements of the 3 × 3 S ij matrix. This will provide us with N (N −1)/2 estimated rotation axes. To infer the symmetry axis(es) of the particle, each estimated axis is normalized to have unit length and then is projected as a point on the unit sphere. The maxima in the density plot of these points reveals the symmetry axes (Fig. 3c, f).

Finally, it turned out that iterating the loop of Lie-algebraic averaging based on the most consistent S ij (those for which trace(S ij) ≈ 3) and the re-computation of the relative transformations can better reveal an indication of symmetry in the trace histogram. This can be considered as an extension of the outlier removal step.

---

### Sample sizes based on three popular indices of risks [^4a61a58f]. General Psychiatry (2018). Low credibility.

Sample size justification is a very crucial part in the design of clinical trials. In this paper, the authors derive a new formula to calculate the sample size for a binary outcome given one of the three popular indices of risk difference. The sample size based on the absolute difference is the fundamental one, which can be easily used to derive sample size given the risk ratio or OR.

---

### Data-driven control of complex networks [^b8f93167]. Nature Communications (2021). High credibility.

Fig. 1
The effect of model uncertainty in the computation of optimal network controls.

Panel (a) shows a schematic of a classic network identification procedure. The reconstructed network is affected by estimation errors δ i j. The symbol { a i j } denote the correct network weights, whereasthe (incorrectly) reconstructed ones. Panel (b) illustrates the error in the final (output) state induced by an optimal control design based on the reconstructed network. The symbol y f denote the desired final state of (a subset of) the network nodes, whereasthe one reached by the optimal input u ⋆ (t). In panel (c), we consider minimum-energy controls designed from exact and incorrectly reconstructed linear dynamical networks, and compute the resulting error in the final state as the network size n varies. We consider connected Erdös–Rényi networks with edge probability, ten randomly selected control nodes, control horizon T = 2 n, and a randomly chosen final state. We assume y f = x f, i.e. the nodes that we want to control coincide with all network nodes. To mimic errors in the network reconstruction process, we add to each edge of the network a disturbance modeled as an i.i.d. random variable uniformly distributed in, δ > 0. Each curve represents the average of the (norm of the) error in the final state over 100 independent realizations of x f. To compute minimum-energy control inputs, we use the classic Gramian-based formula and standard LAPACK linear-algebra routines (see "Methods"). Notice that there is a nonzero error in the final state which grows with the size of the network even in the absence of uncertainty (δ = 0). This is due to numerical errors in the computation of the minimum-energy control which are a consequence of the ill-conditioning of the Gramian.

---

### A computational toolbox for the assembly yield of complex and heterogeneous structures [^0deafad0]. Nature Communications (2023). High credibility.

The self-assembly of complex structures from a set of non-identical building blocks is a hallmark of soft matter and biological systems, including protein complexes, colloidal clusters, and DNA-based assemblies. Predicting the dependence of the equilibrium assembly yield on the concentrations and interaction energies of building blocks is highly challenging, owing to the difficulty of computing the entropic contributions to the free energy of the many structures that compete with the ground state configuration. While these calculations yield well known results for spherically symmetric building blocks, they do not hold when the building blocks have internal rotational degrees of freedom. Here we present an approach for solving this problem that works with arbitrary building blocks, including proteins with known structure and complex colloidal building blocks. Our algorithm combines classical statistical mechanics with recently developed computational tools for automatic differentiation. Automatic differentiation allows efficient evaluation of equilibrium averages over configurations that would otherwise be intractable. We demonstrate the validity of our framework by comparison to molecular dynamics simulations of simple examples, and apply it to calculate the yield curves for known protein complexes and for the assembly of colloidal shells.

---

### The XZZX surface code [^8f6ac9c8]. Nature Communications (2021). High credibility.

This structured noise model thus leads to two distinct regimes, depending on which failure process is dominant. In the first regime where, we expect that the logical failure rate will decay like. We find this behaviour with systems of a finite size and at high bias where error rates are near to threshold. We evaluate logical failure rates using numerical simulations to demonstrate the behavior that characterises this regime; see Fig. 6 (a). Our data show good agreement with the scaling ansatz. In contrast, our data are not well described by a scaling.

Fig. 6
Sub-threshold scaling of the logical failure rate with the XZZX code.

a Logical failure rateat high bias near to threshold plotted as a function of code distance d. We use a lattice with coprime dimensions d × (d + 1) for d ∈ {7, 9, 11, 13, 15} at bias η = 300, assuming ideal measurements. The data were collected usingiterations of Monte-Carlo (MC) samples for each physical rate sampled and for each lattice dimension used. The physical error rates used are, from the bottom to the top curves in the main plot, p = 0.19, 0.20, 0.21, 0.22 and 0.23. Error bars represent one standard deviation for the Monte-Carlo simulations. The solid lines are a fit of the data to, consistent with Eq. (2), and the dashed lines a fit to, consistent with Eq. (3) where we would expect, see Methods. The data fit the former very well; for the latter, the gradients of the best fit dashed lines, as shown on the inset plot as a function of, give a linear slope of 0.61(3). Because this slope exceeds the value of 0.5, we conclude that the sub-threshold scaling is not consistent with. b Logical failure ratesat modest bias far below threshold plotted as a function of the physical error rate p. The data (markers) were collected at bias η = 3 and coprime d × (d + 1) code dimensions of d ∈ {5, 7, 9, 11, 13, 15} assuming ideal measurements. Data is collected using the Metropolis algorithm and splitting method presented in refs. The solid lines represent the prediction of Eq. (3). The data show very good agreement with the single parameter fitting for all system sizes as p tends to zero.

---

### Infants show ratio-dependent number discrimination regardless of set size [^563942da]. Infancy (2013). Low credibility.

Evidence for approximate number system (ANS) representations in infancy is robust but has typically only been found when infants are presented with arrays of four or more elements. In addition, several studies have found that infants fail to discriminate between small numbers when continuous variables such as surface area and contour length are controlled. These findings suggest that under some circumstances, infants fail to recruit either the ANS or object file representations for small sets. Here, we used a numerical change detection paradigm to assess 6-month-old infants' ability to represent small values. In Experiment 1, infants were tested with 1 versus 3, 1 versus 2, and 2 versus 3 dots. Infants successfully discriminated 1 versus 3 and 1 versus 2, but failed with 2 versus 3. In Experiment 2, we tested whether infants could compare small and large values with a 2 versus 4 condition. Across both experiments, infants' performance exhibited ratio dependence, the hallmark of the ANS. Our results indicate that infants can attend to the purely numerical attributes of small sets and that the numerical change detection paradigm accesses ANS representations in infancy regardless of set size.

---

### Statistical ensembles without typicality [^c8e48c72]. Nature Communications (2018). Medium credibility.

Maximum-entropy ensembles are key primitives in statistical mechanics. Several approaches have been developed in order to justify the use of these ensembles in statistical descriptions. However, there is still no full consensus on the precise reasoning justifying the use of such ensembles. In this work, we provide an approach to derive maximum-entropy ensembles, taking a strictly operational perspective. We investigate the set of possible transitions that a system can undergo together with an environment, when one only has partial information about the system and its environment. The set of these transitions encodes thermodynamic laws and limitations on thermodynamic tasks as particular cases. Our main result is that the possible transitions are exactly those that are possible if both system and environment are assigned the maximum-entropy state compatible with the partial information. This justifies the overwhelming success of such ensembles and provides a derivation independent of typicality or information-theoretic measures.

---

### 2019 ASCCP risk-based management consensus guidelines: methods for risk estimation, recommended management, and validation [^c44c6ea6]. Journal of Lower Genital Tract Disease (2020). High credibility.

Risk-based management uncertainty — confidence interval and decision confidence score: Confidence intervals for each risk estimate were calculated using a normal approximation or exact methods based on the binomial distribution, and a "decision confidence score" is defined that combines the uncertainty in the statistical precision and how close the risk estimates fall to the clinical action thresholds; mathematical details on estimating the recommendation confidence scores are presented in Appendix Part D (http://links.lww.com/LGT/A160).

---

### What do you do with the "pink sheets? " [^02a5add0]. Hematology: American Society of Hematology. Education Program (2008). Low credibility.

Applying for a grant seems like an enormous challenge, sometimes akin to Sisyphus' task of rolling the boulder up the hill. In many ways, obtaining funding can be challenging, yet year after year, new investigators do obtain funding, even though there are times when the funding percentage can be low. This review offers some suggestions of what to do after the first or second round of critiques are back. There are some relatively straightforward steps one can take to improve one's chances of success. The joy of funding is worth it!

---

### Finite temperature quantum annealing solving exponentially small gap problem with non-monotonic success probability [^57a8b86b]. Nature Communications (2018). Medium credibility.

Fig. 2
Empirical success probability vs. k * for the ASC problem on the DW2X processor. k * denotes the number of single-fermion energies that fall below the thermal energy gap at the point of the minimum gap s *. The legend entries indicate the chain parameters: (W 1, W 2, N). The error bars everywhere indicate 95% confidence intervals calculated using a bootstrap over different gauges and embeddings. a – c Contrary to closed-system theory expectations, the success probability P G is nonmonotonic in the sector size n, first decreasing and then increasing, exponentially. Inset (a): the minimum gap (in GHz) of the chains as a function of the sector size n ∈{1, …,20}. The solid black line denotes the operating temperature energy scale of the DW2X. d – f For all chains we studied the ground state success probability has a minimum at the sector size n *, where the peak in the number of single-fermion states k * occurs [compare with (a – c)]. The rise and fall pattern, as well as the location of n *, are in agreement with the behavior of P G within the error bars. Inset (d): the total number of energy eigenstates that fall below the thermal energy gap as a function of the sector size n. In this case the peak position does not agree with the ground state success probability minimum

---

### Using an algorithmic approach to shape human decision-making through attraction to patterns [^676b2660]. Nature Communications (2025). High credibility.

Evidence suggests that people are attracted to patterns and regularity. We hypothesized that decision-makers, intending to maximize profit, may be lured by the existence of regularity, even when it does not confer any additional value. An algorithm based on this premise outperformed all other contenders in an international challenge to bias individuals' preferences. To create the bias, the algorithm allocates rewards in an evolving, yet easily trackable, pattern to one option but not the other. This leads decision-makers to prefer the regular option over the other 2:1, even though this preference proves to be relatively disadvantageous. The results support the idea that humans assign value to regularity and more generally, for the utility of qualitative approaches to human decision-making. They also suggest that models of decision making that are based solely on reward learning may be incomplete.

---

### Abstract representations emerge naturally in neural networks trained to perform multiple tasks [^59ff16db]. Nature Communications (2023). High credibility.

Contextual task partitions

We chose this manipulation to match the contextual nature of natural behavior. As motivation, we only get information about how something tastes for the subset of stimuli that we can eat. Here, we formalize this kind of distinction by choosing P classification tasks that each only provide information during training in half of the latent variable space.

We can write each element i of the contextual task vector as follows, where nan values are ignored during training and C is a P × D random matrix. Thus, each of the classification tasks influences training only within half of the latent variable space. This further reduces the average information provided about x by each individual partition.

Partial information task partitions

For contextual task partitions, the contextual information acts on particular tasks. For our partial information manipulation, we take a similar structure, but it instead acts on specific training examples. The intuitive motivation for this manipulation is to mirror another form of contextual behavior: At a given moment (i.e. sampled training example) an animal is only performing a subset of all possible tasks P. Thus, for a training example from that moment, only a subset of tasks should provide information for training.

Mathematically, we can write this partial information structure as follows. For each training example x, the task vector is given by, where p is a uniformly distributed random variable on, which is sampled uniquely for each training example x and M is a parameter also onthat sets the fraction of missing information. That is, M = 0.9 means that, for each training example, 90% of tasks will not provide information.

While results are qualitatively similar for many values of M, in the main text we use a stricter version of this formalization: For each training sample, one task is randomly selected to provide information, and the targets for all other tasks are set to nan.

Grid classification tasks

The grid tasks explicitly break the latent variable structure. Each dimension is broken into n parts with roughly equal probability of occurring (see schematic in Fig. 4 a). Thus, there are n D unique grid compartments, each of which is a D -dimensional volume in latent variable space, and each compartment has a roughly equal probability of being sampled. Then, to define classification tasks on this space, we randomly assign each compartment to one of the two categories – there is no enforced spatial dependence.

---

### Finding defects in glasses through machine learning [^a51d17fc]. Nature Communications (2023). High credibility.

Iterative training procedure

We finally introduce an approach to optimally employ our ML model to process new data: the iterative training procedure. To produce the results reported in Fig. 3 we trained the model once using a subset of the already available data. This is a natural way to proceed when the goal is to process new data that are very similar to the training set, and the training set is itself large enough. However, since the goal of the proposed ML model is to ultimately drive the landscape exploration and collect new samples, the single-training approach may encounter two types of problems. First, at the beginning there may be not enough data and, second, the findings of the model do not provide any additional feedback.

To solve both problems we introduce the iterative training procedure. The idea of iterative training is to use the predictive power of ML to create and expand its own training set, consequently enhancing its performance by iteratively retraining over the new data. Compared to standard active learning methods, iterative training does not focus on new samples with the highest model uncertainty, but instead it iterates the predictions on the samples below the threshold of interest. Details on the method and parameters are discussed in Supplementary Note 3. In practice, we start from a training set of K₀ ~ 10³−10⁴ randomly selected pairs to have an initial idea of the relation between input and output. We then use the ML approach outlined in Fig. 2 to predict the K i = 500 pairs with the lowest QS. For these TLS candidates, we perform the full procedure to calculate the true QS and determine whether the pair is a DW or a TLS. In the Supplementary Fig. 6, we report the result of this procedure when we process a new set of trajectories from the same polydisperse soft sphere potential as in ref. In general the first few iterations of iterative training have a poor performance. In fact, we find that > 70% of the first K i pairs are actually non-DW. After collecting additional K i measurements, we retrain the model. We report in Tab. 2 the average time for each step of the ML procedure. The retraining can be done in ~10 min, after which the model is able to predict the next K i pairs with lowest QS. Overall, to process N ISpairs we estimate that the computational time of the iterative approach is. If N ISpairs > 10⁹ it is possible to significantly reduce t i by permanently discarding the worst pairs, but this is not needed here. We iterate this procedure N i t e r times, until the last batch of K i candidates contains less than 1% of the total number of TLS. We believe that continuing this iterative procedure would lead to the identification of even more TLS/DW, but this is out of the scope of this paper.

---

### A nanofluidic knot factory based on compression of single DNA in nanochannels [^3c9fb5c8]. Nature Communications (2018). Medium credibility.

(see Supplementary Note 7 for detailed derivation of equation 6 and explicit form of β and f 1 (L k, R b,0)). If f wuk is large enough to ensure the free energy at the barrier f 1 (L k, R b,0) < 0, knots of size L k will exist with a spatial distributionleading to an exponential accumulation near the barrier, consistent with the observed single knot distribution (Figs. 6a, c and 7d, Supplementary Note 8).

The barrier (x = 0), where the free energy is minimized, is the most probable location for a single knot to form, but knots can form at all x. Let the partition function z 1 (L k, R b) count the number of ways a single knot of size L k can form on a profile characterized by R b. The number of statistically independent sites at which a knot can form is estimated by n max = r /2 g k, each site i weighted by a Boltzmann factor exp(− f 1 (L k, R b, i 2 g k / r o)), leading to a partition function that can be summed geometrically (see Supplementary Note 9). Similarly, the partition function of a state containing multiple (m) prime knots z m (L k1 ⋯ L k m, R b) is calculated such that the linear ordering of the knots along the profile is preserved. At high compression, these partition functions contain only one state, a ground state configuration consisting of knots stacked single file, with no gaps, directly abutting the barrier (see Fig. 7c, g for one-knot and two-knot ground states, respectively). In this high compression limit Poisson statistics does not hold as only one state is accessible and strong interactions imply knots do not form independently. For low compression, knots can be excited away from the barrier. Interactions are weak as the knots are well separated and many states are accessible, leading to an emergence of Poisson statistics (see Supplementary Note 10).

---

### Quantifying entanglement in a 68-billion-dimensional quantum state space [^b339b52f]. Nature Communications (2019). High credibility.

Adaptive sampling algorithm

For each configuration, experimental data are stored in nodes in a quad-tree decomposition of P whose levels describe increasingly fine detail. The i th node corresponds to a square area ofat locationwith span. Nodes are sampled by placing the corresponding, one-dimensional local patterns on the DMDs and generating a coincidence histogram during acquisition time T a = 0.5 s. Coincidences C i are counted within a 1-ns coincidence window centered on the coincidence peak; accidental coincidences A i are counted in a 1-ns window displaced 2 ns from the coincidence window. Coincidence and accidental values are appended to a list each time the node is sampled. The estimated count rate, whereis a calibrated, relative fiber coupling efficiency. Optionally, A i can be subtracted from C i for accidental removal. Uncertainty is computed by assuming Poissonian counting statistics for C i and A i and applying standard, algebraic propagation of error through the calculation of the entanglement quantity (Eq. (3)).

The data collection algorithm consists of a partitioning phase followed by an iterative phase. During partitioning, the algorithm repeatedly iterates through a scan-list of leaves of the tree. Node i is considered stable when sgn(αRₜ − Rᵢ) is known to at least β standard deviations of certainty, where splitting threshold α (0 ≤ α ≤ 1) and stability criterion β are user-chosen heuristics. Stable nodes are no longer measured. If a node is stable and Rᵢ ≥ αRₜ, the node is split into four equal-sized sub-quadrants, which are initially unstable and added to the scan-list. Optionally, a maximum resolution (maximum tree depth) may be set.

The transition to the iterative phase occurs when the percentage of unstable leaves is < Γ, a user-chosen parameter. At this point, stability is ignored and all leaf nodes are scanned repeatedly and guaranteed to have the same total acquisition time. Various final stopping criteria can be used; we chose a fixed total run time. Note that heuristic parameters α, β, and γ may be changed during operation if desired. For the data shown in this manuscript, α = 0.002, β = 2, and Γ = 0.15 with a 30-h runtime.

The probability distributionis computed by uniformly distributing the estimated count rate (with or without accidental subtraction) from each leaf node across its constituent elements in, followed by normalization.

---

### The estimation of approximate sample size requirements necessary for clinical and epidemiological studies in vision sciences [^4b6a9462]. Eye (2009). Low credibility.

Purpose

The aim of this paper is to provide an overview of sample size estimations for the most frequent type of group studies that result in continuous, binary and ordered categorical outcomes.

Methods

The theory behind power and sample size calculations is explained using the basic probability concepts that underpin the most frequently used statistical significance tests.

Results

Simple formulae and tables are presented for the estimation of sample sizes necessary for efficient and effective clinical and epidemiological trials. These may be used without recourse to sophisticated and complex computer software packages. Mathematical complexity is kept to a minimum. Examples and applications from the vision sciences are specifically highlighted.

Conclusions

The paper highlights, with practical examples, the concepts and computations necessary to make sample size estimations accessible to all eye professionals involved in research, diagnostic and statutory work.

---

### Past-future information bottleneck for sampling molecular reaction coordinate simultaneously with thermodynamics and kinetics [^3e726f35]. Nature Communications (2019). High credibility.

Variational inference and neural network architecture

Typically, both the encoder P (χ | X) and the decoder P (X Δ t | χ) can be implemented by fitting deep neural networksto data in form of time-series of X. Our work stands out in three fundamental ways to typical implementations of the information bottleneck principle and in general of artificial intelligence (AI) methods to sampling biomolecules –. First, we use a stochastic deep neural network to implement the decoder P (X Δ t | χ), but use a simple deterministic linear encoder P (χ | X) (see Fig. 1). The simple encoder ensures that the information bottleneck or RC we learn is actually physically interpretable, which is notably hard to achieve in machine learning. On the other hand, by introducing noise in the decoder, we can control the capacity of the model to ensure that the neural network can delineate useful feature from useless information instead of just memorizing the whole dataset. Second, now that our encoder is a simple linear model, we completely drop the complexity term in Eq. (1) and set γ = 0. Due to a reduced number of variables, this leads to a simpler and more stable optimization problem. Finally, the rare event nature of processes in biomolecules makes it less straightforward to use of information bottleneck/AI methods for enhanced sampling. Here we develop a framework on the lines of ref. which makes it possible to maximize the objective function in Eq. (1) through the use of simulations that are progressively biased using importance sampling as an increasingly accurate information bottleneck variable is learnt.

---

### Faricimab (Vabysmo) [^a46b48c6]. FDA (2024). Medium credibility.

3 DOSAGE FORMS AND STRENGTHS

VABYSMO is a clear to opalescent, colorless to brownish-yellow solution available as:

Injection: 6 mg (0.05 mL of 120 mg/mL solution) in a single-dose prefilled glass syringe
Injection: 6 mg (0.05 mL of 120 mg/mL solution) in a single-dose glass vial

Injection: 6 mg (0.05 mL of 120 mg/mL solution) in a single-dose prefilled syringe (3)
Injection: 6 mg (0.05 mL of 120 mg/mL solution) in a single-dose vial (3)

---

### Incipient ferroelectricity of water molecules confined to nano-channels of beryl [^141987d5]. Nature Communications (2016). Medium credibility.

Within the crystal, water molecules can form doublets, triplets and so on, disposed in adjacent cages separated by the bottlenecks inside the channels. We can estimate the concentrations of such agglomerations as follows. Let the cluster of k molecules have a length k, the total number of water molecules be M and that of cages N; 1 ≤ k ≤ M. Combinatorial consideration allows us to derive the following expression for the mathematical expectation n k of number of clusters that contain k sequential water molecules and that are enclosed at both ends either by lone empty sites or by a sequence thereof

This precise expression allows simplification to a formula that is more convenient for practical use. Assuming that M, N, M − k and N − k are large and using the Stirling expressionwe get

Let us use the relative fraction m k = k · n k M −1 of water molecules residing in a clusters of length k. With a total concentration of water molecules C w = MN −1 we obtain

This formula is self-consistent since it fulfils the condition. The results of the corresponding calculations are presented in Fig. 8 by solid lines. As an independent check, we have performed numerical computer simulations to find out how M water molecules can be distributed among N empty cavities (M < N), at different values of M. We assume an array of N = 10⁵ elements, each of which is assigned a value 0. Using the standard random number generator, an integer number i ∈[1, N] was generated and the i -th element from those not equal to zero was given a value 1. After each such step the values of N and M deceased by 1. The process was repeated as long as the remaining value of M was positive. At the end, the frequencies of occurrence of clusters of varying lengths were determined. The results of such modelling are indicated by dots in Fig. 8. Clearly, both used analyses provide the same results.

---

### Response outcomes gate the impact of expectations on perceptual decisions [^eacb848b]. Nature Communications (2020). High credibility.

Stimulus sequence

The Markov chain generated the sequence of stimulus category c k = {−1,1}, that determined whether the reward in the k th trial was available in the left or the right port, respectively (Fig. 1b, top). The stimulus category c k set which of the two sounds, T L (t) and T R (t), composing each stimulus was dominant this ultimately determined the statistics of the sound amplitudesand(Eq. 1) as described below. In each trial, independently of c k, the stimulus strength s k was also randomly generated (Fig. 1b, bottom). Stimulus strength s k defined the relative weights of the dominant and nondominant sounds: for example, when s k = 1 only the dominant sound was played (i.e. easiest trials), whereas when s k = 0 the two sounds had on average the same amplitude (i.e. hardest trials). We used four possible values for s = 0, 0.23, 0.48, and 1. The stimulus evidence was defined in each trial as the combination e k = c k *s k. The value of e k determined the p.d.f. from which the instantaneous evidence S k, f was drawn in each frame f (i.e. in each 50 ms AM-envelope cycle; Fig. 1d, top): when e k: = ± 1 the p.d.f. was(i.e. a Dirac delta p.d.f.), whereas when e k ∈ (−1,1), it was a stretched beta distribution with support [−1,1], mean equal to e k and variance equal to 0.06 (Fig. 1d, top). Finally, the amplitudesandof the two AM envelopes (Eq. 1) were obtained usingandwith f referring to the frame index that corresponds to the time t (see example in Fig. 1d). With this choice, the sum of the two envelopes was constant in all frames a L k (t) + a R k (t) = 1.