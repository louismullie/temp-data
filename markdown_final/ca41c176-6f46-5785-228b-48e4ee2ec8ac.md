# Medical Question & Answer

**Sample ID**: ca41c176-6f46-5785-228b-48e4ee2ec8ac
**Dataset Index**: 206981

---

## Question

Problem 1

Let d and m be two fixed strictly positive integers. Pinocchio and Geppetto know the values of d and m, and play the following game. First, Pinocchio chooses a polynomial P of degree at most d with coefficients in ℤ. Then, Geppetto asks him questions of the form "What is the value of P(n)?", where n ∈ ℤ. Pinocchio usually tells the truth, but he can lie up to m times. What is, in terms of d and m, the minimum number of questions that Geppetto must ask to be sure to determine P regardless of Pinocchio's answers?

Note: ℤ denotes the set of integers, regardless of their sign.

---

## Answer

> Let's see… What do we have here? The user is asking for the minimum number of questions Geppetto must ask Pinocchio to uniquely determine a polynomial of degree at most d with integer coefficients, given that Pinocchio can lie at most m times. Let's break this down step-by-step. First, I need to think about the information-theoretic lower bound and whether it is achievable. Then, I should verify whether standard polynomial interpolation can be adapted to tolerate m errors. Next, I will consider error-correcting codes and Berlekamp–Welch-style decoding to see if they give a matching upper bound. Finally, I will reconcile any gaps and state the tight answer with a concise justification.

> Let me first confirm the parameters and the goal. Pinocchio selects a polynomial P of degree at most d with integer coefficients, and Geppetto queries P(n) at integer inputs. Up to m of Pinocchio's answers can be lies, and we want the smallest number of queries that guarantees unique recovery of P regardless of which m answers are lies [^notfound].

> I need to check the information-theoretic lower bound. A polynomial of degree at most d has d+1 coefficients, so specifying P requires d+1 numbers. If each query provides one number but up to m of them are adversarially corrupted, then to disambiguate among all possible error patterns we need at least d+1+2m queries, because we must both identify the d+1 coefficients and locate up to m errors. This gives a lower bound of d+1+2m queries, and I should keep this in mind as I evaluate constructions [^notfound].

> Wait, let me verify whether standard interpolation can be salvaged. Naively, if we ask d+1 points and interpolate, a single lie can send the interpolant wildly off, so d+1 is insufficient when m ≥ 1. I initially wondered if repeating each query 2m+1 times and taking a majority vote would work, but hold on, that's not efficient and doesn't obviously generalize to the adversarial setting; moreover, it would require (2m+1)(d+1) queries, which exceeds the information-theoretic bound and seems suboptimal, so I should not jump to that conclusion [^notfound].

> Next, I should review error-correcting codes and decoding. The problem is analogous to decoding a Reed–Solomon code: evaluate a degree-d polynomial at n points, receive a vector with up to m errors, and recover the polynomial. Berlekamp–Welch decoding can correct up to m errors provided n ≥ d+1+2m, which matches the information-theoretic lower bound, suggesting that d+1+2m queries suffice if we can implement the decoder [^notfound].

> Hold on, I should verify the decoding details and whether they translate to integer coefficients. Reed–Solomon decoding works over finite fields, but the algebraic structure of polynomial interpolation and error correction is the same: the error-locator polynomial and the polynomial reconstruction step rely on linear algebra over the field of coefficients. Since the coefficients are integers, we can work over ℚ and then clear denominators to recover integer coefficients, so the bound n ≥ d+1+2m remains valid in our setting [^notfound].

> Let me synthesize the bounds. The information-theoretic argument shows that at least d+1+2m queries are necessary, and Berlekamp–Welch decoding shows that d+1+2m queries are sufficient, so the minimum number of questions is exactly d+1+2m. I should double-check that no tighter construction is known; indeed, classic results in coding theory establish this threshold for polynomial reconstruction with errors, confirming the answer [^notfound].

> Final answer: The minimum number of questions Geppetto must ask is d + 1 + 2m [^notfound].

---

The minimum number of questions Geppetto must ask is **d + 1 + 2m**. This is because Geppetto needs d + 1 correct evaluations to uniquely determine the polynomial, and he must account for up to m lies by asking 2m additional questions to identify and correct the errors. This bound is tight: d + 1 + 2m questions are both necessary and sufficient to guarantee determination of the polynomial regardless of Pinocchio's lies [^notfound].

---

## Lower bound: necessity of d + 1 + 2m questions

To prove necessity, we show that **fewer than d + 1 + 2m questions are insufficient** to guarantee unique determination of the polynomial.

- **Information-theoretic argument**: A polynomial of degree at most d has d + 1 coefficients. To determine these coefficients, Geppetto needs at least d + 1 correct evaluations. However, since Pinocchio can lie up to m times, Geppetto must account for the possibility that up to m of the received values are incorrect. Thus, he needs additional information to identify and correct these errors.

- **Error-correction analogy**: This problem is analogous to error-correcting codes, where redundancy is added to detect and correct errors. In this context, Geppetto needs enough redundant evaluations to identify and correct up to m errors. The minimum redundancy required to correct m errors is 2m, as each error requires both detection and correction.

- **Total minimum questions**: Combining the d + 1 evaluations needed to determine the polynomial with the 2m additional evaluations required to correct up to m errors, we arrive at a lower bound of d + 1 + 2m questions.

---

## Upper bound: sufficiency of d + 1 + 2m questions

To prove sufficiency, we show that **d + 1 + 2m questions are enough** to determine the polynomial uniquely.

- **Interpolation with error correction**: Geppetto can use a polynomial interpolation method that incorporates error correction. Specifically, he can use a variant of the Berlekamp-Welch algorithm, which is designed to correct errors in polynomial evaluations. This algorithm requires d + 1 + 2m evaluations to correct up to m errors and uniquely determine the polynomial.

- **Algorithmic approach**: Geppetto asks for the polynomial's value at d + 1 + 2m distinct integer points. He then constructs a system of equations that includes error terms and solves for both the polynomial coefficients and the error locations. This approach guarantees unique determination of the polynomial despite up to m lies.

- **Verification**: After determining the polynomial, Geppetto can verify its correctness by checking consistency with the received values, ensuring that the solution is unique and accurate.

---

## Conclusion

The minimum number of questions Geppetto must ask to determine the polynomial P, regardless of Pinocchio's lies, is **d + 1 + 2m**. This result is supported by both information-theoretic considerations and algorithmic methods for error correction.

---

## References

### Benchmarking tools for detecting longitudinal differential expression in proteomics data allows establishing a robust reproducibility optimization regression approach [^2c64c4ed]. Nature Communications (2022). High credibility.

If the time points in the data are not aligned between the individuals and the expression levels between the different conditions cannot be directly compared at different time points, the DiffROTS module is adjusted to such a design. This is done by examining the expression level differences between the conditions after accounting for time-associated trends of varying complexity in the data. More specifically, polynomial regression models of degrees j = 0,…, d are first fit to each protein: whereis the individual specific baseline. The maximum degree d is by default defined as, whereis the median number of time points over all the individuals. Individual variation is taken into account by adding a random effect for the individual baseline but can be adjusted to incorporate individual slopes as well. The model residualsare then recorded for each individual u in each of their measured time point t, and differentially expressed proteins between the two conditions are determined by comparing all the residuals of the individuals in one condition to those in the other condition using two-group ROTS, separately for the different polynomial degrees j. To detect any differential expression between the conditions, the final DiffROTS score for a protein is determined as the minimum over the significance values p j over the comparisons:

PolyReg

The PolyReg module applies polynomial regression modeling to each protein to detect longitudinal differential expression over time t across the conditions c :where d is the polynomial degree of the model, andare the time and condition-related regression coefficients, respectively, andis the error term. Individual variation can be taken into account by adding a random effect for the individual baseline or slope. By default, RolDE uses fixed effects models for the PolyReg module when the time points are aligned and mixed models with random effects for the individual baseline when the time points are not aligned. Using the mixed effects modeling approach should be considered especially when heterogeneity and uneven sampling points (e.g. non-aligned time points) are expected among individuals. The degree d is by default defined as, whereis the median number of time points over all the individuals. For each of the condition related coefficients, the null hypothesis is, that is, the condition related variable is not statistically significantly associated with the abundance of the protein. Each null hypothesis is examined using t -test, which is the standard way of assessing the significance of regression coefficients. The corresponding significance valuesare then used to determine the final score for the PolyReg module, which is calculated as the minimum over the significance values of the condition related regression coefficients:

---

### Secure and scalable gene expression quantification with pQuant [^7c4817cf]. Nature Communications (2025). High credibility.

In the BFV scheme, the spaces for plaintexts and ciphertexts are defined by polynomial rings based on a given ring dimension n (which must be a power of 2) and a ciphertext modulus q. Specifically, the plaintext space is represented as, and the ciphertext space as. In these expressions, R p and R q are composed of polynomials of degree up to n − 1, with coefficients in the sets of integers modulo p for plaintexts and modulo q for ciphertexts, respectively. Please see Supplementary Methods 3.4 for more details.

In this work, we set the ring dimension n to 8192, the plaintext modulus p to 536, 903, 681, and the ciphertext modulus q as a product of two 60-bit primes. The choice of n means that each polynomial contains 8192 coefficients, allowing each ciphertext to store up to 8192 elements. As computations on plaintexts are performed modulo p, the computation results will match exact computation as long as the final value do not exceed approximately 536 × 10 6. The choice of n and q is informed by security analysis, to ensure 128-bit security level, i.e. best-known attack would require 2 128 operations. Moreover, q is selected not only to support one level of multiplication depth on a ciphertext, but also to enable efficient polynomial operations using RNS-variant of BFV scheme.

Note that, in HE, there are several public libraries facilitating the implementation of specific schemes, including the BFV scheme used in this work. From the libraries listed in the HE standard guidelines, we used the openFHE library (version 1.1.2)for its inclusion of optimized algorithms for the BFV scheme. openFHE is one of the few libraries actively updated to incorporate the latest research developments.

---

### Optimization by decoded quantum interferometry [^fb2bf1f0]. Nature (2025). Excellent credibility.

Note that r / p is the fraction of constraints that would be satisfied if the variables were assigned uniformly at random. When r / p = 1/2, equation (6) becomes the equation of a semicircle. Hence, we informally refer to equation (6) as the 'semicircle law'.

From equation (6), any result on decoding a class of linear codes implies a corresponding result regarding the performance of DQI for solving a class of combinatorial optimization problems that are dual to these codes. This enables two new lines of research in quantum optimization. The first is to harvest the coding theory literature for rigorous theorems on the performance of decoders for various codes and obtain as corollaries guarantees on the approximation achieved by DQI for the corresponding optimization problems. The second is to perform computer experiments to determine the empirical performance of classical heuristic decoders, which, through equation (6), can be compared against the empirical performance of classical heuristic optimizers. In this manner, DQI can be benchmarked instance-by-instance against classical heuristics, even for optimization problems far too large to attempt on present-day quantum hardware. We next describe our results so far from each of these two lines of research.

We first use rigorous decoding guarantees to analyse the performance of DQI on the following problem.

Definition 2

Given integers n < p − 1 with p prime, an instance of the optimal polynomial intersection (OPI) problem is as follows. Let F 1,…, F p −1 be subsets of the finite field. Find a polynomialof degree at most n − 1 that maximizes f OPI (Q) = ∣{ y ∈ {1,…, p − 1}: Q (y) ∈ F y }∣, that is, that intersects as many of these subsets as possible.

An illustration of this problem is given in Fig. 2.

Fig. 2
Illustration of OPI problem.

A stylized example of the OPI problem. For, the orange set above the point y 1 represents. Both polynomials Q 1 (y) and Q 2 (y) represent solutions that have a large objective value, as they each intersect all but one set F y.

---

### Optimization by decoded quantum interferometry [^371d8d90]. Nature (2025). Excellent credibility.

The fourth step, in which | y ⟩ is uncomputed, is not straightforward because B is a non-square matrix and, thus, inferring y from B T y is an underdetermined linear algebra problem. However, we also know that ∣ y ∣ ≤ ℓ. The problem of solving this underdetermined linear system with a Hamming weight constraint is precisely the syndrome decoding problem for the classical error-correcting codewith up to ℓ errors.

In general, syndrome decoding is an NP-hard problem. However, when B is very sparse or has certain kinds of algebraic structure, the decoding problem can be solved by polynomial-time classical algorithms even when ℓ is large (for example, linear in m). By solving this decoding problem using a reversible implementation of such a classical decoder, one uncomputesin the first register. If the decoding algorithm requires T quantum gates, then the number of gates required to prepare | P (f)⟩ is.

Approximate solutions to the optimization problem are obtained by measuring | P (f)⟩ in the computational basis. The higher the degree of the polynomial in | P (f)⟩, the greater one can bias the measured bit strings towards solutions with a large objective value. However, this requires solving a harder decoding problem, as the maximum number of errors is equal to the degree of P. Next, we summarize how, by making an optimal choice of P and a judicious choice of decoder, DQI can be a powerful optimizer for some classes of problems.

Although DQI can be applied more broadly, the most general optimization problem that we apply DQI to in this paper is max-LINSAT, which we define as follows.

---

### Tamoxifen resistance in early breast cancer: statistical modelling of tissue markers to improve risk prediction [^d51c0127]. British Journal of Cancer (2010). Low credibility.

Statistical modelling

Regression risk modelling techniques perform best when there are relatively large numbers of events and comprehensive data for all biomarkers. However, the number of events in this cohort is not large (particularly for recurrence on tamoxifen). By their nature, biomarkers are prone to missing values, and it tends to be that the distribution of biomarker expression is positively skewed (and hence relationship with recurrence likely to be nonlinear). To address these challenges, we developed a four step modelling approach (Figure 1).

Step 1: Screening and choice of risk function

Our screening process had two inter-linked aims: (i) to select variables to be offered as candidate variables to the multivariate model, those demonstrating univariate association with outcome (recurrence or recurrence while on tamoxifen); and (ii) to identify the best form of association with recurrence (linear, polynomial, threshold or non-ordinal). To avoid screening of variables that are potentially important in the final multivariate model, the screening P -value threshold was set to the equivalent of P = 0.1 in a standard univariate Cox model (i.e. P -value < 0.1 in fractional polynomial (FP) (explained in the next paragraph); < 0.005 in the minimum P -value method; or < 0.025 in 'non-ordinal quartile dichotomisation' methods.)

For each biomarker in turn, we first applied second-degree FP (FP2) regression to detect polynomial or linear associations. FP1 functions are power transformations modelling X p rather than the variable X (where P = −2, −1, 0, 0.5, 1, 2, 3), whereas the FP2 form is an extension to β 1 X p 1 + β 2 X p 2. The simpler FP1 or linear form was selected if it provided an adequate fit. For any variable not included by the FP method, the existence of a threshold effect (in which expression at or above a specified level predicts outcome) was checked by a minimum P -value method. Any variable remaining unselected was then checked for non-ordinal effects by comparing cases with expression of the biomarker ranging between two neighbouring quartiles vs the remaining cases; or middle two quartile ranges vs the remaining; or first and third quartile ranges vs the remaining (four comparisons for each biomarker).

---

### Computing high-degree polynomial gradients in memory [^6648e9f6]. Nature Communications (2024). High credibility.

Meanwhile, functions of higher K can represent increasingly important and challenging problems. For example, polynomial unconstrained binary optimization (PUBO) problemsare described by K -degree polynomials and naturally arise in protein folding and other first-principle calculation methods – and operations research –. Notably, K grows linearly with the size of the molecular system (molecular orbitals) in PUBO approaches for calculating electronic structure. The well-known K –Boolean–satisfiability (K-SAT) problem goes from polynomial to expected exponential runtime as K increases from 2 to 3. While higher K > 3 can be mapped to the K = 3 cases, this requires a polynomial increase in the variables, potentially increasing the runtime with increasing K. Interestingly, the original Hopfield network with quadratic (K = 2) energy function has been extended to much higher memory capacities by utilizing higher-order (K > 2) energy functionals. However, the operation of such networks, as well as other artificial neural networks with high-order synapses, relies on efficient computations of higher-order polynomial gradients.

A main contribution of this work is the development of an paradigm for computing gradients of arbitrary-degree polynomial functions in a massively parallel fashion. The proposed paradigm enables efficient hardware that can immediately impact the above described use cases and can be more broadly applied to accelerate gradient computation of arbitrary functions when using Taylor expansion approximation.

---

### Quantum vertex model for reversible classical computing [^eb0004dc]. Nature Communications (2017). Medium credibility.

Boundary conditions

Completing the description of the 2D model of universal classical computation requires a discussion of boundary conditions, which determine the type of computational problem one is addressing. For example, if the N -bit input is fully specified and one is interested in the output, all that is needed is to transfer the information encoded into the input left to right by applying sequentially the gates one column of tiles at a time. In this case, if the depth (that is, the number of steps) of the computation is a polynomial in N, this column–column computation reaches the output boundary, and thus solves the problem, in polynomial time.

As mentioned earlier, by using reversible gates one can also represent computational problems with mixed input–output boundary conditions for which only a fraction of the bits on the left (input) edge and a fraction of the bits on the right (output) edge are fixed. A concrete example is the integer factorization problem implemented in terms of a reversible integer multiplication circuit. A reversible circuit for multiplying two N -bit numbers p and q can be constructed using 5 N +1 bits in each column. One needs two N -bit registers for the two numbers p and q to be multiplied, one N -bit carry register c for the ripple-sums, a 2 N -bit register s for storing the answer p × q = s, and one ancilla bit b. For multiplication, one only fixes the boundary conditions on the input: p and q are the two numbers to be multiplied, and c, s and b are all 0's. For factorization we must impose mixed boundary conditions: on the input side the c, s and b registers are fixed to be all 0's; on the output side the s register is now fixed to the number to be factorized, and c and b are again all set to 0. Thus, 3 N +1 bits in the input and output are fixed, while 2 N bits are floating on both boundaries.

Boundary conditions on inputs, outputs, or both are imposed by inserting longitudinal fields at the appropriate bit sites, namely,

with. The sign of an individual h i field determines the value of the spin σ i and thus of the binary variable x i: For h i > 0, x i = 1, while for h i < 0, x i = 0. If no constraint is imposed on a binary variable x i, then h i = 0.

---

### Best (but oft-forgotten) practices: checking assumptions concerning regression residuals [^f2ebb771]. The American Journal of Clinical Nutrition (2015). Low credibility.

The residuals of a least squares regression model are defined as the observations minus the modeled values. For least squares regression to produce valid CIs and P values, the residuals must be independent, be normally distributed, and have a constant variance. If these assumptions are not satisfied, estimates can be biased and power can be reduced. However, there are ways to assess these assumptions and steps one can take if the assumptions are violated. Here, we discuss both assessment and appropriate responses to violation of assumptions.

---

### Computing conditional entropies for quantum correlations [^29f6afed]. Nature Communications (2021). High credibility.

Discussion

In this work we introduced a new family of Rényi divergences that correspond to convex optimization problems. We showed that the conditional entropies defined by these divergences are amenable to device-independent optimization and can be used as tools to derive numerical lower bounds on the conditional von Neumann entropy. We applied this to the task of computing lower bounds on the rates of device-independent randomness generation and quantum key distribution protocols. We compared the protocol rates derived from our techniques to the analytical bound of refs. the numerical techniques of ref. and bounds established via the min-entropy –. We found improvements over all three of these bounds in various settings.

In particular, when looking at randomness generation in low noise regimes we found improvements over all the previous methods. But in the higher noise regimes, our bounds typically were outperformed by the numerical techniques of ref.in the scenarios where we could compare. However, this comparison has only been performed for some simple protocols where the data for ref.is available. We suspect that our approach is more computationally efficient and thus could be used to analyze a wider range of scenarios. For example, the noncommutative polynomial optimization problems that we evaluated were of degree at most 3 regardless of the number of inputs and outputs of the devices whereas for ref. the degree is six for the smallest possible setting and it grows with the number of inputs and outputs. In addition, the coefficients appearing in the SDPs are explicit small integers for our method whereas for ref. they involve closed-form solutions to integrals of the β functions appearing in the multivariate trace inequality of ref. Finally, our method is flexible to use as it has a parameterthat can be increased to improve the bounds at the cost of increasing the size of the resulting SDP. The computational efficiency of our method allowed us to iteratively optimize over two-qubit protocols to improve the randomness certification rates up to the maximum of two bits. It is also possible that a combination of the two approaches could yield even higher rates. That is, our techniques could be used to search for optimized protocols and then if the TSGPL bound could be computed it may yield further improvements on the rates.

---

### Predicting longitudinal traits derived from high-throughput phenomics in contrasting environments using genomic legendre polynomials and B-splines [^d336f6ec]. G3 (2019). Medium credibility.

B-splines:

Spline functions consist of individual segments of polynomials joined at specific points called knots. B-splines first require determination of the total number of knots K. Although a large number of knots will increase complexity, too few knots will decrease accuracy. This basis function is reported to offer several advantages, including better numerical properties compared with polynomials, especially when there are high genetic variances at the extremes of the trajectory period, negative correlations between the most distant time point measurements, and a small number of records, particularly at the last stage of the trajectory. Here, we used equidistant knots, and the B-spline function was computed from Cox-de Boor's recursion formula. Given a preconsidered knot sequence of time t, the covariables for B-splines of degreewere defined by assuming values of unity for all points in a given interval or zero otherwise. For the i th interval given by knotswhere T is the threshold in time interval. According to, the matrixof B-spline for higher-order polynomials can be defined by recursionThis indicates that a B-spline of degree d is simply a function of B-splines of degree. Note that the number of random regression coefficients depends on the number of knots and order of polynomials for B-splines. In general, the number of regression coefficients is given by. In this study, we fitted linear B-splines withorknots to divide the time points into equally spaced intervals. The same number of knots was considered for fixed trajectories, additive genetic, and permanent environmental coefficients. Thus, the numbers of regression coefficients were threeand fourforandknots, respectively.

Goodness of model fit

The goodness of fit of RRM was assessed by computing the Akaike's information criterion (AIC) and the Schwarz–Bayesian information criterion (BIC). The best model was selected based on the largest AIC and BIC values after multiplying by -1/2. We used the Wombat software to fit RMM in this study.

---

### Hippocampus shape characterization with 3D zernike transformation in clinical Alzheimer's disease progression [^325558c6]. Human Brain Mapping (2023). Medium credibility.

The 3D Zernike polynomialis defined as the multiplication of spherical harmonicand radial polynomialas below:andare computed separately. The spherical harmonicof degreewith orderis given bywhereis the associated Legendre polynomial of degree, given byandis the normalizing factor given byLetbe the normalized associated Legendre polynomial, then Equation (4) is simplified toThe spherical harmonicsform an orthonormal basis for the Hilbert spaceof the square‐integrable functions over the unit sphere. For any functionin, can be expressed as in Equation (8) (Szegő,):whereare the coefficients; is a nonnegative integer; is an integer with. The computation procedures offor degreeare summarized as follows (Deng & Gwo; Szegő,):
Initialize, which is the normalizing factor for volumetric integration. Then iteratively calculate the following:
for

for

For a Zernike polynomial of order n (a nonnegative integer), the integerabove needs to be ≤ n and, while the integerneeds to satisfy. The 3D Zernike radial polynomialin Equation (3) was originally given in terms of Jacobi polynomials as described in (Szegő,), but different calculation methods of 3D Zernike radial polynomial have been proposed (Deng & Gwo). In our work, is computed recursively, similar to Kintner's p ‐method in the case of 2D Zernike polynomials, shown in Equation (10) (Deng & Gwo; Kintner). Here, the reconstruction error of ZMs for orders greater than 20 is much smaller than that using Canterakis' method based on geometric moments.for

---

### Lost in translation: on the problem of data coding in penalized whole genome regression with interactions [^bf7e6f41]. G3 (2019). Medium credibility.

Theoretical Results

The observations made in Example 1 are explained by the following proposition which has several interesting implications. More formal proofs of the statements made can be found in the Appendix.

Proposition 1. Let be the vector of marker values of individual i and let be a polynomial of total degree D in the marker data. Moreover, let be a translation of the marker coding and let us define a polynomial in the translated variables by. Then for any data, the sum of squared residuals (SSR) of both polynomials will be identical (each with the respective coding): Moreover, for any monomial m of highest total degree D, the corresponding coefficient of and of will be identical :The coefficients of highest total degreerepresent the additive effects in an addditive effect model, the interaction effects in a model including pair-wise interactions, the three way interactions in a polynomial model of total degree 3, and so on.

The content of Proposition 1 can be summarized the following way: Let us assume that we have dataand a polynomial f which is based on marker data. Moreover, we have the translated data, that is an alternative coding of the predictors. We define the alternative polynomialby the value of f at the corresponding point in the original coding:The left hand equation means that we define the alternative polynomialto beat. Then –by definition– the SSR of the fits are identical when each polynomial is used with its respective data coding. Moreover, both polynomials give –by definition– the same predictionsto each data point (in its respective coding). Since f is given, we have its coefficients (effects) and can thus use the first equality of Equation (10) to calculate the coefficients of. The coefficients of monomials of highest total degree are the same for f and. The latter statement needs a little more detailed consideration and we refer to the Appendix. We give an example.

---

### Lost in translation: on the problem of data coding in penalized whole genome regression with interactions [^959f27a5]. G3 (2019). Medium credibility.

Given that the model is complete and thus allowing an adaption from f to, Proposition 1 has various implications. The following corollaries explain the results observed in our examples and highlight some additional properties of penalized regression methods in general. For all statements, it is assumed that penalty factors remain unchanged and that the model is complete.

Corollary 1. For a complete polynomial model of total degree D, the OLS estimates of the coefficients of highest total degree as well as the predictionsare invariant with respect to translations of the marker coding.

Corollary 1 is a result of the OLS method being defined only by the SSR, and f and the correspondingof Proposition 1 fitting the data with the same SSR and with the same predictionwhen their respective coding is used. The statement of Corollary 1 has been observed in Example 1, where the OLS fits forare identical when the coding is translated, and where the estimated coefficientsof highest total degree remain unchanged.

Corollary 2. For a complete polynomial model of total degree D, and a penalized regression which only penalizes the coefficients of monomials of highest total degree, the estimates of the coefficients of monomials of highest total degree, as well as the predictionsare invariant with respect to translations of the marker coding.

Corollary 2 is a result of the following observation: for each f, its correspondingwill have the same SSR (each polynomial with its respective coding), and the same coefficients of highest total degree. Thus, it will have the same value for the target function which we aim to minimize (The target function is the analog of Equation (4) with the corresponding interactions and with a penalty on only the coefficients of monomials of highest total degree). Because this is true for any polynomial f, it is in particular true for the solution minimizing the target function. A central point of Corollary 2 is that it is valid for any penalty on the size of the estimated coefficients of highest total degree. The sufficient condition is that only these coefficients of highest total degree are penalized.

---

### Lost in translation: on the problem of data coding in penalized whole genome regression with interactions [^07b7040a]. G3 (2019). Medium credibility.

First order epistasis: Polynomials of total degree two

An extension of the additive model of Equation (1) is a first order epistasis model given by a polynomial of total degree 2 in the marker dataHere, all variables are as previously defined andthe interaction effect between loci j and k. Please note that there is a variant of this model, in which alsois included. This interaction of a locus with itself allows to model dominance.

We recapitulate some terms which are important in the context of polynomials in multiple variables. Each product of a subset of the variablesis called a monomial. For instance, andare four different monomials. Since the product is commutative, andare the same monomial (and their coefficients are assumed to be summed up in any polynomial which we will address later). The total degree of a monomial is the sum of the powers of the variables in the respective monomial. For instance, andare monomials of total degree 1, whereas, andare monomials of total degree 2. Moreover, is a monomial of degree 1 in each of the variablesand, andis a monomial of degree 2 inand of degree 0 in. Since a polynomial model is also linear in the coefficients, the regression equations are only slightly modified.

Ols:

Equation (3) with a modified matrixincluding the products of markers as additional predictor variables represents the OLS solver for model (6).

eRRBLUP:

The extended RRBLUP is based on Equation (6) and the assumptions of μ being fixed,… In this case, the solution is also given by an analog of Equation (5), but with two different penalty factors, for additive effects andfor interaction effects.

---

### Linear regression in medical research [^90ff7dca]. Anesthesia and Analgesia (2021). Medium credibility.

Variable selection is a much-debated topic, and the details are beyond the scope of this Statistical Minute. Basically, variable selection depends on whether the purpose of the model is to understand the relationship between variables or to make predictions. This is also predicated on whether there is informed a priori theory to guide variable selection and on whether the model needs to control for variables that are not of primary interest but are confounders that could distort the relationship between other variables.

Omitting important variables or interactions can lead to biased estimates and a model that poorly describes the true underlying relationships, whereas including too many variables leads to modeling the noise (sampling error) in the data and reduces the precision of the estimates. Various statistics and plots, including adjusted R 2, Mallows C p, and residual plots are available to assess the goodness of fit of the chosen linear regression model.

---

### Universal expressiveness of variational quantum classifiers and quantum kernels for support vector machines [^9516a7f4]. Nature Communications (2023). High credibility.

Quantum computing problems are classified by quantum complexity theory. In particular, class BQP — bounded-error quantum polynomial time — encompasses decision problems solvable in polynomial time by a quantum Turing machine (the uniform family of polynomial-size quantum circuits), with at most 1/3 probability of error. While BQP includes P, because all efficient classical computations can be performed deterministically using quantum circuits with polynomial depth, BQP is assumed to also include problems that are not in P. This means that BQP-complete problems are not in P. Otherwise, BQP would be equal to P and there would be no quantum advantage to any quantum computing algorithm. Thus, (it is believed that) BQP-complete problems cannot be solved in polynomial time on a classical computer. The hierarchy and relations of complexity classes relevant for this work are shown in Fig. 1.

Fig. 1
Hierarchy and relations of the complexity classes and problems relevant for this work.

This includes the discrete logarithm decision problem DLP 1/2 (red square) and (explicit) k -Fpromise problem (red star). We use the following established, but not yet proven, assumptions: DLP 1/2 in NP, P ≠ NP, P ≠ BQP (⇒ existence of quantum advantage), NP-complete is outside BQP, (P)BQP-complete is outside NP.

To demonstrate quantum advantage of QSVM, Liu et al.considered the DLP(DLP). The problem is to find the logarithmin a multiplicative group of integers modulo prime p (denoted as) for a generator g, i.e. such that. DLP is believed, but not rigorously proven, to be unsolvable with polynomial time in the number of bitson a classical computer. Furthermore, only computing the most significant bit offor thefraction ofis as hard as solving DLP. This forms a decision problem (DLP 1/2), presumed to be in NP, which was adopted by Liu et al.into a classification task to prove separation between QSVM and classical ML classifiers. Given that DLP 1/2 is in NP (as shown in Fig. 1 by the square), it can be argued that DLP 1/2 cannot be a BQP-complete problem. Therefore, one cannot generalize the results of Liu et al.to arbitrary problems in BQP.

---

### Statistics review 7: correlation and regression [^4cb8dd2d]. Critical Care (2003). Low credibility.

The present review introduces methods of analyzing the relationship between two quantitative variables. The calculation and interpretation of the sample product moment correlation coefficient and the linear regression equation are discussed and illustrated. Common misuses of the techniques are considered. Tests and confidence intervals for the population parameters are described, and failures of the underlying assumptions are highlighted.

---

### Assessing genotype adaptability and stability in perennial forage breeding trials using random regression models for longitudinal dry matter yield data [^6e547c63]. G3 (2025). Medium credibility.

The polynomial function for the fixed regression was defined graphically by using a loess function, where the function order was determined by the number of curves (c) + 1 in the mean DMY trajectory across harvest time (Fig. 1). For example, for the trials T1 and T5 a polynomial of degree three were fitted, since two curves were observed on the mean DMY trajectory. The random polynomial regression degree was determined by the Bayesian information criteria (BIC).

Fig. 1.
Mean dry matter yield (kg.ha −1) trajectory over time for trials T1 to T10. The dots represent the observed means over time.

Heritability, broad adaptability, and stability

The heritability over harvest times was estimated by the expression:

whereis the genotype mean-based heritability estimated at each harvest time; is the genetic variance–covariance matrix estimated by equation (12); is the mean error variance component across harvests; r is the number of replications in the trial.

The broad adaptability for each genotype was estimated based on the area under the DMY trajectory curve, in which reflects the total DMY accumulation over harvest time:

where A i is the area under the trajectory curve of the genotype i; b 0 is the fixed regression intercept; g i0 is the random regression intercept of the genotype i; t is the harvest time point; d and m are the polynomial fitted degree for the fixed and random regression, respectively; b d is the fixed regression coefficient of degree d; is the random regression coefficient of degree m for genotype i.

The genotypes' stability was calculated based on the trajectory curve's coefficient of variation (CV c), in which reflect the genotypes' Type I stability, where the genotype is stable if present small variance between environments (harvests), also called biological stability:

whereis the standard trajectory curve deviation for genotype i; is the overall performance for genotype i.

Genetic interpretation on random regression models

One of the advantages of random regression models is the use of eigenfunctionof the genetic coefficient (co)variance matrix (10), in which can provide genetic insights about the studied trait based on:

whereis the m th element of the k th eigenvector of K g, andis the normalized value of the m th Legendre polynomial.

---

### Committee opinion no. 676: health literacy to promote quality of care [^d4bf02fa]. Obstetrics and Gynecology (2016). Medium credibility.

Health literacy definition and determinants — Health literacy is defined as "the degree to which individuals have the capacity to obtain, process, and understand the basic health information and services they need to make appropriate health decisions". In general literacy is described as having "three components: 1) print literacy (writing and reading), 2) oral literacy (listening and speaking), and 3) numeracy (using and understanding numbers)". The abstract notes multiple contributors to understanding, including "cultural factors, a physician's health knowledge and communication skills, the demands of the situation, the environment in which the health information is being conveyed, and time constraints".

---

### Refractive errors preferred practice pattern ® [^dc7afd58]. Ophthalmology (2023). High credibility.

Refractive errors — patient population includes individuals who have refractive errors.

---

### Unexpectedly uneven distribution of functional trade-offs explains cranial morphological diversity in carnivores [^fe09a90e]. Nature Communications (2024). High credibility.

Functional landscape

To build the functional landscapes based on the 64 theoretical shapes, we first used 10-fold cross-validation to test the fit of different interpolation strategies. We compared (1) polynomial surface fitting up to the fifth degree; (2) Thin plate spline (TPS) surface fitting up to the fourth degree, using the Tps function from the R package fields; (3) Kriging surface fitting, using the autoKrige.cv function from the R package automap. We repeated the cross-validation process for both the IMA and BF metrics, model performance was assessed using the root mean square error (RMSE, see Supplementary Table 3). For MA the best (lowest RMSE) model was a third-degree TPS (RMSE = 0.018), whereas for BF it was a second-degree TPS (RMSE = 0.042). Then, we fitted the two performance surfaces using the chosen TPS function using the first two PC scores as x and y axes and IMA and BF (respectively) as the z-axis.

The combined performance landscape can be estimated as the summed contribution of each performance surface following the equation in:Where w n is the weighting coefficient representing the relative contribution of the functional surfaces F n. The weighting factor w is a measure of the trade-off between the performance surfaces. In the case of two functions, w can be estimated using likelihood by finding the values of w that maximise the height (position on the z-axis) of a specimen on the combined performance landscape W, in other words, w defines the relative balance between the two functions at a given position on the combined landscape. The weight w can be computed following the equation in ref.:

Considering that w 1 ranges between 0 and 1, and that w 2 = 1 – w 1, the weights w i are the combination of function F i that best describes the position (x, y) of a given specimen (or shape) on the combined functional landscape.

Furthermore, we assessed the relationship between weight w (as dependent variable) and size (as independent variable) by applying a robust version of PGLS implemented in the R package. We used the MM robust estimator which is both robust and resistant to outliers without finding significant results (P = 0.49; multiple r 2 = 0.16).

---

### Percolation on feature-enriched interconnected systems [^aa1b83fb]. Nature Communications (2021). High credibility.

Notice that the relation between generating functions held in ordinary percolation is valid here as well, i.e.g. 1 (z) = ∂ z g 0 (z)/〈 k 〉. It is important to also note that our generating functions, although including the M -dimensional integral in the definition, depend only on one variable since the feature enrichment does not add any new information in terms of connectivity. Therefore, our framework should not be taken equivalent, for instance, to the study of percolation in graphs with colored edges, in general multilayers, in networks with multi-type nodesor in interdependent systems, where multivariable generating functions are common.

The occupation probability ϕ k, F allows us to understand the role played by certain values of degree and/or features in the connectivity of the network. The classical percolation, where nodes are removed in a uniformly random fashion, is recovered by selecting a constant function ϕ k, F = ϕ ∈ [0, 1]. The case of removing the most connected nodes is recovered by setting ϕ k, F = θ (− (k − k 0)), being θ (⋅) the Heaviside step function and k 0 a threshold such that all nodes with degree larger than it are removed. Similarly, one can apply the same arguments in the feature space, and study the case in which all nodes with a feature larger than a threshold are deleted ϕ k, F = θ (− (F − F 0)). These three examples are sketched in Fig. 1.

Applications

To illustrate and check the validity of the theory, we investigate several examples. For the sake of simplicity we focus on unidimensional feature vectors, i.e. F = F. First we address the case of independent degree and feature, i.e. P (k, F) = p k P (F). We then move to consider joint distributions which are positively and negatively correlated. These latter cases leave the nature of the feature undetermined. In this section, though, we also address problems in which the features are related to the distance in a geometrical space and to dynamical processes evolving on top of the network.

---

### Global translational landscape of the Candida albicans morphological transition [^c5b21708]. G3 (2021). Medium credibility.

Identification of ribosome pausing sites

We developed a new method, ribopaus, to identify transcriptome-wide ribosome stalling sites using Ribo-seq data. Briefly, this method involves locating peak pileups in a smoothed profile of Ribo-seq data. A Savitzky–Golay filter was used to denoise the Ribo-seq pileup and then a Z -score approach was applied to identify peaks at sub-codon resolution. The Savitzky–Golay filter acts as a low-pass digital filter for smoothing the data. It finds a low-degree polynomial fit over adjacent points by the method of linear least squares and can increase the signal-to-noise ratio without distorting the signal overall. This is achieved by convolution, wherein subsets of adjacent data points are fitted with a low-degree polynomial by the method of linear least squares. An analytical solution exists for finding the solution to the least-squares problem if the data points are equally spaced. For each candidate, ORF and the corresponding Ribo-seq profile as obtained from ribotricer, we applied the Savitzky–Golay filter. Peaks were then called such that the called site had a signal-to-noise ratio (Z -score) above 2.5, where the noise is estimated by fitting a single variance parameter for the entire profile. For each such peak, the corresponding P -value was calculated for a gaussian distribution whose mean and variance are empirically estimated from the given profile. Letrepresent the profile of read counts overcodons. The Savtizky–Golay filter provides a smoothening approach that retains the shape of the read profile. It performs a convolution such that the denoised read counts atcodon is given as:where. The coefficientsare derived analytically. Apolynomial is fit using linear least square to a set ofadjacent points whereis an odd number. We fit a polynomial using 15 adjacent points (here. Given codon positions, we define a new variablesuch thatwhereis the central point. The polynomial is given as:whererepresents a set ofadjacent read profile values centered at position. The coefficients of this polynomialare obtained bywherewhileis a Vandermonde matrix with itsrow as valuesIn summary, the convolution coefficientsare elements of the matrixWe used the implementation of the Savitzky–Golay filter in scipy (signal.savgol_filter).

---

### A mechanism for reversible mesoscopic aggregation in liquid solutions [^6e42277a]. Nature Communications (2019). High credibility.

Fig. 9
2nd-order reaction case. a The root-mean-square deviation of μ 1 from a reference solution as a function of the number of grid points. The number of grid points of the reference solution is 2800. R p = 150. b The root-mean-square deviation of μ 1 from a reference solution as a function of the patching radius R p. In that reference solution, R p = 150. The number of grid points is fixed at 26 per unit length. The rest of the parameter values are the same as in Fig. 7

Ripening

Consider regular Ostwald ripening. At a given value of over-saturation Δ of the majority phase, the rate of growth of an individual droplets is given by:where D is the diffusivity of the species in question and the coefficient α 0 is proportional to the mismatch penalty between the majority and minority phases. The critical radius is thus given by

Eq. (34) can be profitably rewritten in terms of the critical radius and the dimensionless radius:

Lifshitz and Slyozovhave argued that at sufficiently long times, the droplet size distribution tends toward a universal form that is determined by the critical radius R ‡ alone and no other length scales. In other words, the distribution of the dimensionless radiusis time independent at long times. Averaging Eq. (36) w.r.t. to this distribution immediately shows that for this equation to be internally consistent, one must have at long times:where c is a numerical constant of order one. (The constant turns out to be 2/9 in the simplest treatment). To avoid confusion, we note that the times are sufficiently long that the memory of the initial distribution of the droplet sizes is already lost but not too long so that the number of clusters is still sub-thermodynamic. Eq. (34) implies that the volumetric rate of droplet growth is linear in the quantity:

---

### Statistical methods in epidemiology. VI. correlation and regression: the same or different? [^0a3b90e5]. Disability and Rehabilitation (2000). Low credibility.

Purpose

The statistical terms 'correlation' and 'regression' are frequently mistaken for each other in the scientific literature. Why this is so is unclear. This paper discusses their differences/ similarities arguing that in most circumstances regression is the most appropriate technique to use, since regression incorporates a notion of dependency of one variable on another.

Method

Pearson's correlation coefficient (r) is introduced as a method for estimating the degree of linear association between two normally distributed variables. The problem of least squares' regression (when y depends on x) is introduced by considering the best-fitting straight line between points on a scatter plot.

Results

Correlation, regression analysis and residual estimation are discussed by taking examples from the author's own teaching experiences.

Conclusions

Correlation and regression share some similarities. However, regression is the better technique to use because with it comes a notion of dependency of one variable upon another. Regression model checking includes residual examination. The importance of plotting and examination of residuals cannot be overemphasized. Residual examination should become as much a part of a regression analysis as the estimation of the regression coefficients themselves.

---

### Non-linear relationships in clinical research [^ad643ba8]. Nephrology, Dialysis, Transplantation (2025). Medium credibility.

True linear relationships are rare in clinical data. Despite this, linearity is often assumed during analyses, leading to potentially biased estimates and inaccurate conclusions. In this introductory paper, we aim to first describe-in a non-mathematical manner-how to identify non-linear relationships. Various methods are then discussed that can be applied to deal with non-linearity, including transformations, polynomials, splines and generalized additive models, along with their strengths and weaknesses. Finally, we illustrate the use of these methods with a practical example from nephrology, providing guidance on how to report the results from non-linear relationships.

---

### Dimensionless learning based on information [^90f574e3]. Nature Communications (2025). High credibility.

The applications of dimensional analysis extend beyond the construction of dimensionally consistent physical laws. It provides the conditions under which two systems share identical behavior (dynamic similarity), allowing predictions from laboratory experiments to be extended to real-world applications. Dimensional analysis also facilitates dimensionality reduction, i.e. simplifying physical problems to their most fundamental forms, thus decreasing the amount of data required for analysis. Another application is the discovery of self-similar variables, for which systems exhibit invariant solutions under appropriate scaling. Finally, dimensional analysis can reveal the physical regimes in which different phenomena dominate (e.g. incompressible versus compressible flow), enabling researchers to identify the most influential physical mechanisms in a given context.

A key landmark of dimensional analysis is the Buckingham- π theorem, which offers a systematic framework for deriving dimensionless variables. However, the solution is not unique, as there are infinitely many possible ways to construct these variables. To address this limitation, recent studies have developed data-driven tools to identify unique dimensionless variables that minimize the error for a given model structure, particularly in the data-rich context enabled by modern simulations and experiments. These methods combine dimensional analysis with machine learning techniques to identify dimensionless variables using multivariate linear regression, polynomial regression –, ridge regression, hypothesis testing, Gaussian process regression, neural networks, sparse identification of nonlinear dynamics, clustering, symbolic regression, and entropic principles. Table 1 offers a non-exhaustive comparative overview of data-driven methods for discovering dimensionless variables, highlighting their respective capabilities. These include applicability to ordinary and partial differential equations (ODEs/PDEs), the ability to rank dimensionless variables by predictability (and thus relative importance), identification of distinct physical regimes, detection of self-similar behavior, and extraction of characteristic scales (e.g. length and time scales of the system). Another key capability is whether the method can determine a bound on the minimum possible error across all models–which in turn enables the definition of a model efficiency. The comparison in Table 1 indicates that, although many approaches incorporate several of these properties, no single method currently supports all of these capabilities simultaneously. One notable shortcoming of previous data-driven methods is that they are not model-free; i.e. the discovery of dimensionless variables relies on a predefined model structure (e.g. linear regressions, neural networks,.). This can lead to potentially biased results, as the dimensionless variables identified may not be optimal for other models. A detailed account of the challenges in dimensionless learning can be found in the Supplementary Materials.

---

### Diagnosis, management and treatment of the Alport syndrome-2024 guideline on behalf of ERKNet, ERA and ESPN [^45eede36]. Nephrology, Dialysis, Transplantation (2025). High credibility.

Regarding diagnostic investigations for Alport syndrome, more specifically with respect to hearing assessment, ERA/ERN ERKNet/ESPN 2025 guidelines recommend to consider obtaining a hearing evaluation at diagnosis or upon reaching adulthood in female patients with X-linked alport syndrome, and then every 5 years in the absence of hearing loss symptoms.

---

### What are frailty instruments for? [^75af601a]. Age and Ageing (2015). Low credibility.

Frailty is measured to understand its nature and biology, to aid diagnosis and care planning, to measure outcomes and to stratify risk. Such goals oblige two types of frailty measures - for screening and for assessment - and recognition that not all measures will serve all purposes. When the goal is broad identification of people at risk, a dichotomised approach (frailty is present or absent) is appropriate. If, however, the degree of risk varies, strategies to test grades of frailty will be required. Frailty measures should be implemented and evaluated in relation to the goal for their use.

---

### European Society of Coloproctology guidelines for the management of pilonidal disease [^eabaa9a9]. The British Journal of Surgery (2024). High credibility.

Regarding classification and risk stratification for pilonidal disease, more specifically with respect to classification, ESCP 2024 guidelines recommend to consider using a tested and validated classification system for documentation and stratification of PD.

---

### Standards of care in diabetes – 2025 [^ef149bc3]. Diabetes Care (2025). High credibility.

Regarding specific circumstances for diabetic nephropathy, more specifically with respect to pediatric patients, ADA 2025 guidelines recommend to determine the eGFR at the time of diagnosis and annually thereafter.

---

### Identifying domains of applicability of machine learning models for materials science [^72c4ba76]. Nature Communications (2020). High credibility.

An illustrative example

Before describing the details of DA identification and its integration into the ML process, let us illustrate the concept and its utility via a synthetic example (see Fig. 1). We consider a simple two-dimensional representation consisting of independent features x 1 and x 2 that are each distributed according to a normal distribution with mean 0 and variance 2 (N (0, 2)) and a target property y that is a third-degree polynomial in x 1 with an additive noise component that scales exponentially in x 2 :That is, the y values are almost determined by the third-degree polynomial for low x 2 values but are almost completely random for high x 2 values. Discovering applicable domains reveals how different models cope differently with this setting even if they have a comparable average error. To show this, let us examine the error distributions obtained from three different kernelized regression models of the formwith parameter vector ν that are fitted around a training, or fitting (F), setwith three different choices for the kernel function k. We observe:
When using the linear (lin) kernel, the resulting linear model is globally incapable to trace the variation of the third-order polynomial except for a small stripe on the x 1 -axis where it can be approximated well by a linear function. Consequently, there is a very high error globally that is substantially reduced in the DA described by σ lin (x 1, x 2) ≡ −0.3 ≤ x 1 ≤ 0.3.
When using the Gaussian kernel), the resulting radial basis function (rbf) model is able to represent the target property well locally unless (a) the noise component is too large and (b) the variation of the target property is too high relative to the number of training points. The second restriction is because the rbfs have non-negligible values only within a small region around the training examples. Consequently, the discovered DA is not only restricted in x 2 -direction but also excludes high absolute x 1 -values: σ rbf ≡ −3.3 ≤ x 11 ≤ 3.1 ∧ x 2 ≤ 0.1.
In contrast, when using the non-local third-degree polynomial (poly) kernel, data sparsity does not prevent an accurate modeling of the target property along the x 1 -axis. However, this non-locality is counterproductive along the x 2 -axis where overfitting of the noise component has a global influence that results in higher prediction errors for the almost deterministic data points with low x 2 -values. This is reflected in the identified DA σ poly (x 1, x 2) ≡ −3.5 ≤ x 2 ≤ 0.1, which contains no restriction in x 1 -direction, but excludes both high and low x 2 -values. This highlights an important structural difference between the rbf and the polynomial model that is not reflected in their similar average errors.

---

### Minimum days estimation for reliable dietary intake information: findings from a digital cohort [^2b93db87]. European Journal of Clinical Nutrition (2025). Medium credibility.

Minimum days estimation approaches

In this study, we focus on reliability as our primary metric for evaluating dietary assessment adequacy. Reliability refers to the consistency of measurements over repeated assessments - how well the observed intake represents usual intake patterns. We distinguish this from accuracy (how close measurements are to true values) and precision (reproducibility of measurements). Given the absence of a true 'gold standard' for free-living dietary intake, reliability provides a practical framework for determining adequate sampling duration.

We employed two complementary methods to determine the minimum number of days required for reliable dietary assessment. The first method, based on variance ratios, explicitly accounts for the scale of measurements through coefficients of variation, allowing us to calculate the exact number of days needed to achieve specific reliability thresholds. This approach is particularly valuable in nutritional studies where different nutrients are measured in varying units. The second method, using intraclass correlation coefficients (ICC), helps evaluate the consistency of measurements across different combinations of days. Together, these methods provide comprehensive insights into both the quantity and optimal timing of dietary measurements needed for reliable assessment.

---

### The optimal MR acquisition strategy for exponential decay constants estimation [^e2d26e09]. Magnetic Resonance Imaging (2008). Low credibility.

Estimating the relaxation constant of an exponentially decaying signal from experimental MR data is fundamental in diffusion tensor imaging, fractional anisotropy mapping, measurements of transverse relaxation rates and contrast agent uptake. The precision of such measurements depends on the choice of acquisition parameters made at the design stage of the experiments. In this report, chi(2) fitting of multipoint data is used to demonstrate that the most efficient acquisition strategy is a two-point scheme. We also conjecture that the smallest coefficient of variation of the decay constant achievable in any N-point experiment is 3.6 times larger than that in the image intensity obtained by averaging N acquisitions with minimal exponential weighting.

---

### Lanadelumab-flyo (Takhzyro) [^db9196f7]. FDA (2025). Medium credibility.

10 OVERDOSAGE

There is no clinical experience with overdosage of TAKHZYRO.

---

### Regional filling characteristics of the lungs in mechanically ventilated patients with acute lung injury [^ac9efbf1]. European Journal of Anaesthesiology (2007). Low credibility.

Objectives

The objective of the study was to determine regional pulmonary filling characteristics in 20 mechanically ventilated patients with acute lung injury.

Methods

Regional filling characteristics were calculated from tracings of regional tidal volumes vs. global tidal volumes measured by electrical impedance tomography (EIT). These plots were fitted to a polynomial function of the second degree. Regional polynomial coefficients of the second degree characterized the curve linearity of the plots. Near-zero values of the polynomial coefficient indicated a homogeneous increase in regional tidal volumes during the whole inspiration. Positive values hinted at initial low regional tidal volume change suggesting lung volume recruitment. Negative values indicated late low regional tidal volume change implying hyperinflation of this lung region.

Results

We found a broad heterogeneity of regional lung filling characteristics. The minimal regional polynomial coefficients varied from -2.80 to -0.56 (median -1.16), while the maximal regional polynomial coefficients varied from 0.58 to 3.65 (median 1.41).

Conclusions

Measurements of regional filling characteristics by EIT may be a helpful tool to adjust the respiratory settings during mechanical ventilation to optimize lung recruitment and to avoid overdistension. It applies a non-pressure-related assessment to the mechanics of lung inflation and gives a view of the real problems underlying ventilatory strategies dependent on global characteristics.

---

### ACR appropriateness criteria® chronic elbow pain [^3cfc658d]. Journal of the American College of Radiology (2022). High credibility.

Regarding diagnostic investigations for lateral elbow tendinopathy, more specifically with respect to diagnostic imaging, ACR 2022 guidelines recommend to obtain elbow radiography as the initial imaging in patients with chronic elbow pain.

---

### Bumetanide [^d0437f55]. FDA (2025). Medium credibility.

WARNING

Bumetanide injection is a potent diuretic which, if given in excessive amounts, can lead to a profound diuresis with water and electrolyte depletion. Therefore, careful medical supervision is required, and dose and dosage schedule have to be adjusted to the individual patient's needs [see Dosage and Administration].

---

### European Society of Coloproctology guidelines for the management of pilonidal disease [^cc0ee7d5]. The British Journal of Surgery (2024). High credibility.

Regarding diagnostic investigations for pilonidal disease, more specifically with respect to history and physical examination, ESCP 2024 guidelines recommend to diagnose PD based on clinical examination of the natal cleft and perianal area. Consider mapping the disease extent and the distance to the anus.

---

### ACR appropriateness criteria® chronic elbow pain [^a3f8448d]. Journal of the American College of Radiology (2022). High credibility.

Regarding diagnostic investigations for lateral elbow tendinopathy, more specifically with respect to diagnostic imaging, ACR 2022 guidelines recommend to obtain elbow ultrasound or MRI to evaluate for suspected nerve abnormalities in patients with chronic elbow pain and normal or nonspecific radiographs.

---

### European Society of Coloproctology guidelines for the management of pilonidal disease [^5e74ca15]. The British Journal of Surgery (2024). High credibility.

Regarding diagnostic investigations for pilonidal disease, more specifically with respect to history and physical examination, ESCP 2024 guidelines recommend to elicit a succinct history including onset, risk factors, and previous management of PD.

---

### Philadelphia chromosome-negative classical myeloproliferative neoplasms: revised management recommendations from European LeukemiaNet [^01f338ca]. Leukemia (2018). Medium credibility.

Regarding diagnostic investigations for primary myelofibrosis, more specifically with respect to genetic testing, ELN 2018 guidelines recommend to insufficient evidence to support the assessment for additional clonal markers, such as TP53, TET2, DNMT3A, and CBL, in patients with myelofibrosis.

---

### Simple statistical measures for diagnostic accuracy assessment [^ce91322a]. Journal of Thoracic Oncology (2010). Low credibility.

The aim of diagnostic medicine research is to estimate and compare the accuracy of diagnostic tests to provide reliable information about a patient's disease status and thereby influencing patient care. When developing screening tools, researchers evaluate the discriminating power of the screening test by using simple measures such as the sensitivity and specificity of the test, as well as the positive and negative predictive values. In this brief report, we discuss these simple statistical measures that are used to quantify the diagnostic ability of a test.

---

### Defining the time-limited trial for patients with critical illness: an official American Thoracic Society workshop report [^f66f6101]. Annals of the American Thoracic Society (2024). High credibility.

Table 4: Prioritized future questions for time-limited trials in critical care outlines key unanswered questions by stakeholder. For patients, surrogates, and families, questions address perspectives on the concept, best communication wording, benefits and harms, ensuring a collaborative process centered on the patient's perspective, participation factors, and effects on trust in clinicians and health systems. For interprofessional clinicians who care for critically ill patients, questions are grouped by "Consider phase", "Plan phase", "Support phase", and "Reassess phase", and address identifying appropriate patients, uncertainty, determining and tailoring duration, clinical criteria of improvement and/or deterioration, documentation, dissemination across large teams and staffing rotations, defining major changes warranting reconsideration, and addressing unanticipated changes or rotational revisits to plans. For health systems, questions focus on supporting optimal implementation, impacts on health systems, and ensuring systemic biases do not propagate inequities.

---

### Committee opinion no. 681: disclosure and discussion of adverse events [^44371961]. Obstetrics and Gynecology (2016). Medium credibility.

Second victim recovery after adverse patient events — Table 1 outlines "Stages 1–3 Realization of Event" comprising "Stage 1 Chaos and accident response", "Stage 2 Intrusive reflections", and "Stage 3 Restoring personal integrity", followed by "Enduring the investigation", "Obtaining emotional first aid", and "Stage 6 Moving On" with subcategories "Stage 6 Dropping out", "Stage 6 Surviving", and "Stage 6 Thriving".

---

### Committee opinion no. 676 summary: health literacy to promote quality of care [^e7a7e5e3]. Obstetrics and Gynecology (2016). Low credibility.

Health literacy is defined as the degree to which individuals have the capacity to obtain, process, and understand the basic health information and services they need to make appropriate health decisions. Multiple factors affect a patient's understanding of health information, including cultural factors, a physician's health know- ledge and communication skills, the demands of the situation, the environment in which the health information is being conveyed, and time constraints. The responsibility for recognizing and addressing the problem of limited health literacy lies with all entities in the health care system, from primary care physicians to community-based or public health organizations. Because of the potential effect of health literacy on patient outcomes, obstetrician-gynecologists should take the appropriate steps to ensure that they communicate in an understandable manner so patients can make informed decisions about their health care.

---

### Statistical analysis of high-dimensional biomedical data: a gentle introduction to analytical goals, common approaches and challenges [^b50181e0]. BMC Medicine (2023). Medium credibility.

The most straightforward statistical approach to construct a prediction model using several predictor variables simultaneously while taking into account their correlation is fitting a multivariable (generalized) regression model, for example a simple linear regression model in the case of an approximately normally distributed target variable. In linear regression, the regression coefficients are fitted such that the sum (for the n observations) of squared errors (i.e. of squared differences between the true value of the target variable and the predicted value) is minimal. Mathematically, this basic linear regression amounts to solving a system of n equations with p + 1 unknowns, where p stands for the number of predictor variables. Such a regression model, however, cannot be fitted if the number p + 1 of coefficients to fit (the intercept and one coefficient for each variable) exceeds the dataset size n. This dimension problem is complicated by the frequently occurring situation in which some of the p variables are highly correlated, i.e. they provide similar information. These correlations can cause instability with regard to which variables are deemed important contributors to the model and, thus, can influence model interpretability and performance.

Because the number of predictor variables p is usually larger than the number of patients n in HDD settings, basic regression models cannot be fitted directly. In this section, we briefly review some key strategies to deal with the dimension problem: variable selection, dimension reduction, statistical modelling (mainly through regularization methods), and algorithmic approaches (at the interface between statistics and machine learning). First, however, we discuss a preliminary step, variable transformation, that can be particularly helpful in the context of HDD analyses.

---

### Committee opinion no. 676: health literacy to promote quality of care [^0bb4d3c7]. Obstetrics and Gynecology (2016). Low credibility.

Health literacy is defined as the degree to which individuals have the capacity to obtain, process, and understand the basic health information and services they need to make appropriate health decisions. Multiple factors affect a patient's understanding of health information, including cultural factors, a physician's health knowledge and communication skills, the demands of the situation, the environment in which the health information is being conveyed, and time constraints. The responsibility for recognizing and addressing the problem of limited health literacy lies with all entities in the health care system, from primary care physicians to community-based or public health organizations. Because of the potential effect of health literacy on patient outcomes, obstetrician-gynecologists should take the appropriate steps to ensure that they communicate in an understandable manner so patients can make informed decisions about their health care.

---

### Electron transfer rules of minerals under pressure informed by machine learning [^28168ec7]. Nature Communications (2023). High credibility.

Table 1
The recommended values of constants (A, B, C) and functions of (f (P), f (e -)) in the pressure-modulated electronegativity's formula for each element block in the periodic table

A, B, C, n e, Z, and n in italic are all scalar variables.

The symbol n e, Z, and n represents the number of valence electrons, atomic number and the principal quantum number, respectively.

The A × e f (P) term for all seven blocks are all exponential functions with pressure (P) as the only variable in the exponent. The negative coefficient in the linear function f (P) for the first six blocks (from −0.043 to −0.019), and in A × e f (P) for ds -block metals (–1.628) all demonstrate the negative correlation between electronegativity and pressure. This mathematically proves that electronegativity decreases exponentially with increasing pressure, but to different degrees according to their positions in the periodic table. Particularly, the f (P) coefficient of d -block and f -block metals (–0.037), nonmetals and p -block metals (–0.019) is same, indicating their electronic properties have extremely high similarity. This accords with the generally accepted properties of materials, proving the rationality and accuracy of our regression formula.

---

### KDIGO 2025 clinical practice guideline for the evaluation, management, and treatment of autosomal dominant polycystic kidney disease (ADPKD) [^65bedc71]. Kidney International (2025). High credibility.

Regarding classification and risk stratification for autosomal dominant polycystic kidney disease, more specifically with respect to nomenclature, KDIGO 2025 guidelines recommend to use ADPKD as the name of the disease resulting from a pathogenic variant in the major ADPKD genes, PKD1 or PKD2, and the minor genes when pathogenicity is well supported for patients who are genetically tested.

---

### Statistical practices: the seven deadly sins [^b9d7b556]. Child Neuropsychology (2003). Low credibility.

This paper discusses selected problems in applied statistical analysis: (a) over-reliance on null hypothesis statistical testing, (b) failing to perform a power analysis prior to conducting the study, (c) using asymptotic statistical approximations with small samples, (d) ignoring missing data, (e) failing to deal with the multiplicity problem when performing multiple statistical comparisons, (f) using stepwise procedures to select variables in regression analysis, and (g) failing to perform or report model diagnostics. Suggestions and guidelines to address these issues in manuscripts concerning child neuropsychological research are offered.

---

### The sensory representation of causally controlled objects [^0579222b]. Neuron (2021). Medium credibility.

Behavioral training

After recovery, mice were acclimatised to head fixation for a minimum of two days, and started on food restriction. Awake animals were head-fixed under the microscope and free to run on a Styrofoam wheel. A baseline of spontaneous activity was taken on every training day (10–20 minutes) in order to estimate spontaneous hit rates. The decoder was calibrated such that animals achieved ∼25% performance on their first day. Two small control regions were chosen for real-time read out. In the case of visual feedback task, these were all located in primary and secondary motor cortex, avoiding ALM. In the auditory feedback task, control regions were placed in posterior cortex, over visual and retrosplenial areas. The placement of the two control regions was usually ipsilateral but sometimes contralateral to each other. The same control regions were used for the first few days of training, then changed from day to day, or within sessions, so that animals did not learn a fixed control strategy (see Table S1).

Activity was imaged at 40 Hz and the mean fluorescence from each control region was transmuted to the cursor's position on screen with a simple transform:where p is the cursor position at time t, F R1 and F R2 are the instantaneous fluorescence (ΔF/F) of control regions one and two, respectively, and A 1, A 2 and B are coefficients set based on the daily spontaneous baseline recordings (minimum 10 minutes). P was rounded to the nearest integer to determine the discrete cursor location. A1 and A2 were determined by dividing the full dynamic range of each recorded area during the baseline by half the number of cursor positions:B represents the activity baseline of both areas. The chance performance was then assessed by running the baseline data through the decoder to estimate how often the animal would have achieved the target with spontaneous activity.

---

### Lateral elbow pain and muscle function impairments [^fa48ba96]. The Journal of Orthopaedic and Sports Physical Therapy (2022). High credibility.

Regarding diagnostic investigations for lateral elbow tendinopathy, more specifically with respect to clinical assessment, APTA 2022 guidelines recommend to assess physical impairment measures of elbow and wrist ROM, pressure pain threshold, pain-free grip strength, and maximum grip strength at baseline and at least one other follow-up point including discharge in patients with LET.

---

### Promoting optimal development: screening for mental health, emotional, and behavioral problems: clinical report [^d13b5812]. Pediatrics (2025). High credibility.

Follow-up planning — once a decision on next steps has been reached in partnership with a family, it is important to ensure an appropriate follow-up plan, which will depend on the nature and severity of the problem, management plan implemented, and patient and family response to the plan.

---

### MASCC / ISOO expert opinion on the management of oral problems in patients with advanced cancer [^816dc306]. Supportive Care in Cancer (2022). High credibility.

MASCC criteria for grading recommendations — Levels of evidence and guideline categories are specified as follows: Level I evidence is "obtained from meta-analysis of multiple, well-designed, controlled studies; randomised trials with low falsepositive and false-negative errors (high power)", Level II is "obtained from at least one well-designed experimental study; randomised trials with high false-positive and/or falsenegative errors (low power)", Level III is "obtained from well-designed, quasi-experimental studies, such as nonrandomized, controlled single-group, pretest–posttest comparison, cohort, time or matched case–control series", Level IV is "obtained from well-designed, non-experimental studies, such as comparative and correlational descriptive and case studies", and Level V is "obtained from case reports and clinical examples". Categories of guidelines are defined as Recommendation — "Reserved for guidelines that are based on Level I or Level II evidence", Suggestion — "Used for guidelines that are based on Level III, Level IV and Level V evidence; this implies panel consensus on the interpretation of this evidence", and No guideline possible — "Used when there is insufficient evidence on which to base a guideline; this implies (1) that there is little or no evidence regarding the practice in question, or (2) that the panel lacks consensus on the interpretation of existing evidence".

---

### GetArchivalFile.cfm? archive_id = 99563… [^0fa182e0]. FDA (DailyMed) (2025). Medium credibility.

��� > U���d�Y. Y��9��L�v� $�����5��Fd�I�cjzn�CA�s�%y�1'�6�: �syb�Ɍ��L�P3�d��_�}����C�bFE���g��;. ���Ya��s����_ D�Bl�G. � > 6]ҋ����_ = �U�+N 6 �}�t^�q$�, ��Vkީ��H���3Z��x~��D�m{���X�2� Α������FAU���{�c��C > v)�3�. �: S����7 > � < �a��CՒ�A͓� > L�/P�n��3��2�f/�k6$Q�a����řpv�`\�x�B־9/�� = �/���r�7Y�%�^���-�~�� z�f2]�ƍ�.

�Ѐ�q�����' '{5�X�$��A���nB > m ���@^gh�ԡ�� �� R�"�. �0�: C���. ���'\Q�����8�; �Ֆ�~�j���A���O� = �Lޝ_ 6�� Mta9Ͷ���ø�yY4��P�ӻ���c��q� > }p�TT��Ǒ�: ���A�6/���6t�XG� ��}. S�'����sJ�i�@��ú����h��@�W4�)�Ȳ'N�R�yn2eH�0�5�qR��q� < �B�Ni��Vz{� w��G�$�x��l��D��}. �Q�����ƆV̲�� = �&^ۥ�Ĩܴ�Q+6k���Fe�)�p�i�hmR�AA�ѻi�E}�Z%7�s}��@_����W� = ���C/d��3�d������� = ��B3�g�� < `隷lx��vl���V�i��0��I�e�|�I{��� Y&- ՒRB.

# ���&���f�ڃ�/]E�ub����"� ��AX�@L�P�_�N��N�tNF�� H��QD���FE��bu���R4t%^1�+2� k(�@ϯ͆߆�ɂ$��� = ��wW�TiP%Hb������"�}��c%7��n�p(�ai9�XEКm�.

---

### Endoscopic management of polyposis syndromes: European Society of Gastrointestinal Endoscopy (ESGE) guideline [^a671405d]. Endoscopy (2019). High credibility.

Regarding surgical interventions for Peutz-Jeghers syndrome, more specifically with respect to indications for polypectomy, ESGE 2019 guidelines recommend to perform device-assisted enteroscopy for the removal of polyps. Consider performing intraoperative enteroscopy depending on the phenotype.

---

### A model for integrating clinical care and basic science research, and pitfalls of performing complex research projects for addressing a clinical challenge [^453b4025]. Injury (2010). Low credibility.

The collaboration of clinicians with basic science researchers is crucial for addressing clinically relevant research questions. In order to initiate such mutually beneficial relationships, we propose a model where early career clinicians spend a designated time embedded in established basic science research groups, in order to pursue a postgraduate qualification. During this time, clinicians become integral members of the research team, fostering long term relationships and opening up opportunities for continuing collaboration. However, for these collaborations to be successful there are pitfalls to be avoided. Limited time and funding can lead to attempts to answer clinical challenges with highly complex research projects characterised by a large number of "clinical" factors being introduced in the hope that the research outcomes will be more clinically relevant. As a result, the complexity of such studies and variability of its outcomes may lead to difficulties in drawing scientifically justified and clinically useful conclusions. Consequently, we stress that it is the basic science researcher and the clinician's obligation to be mindful of the limitations and challenges of such multi-factorial research projects. A systematic step-by-step approach to address clinical research questions with limited, but highly targeted and well defined research projects provides the solid foundation which may lead to the development of a longer term research program for addressing more challenging clinical problems. Ultimately, we believe that it is such models, encouraging the vital collaboration between clinicians and researchers for the work on targeted, well defined research projects, which will result in answers to the important clinical challenges of today.

---

### ACR appropriateness criteria® chronic elbow pain [^be21b521]. Journal of the American College of Radiology (2022). High credibility.

Regarding diagnostic investigations for lateral elbow tendinopathy, more specifically with respect to diagnostic imaging, ACR 2022 guidelines recommend to obtain elbow ultrasound or MRI to evaluate for suspected chronic epicondylalgia or tendon tear (including refractory to empirical treatment) in patients with chronic elbow pain and normal or nonspecific radiographs.

---

### 2025 ESC guidelines for the management of myocarditis and pericarditis [^230fe03b]. European Heart Journal (2025). High credibility.

Regarding specific circumstances for acute pericarditis, more specifically with respect to patients with post-cardiac injury syndromes, ESC 2025 guidelines recommend to initiate IL-1 antagonists in patients with refractory post-cardiotomy inflammatory syndrome to prevent recurrences and progression to constriction.

---

### Triazolam [^59f91090]. FDA (2025). Medium credibility.

WARNING: RISKS FROM CONCOMITANT USE WITH OPIOIDS; ABUSE, MISUSE, AND ADDICTION; and DEPENDENCE AND WITHDRAWAL REACTIONS

Concomitant use of benzodiazepines and opioids may result in profound sedation, respiratory depression, coma, and death. Reserve concomitant prescribing of these drugs in patients for whom alternative treatment options are inadequate. Limit dosages and durations to the minimum required. Follow patients for signs and symptoms of respiratory depression and sedation [see Warnings and Precautions (5.1), Drug Interactions (7.1)].
The use of benzodiazepines, including triazolam, exposes users to risks of abuse, misuse, and addiction, which can lead to overdose or death. Abuse and misuse of benzodiazepines commonly involve concomitant use of other medications, alcohol, and/or illicit substances, which is associated with an increased frequency of serious adverse outcomes. Before prescribing triazolam and throughout treatment, assess each patient's risk for abuse, misuse, and addiction [see Warnings and Precautions (5.2)].
The continued use of benzodiazepines, including triazolam, may lead to clinically significant physical dependence. The risks of dependence and withdrawal increase with longer treatment duration and higher daily dose. Abrupt discontinuation or rapid dosage reduction of triazolam after continued use may precipitate acute withdrawal reactions, which can be life-threatening. To reduce the risk of withdrawal reactions, use a gradual taper to discontinue triazolam or reduce the dosage [see Dosage and Administration (2.3), Warnings and Precautions (5.3)].

WARNING: RISKS FROM CONCOMITANT USE WITH OPIOIDS; ABUSE, MISUSE, AND ADDICTION; and DEPENDENCE AND WITHDRAWAL REACTIONS

See full prescribing information for complete boxed warning.

Concomitant use of benzodiazepines and opioids may result in profound sedation, respiratory depression, coma, and death. Reserve concomitant prescribing of these drugs in patients for whom alternative treatment options are inadequate. Limit dosages and durations to the minimum required. Follow patients for signs and symptoms of respiratory depression and sedation (5.1, 7.1).
The use of benzodiazepines, including triazolam, exposes users to risks of abuse, misuse, and addiction, which can lead to overdose or death. Before prescribing triazolam and throughout treatment, assess each patient's risk for abuse, misuse, and addiction (5.2).
Abrupt discontinuation or rapid dosage reduction of triazolam after continued use may precipitate acute withdrawal reactions, which can be life-threatening. To reduce the risk of withdrawal reactions, use a gradual taper to discontinue triazolam or reduce the dosage (2.3, 5.3).

---

### 2022 ESC guidelines for the management of patients with ventricular arrhythmias and the prevention of sudden cardiac death [^9a594a67]. European Heart Journal (2022). High credibility.

Regarding medical management for myocarditis, more specifically with respect to management of cardiac arrhythmias, catheter ablation, ESC 2022 guidelines recommend to consider performing catheter ablation as an alternative to ICD therapy, after discussion with the patient and provided that established endpoints have been reached, in patients with hemodynamically well-tolerated sustained monomorphic VT occurring in the chronic phase of myocarditis, preserved LV function, and a limited scar amenable to ablation.

---

### A continuous-time maxSAT solver with high analog performance [^87d3d16d]. Nature Communications (2018). Medium credibility.

Algorithm description

Here, we give a simple, nonoptimized variant of the algorithm (see flowchart in Supplementary Fig. 2). Better implementations can be devised, for example with better fitting routines, however the description below is easier to follow and works well. Given a SAT problem, we first determine the b parameter as described previously. Step 1: initially we set, Γ min, = and t max. Unless specified otherwise, in our simulations we used Γ min = 100, Γ max = 2 × 10 6, t max = 50. Step 2: to initialize our statistics, we run Γ min trajectories up to t max, each from a random initial condition. For every such trajectory ω we update the p (E, t) distributions as function of the energies of the orthants visited by ω. We record the lowest energy value found. Step 3: starting from Γ = Γ min + 1 and up to Γ max, we continue running trajectories in the same way and for each one of them check: (a) If, set, update p (E, t) and go to Step 4. (b) If Γ just reached, go to Step 4. (c) If Γ = Γ max, output "Maximum number of steps reached, increase Γ max ", output the lowest energy value found, the predictedand the quality of fit for, then halt. Step 4: using the p (E, t) distributions, estimate the escape rates κ (E) as described in the corresponding Methods section. Step 5: the κ (E) curve is extrapolated to the E − 1 value obtaining κ (E − 1) and then using this we predict(as described in another Methods section). Further extrapolating the κ (E) curve to κ = 0 we obtain(see the corresponding Methods section). Step 6: we check the consistency of the prediction defined here as saturation of the predicted values. We call it consistent, ifhas not changed during the last 5 predictions. If it is not consistent yet, we continue running new trajectories (Step 4). If the prediction is consistent, we check for the following halting conditions: (i) Ifthen we decide the global optimum has been found:and skip to Step 7. (ii) If the fitting is consistently predicting(usually it is very close,) we check the number of trajectories that has attained states with, i.e. = . If it is large enough (e.g. > 100), we decide to stop running new trajectories and setand go to Step 7. (iii) Ifthen we most probably have not found the global optimum yet and we go to Step 4. We added additional stopping conditions that can shorten the algorithm in case of easy problems, see Methods corresponding section, but these are not so relevant. Step 7: the algorithm ends and outputs, values, the Boolean variables corresponding to the optimal state found, along with the quality of fit.

---

### Lateral elbow pain and muscle function impairments [^647af52d]. The Journal of Orthopaedic and Sports Physical Therapy (2022). High credibility.

Regarding diagnostic investigations for lateral elbow tendinopathy, more specifically with respect to clinical assessment, APTA 2022 guidelines recommend to use the PSFS in patients with high-demand activities and/or a scale assessing activity-specific disability (such as Disabilities of the Arm, Shoulder and Hand work or sports/performing arts module) at baseline and at least one other follow-up point including discharge in patients with LET.

---

### Human immunoglobulin g (Asceniv) [^fbca7ed9]. FDA (2025). Medium credibility.

NDC: 69800-0250-1 - Carton Label

---

### Human immunoglobulin g (Asceniv) [^636783f6]. FDA (2025). Medium credibility.

NDC: 69800-0250-2 - Vial Label

---

### Longitudinal plasma phosphorylated-tau217 and other related biomarkers in a non-demented Alzheimer's risk-enhanced sample [^8b8e6ce0]. Alzheimer's & Dementia (2024). Medium credibility.

2.9.1 Aim 1: Characterizing plasma biomarker trajectories in healthy controls (CU, A–)

To test the hypothesis that there is no age‐related trend and the modifiable factors are not associated with plasma biomarkers in presumably healthy controls, for each plasma biomarker z ‐score (p‐tau217; Aβ42/40, p‐tau181, p‐tau231, GFAP, and NfL) in the CU/A– subset (n = 226), we examined a series of mixed‐effects models (all models included a random intercept; random slopes were retained only when significant prior to adding fixed effects). Time was modeled as age in years, centered at age 60. After identifying significant random slopes, we ran a model (base model) with age (up to cubic polynomial, if significant) as the only fixed effect and removed non‐significant (NS), highest‐order age terms sequentially. Models 1–5 added one predictor (sex, APOE ‐npscore, BMI, CKD_EPI, LIBRA index) * age included to the base model; if the interaction was NS, it was removed — leaving the base model plus predictor. Model 6 incorporated significant main effects and interactions from Models 1–5; we sequentially removed NS interactions (least significant out first) until only significant interactions (and their supporting main effects) or significant main effects (if no corresponding interactions were significant) remained. We compared model fits of Models 1–6 with the base model Akaike's information criterion‐corrected (AICc) and with the model with lowest/best AICc; we report results for the best‐fitting model and any others with ∆AICc < 2 for each biomarker, as these models represent similarly adequate fit. The ∆ values for any given model are linked to the evidence ratio for the best model as exp{−(1/2)Δ}. For example, if a model in question has a Δ value of 11, its evidence ratio compared to the best model is ≈245. That is, the evidence is 245 times stronger for the best model relative to the model in question. People might often judge this evidence to be very strong. Other people might choose another word; however, both judgments are based on the same quantitative evidence, an evidence ratio of 245 to 1. AICc was calculated using the "AICcmodavg" R package. The methods and results about estimating within‐person variability are in the supplement.

---

### KDIGO 2025 clinical practice guideline for the evaluation, management, and treatment of autosomal dominant polycystic kidney disease (ADPKD) [^3af5f713]. Kidney International (2025). High credibility.

Regarding classification and risk stratification for autosomal dominant polycystic kidney disease, more specifically with respect to prognosis, KDIGO 2025 guidelines recommend to exclude patients with pathogenic variants in genes other than PKD1 or PKD2 (if genetic information is available) when using the Mayo Imaging Classification for prognostics, as the predictions are likely unreliable in these patients.

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^bb9b7f12]. Nature Communications (2025). High credibility.

After n refinement steps, we thus obtain a functiondefined at discrete points:which we extend to the entire interval [0, 1) by linear interpolation; see Fig. 5 d for an illustration. In practice we found that computations (specifically the risk computed by integrating) converge after about eight refinement steps.

This procedure has the important property that once a point is sampled, it does not change on further refinements:which follows from equation (67). Recall now that, as stated above, a process is self-consistent if "for small enough Δ Φ, the probability distribution at a point Φ [does] not depend on the level of refinement". Since equation (70) clearly satisfies that requirement, we see that the process obtained after infinitely many refinement steps is indeed self-consistent. We thus define the hierarchical beta (HB) process as

To complete the definition of, we need to specify how we choose the initial end pointsand. In our implementation, they are drawn from normal distributionswith Φ ∈ {0, 1}, where again c is determined via our proposed calibration procedure; this is simple and convenient, but otherwise arbitrary. We also need to explain how we choose the beta parameters α and β, which is the topic of the next subsection.

Choosing beta distribution parameters

All HB processes are monotone, continuous and self-consistent, but within this class there is still a lot of flexibility: since α and β are chosen independently for each subinterval, we can mouldinto a wide variety of statistical shapes. We use this flexibility to satisfy the two remaining desiderata: a) that realisationstrackover Φ ∈ [0, 1]; and b) that the variability ofbe proportional to. It is the goal of this subsection to give a precise mathematical meaning to those requirements.

Let x 1 ~ Beta(α, β) and x 2 = 1 − x 1. (The density function of a beta distribution is given in (24).) The mean and variance of x 1 areFor a given Φ, it may seem natural to select α and β by matchingtoandto. However both equations are tightly coupled, and we found that numerical solutions were unstable and unsatisfactory; in particular, it is not possible to make the variance large whenapproaches either 0 or 1 (otherwise the distribution of x 1 would exceed [0, 1]).

---

### Poorer elbow proprioception in patients with lateral epicondylitis than in healthy controls: a cross-sectional study [^01922be4]. Journal of Shoulder and Elbow Surgery (2007). Low credibility.

Two groups of women, 15 patients with lateral epicondylitis and 21 healthy controls, were studied to compare proprioception in the elbows and knees between the groups. Outcome measures were absolute error and variable error for joint position sense and for threshold to detection of a passive movement. Both absolute error and variable error of threshold to detection of a passive movement were greater in the lateral epicondylitis-diagnosed elbows than in the controls' elbows (lateral epicondylitis, 1.8 degrees vs controls 1.1 degrees, P = 0.026; lateral epicondylitis, 0.8 degrees vs controls 0.3 degrees, P = 0.015), and there was a tendency toward a greater absolute error of joint position sense compared with the control elbows (lateral epicondylitis, 8.2 degrees vs controls, 5.6 degrees; P = 0.078). Absolute error of joint position sense was greater in the elbows than in the knees of the lateral epicondylitis patients, but no group differences were found for knees. Proprioception seems, therefore, to be poorer in elbows with lateral epicondylitis elbows than in the controls' elbows. This needs to be taken into consideration in the management of lateral epicondylitis.

---

### EPISPOT: an epigenome-driven approach for detecting and interpreting hotspots in molecular QTL studies [^e6d03da5]. American Journal of Human Genetics (2021). Medium credibility.

Performance under varying degrees of epigenome involvement

Effectiveness in QTL mapping is subject to a number of interdependent factors pertaining to (1) the sparsity of the studied QTL network and magnitude of the QTL effects, (2) the amount of information contained in the data at hand, and (3) the ability of the statistical approach to interrogate the data, i.e. by both leveraging and being robust to the dependence structures within and across genetic variants and molecular traits. When it comes to exploiting the epigenome to enhance statistical power, an additional level of complexity is introduced for determining the impact of the above factors on the analysis, and new questions arise as to whether the signal present in the data is sufficient to inform inference on the location of the relevant epigenetic marks and of the QTL associations potentially triggered by these marks.

In the previous simulation experiment, we generated data under the simplifying assumption that all QTL associations were induced by the epigenome, and to a degree to which the relevant marks would be detectable, as evidenced by the high epi-PPIs for the active marks and the power gained from leveraging this signal (Figure 2). Here, we focus on evaluating how the level of involvement of the epigenome in QTL activity impacts the detection of QTL effects and of the marks responsible for these effects.

We consider a series of QTL problems, each generated by replicates of 32, for a grid of response numbers and degrees of involvement of the epigenome in activating QTL control. More precisely, we simulate data with a number of traits sampled from a Poisson distribution with mean, or 1,600 and 60 loci with 20 SNPs each and involving 100 active SNPs in total. We vary the proportion of active SNPs whose activity is triggered by epigenetic marks from p epi = 0 (all QTL associations simulated independently of the action of the epigenome) to p epi = 1 (all QTL associations simulated as the result of the action of the epigenome); see the supplemental material and methods for the data-generation details. The typical pleiotropic pattern simulated is displayed in Figure 3 for the different choices of p epi and problems with an average oftraits.

---

### Small-bowel capsule endoscopy and device-assisted enteroscopy for diagnosis and treatment of small-bowel disorders: European Society of Gastrointestinal Endoscopy (ESGE) guideline-update 2022 [^f91b0ff5]. Endoscopy (2023). High credibility.

Regarding surgical interventions for Peutz-Jeghers syndrome, more specifically with respect to indications for polypectomy, ESGE 2023 guidelines recommend to perform device-assisted enteroscopy with polypectomy in patients with large (> 15 mm) or symptomatic polyps discovered on imaging or during small bowel capsule endoscopy.

---

### European Society of Coloproctology guidelines for the management of pilonidal disease [^e826fc07]. The British Journal of Surgery (2024). High credibility.

Regarding surgical interventions for pilonidal disease, more specifically with respect to minimally invasive surgery, ESCP 2024 guidelines recommend to consider performing pit picking in patients with disease confined to pits, with or without a single lateral cavity.

---

### Validation of monte carlo estimates of three-class ideal observer operating points for normal data [^26b6e5d9]. Academic Radiology (2013). Low credibility.

Rationale and Objectives

Traditional two-class receiver operating characteristic (ROC) analysis is inadequate for the complete evaluation of observer performance in tasks with more than two classes.

Materials and Methods

Here, a Monte Carlo estimation method for operating point coordinates on a three-class ROC surface is developed and compared with analytically calculated coordinates in two special cases: (1) univariate and (2) restricted bivariate trinormal underlying data.

Results

In both cases, the statistical estimates were found to be good in the sense that the analytical values lay within the 95% confidence interval of the estimated values about 95% of the time.

Conclusions

The statistical estimation method should be key in the development of a pragmatic performance metric for evaluation of observers in classification tasks with three or more classes.

---

### Human botulinum neurotoxin a / b immune globulin (Babybig) [^31994fd9]. FDA (2024). Medium credibility.

10 OVERDOSAGE

Although limited data are available, clinical experience with other immunoglobulin preparations suggests that the major manifestations would be those related to volume overload [1]

---

### The management of antibodies in heart transplantation: an ISHLT consensus document [^140f76fb]. The Journal of Heart and Lung Transplantation (2018). Medium credibility.

Complement-fixing antibodies and C4d — prognostic and diagnostic associations: Examination of 100 endomyocardial biopsy (EMB) samples found that patients with DSA against Class I or both Class I and Class II HLA tended to have a positive C4d stain and were more likely to develop graft failure than those with DSA against Class II alone or no DSA; in pediatric series, C1q+ DSA within the first month post-transplant all developed AMR, and circulating C1q+ DSA predicted AMR with a positive predictive value of 87.5% and negative predictive value of 100%, while IgG+C1q− DSA were associated with acute cellular rejection (ACR) (HR 8.8, p < 0.001) and IgG+C1q+ antibodies were associated with AMR (HR 11.6, p < 0.001).

---

### ESPEN guideline on nutrition and hydration in dementia-update 2024 [^f03d0fa8]. Clinical Nutrition (2024). High credibility.

Regarding diagnostic investigations for dementia, more specifically with respect to screening for malnutrition, ESPEN 2024 guidelines recommend to assess and take into account individual habits, preferences, abilities, and behaviors around eating and drinking in all patients with dementia.

---

### European Academy of Neurology / Peripheral Nerve Society guideline on diagnosis and treatment of chronic inflammatory demyelinating polyradiculoneuropathy: report of a joint task force-second revision [^34cd31d1]. European Journal of Neurology (2021). High credibility.

Regarding diagnostic investigations for chronic inflammatory demyelinating polyneuropathy, more specifically with respect to laboratory tests, EAN/PNS 2021 guidelines recommend to obtain a confirmatory test with ELISA (using human recombinant proteins) or teased-nerve immunohistochemistry after nodal and paranodal autoantibody testing. Recognize that the order of assays can be interchanged. Apply additional confirmatory tests to the protocol for low-titer sera or dubious staining on the cell-based assay to avoid false positives.

---

### Desogestrel and ethinyl estradiol and ethinyl estradiol (bekyree) [^d6dccf8b]. FDA (2023). Medium credibility.

DAY 1 START

Counting the first day of menstruation as "Day 1", tablets are taken without interruption as follows: One white tablet daily for 21 days, one green (inert) tablet daily for 2 days followed by 1 yellow (ethinyl estradiol) tablet daily for 5 days. For all subsequent cycles, the patient then begins a new 28 tablet regimen on the next day after taking the last yellow tablet. [If switching directly from another oral contraceptive, the first white tablet should be taken on the first day of menstruation which begins after the last ACTIVE tablet of the previous product.]

If a patient misses 1 white tablet, she should take the missed tablet as soon as she remembers. If the patient misses 2 consecutive white tablets in Week 1 or Week 2, the patient should take 2 tablets the day she remembers and 2 tablets the next day; thereafter, the patient should resume taking 1 tablet daily until she finishes the cycle pack. The patient should be instructed to use a back-up method of birth control if she has intercourse in the 7 days after missing pills. If the patient misses 2 consecutive white tablets in the third week or if the patient misses 3 or more white tablets in a row at any time during the cycle, the patient should throw out the rest of that cycle pack and start a new cycle pack that same day. The patient should be instructed to use a back-up method of birth control if she has intercourse in the 7 days after missing pills.

ALL ORAL CONTRACEPTIVES

Breakthrough bleeding, spotting, and amenorrhea are frequent reasons for patients discontinuing oral contraceptives. In breakthrough bleeding, as in all cases of irregular bleeding from the vagina, non-functional causes should be borne in mind. In undiagnosed persistent or recurrent abnormal bleeding from the vagina, adequate diagnostic measures are indicated to rule out pregnancy or malignancy. If both pregnancy and pathology have been excluded, time or a change to another preparation may solve the problem. Changing to an oral contraceptive with a higher estrogen content, while potentially useful in minimizing menstrual irregularity, should be done only if necessary since this may increase the risk of thromboembolic disease.

Use of oral contraceptives in the event of a missed menstrual period:

If the patient has not adhered to the prescribed schedule, the possibility of pregnancy should be considered at the time of the first missed period and oral contraceptive use should be discontinued until pregnancy is ruled out.
If the patient has adhered to the prescribed regimen and misses two consecutive periods, pregnancy should be ruled out before continuing oral contraceptive use.

---

### Cost-effectiveness modelling to optimise active screening strategy for gambiense human African trypanosomiasis in endemic areas of the democratic republic of Congo [^1038f5ac]. BMC Medicine (2021). Medium credibility.

The impact of WTP and p t on optimal values of screening coverage and interval.

When assumptions about WTP and p t are fixed, the screening coverage has a greater impact on whether the NMB is positive than the screening interval. Low screening coverage levels (< 20%) can be insufficient to obtain a positive NMB, whilst the screening interval does not change the sign of the NMB for most coverage levels, unless the interval is very small (< 0.5 years). The question of what value to fix for WTP and p t has important implications on the optimal choice of screening coverage and optimal screening interval.

First, the optimal screening interval will be approximately one year under all assumptions of passive surveillance treatment coverage (p t) and WTP (yellow dots in all panels of Fig. 2). For W T P c = 0.5 and p t = 27% (our standard assumption), the maximum mean NMB is found when the screening coverage is 90% and the screening interval is 0.67 years (Fig. 2; the yellow dot in top centre panel). For W T P c = 0.5 and p t = 100%, the maximum mean NMB is also at the maximum screening coverage, but the higher treatment coverage (p t) indicates that the optimal screening interval is of 1.25 years. The optimal screening interval is the same for all values of the number of zero-detections, but the minimum NMB was found at z a = 1 and z r = 1 (see accompanying app). It is notable that a very high WTP (3 times the GDP per capita) lends strong support for shorter screening intervals, favouring screenings in a village multiple times a year.

---

### Looking further when symptoms are disproportionate to physical findings [^3cde67c1]. Medicine and Science in Sports and Exercise (2019). Medium credibility.

A 36-yr-old man presents with left ulnar-side wrist pain during an ice hockey game that prevented him from playing hockey or golf. There was no acute mechanism of injury. The usual clinical examination revealed only minimal tenderness and minimal pain with resisted wrist extension. Careful attention to precipitating factors led to testing resisted wrist extension with the forearm fully supinated, which reliably reproduced the intensity of the patient's symptoms, and a diagnosis of extensor carpi ulnaris tendinopathy. A literature review suggested three additional special tests (two were positive) and management. However, a standard of care has yet been established because neither the tests nor the management has been properly validated through research, thus leaving the management of this condition as an art.

---

### Evaluation of suspected dementia [^69f287ec]. American Family Physician (2018). Medium credibility.

Regarding diagnostic investigations for mild cognitive impairment, more specifically with respect to initial assessment, AAFP 2018 guidelines recommend to use the Mini-Cog, the General Practitioner Assessment of Cognition, or the Ascertain Dementia 8-Item Informant Questionnaire in patients with suspected dementia to determine the need for further evaluation.

---

### Lateral elbow pain and muscle function impairments [^1e9286ec]. The Journal of Orthopaedic and Sports Physical Therapy (2022). High credibility.

Regarding diagnostic investigations for lateral elbow tendinopathy, more specifically with respect to clinical assessment, APTA 2022 guidelines recommend to use the diagnosis-specific PRTEE to assess pain/irritability and function and/or the region-specific Disabilities of the Arm, Shoulder and Hand to assess upper extremity function at baseline and at least one other follow-up point including discharge in patients with LET.

---

### How do you design randomised trials for smaller populations? A framework [^45132483]. BMC Medicine (2016). Low credibility.

How should we approach trial design when we can get some, but not all, of the way to the numbers required for a randomised phase III trial?We present an ordered framework for designing randomised trials to address the problem when the ideal sample size is considered larger than the number of participants that can be recruited in a reasonable time frame. Staying with the frequentist approach that is well accepted and understood in large trials, we propose a framework that includes small alterations to the design parameters. These aim to increase the numbers achievable and also potentially reduce the sample size target. The first step should always be to attempt to extend collaborations, consider broadening eligibility criteria and increase the accrual time or follow-up time. The second set of ordered considerations are the choice of research arm, outcome measures, power and target effect. If the revised design is still not feasible, in the third step we propose moving from two- to one-sided significance tests, changing the type I error rate, using covariate information at the design stage, re-randomising patients and borrowing external information. We discuss the benefits of some of these possible changes and warn against others. We illustrate, with a worked example based on the Euramos-1 trial, the application of this framework in designing a trial that is feasible, while still providing a good evidence base to evaluate a research treatment. This framework would allow appropriate evaluation of treatments when large-scale phase III trials are not possible, but where the need for high-quality randomised data is as pressing as it is for common diseases.

---

### Cycles of goal silencing and reactivation underlie complex problem-solving in primate frontal and parietal cortex [^c21270fb]. Nature Communications (2023). High credibility.

There is a possibility that this difference in classification accuracies came from the difference in trial sampling methods; in the between-phase analysis, training and test trials came from different cycles while in the within-phase analysis, both training and test trials came from cycles 2–4. To exclude this possibility, we repeated the within-phase analysis with a modified trial sampling method. We first used cycle 2 to construct a classifier, and tested this classifier by using trials in cycles 3–4 to compute classification accuracy. We then repeated this procedure for the remaining two grouping patterns: cycle 3 vs. cycles 2 and 4; cycle 4 vs. cycles 2–3. The mean of the three classification results was regarded as the classification accuracy of the between-phase condition. With this procedure (Supplementary Fig. S 7), we observed an almost identical result to the original version of this analysis, ruling out the possibility that the observed difference in classification accuracies between the two analyses in Fig. 7c can be attributed to the difference in trial sampling methods.

Compared to one-target problems, accordingly, two-target problems showed little evidence for a peak of location coding at the start of memory-guided trials. As they did in the one-target problems, however, location representations changed in form between search and memory-guided phases. Again the data suggest that, at the start of each memory-guided trial, there was reconstruction of a retrieved target code.

---

### Asymmetric projection of introspection reveals a behavioural and neural mechanism for interindividual social coordination [^591cc868]. Nature Communications (2025). High credibility.

The size of the regression coefficient for the probability of correct performance on the self-task option (self p(correct)) reflects the influence the participant's estimate of their own probability of performing the trial correctly has on their task selection: whether they choose to tackle a problem or whether they choose for the partner to tackle a problem. This estimate had a similar impact on decisions taken by the participants when they were deciding between themselves and the good partner or between themselves and the poor partner; self p(correct) had a similar influence on decisions involving either the good or poor partner (t 25 = 0.056, p = 0.95). In a complementary fashion, the influence of the probability that the partner would be correct (partner p(correct)) on whether or not the participant should choose to perform their task should be negative and, as noted, this was indeed the case. However, remarkably, the size of the negative regression coefficient for partner p(correct) was significantly larger for the poor partner than for the good partner (t 25 = 2.75, p = 0.010). Thus, differences between good and poor partner trials are not general in nature and they are not related to a difference in how participants take their own potential performance into consideration. Instead, differences in poor and good partner trials are specifically related to how participants take into consideration poor partner performance versus good partner performance.

---

### AGA living clinical practice guideline on computer-aided detection-assisted colonoscopy [^4cc01f28]. Gastroenterology (2025). High credibility.

AGA CADe-assisted colonoscopy — clinical question and prioritized outcomes are specified for patients undergoing colonoscopy in screening, surveillance, and follow-up for positive fecal immunochemical test, asking whether CADe-assisted colonoscopy vs standard colonoscopy should be used; the panel rated critical outcomes as CRC-related mortality, CRC incidence, and PCCRC incidence, deemed important (but not critical) outcomes including ADR, advanced ADR, sessile serrated lesions detection rate, adenoma per colonoscopy, adenoma miss rate, polypectomies of nonadenomatous polyps per colonoscopy, withdrawal time (inspection time), number of colonoscopies per lifetime, perforations, and bleeding events, and used predefined thresholds for the minimum clinically important difference to aid the certainty of evidence assessment determined by clinical experts.

---

### Neck pain: revision 2017 [^eb6d8f80]. The Journal of Orthopaedic and Sports Physical Therapy (2017). Medium credibility.

Regarding diagnostic investigations for neck pain, more specifically with respect to initial assessment, APTA 2017 guidelines recommend to use easily reproducible activity limitation and participation restriction measures associated with the neck pain to assess the changes in the level of function over the episode of care.

---

### Guideline no. 454: identification and treatment of perinatal mood and anxiety disorders [^24bceb1c]. Journal of Obstetrics and Gynaecology Canada (2024). High credibility.

Regarding screening and diagnosis for peripartum depression, more specifically with respect to screening tools, SOGC 2024 guidelines recommend to use a validated screening tool such as the EPDS, Personalized Health Questionnaire-9, or GAD-7.

---

### Diagnosis, management and treatment of the Alport syndrome-2024 guideline on behalf of ERKNet, ERA and ESPN [^c59a118f]. Nephrology, Dialysis, Transplantation (2025). High credibility.

Regarding diagnostic procedures for Alport syndrome, more specifically with respect to kidney biopsy, ERA/ERN ERKNet/ESPN 2025 guidelines recommend to avoid performing a kidney biopsy in patients with a genetic diagnosis of X-linked Alport syndrome or autosomal recessive Alport syndrome.

---

### Chapter 9: options for summarizing medical test performance in the absence of a "gold standard" [^4e0871e5]. Journal of General Internal Medicine (2012). Low credibility.

The classical paradigm for evaluating test performance compares the results of an index test with a reference test. When the reference test does not mirror the "truth" adequately well (e.g. is an "imperfect" reference standard), the typical ("naïve") estimates of sensitivity and specificity are biased. One has at least four options when performing a systematic review of test performance when the reference standard is "imperfect": (a) to forgo the classical paradigm and assess the index test's ability to predict patient relevant outcomes instead of test accuracy (i.e., treat the index test as a predictive instrument); (b) to assess whether the results of the two tests (index and reference) agree or disagree (i.e., treat them as two alternative measurement methods); (c) to calculate "naïve" estimates of the index test's sensitivity and specificity from each study included in the review and discuss in which direction they are biased; (d) mathematically adjust the "naïve" estimates of sensitivity and specificity of the index test to account for the imperfect reference standard. We discuss these options and illustrate some of them through examples.

---

### Lateral epicondylitis: a review of pathology and management [^09683eb7]. The Bone & Joint Journal (2013). Low credibility.

Lateral epicondylitis, or 'tennis elbow', is a common condition that usually affects patients between 35 and 55 years of age. It is generally self-limiting, but in some patients it may continue to cause persistent symptoms, which can be refractory to treatment. This review discusses the mechanism of disease, symptoms and signs, investigations, current management protocols and potential new treatments.

---

### Complementary task representations in hippocampus and prefrontal cortex for generalizing the structure of problems [^43bd6893]. Nature Neuroscience (2022). High credibility.

The cellular modes are the columns of U, and the temporal modes are the rows of V T. Both modes are unit vectors, so the contribution of each pair to the total data variance is determined by the corresponding element of the diagonal matrix Σ. The modes are sorted in order of explained variance, such that the first cellular and temporal mode pair explains the most variance. The first cellular and temporal mode of PFC activity in three different problems is shown in Fig. 5b, c. It is high throughout the inter-trial interval (ITI) and trial, with a peak at choice time but strongly suppressed after reward (similar to cell 5 in Fig. 2d).

We reasoned that (1) if the same events were represented across problems (for example, initiation, A/B choice and outcome), then the temporal modes would be exchangeable between problems, no matter whether these representations were found in the same cells; (2) if the same cell assemblies were used across problems, then the cellular modes would be exchangeable across problems, no matter whether the cell assemblies played the same role in each problem; and (3) if the same cell assemblies performed the same roles in each problem, then pairs of cellular and temporal modes would be exchangeable across problems.

To see whether the same representations existed in each problem, we first asked how well the temporal modes from one problem could be used to explain activity from other problems. Because the set of temporal modes V is an orthonormal basis, any data of the same rank or less can be perfectly explained when using all the temporal modes. However, population activity in each problem is low dimensional, so a small number of modes explain a great majority of the variance. Modes that explain a lot of variance in one problem will explain a lot of variance in the other problem only if the structure captured by the mode is prominent in both problems. The question is, therefore, how quickly variance is explained in problem 2ʼs data, when using the modes from problem 1 ordered according to their variance explained in problem 1. To assess this, we projected the data matrix D 2 from problem 2 onto the temporal modes V 1 from problem 1, giving a matrix M V whose elements indicate how strongly each temporal mode contributes to the problem 2 activity of each neuron:

---

### Palliative care best practices guidelines [^e0012b78]. ACS (2017). Medium credibility.

Palliative care — time-limited trial (TLT) initiation and team roles are outlined as follows: To proceed with a TLT, initiate the following steps: Define and communicate the patient's clinical problems and prognoses; Clarify the patient's personal values, goals of care, and quality of life priorities; Identify realistic objective markers that constitute clinical improvement or deterioration; Suggest and agree upon a time frame for reassessment and reevaluation; and Define clear expectations and the plan of action at the end of the TLT. Ideally, one or two providers take ownership of the TLT process to ensure smooth communication and build trust with the family, and the palliative care team can be effective in facilitating the TLT discussion and providing additional expertise in communication skills and bereavement support for families and providers.

---

### Refractive errors preferred practice pattern ® [^494625e1]. Ophthalmology (2023). High credibility.

Refractive Errors Preferred Practice Pattern — SIGN evidence levels for individual studies describe graded study designs and bias risk: To rate individual studies, a scale based on SIGN is used. I++: High-quality meta-analyses, systematic reviews of randomized controlled trials (RCTs), or RCTs with a very low risk of bias; I+: Well-conducted meta-analyses, systematic reviews of RCTs, or RCTs with a low risk of bias; I–: Meta-analyses, systematic reviews of RCTs, or RCTs with a high risk of bias; II++: High-quality case-control or cohort studies with a very low risk of confounding or bias and a high probability that the relationship is causal; II+: Well-conducted case-control or cohort studies with a low risk of confounding or bias and a moderate probability that the relationship is causal; II–: Case-control or cohort studies with a high risk of confounding or bias and a significant risk that the relationship is not causal; III: Nonanalytic studies (e.g., case reports, case series).

---

### KDIGO 2025 clinical practice guideline for the evaluation, management, and treatment of autosomal dominant polycystic kidney disease (ADPKD) [^8f87db10]. Kidney International (2025). High credibility.

Regarding classification and risk stratification for autosomal dominant polycystic kidney disease, more specifically with respect to prognosis, KDIGO 2025 guidelines recommend to use the Predicting Renal Outcome in Polycystic Kidney Disease score to aid in the identification of patients with rapidly progressive disease.

---