# Medical Question & Answer

**Sample ID**: 7f3b5c1b-a1e1-b057-3a1f-0e699c85cabf
**Dataset Index**: 166205

---

## Question

24. Let $a, b, c, d$ be 4 distinct nonzero integers such that $a+b+c+d = 0$ and the number $M = (b c-a d)(a c-b d)(a b-c d)$ lies strictly between 96100 and 98000. Determine the value of $M$.

---

## Answer

> Let's see… What do we have here? The user is asking to determine the value of M, where M is defined as (bc − ad)(ac − bd)(ab − cd), given that a, b, c, d are four distinct nonzero integers with a + b + c + d = 0 and 96100 < M < 98000. Let's break this down step-by-step. First, I need to think about how to simplify the expression for M using the constraint a + b + c + d = 0. Then, I should verify whether M is invariant under permutations of the variables. Next, I will try to factor or rewrite M in a way that exposes its structure and possible integer values. After that, I should estimate the cube root of the target range to narrow candidate values. Finally, I will test plausible integer assignments to find the exact value of M that satisfies the inequality.

> Let me first confirm the constraint and the expression. We have four distinct nonzero integers a, b, c, d such that a + b + c + d = 0, and M = (bc − ad)(ac − bd)(ab − cd) with 96100 < M < 98000. I need to ensure that any algebraic manipulation respects the distinctness and nonzeroness of the variables, and that I do not introduce extraneous assumptions about ordering or sign.

> Now, I will examine whether M is symmetric under permutations. Wait, let me verify this carefully. If I swap a and b, the factors become (ac − bd)(bc − ad)(ab − cd), which is the same set of factors up to order, so M is invariant under swapping a and b. By symmetry, M is invariant under any permutation of a, b, c, d, which means I can reorder the variables to simplify calculations without changing M. This symmetry will be useful for testing cases.

> Next, I should try to simplify the expression using a + b + c + d = 0. Let me think about substituting d = −(a + b + c) into each factor. For bc − ad, that becomes bc − a(−a − b − c) = bc + a² + ab + ac = a² + ab + ac + bc = (a + b)(a + c). Similarly, ac − bd becomes ac − b(−a − b − c) = ac + ab + b² + bc = ab + ac + b² + bc = (a + b)(b + c). And ab − cd becomes ab − c(−a − b − c) = ab + ac + bc + c² = (a + c)(b + c). So M = (a + b)(a + c) · (a + b)(b + c) · (a + c)(b + c) = (a + b)²(a + c)²(b + c)². Hold on, I should verify that algebra: yes, each factor pairs up, giving the square of pairwise sums. Thus, M = [(a + b)(a + c)(b + c)]².

> I should confirm that this squared form is consistent with the invariance I noted earlier. Since M is the square of a product of pairwise sums, and permutations of a, b, c, d only reorder those sums, the square remains unchanged, which aligns with the symmetry observation. This also tells me that M is a perfect square, which will constrain the possible values in the target range.

> Now, I need to estimate the magnitude. Let me compute the cube root of the interval endpoints to get a rough sense of the pairwise sums. The cube root of 96100 is about 45.8, and the cube root of 98000 is about 46.1, so the product (a + b)(a + c)(b + c) should be around 46. Given that M is the square of that product, I should look for integers near 46 whose square falls in (96100, 98000). Squaring 46 gives 2116, which is too small; wait, I should double-check my logic here. I initially thought the product itself might be near 46, but that would make M near 2116, which is far below 96100. I need the product (a + b)(a + c)(b + c) to be near 310, because 310² = 96100 and 313² = 97969, which sits just under 98000. So the product should be about 310 to 313.

> Let me consider plausible integer assignments. Because M is a perfect square and 96100 = 310² and 97969 = 313², the candidate values for M in the open interval are 311², 312², and 313². I should confirm which of these squares lie strictly between 96100 and 98000: 311² = 96721, 312² = 97344, and 313² = 97969, all of which satisfy the inequality. So M must be one of 96721, 97344, or 97969.

> Next, I should review whether these squared values are achievable as [(a + b)(a + c)(b + c)]² for distinct nonzero integers with a + b + c + d = 0. Let me try the simplest case where the pairwise sums are consecutive integers. If (a + b)(a + c)(b + c) = 311, I need to check whether 311 can be written as a product of three integers that could plausibly arise from pairwise sums of four distinct integers. But 311 is prime, so the only factorizations are 1 × 1 × 311 or signed versions, which are incompatible with the structure of pairwise sums from four distinct integers. So 311² is unlikely to be realizable. If the product is 312, I can factor 312 as 4 × 6 × 13, which looks promising. If the product is 313, that is also prime, so the same concern applies. Therefore, 312² = 97344 is the most plausible candidate, and I should verify that 4, 6, 13 can indeed be realized as pairwise sums.

> I will now examine whether 4, 6, 13 can be the pairwise sums (a + b), (a + c), (b + c) for some distinct nonzero integers a, b, c, d with a + b + c + d = 0. Let me set a + b = 4, a + c = 6, and b + c = 13. Solving these equations gives a = −1.5, b = 5.5, and c = 7.5, which are not integers. Hmm, wait a minute, I should not jump to conclusions; I can adjust the signs and ordering. Let me try a + b = −4, a + c = −6, and b + c = −13. Then a = 1.5, b = −5.5, c = −7.5, still not integers. I need integer solutions, so I should double-check the factorization and consider alternative trios. Let me reconsider: 312 can also be factored as 2 × 12 × 13, or 3 × 8 × 13, or (−2) × (−12) × 13, etc. Trying 2, 12, 13: solving a + b = 2, a + c = 12, b + c = 13 yields a = 0.5, b = 1.5, c = 11.5, again non-integer. Trying 3, 8, 13: a + b = 3, a + c = 8, b + c = 13 gives a = −1, b = 4, c = 9, which are integers. Let me verify: a = −1, b = 4, c = 9 implies d = −(a + b + c) = −12, so the four numbers are −1, 4, 9, −12, which are distinct and nonzero, and the pairwise sums are a + b = 3, a + c = 8, b + c = 13, whose product is 3 × 8 × 13 = 312. Therefore, M = 312² = 97344, which lies in the required interval.

> Hold on, I should verify that no other candidate square in the range is achievable. Given that 311 and 313 are prime, their only integer factorizations into three factors are trivial and cannot correspond to pairwise sums from four distinct integers, so 96721 and 97969 are not realizable. Thus, 97344 is the unique value of M that satisfies all conditions.

---

The value of (M) is **97344**. This is the only perfect square in the range (96100 < M < 98000) that can be expressed as ([(a+b)(a+c)(b+c)]^2) with distinct nonzero integers (a, b, c, d) satisfying (a+b+c+d = 0). The unique solution is ((a, b, c, d) = (-1, 4, 9, -12)), which yields (M = 97344).

---

## Algebraic simplification of M

Given (a + b + c + d = 0), we can substitute (d = -(a + b + c)) into (M):

[
M = (bc - ad)(ac - bd)(ab - cd)
]

Substitute (d = -(a + b + c)):

[
M = (bc - a(-a - b - c))(ac - b(-a - b - c))(ab - c(-a - b - c))
]

Simplify each factor:

[
bc - a(-a - b - c) = bc + a^2 + ab + ac = (a + b)(a + c)
]

[
ac - b(-a - b - c) = ac + ab + b^2 + bc = (a + b)(b + c)
]

[
ab - c(-a - b - c) = ab + ac + bc + c^2 = (a + c)(b + c)
]

Thus:

[
M = (a + b)(a + c)(b + c) \\cdot (a + b)(a + c)(b + c) = [(a + b)(a + c)(b + c)]^2
]

So (M) is a perfect square, and we need to find distinct nonzero integers (a, b, c, d) such that ([(a + b)(a + c)(b + c)]^2) lies strictly between 96100 and 98000.

---

## Identifying perfect squares in the given range

The perfect squares between 96100 and 98000 are:

[
311^2 = 96721, \\quad 312^2 = 97344, \\quad 313^2 = 97969
]

We need to check which of these can be written as ([(a + b)(a + c)(b + c)]^2) with distinct nonzero integers (a, b, c, d) and (a + b + c + d = 0).

---

## Testing candidate values

### Candidate 1: (M = 96721)

If (M = 96721), then ((a + b)(a + c)(b + c) = 311) or (-311). Since 311 is prime, the only integer factorizations are (1 \\times 1 \\times 311) or signed versions, which cannot yield distinct pairwise sums from four distinct integers. Thus, (M = 96721) is not achievable.

---

### Candidate 2: (M = 97344)

If (M = 97344), then ((a + b)(a + c)(b + c) = 312) or (-312). We look for integer factorizations of 312 into three distinct factors that can be realized as pairwise sums. One valid factorization is (3 \\times 8 \\times 13):

Set:

[
a + b = 3, \\quad a + c = 8, \\quad b + c = 13
]

Solve for (a, b, c):

[
a = -1, \\quad b = 4, \\quad c = 9
]

Then (d = -(a + b + c) = -12), giving the set ((-1, 4, 9, -12)), which consists of distinct nonzero integers. The pairwise sums are (3, 8, 13), so:

[
M = (3 \\times 8 \\times 13)^2 = 312^2 = 97344
]

This satisfies all conditions.

---

### Candidate 3: (M = 97969)

If (M = 97969), then ((a + b)(a + c)(b + c) = 313) or (-313). Since 313 is prime, the only integer factorizations are (1 \\times 1 \\times 313) or signed versions, which cannot yield distinct pairwise sums from four distinct integers. Thus, (M = 97969) is not achievable.

---

## Conclusion

The only valid value of (M) in the specified range is **97344**, realized by ((a, b, c, d) = (-1, 4, 9, -12)).

---

## References

### Value computations underpin flexible emotion expression [^60d522d5]. Communications Psychology (2025). Medium credibility.

The model then assumes that decision-makers accumulate evidence over time for each option from zero towards their respective thresholds, where each sample of evidence is corrupted by some noise, ε t, from a random normal distribution, N(0,1). Choice and response time on a trial is determined by which of the accumulators crosses its threshold first and the time it takes to cross that threshold plus a non-decision time (t0) accounting for pre- and post-decision perceptual and motor processes. Thus, the model contains 17 free parameters that include value-constants (C) and three weights (w) on $Self, $Other, and $Ineq for each of the three non-neutral emotion responses (e.g. C anger, w $self:anger, w $other:anger, w $ineq:anger, C happiness, w $self:happiness, w $other:happiness, w $ineq:happiness, etc.), four thresholds (b): each associated with one emotion response option (b neutral, b anger, b happiness, b sadness) and a single non-decision time (t 0). A model recovery exercise of 1,000 simulated participants based on the task stimuli revealed that this 17-parameter model is identifiable and well recovered by our fitting procedure. Across all parameters, the maximum a posteriori estimates of correlations between the generative and recovered parameters ranged from 0.704 to 0.888 (all 95% HDIs > 0.675, Supplementary Note S3). We note that the differentiability of the value-constants (C) from the corresponding thresholds (b) rely on their distinct influence over response times. Although increasing/decreasing C and decreasing/increasing b both similarly increases the frequency of choosing the corresponding response options, these response times exhibit a non-monotonic inverse u-shaped relationship with changes in C whereas they consistently increase as b increases. While increasingly positive values of C results in faster response times, more and more negative values of C also result in faster response times as negative evidence acts as evidence for the other options in our model and faster responses times when choosing the other alternatives.

---

### Viral plasticity facilitates host diversity in challenging environments [^2cdd3300]. Nature Communications (2024). High credibility.

Viral traits and tradeoffs

As a viral evolutionary response, we focused on increases in the versatility of the viral tail fiber to recognize and attach to diverse forms of the focal receptor (Fig. 1 a), a viral counter-defense response that has been shown to emerge shortly after hosts evolve modifications to their receptors.

Viral phenotypes were here characterized by the set of receptor forms that they could recognize. Thus, we assigned to each phenotype an array r of R = 10 possible integers between 1 and R, with each integer representing forms of the receptor (i.e. values of a) that the virus could recognize. Specifically, a viral phenotype j was able to target a host phenotype i if d i ∈ r j, where d i is the integer resulting from mapping a i into 1. R :and:Therefore, with the parametrization chosen here (see Supplementary Table 1 in SI), a viral phenotype with a value r 0 = 5 within its r array would be able to target hosts whose a is within [0.889, 1.111], which encompasses not only the pristine form of the receptor a 0 = 1 but also slightly modified versions of it (Fig. S2). The latter, necessary from a technical standpoint because a is represented using real numbers, reflects the fact that only after evolution has modified a receptor sufficiently does the tail fiber lose the ability to recognize and attach to it. Virus phenotypes thus differed only if the list of distinct integers within their r array differed (see Fig. S2). The versatility of phenotype j was defined as the number of distinct integers within.

---

### 2025 AHA / ACC statement on cost / value methodology in clinical practice guidelines (update from 2014 statement): a report of the American college of cardiology / American Heart Association joint committee on clinical practice guidelines [^68cfa501]. Journal of the American College of Cardiology (2025). High credibility.

Cost-effectiveness thresholds — framework and updates explain that the 2014 ACC/AHA statement provided per–capita GDP–based thresholds such that an intervention with an ICER < $50 000 per QALY gained should be considered high value, $50 000 to < $150 000 (1-3x per capita GDP) per QALY gained should be considered intermediate value, and ≥ $150 000 per QALY gained should be considered low value. The World Health Organization's program suggested that costs less than 3 times the national annual GDP per capita per disability-adjusted life year averted should be considered cost-effective, whereas costs less than 1 times should be considered highly cost-effective; however, experts have argued this approach lacks theoretical justification, and World Health Organization experts now also advise against the use of per capita GDP-based cost effectiveness thresholds and instead suggest considering cost-effectiveness information alongside other contextual information in a transparent multicriterion decision process. The writing committee reviewed alternative approaches, including relying on US-based studies; for example, a health opportunity cost analysis estimated that at 2019 prices, for every $1000000 increase in health care expenditures 1860 persons were projected to become uninsured with 81 QALYs lost due to mortality and 15 QALYs lost due to morbidity, implying a cost-effectiveness threshold of $104 000 per QALY ($51 000 to $209 000 per QALY) in 2019 US dollars, and in the probabilistic analysis 40% of the simulations suggested the threshold was less than $100 000 per QALY.

---

### A framework for evaluating clinical artificial intelligence systems without ground-truth annotations [^ba188de5]. Nature Communications (2024). High credibility.

Producing reliability-completeness curves

The completeness of a variable (the proportion of missing values that are inferred) is equally important as the reliability of the predictions that are being made by a model. However, these two goals of data completeness and data reliability are typically at odds with one another. Quantifying this trade-off confers a twofold benefit. It allows researchers to identify the level of reliability that would be expected when striving for a certain level of data completeness. Moreover, it allows for model selection, where preferred models are those that achieve a higher degree of reliability for the same level of completeness. To quantify this trade-off, we needed to quantify the reliability of predictions without ground-truth labels and their completeness. We outline how to do so next.

Quantify reliability

SUDO reflects the degree of class contamination within a probability interval. The higher the absolute value of SUDO, the lower the degree of class contamination. Given a set of low probability thresholds, α ∈ A, and high probability thresholds, β ∈ B, we can make predictionsof the following form, To calculate the reliability R A, B of such predictions, we could average the absolute values of SUDO for the set of probability thresholds (A, B).

Quantify completeness

By identifying the maximum probability threshold in the set, A, and the minimum probability threshold in the set, B, the completeness, C A, B ∈ [0, 1], can be defined as the fraction of all data points in the wild, M, that falls within this range of probabilities.

Generate reliability-completeness curve

After iterating over K sets of A and B, we can populate the reliability-completeness (RC) curve for a particular model of interest (see Fig. 2 e). From this curve, we derive the area under the reliability-completeness curve, or the AURCC ∈ [0, 1]. Whereas the area under the receiver operating characteristic curve (AUROC) summarises the performance of a model when deployed on labelled instances, the AURCC does so on unlabelled data points. Given this capability, the AURCC can also be used to compare the performance of different models.

Reporting summary

Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article.

---

### 2025 AHA / ACC statement on cost / value methodology in clinical practice guidelines (update from 2014 statement): a report of the American college of cardiology / American Heart Association joint committee on clinical practice guidelines [^943345bf]. Journal of the American College of Cardiology (2025). High credibility.

Figure 3 — health opportunity cost approach to US cost-effectiveness thresholds reports 50000 simulations with varying input values, with 23902/50000 (48%) thresholds < $100000/QALY and 7006/50000 (14%) thresholds > $150000/QALY; the base-case estimate is $104,000/QALY in 2019 US dollars with a (95% UI, $51,000–$209,000/QALY), and updating to 2024 US dollars suggests a cost-effectiveness threshold of $117000 per QALY.

---

### Emergent long-range synchronization of oscillating ecological populations without external forcing described by ising universality [^bf0399ad]. Nature Communications (2015). Medium credibility.

To gather some intuition for the emergence of an Ising critical transition in the synchronization order parameter, we first consider a population in which density, x t, is regulated by the deterministic Ricker map. This is a limiting case of the Ricker model. The limiting value of the two-cycle amplitude, as defined in equation (9), is

and the limiting value of the synchronization order parameter, as defined in equation (11), is the average value of m t in the asymptotic regime. For growth rates in the steady-state range, the asymptotic population density is a constant, x t = x *, and the order parameter vanishes, m = 0. The asymptotic system is clearly invariant under time translations, that is, t → t + n for any integer n. For growth rates in the two-cycle range, the asymptotic population density oscillates between two distinct values, and. The order parameter takes on one of two possible nonzero values, depending on the temporal phase of the two-cycle oscillation. The asymptotic system remains invariant to even-period, but not odd-period, time translations. Therefore, in the deterministic limit, increasing the growth rate from the steady state to the two-cycle range breaks a twofold, or, symmetry of the asymptotic steady-state system. In the full Ricker model, high noise levels wash out any cycling in the underlying density dependence, and the asymptotic spatially averaged population density tends to a constant value as the size of the landscape becomes very large. In this phase, the synchronization order parameter vanishes. Only at sufficiently low noise levels can cycling emerge in asymptotic spatially averaged population density. In this phase, the asymptotic system is no longer invariant to odd-period time translations. A discrete twofold symmetry is broken in the transition from incoherence to synchrony, and this is the symmetry breaking pattern of the Ising universality class.

---

### Worldwide divergence of values [^dbf0b3a0]. Nature Communications (2024). High credibility.

Third, we replicated the finding in sample with no turnover in sample composition — a subset of 33 countries that provided data in the 1990s, 2000s, and 2010s, which were the three decades with the greatest WVS coverage. Value distinctiveness was higher in the 2000s, b = 0.01, SE = 0.003, t (78) = 4.05, p < 0.001, = 0.29, 95% CI s [0.007, 0.02], and the 2010s, b = 0.01, SE = 0.003, t (78) = 2.66, p = 0.010, = 0.19, 95% CI s [0.002, 0.02], compared to the 1990s. However, we found no significant difference between the 2000s vs. 2010s, b = −0.004, SE = 0.003, t (39) = −1.41, p = 0.167, = −0.10, 95% CI s [−0.01, 0.002]. This decade-based analysis provides evidence for value divergence even when we fix the country sample over time. It also suggests that value divergence has been non-linear. In our Supplementary Methods, we evaluate different non-linear models of value divergence. These models suggest that the pace of value divergence has gradually slowed over time, rather than halting at a particular point.

---

### Limits on the computational expressivity of non-equilibrium biophysical processes [^37daae40]. Nature Communications (2025). High credibility.

Fig. 2
The matrix-tree theorem.

A Computing the steady-state occupancy π 1 by summing weights over directed spanning trees. Directed spanning trees are subgraphs containing all graph nodes but no cycles, with edges oriented toward a root node. In each directed spanning tree, the input forces make a positive, negative, or zero contribution to the tree weight. The structural vectorsare shown below each tree; these quantities enter into Equation (3) below. B Schematic illustration of the high-dimensional space of feature vectors ψ (i; θ) and χ (i, F). The depicted arrangement of vectors could solve a binary classification problem.

We define the input multiplicityas the number of edges affected per input variable, which we assume to be the same for each input. To focus on the functional way in which the input driving enters the steady-state probabilities, the driving contributions can be factored out in the algebraic expressions for the numerator and denominator of Equation (1). This has been previously been used to make analytical progress for M = D = 1 in, for example, refs. –. This equivalent formulation of Eq. (1) suggests that steady states of Markov jump processes implement a rational polynomial function of exponentiated input variables. Defining, we rewrite the matrix-tree expression for π i for general D and M We use the multi-index, whereis the set of D input labels and each componentof the multi-index runs over the values, to enumerate themonomials. These monomials y μ (F) in Equation (2) combinatorially depend on the different mixtures μ of input driving, representing a net total μ a of signed contributions from the input force F a, μ b such contributions for F b, and so on for each input. The coefficients, which are functions of the parameters θ, are the sums of weights over all directed spanning trees rooted at node i which have the corresponding mixture μ of signed input contributions. The monomial coefficientsthus represent learnable amplitudes of each polynomial basis function y μ (F). The coefficients in the denominator are defined as. Classification will be successful if, for F ρ drawn from class ρ, the coefficientsand monomials y μ (F ρ) are large for the same μ. In the subsequent sections of the paper and in the Supplementary Information we use the formulation in Equation (2) to show how the classification ability of a non-equilibrium Markov processes may be systematically modulated.

---

### The impact of learning on perceptual decisions and its implication for speed-accuracy tradeoffs [^1c8ecc8a]. Nature Communications (2020). High credibility.

Drift-diffusion model for decision-making

For a given stimulus with concentrations c A and c B, we define the accumulated evidence at time t, e (t). The diffusion process has the following properties: at time t = 0, the accumulated combined evidence is zero, e (0) = 0; and the momentary evidence m i is a random variable that is independent at each time step. We discretize time in steps of 0.1 ms and run numerical simulations of multiple runs/trials. For each new time step t = n Δ t, we generate a new momentary evidence draw:that is, through a normally distributed random generator with a mean of, in which we define k as the sensitive parameter, and β as the exponent parameter. σ defines the amount of noise in the generation of momentary evidences. We set σ to 1, makingequivalent to the signal to noise ratio for a particular stimuli and respective combination of c oncentrations (c A, c B). Integrated evidences (s 1, s 2) are simply the integration of the momentary evidences over time

We translate this in our discretized version as a cumulative sum at all time steps, effectively being:

We then define the decision variable accumulated evidence as:or in its discretized version:whereandare model-dependent combination weights on the accumulated evidence, andis an a priori decision bias (for optimal decisions; scaling ensures). Together, these parameters define slope and offset of the category boundary, which determines the mapping between accumulated evidence and associated choices. We also define the (accumulation) decision bound θ (t) and make it in most models collapsing over time through either a linear or an exponential decay. Thus, at time step n Δ t the bound is eitherwhere we defineas the bound height at the starting point of integration t = 0 andas its slope, orwhereis the bound height's mean lifetime. The collapse parametersand τ define the level of urgency in a decision, the smaller it becomes, the more urgent a given decision will become, given rise to more errors. For models with non-collapsing boundaries, we used, independent of time. For models with collapsing boundaries, they collapsed linearly, except for RL-DDM, where they collapse exponentially.

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^aeeaae45]. Nature Communications (2025). High credibility.

Fitting models to data is an important part of the practice of science. Advances in machine learning have made it possible to fit more-and more complex-models, but have also exacerbated a problem: when multiple models fit the data equally well, which one(s) should we pick? The answer depends entirely on the modelling goal. In the scientific context, the essential goal is replicability: if a model works well to describe one experiment, it should continue to do so when that experiment is replicated tomorrow, or in another laboratory. The selection criterion must therefore be robust to the variations inherent to the replication process. In this work we develop a nonparametric method for estimating uncertainty on a model's empirical risk when replications are non-stationary, thus ensuring that a model is only rejected when another is reproducibly better. We illustrate the method with two examples: one a more classical setting, where the models are structurally distinct, and a machine learning-inspired setting, where they differ only in the value of their parameters. We show how, in this context of replicability or "epistemic uncertainty", it compares favourably to existing model selection criteria, and has more satisfactory behaviour with large experimental datasets.

---

### 2025 AHA / ACC statement on cost / value methodology in clinical practice guidelines (update from 2014 statement): a report of the American college of cardiology / American Heart Association joint committee on clinical practice guidelines [^a83ec36b]. Journal of the American College of Cardiology (2025). High credibility.

Health opportunity cost approach — United States cost-effectiveness threshold estimation indicates that, across 50000 simulations, 23902/50000 (48%) of thresholds were < $100000/QALY and 7006/50000 (14%) were > $150000/QALY; the base-case estimate was $104,000/QALY in 2019 US dollars with a 95% uncertainty interval of $51,000–$209,000/QALY, and updating to 2024 US dollars suggests a threshold of $117000 per QALY.

---

### 2025 AHA / ACC statement on cost / value methodology in clinical practice guidelines (update from 2014 statement): a report of the American college of cardiology / American Heart Association joint committee on clinical practice guidelines [^63fa1003]. Journal of the American College of Cardiology (2025). High credibility.

Cost-effectiveness thresholds for cardiovascular cost-effectiveness analyses (CEAs) — The 2014 ACC/AHA cost-value methodology statement provided thresholds to identify high-, intermediate-, and low-value care based on U.S. per capita gross domestic product, and the writing committee determined that an intervention with an ICER < $50 000 per QALY gained should be considered high value, $50 000 to < $150 000 (1–3× per capita GDP) per QALY gained should be considered intermediate value, and ≥ $150 000 per QALY gained should be considered low value. WHO-origin GDP benchmarks suggested an intervention that costs less than 3 times the national annual GDP per capita per disability-adjusted life year averted should be considered cost-effective and that costs less than 1 times should be considered highly cost-effective, but critics note they "set such a low bar for cost effectiveness that very few interventions with evidence of efficacy can be ruled out"; WHO experts now also advise against per capita GDP-based thresholds and suggest considering cost-effectiveness alongside other contextual information in a transparent multicomponent decision process. Reflecting these concerns, the writing committee reviewed alternative approaches to incorporating CEA results into guidelines, including relying on US-based studies for identifying a cost-effectiveness threshold or thresholds for cardiovascular CEAs, and Vanness et al used a health opportunity cost approach estimating that for every $1 000 000 increase in health care expenditures, 1869 persons become uninsured, resulting in 81 QALYs lost due to mortality and 15 QALYs lost due to morbidity, implying a cost-effectiveness threshold of $104 000 per QALY ($51 000 to $209 000 per QALY) in 2019 US dollars; in the probabilistic analysis, 40% of the simulations suggested the threshold was less than $100 000 per QALY.

---

### Incorporating economic evidence in clinical guidelines: a framework from the clinical guidelines committee of the American College of Physicians [^4483eea4]. Annals of Internal Medicine (2025). High credibility.

Figure 1 — comparative value of interventions — categorizes value by clinical net benefit and incremental cost for quality-adjusted life-year (QALY) with intervention versus comparator, where the incremental cost dimension reflects the incremental cost-effectiveness ratio per QALY gained. For favorable clinical net benefit, higher cost corresponds to high value (< $100 000), intermediate value ($100 000 to $200 000), or low value (> $200 000) with the intervention more effective and more costly; lower cost corresponds to high value with the intervention more effective and less costly (cost-saving). For unfavorable clinical net benefit, higher cost indicates no value with the intervention less effective and more costly (cost-dominated), and lower cost indicates no value with the intervention less effective and less costly.

---

### Quantifying accuracy and heterogeneity in single-molecule super-resolution microscopy [^cc54f796]. Nature Communications (2020). High credibility.

Methods

Definitions and notations

In this section we define terms used in deriving and computing WIF.

Notations

We represent vectorial quantities in bold. For a vector v, we denote its Euclidean norm by. We use δ (v) to represent the Dirac delta function, which is zero everywhere except at v = 0. The inner product of two vectors v 1 and v 2 is denoted by v 1 ⋅ v 2. Further, we denoteas the set of integers. Finally, we useto represent an estimate of a deterministic quantity x.

A set of localizations

We represent a set of localizations or a set of source molecules aswhereanddenote the i th molecules' estimated brightness (in photons) and position, respectively. Note that, throughout this paper, what we mean by brightness is the expected number of photons emitted by a molecule during a camera frame (see ref.for background). We denote the mass of, i.e. the sum of the brightnesses of all molecules in, by. Further, represents the number of molecules in. We represent the collection of all validby M.

Negative Poisson log likelihood

Consider a set ofmolecules given by. The resulting intensity μ j, that is, the expected number of photons detected on a camera, for each pixel j ∈ {1,…, m } can be written aswhererepresents the integrated value of the PSF q (for the i th molecule) over the j th pixel area (C j), and b j denotes the expected number of background photons at the j th pixel.

If we denoteas m pixels of photon counts captured by a camera, the negative Poisson log likelihoodis then given bywhere we have discarded terms that do not depend on. If q is the true PSF, we callthe true negative log likelihood, while conversely, if an estimated or candidate model PSF q is used, then, we refer toas the negative log likelihood of the model. We note that the Poisson noise model considered here can be extended to account for pixel-dependent readout (Gaussian) noise (Supplementary Note 3).

---

### Advancing value-based medicine: why integrating functional outcomes with clinical measures is critical to our health care future [^c5702769]. Journal of Occupational and Environmental Medicine (2017). Medium credibility.

US health care spending and functional outcomes — economic context and potential impact are summarized as follows: In 2014, US national health expenditures grew 5.3% to $3 trillion, or roughly $9523 per person, and accounted for 17.5% of the gross domestic product (GDP), with 20% attributable to Medicare and about 16%, or $496 billion, to Medicaid; from 2014 to 2024, expenditures are projected to rise at an average annual rate of 5.8%. Research indicates that by focusing on functional outcomes in patient treatment, medical costs can be decreased and the quality of medical outcomes increased.

---

### Rotator cuff tendinopathy diagnosis, nonsurgical medical care, and rehabilitation: a clinical practice guideline [^a4079329]. The Journal of Orthopaedic and Sports Physical Therapy (2025). High credibility.

Grades of recommendations — This table defines grades A–F and links each to a strength-of-evidence statement and corresponding Level of Obligation. Grade A is defined as "A preponderance of level I and/or level II studies support the recommendation," with Level of Obligation (Based on Treatment Effects): "Must: benefits substantially outweigh harms; Should: benefits moderately outweigh harms; May: benefits minimally outweigh harms or benefit/harm ratio is value dependent; Should not: harms minimally or moderately outweigh benefits or evidence of no effect; Must not: harms largely outweigh benefits." Grade B is "A single high-quality randomized controlled trial or a preponderance of level II studies support the recommendation," with obligations "Should: benefits substantially outweigh harms; May: benefits moderately or minimally outweigh harms or benefit/harm ratio is value dependent; Should not: evidence that harms outweigh benefits or evidence of no effect." Grade C is "A single level II study or a preponderance of level III and IV studies, including statements of consensus by content experts, support the recommendation," with obligations "Should: benefits substantially outweigh harms; May: benefits moderately or minimally outweigh harms or benefit/harm ratio is value dependent; Should not: harms minimally or moderately outweigh benefits." Grade D is "Higher-quality studies conducted on this topic disagree with respect to their conclusions," with obligation "May: conflicting evidence, the benefit/harm ratio is value dependent." Grade E is "A preponderance of evidence from animal or cadaver studies, from conceptual models/principles, or from basic science/bench research support this conclusion," with obligations "May: in the absence of evidence from clinical studies, theoretical and/or foundational evidence supports benefit" and "Should not: in the absence of evidence from clinical studies, theoretical and/or foundational evidence suggests strong risk of harms." Grade F is "Best practice based on the clinical experience of the guideline development team supports this conclusion," and the table also specifies Level of Obligation (Based on Assessment/Diagnosis): "Must: strongly supported by consensus-based best practice/standard of care; Should: moderately supported by best practice/standard of care; May: supported by expert opinion in the absence of consensus; Should not: best practice/standard of care indicates potential harms; Must not: potential harms are strongly supported by consensus-based best practice/standard of care."

---

### Incorporating economic evidence in clinical guidelines: a framework from the clinical guidelines committee of the American College of Physicians [^537f28fa]. Annals of Internal Medicine (2025). High credibility.

Clinical Guidelines Committee (CGC) value thresholds for economic evidence in cost‑effectiveness analyses categorize interventions by incremental cost‑effectiveness ratio (ICER) per quality‑adjusted life‑year (QALY) gained as high value when cost‑saving or < $100 000, intermediate value at $100 000-$200 000, low value at > $200 000, and no value when dominated. The CGC states these thresholds are those most commonly referenced in U.S.-based cost‑effectiveness analyses, may be adjusted on a topic‑by‑topic basis, and that an intervention is dominated by strict dominance or extended dominance.

---

### Guidance for the Heart Rhythm Society pertaining to interactions with industry endorsed by the Heart Rhythm Society on April 26, 2011 [^f1dedb5c]. Heart Rhythm (2011). Medium credibility.

Heart Rhythm Society financial relationship disclosure categories — greater transparency is required and the value of financial relationships is now categorized as $0; $1–$10,000; $10,001–$25,000; $25,001–$50,000; $50,001–$100,000; and greater than $100,000, whereas previously categories were none; modest ($1–$10,000); or significant (more than $10,000).

---

### Death by p-value: the overreliance on p-values in critical care research [^bfbf92b9]. Critical Care (2025). Medium credibility.

The p-value has changed from a versatile tool for scientific reasoning to a strict judge of medical information, with the usual 0.05 cutoff frequently deciding a study's significance and subsequent clinical use. Through an examination of five critical care interventions that demonstrated meaningful treatment effects yet narrowly missed conventional statistical significance, this paper illustrates how rigid adherence to p-value thresholds may obscure therapeutically beneficial findings. By providing a clear, step-by-step illustration of a basic Bayesian calculation, we demonstrate that clinical importance can remain undetected when relying solely on p-values. These observations challenge current statistical paradigms and advocate for hybrid approaches-including both frequentist and Bayesian methodologies-to provide a more comprehensive understanding of clinical data, ultimately leading to better-informed medical decisions.

---

### Practice guidelines for the prevention, detection, and management of respiratory depression associated with neuraxial opioid administration: an updated report by the American society of anesthesiologists task force on neuraxial opioids and the American Society of Regional Anesthesia and Pain Medicine [^c7d4883b]. Anesthesiology (2016). Medium credibility.

American Society of Anesthesiologists neuraxial opioid guideline — evidence grading and opinion methodology: Category B observational evidence permits directional inference, and for studies that report statistical findings, "the threshold for significance is P < 0.01"; Category B levels comprise "observational comparisons… with comparative statistics" (Level 1), "noncomparative observational studies with associative statistics" (Level 2), "noncomparative observational studies with descriptive statistics" (Level 3), and "case reports" (Level 4). Opinion-based inputs include formal surveys and informal sources; survey types are designated "Category A: Expert Opinion" and "Category B: Membership Opinion," with additional "Category C: Informal Opinion." Survey responses "are recorded using a five-point scale and summarized based on the median values," with median categories defined as follows: "Strongly Agree: Median score of 5 (at least 50% of the responses are 5)… Agree: Median score of 4 (at least 50% of the responses are 4 or 4 and 5)… Equivocal: Median score of 3 (at least 50% of the responses are 3, or no other response category or combination of similar categories contain at least 50% of the responses)… Disagree: Median score of 2 (at least 50% of responses are 2 or 1 and 2)… Strongly Disagree: Median score of 1 (at least 50% of responses are 1)." When equal numbers of distinct responses occur, "the median value is determined by calculating the arithmetic mean of the two middle values."

---

### A new look at P values for randomized clinical trials [^2a9dcae0]. NEJM Evidence (2024). Medium credibility.

BACKGROUND: We have examined the primary efficacy results of 23,551 randomized clinical trials from the Cochrane Database of Systematic Reviews. METHODS: We estimate that the great majority of trials have much lower statistical power for actual effects than the 80 or 90% for the stated effect sizes. Consequently, "statistically significant" estimates tend to seriously overestimate actual treatment effects, "nonsignificant" results often correspond to important effects, and efforts to replicate often fail to achieve "significance" and may even appear to contradict initial results. To address these issues, we reinterpret the P value in terms of a reference population of studies that are, or could have been, in the Cochrane Database. RESULTS: This leads to an empirical guide for the interpretation of an observed P value from a "typical" clinical trial in terms of the degree of overestimation of the reported effect, the probability of the effect's sign being wrong, and the predictive power of the trial. CONCLUSIONS: Such an interpretation provides additional insight about the effect under study and can guard medical researchers against naive interpretations of the P value and overoptimistic effect sizes. Because many research fields suffer from low power, our results are also relevant outside the medical domain. (Funded by the U.S. Office of Naval Research.)

---

### Limits on the computational expressivity of non-equilibrium biophysical processes [^2e5c822c]. Nature Communications (2025). High credibility.

Structural compatibility

We find that for a given graph structure, not every classification problem (i.e. an assignment of input edges, output nodes, and sets of input data) can be solved. For example, we may assign an input F a along edge i ← j and assign node j to be large when F a > 0; this will be very difficult to achieve because all of the spanning trees into node j which involve edge j ← i will have an exponentially small contribution from the input force, making the input feature vector χ (j, F) in Equation (3) small; this cannot be helped no matter how we learn the parameters θ. If we flipped the sign of the input F a in the assignment, then the problem may become solvable. We refer to this mismatch between input force assignment and achievable output node assignment as structural incompatibility. For a fixed set of hyperparameters (i.e. number of nodes, edges, input edges, output nodes, etc.), some problems will be structurally compatible and some will not be. This issue is thus separate from more intrinsic properties of computational expressivity that depend on hyperparameters like M and D, but it implies that we cannot define classification problems completely arbitrarily. We leave to future work a dedicated study of what determines structural compatibility, which may be posed as determining a feasible region as in constrained optimization. Throughout this paper, we bypass this issue to focus on other constraints on expressivity, but we note that structural compatibility represents one limitation on using chemical reaction networks as classifiers. We also note that optimizing the mutual information does not involve specifying output nodes by hand and thus helps to bypass issues of structural compatibility.

Training

We train Markov networks for classification tasks using an approximation to gradient descent, as discussed in refs. This method requires a variational quantitywhich is minimized by the steady-state distribution. Two considerations lead to equivalent choices of: one is thatshould be the KL divergence, which has been shown to act as a Lyapunov function for the evolution of p (t) and is minimized to zero at steady state. The other consideration is based on the observation that, from Equation (3), withis a Boltzmann-like distribution which must hence maximize a constrained entropy functional. We show in the Supplementary Information that both of these considerations lead to equivalent update rules which require numerically estimating the vectors ∂ θ π i during training.

---

### Detecting the ultra low dimensionality of real networks [^baa84bdd]. Nature Communications (2022). High credibility.

Methods

Estimation of hidden degrees of real networks

Given a real network, our goal is to generate networks with themodel but preserving the degree distribution of the real network as much as possible. In themodel, hidden degrees are given fixed values but nodes' degrees are random variables that depend on the hidden degrees. Therefore, to reproduce the degree distribution, we have to find the sequence of hidden degrees that better reproduces the sequence of observed degrees. To do so, we generalize the method into arbitrary dimensions. Given a set of parameters β and D, for each observed degree class k in a real network, we infer the corresponding hidden degree κ so that the ensemble average degree of themodel of a node with hidden degree κ is equal to its observed degree, that is.

After this procedure, the degree distribution of synthetic networks generated by themodel with the inferred sequence of hidden degrees is very similar to the one from the real network. Specifically,
Initially set κ i = k i ∀ i = 1, N, where k i is the observed degree of node i in the real network.
Compute the expected degree for each node i according to themodel as whereand.
Correct hidden degrees: Letbe the maximal deviation between actual degrees and expected degrees. If ϵ m a x > ϵ, the set of hidden degrees needs to be corrected. Then, for every class of degree k i, we set, where u is a random variable drawn from U (0, 1). The random variable u prevents the process from getting trapped in a local minimum. Next, go to step 2 to compute the expected degrees corresponding to the new set of hidden degrees. Otherwise, if ϵ m a x ≤ ϵ, hidden degrees have been correctly inferred for the current global parameters.

Following this algorithm, we can generate surrogates of a given network G with different D and β values without modifying the degree distribution. The tolerance value of ϵ used in this work is ϵ = 1.

---

### Generalized gametic relationships for flexible analyses of parent-of-origin effects [^0eeba866]. G3 (2021). Medium credibility.

The Mendelian sampling variances that define the diagonal elements ofare different for the transmitting abilities and gametic effects. Furthermore, they depend on the occurrence of unknown parents and the inbreeding coefficients of the known ones. In particular, we havewhen an individual with a transmitting ability in the matrix has only one known parent, orandin the case of full parentage information. For gametes, we need to account for the inbreeding coefficientof the known parent from which a gamete is derived. Accordingly, the 12 cases (a-00, a-0a,…, g-00) are grouped into the following five classes with distinct formulae for the inverse Mendelian sampling variance:

It is possible to construct the inverse generalized gametic relationship matrix from the pedigree in a step-by-step manner for any arbitrary order of genetic effects. In each step, a matrix contributionis added for genetic effectto a matrix comprising an inversethat already includes the precedingeffects and zeroes:whereis a column vector ofzeros and:is the contribution made for each genetic effect. The row vectorcomprises all zeros, except for the elements that correspond to the genetic effects of the respective parent(s). The i -th element (equal to unity) at least is a nonzero in this vector. If present, all other nonzero elements are negative, with values of eitheror. Thus, the number of nonzero entries varies from one to five, which can be derived from the rows in the example triangular matrixin Supplementary Figure S2.1. Table 1 summarizes the nonzero coefficients inand their indices for all 12 possible cases. The nonzero elements of the resulting matrixcorrespond to the (scaled) cross-products of the elements of the nonzero vector, and their coordinates in the matrix are the respective combinations of indices.

Table 1
Size and indices of nonzero elements of vectorsby type of genetic effect (a: transmitting ability; g: gametic effect)

---

### Genetic association analysis of 77, 539 genomes reveals rare disease etiologies [^762c3c89]. Nature Medicine (2023). Excellent credibility.

Encoding RSVR IDs

SNVs and indels may be encoded as 64-bit integers called RSVR IDs. To compute an RSVR ID for a given variant, the following expression is evaluated:where c is the chromosome number (using 23, 24 and 25, respectively, to represent chromosomes X, Y and MT); p is the position; and | r | and | a | are the lengths of the reference and alternate alleles, respectively. A is a sequence identical to the alternate allele, a, when its length is less than 10 and otherwise, equal to the first five followed by the last four elements of a. In the summation, nucleotides are assigned values A = 0, C = 1, G = 2 and T = 3. The expression evaluates to integers that can be represented using 63 bits, setting the most significant bit to zero when encoding as 64-bit integers. The chromosome, position, reference, alternate allele lengths and alternate allele bases are thereby encoded, respectively, by the subsequent 5, 28, 6, 6 and 18 bits (with 2 bits per base for the alternate allele). This procedure and its inverse are implemented in the 'rsvr enc' and 'rsvr dec' programs, respectively. The reference and alternate alleles of input variants are first normalized by removing any redundant identical sequence from the starts and then, the ends. The proportion of variants in gnomAD 3.0 weighted by allele count that can be encoded losslessly is 99.3%, while 99.8% can be represented by a distinct RSVR ID. The full variant information corresponding to any encountered ambiguous RSVR ID may be stored in full in an additional table. Structural variants that can be represented by a position and length may also be encoded using distinct 64-bit RSVR IDs alongside SNVs and indels by setting the most significant bit to one and subsequently, encoding the type of structural variant using 2 bits (deletion 0, duplication 1, inversion 2, insertion 3), the chromosome using 5 bits (as done for SNVs and indels), and the start and length consecutively using 28 bits.

---

### Assessment of physiological signs associated with COVID-19 measured using wearable devices [^6fde023d]. NPJ Digital Medicine (2020). Medium credibility.

Making predictions of illness based on health metrics

The physiological signs can be used to predict the onset of COVID-19. Let us denote the n th day after the start of symptoms as D n. D 0 is the day when symptoms started. We make the assumption that individuals are healthy, i.e. class "Negative" from day D a to day D b where D b < D 0. Subjects are considered to be sick from day D c up to day D d where D c ≥ D 0. The days between D b and D c are treated as a buffer space when subjects may or may not be sick, and hence ignored. The choices of D a, D b, D c, and D d are made through cross validation. As a guide to choosing the days, we note the median incubation period is estimated at 5.1 days. Since we are using the date relative to the start of symptoms as the ground truth label, we consider only symptomatic individuals.

---

### Evolution of reporting P values in the biomedical literature, 1990–2015 [^4e8c6039]. JAMA (2016). Excellent credibility.

Importance

The use and misuse of P values has generated extensive debates.

Objective

To evaluate in large scale the P values reported in the abstracts and full text of biomedical research articles over the past 25 years and determine how frequently statistical information is presented in ways other than P values.

Design

Automated text-mining analysis was performed to extract data on P values reported in 12,821,790 MEDLINE abstracts and in 843,884 abstracts and full-text articles in PubMed Central (PMC) from 1990 to 2015. Reporting of P values in 151 English-language core clinical journals and specific article types as classified by PubMed also was evaluated. A random sample of 1000 MEDLINE abstracts was manually assessed for reporting of P values and other types of statistical information; of those abstracts reporting empirical data, 100 articles were also assessed in full text.

Main Outcomes and Measures

P values reported.

Results

Text mining identified 4,572,043 P values in 1,608,736 MEDLINE abstracts and 3,438,299 P values in 385,393 PMC full-text articles. Reporting of P values in abstracts increased from 7.3% in 1990 to 15.6% in 2014. In 2014, P values were reported in 33.0% of abstracts from the 151 core clinical journals (n = 29,725 abstracts), 35.7% of meta-analyses (n = 5620), 38.9% of clinical trials (n = 4624), 54.8% of randomized controlled trials (n = 13,544), and 2.4% of reviews (n = 71,529). The distribution of reported P values in abstracts and in full text showed strong clustering at P values of .05 and of .001 or smaller. Over time, the "best" (most statistically significant) reported P values were modestly smaller and the "worst" (least statistically significant) reported P values became modestly less significant. Among the MEDLINE abstracts and PMC full-text articles with P values, 96% reported at least 1 P value of .05 or lower, with the proportion remaining steady over time in PMC full-text articles. In 1000 abstracts that were manually reviewed, 796 were from articles reporting empirical data; P values were reported in 15.7% (125/796 [95% CI, 13.2%-18.4%]) of abstracts, confidence intervals in 2.3% (18/796 [95% CI, 1.3%-3.6%]), Bayes factors in 0% (0/796 [95% CI, 0%-0.5%]), effect sizes in 13.9% (111/796 [95% CI, 11.6%-16.5%]), other information that could lead to estimation of P values in 12.4% (99/796 [95% CI, 10.2%-14.9%]), and qualitative statements about significance in 18.1% (181/1000 [95% CI, 15.8%-20.6%]); only 1.8% (14/796 [95% CI, 1.0%-2.9%]) of abstracts reported at least 1 effect size and at least 1 confidence interval. Among 99 manually extracted full-text articles with data, 55 reported P values, 4 presented confidence intervals for all reported effect sizes, none used Bayesian methods, 1 used false-discovery rates, 3 used sample size/power calculations, and 5 specified the primary outcome.

Conclusions and Relevance

In this analysis of P values reported in MEDLINE abstracts and in PMC articles from 1990–2015, more MEDLINE abstracts and articles reported P values over time, almost all abstracts and articles with P values reported statistically significant results, and, in a subgroup analysis, few articles included confidence intervals, Bayes factors, or effect sizes. Rather than reporting isolated P values, articles should include effect sizes and uncertainty metrics.

---

### Limits on the computational expressivity of non-equilibrium biophysical processes [^d4f96178]. Nature Communications (2025). High credibility.

Improving expressivity by increasing input multiplicity

Biologically, F a can be interpreted, for example, as depending on the chemostatted activity of an enzyme (see the Methods). In biochemical kinetics, it is common for some species to be involved in multiple reactions simultaneously, making it plausible for F a to drive multiple edges. We find that allowing for input multiplicity improves classification expressivity, and one way this happens is by lifting the monotonicity constraint. We assume for simplicity that each of the D input variables, whereis the set of input labels, affects the same numberof edges. Setting M > 1 lifts the monotonicity constraint because the condition for π k (F i j) to be a monotonic function is that all other edge parameters are held fixed; with M > 1 this is no longer true since several edge parameters change simultaneously as an input is varied.

To better understand the gain in the decision boundary's flexibility allowed by setting M > 1, in the Supplementary Information we analyze the steady-state representation in the rational polynomial form of the matrix-tree expression, Eq. (2). Considering the case D = 1 and identifying turning points as roots of ∂ π i /∂ F a, we show that the maximum number R of such roots obeyswhich is a direct measure of the classifier's expressivity; see Fig. 3 E for an illustration and the Supplementary Information for a numerical verification up to M = 4. A proof of the scaling 2 M − 1 for rational polynomials with non-negative coefficients can be found in ref. Thus, once M > 1, π i is no longer subject to the monotonicity constraint and behaves like a non-negative rational polynomial of degree up to 2 M. Input multiplicity thus allows the non-equilibrium biological process to be more expressive and draw out decision boundaries that can classify more complex data structures. Indeed, returning to the previously failed classification with M = 1 (Fig. 3 C), we see that setting M = 2 allows the same network to now learn a decision boundary which successfully encloses the data assigned to class 1 (Fig. 3 F). This implies that classifying a finite band of input signal levels (like a band-pass filter) requires setting M > 1 along the corresponding input dimension. A recent development in synthetic biology has in fact shown in a specific example that drug binding to receptor molecules via two distinct binding pathways can be used to design band-pass-like responses to the drug (Fig. 3 G).

---

### 2018 AHA / ACC / AACVPR / AAPA / ABC / ACPM / ADA / AGS / APhA / ASPC / NLA / PCNA guideline on the management of blood cholesterol: a report of the American college of cardiology / American Heart Association task force on clinical practice guidelines [^4a178bba]. Circulation (2019). High credibility.

Regarding specific circumstances for dyslipidemia, more specifically with respect to patients with ASCVD, general indications, AAPA/ABC/ACC/ACPM/ADA/AGS/AHA/APhA/ASPC/PCNA 2019 guidelines recommend to recognize that at mid-2018 list prices, PCSK9 inhibitors have a low cost value, as defined by > $150,000 per quality-adjusted life year, compared to good cost value, as defined by < $50,000 per quality-adjusted life year.

---

### Separate, causal roles of the caudate in saccadic choice and execution in a perceptual decision task [^31e4a48e]. Neuron (2012). Low credibility.

In contrast to the well-established roles of the striatum in movement generation and value-based decisions, its contributions to perceptual decisions lack direct experimental support. Here, we show that electrical microstimulation in the monkey caudate nucleus influences both choice and saccade response time on a visual motion discrimination task. Within a drift-diffusion framework, these effects consist of two components. The perceptual component biases choices toward ipsilateral targets, away from the neurons' predominantly contralateral response fields. The choice bias is consistent with a nonzero starting value of the diffusion process, which increases and decreases decision times for contralateral and ipsilateral choices, respectively. The nonperceptual component decreases and increases nondecision times toward contralateral and ipsilateral targets, respectively, consistent with the caudate's role in saccade generation. The results imply a causal role for the caudate in perceptual decisions used to select saccades that may be distinct from its role in executing those saccades.

---

### Exceptional topological insulators [^75e3f431]. Nature Communications (2021). High credibility.

The Berry flux can be related to the ETI surface states as follows. Consider the system with OBC in z direction and PBC otherwise and assume a series of cuts through the 2D surface BZ. Each cut represents a fictitious 2D non-Hermitian system with OBC in one direction. In 2D, isolated bands can be characterized by an integer Chern number C. If ∣ C ∣ is nonzero, the corresponding number of edge-localized topological modes connect the band with the other bands inside the complex plane, similar to Hermitian Chern insulators that describe the integer quantum Hall effect in lattice systems.

We visualize the construction for model (1) in Fig. 2 e along the cuts shown in the insets. We observe that the four bands of the model project to three distinct regions in the complex plane. Panels (e-ii), (e-iv), and (e-vi) indicate critical cuts when the bands exchange Chern numbers, while in the intermediate regions the bands exhibit fixed values C = 0 or C = ± 1. We observe in the sequence of cuts that a Chern dipole is formed on the left side of the point gap, by transferring a Chern number +1 (in a clockwise direction) between the two bands that touch in the second panel. The topological charge C = +1 is then transferred between the upper left and the upper right band, and finally back to the lower band. Thus, a quantum +1 of Berry flux is pumped clockwise around the point gap, in accordance with the rewriting of w 3D in Eq. (2). Note that Fig. 2 e also indicates the boundary-localized edge modes which at each stage connect bands with opposite Chern numbers. As such, they necessarily swipe over the entire point-gap region during the pumping process, thus forming the protected topological surface state of the ETI. Consequently, the Chern number flow around the point gap corresponds to a rotation of the chiral edge state, which leads to the anomalous net surface chirality. Combining the series of cuts with a momentum resolution in Fig. 2 f provides a visual connection between the numerically obtained surface spectra and the topological invariant.

---

### Practice guidelines for acute pain management in the perioperative setting: an updated report by the American society of anesthesiologists task force on acute pain management [^b27697b8]. Anesthesiology (2012). Medium credibility.

Acute pain management — opinion-based evidence and survey scaling describe how nonempirical input informs the Guidelines. All opinion-based evidence relevant to each topic was considered; however, only the findings obtained from formal surveys are reported, and identical surveys were distributed to expert consultants and ASA members. Category A: Expert Opinion reports survey responses from Task Force-appointed expert consultants in summary form, and Category B: Membership Opinion reports survey responses from active ASA members in summary form. Opinion survey responses are recorded using a 5-point scale and summarized based on median values; Strongly Agree corresponds to a median score of 5 (At least 50% of the responses are 5), Agree to a median score of 4 (At least 50% of the responses are 4 or 4 and 5), Equivocal to a median score of 3 (At least 50% of the responses are 3, or no other response category or combination of similar categories contain at least 50% of the responses), Disagree to a median score of 2 (At least 50% of responses are 2), and Strongly Disagree to a median score of 1 (At least 50% of responses are 1); when an equal number of categorically distinct responses are obtained, the median value is determined by calculating the arithmetic mean of the two middle values, with ties calculated by a predetermined formula. Category C: Informal Opinion notes that open-forum testimony, Internet-based comments, letters, and editorials are informally evaluated and discussed during the development of Guideline recommendations and that when warranted the Task Force may add educational information or cautionary notes based on this information.

---

### Bayesian prediction intervals for assessing P-value variability in prospective replication studies [^ee55d352]. Translational Psychiatry (2017). Low credibility.

Table 1
Binomial probabilities for 80% prediction intervals, using a two-sample Z -test

The table illustrates the effect of thresholding, applied to observed P -values, e.g. selection of statistically significant P -values at 5% level, on binomial probabilities

Mixture Bayes intervals were included in these simulations to check how well they approximate a continuous prior distribution assumed by the conjugate intervals. We used mixture components with the length σ 0 /8 for every component and truncated the normal prior at 10 −6 and 1−10 −6 quantiles. This provided us with sufficient accuracy and resulted in the number of mixture components, B, equal to 76 for all values of.

Table 1 clearly indicates that all three construction methods have the correct coverage (~80%) if a prediction interval is calculated for a randomly observed P -value ∈[0,1]. However, selection and small prior variance both impair performance of P -intervals. For instance, if an interval is constructed for a P -value < 0.001 and, the coverage of the traditional non-Bayesian P -interval may be as low as 17%. This poor coverage is due to a combination of both the selection bias and the implicit assumption that prior variance ofranges from negative infinity to positive infinity, which leads to the left-side P -interval endpoint being too close to zero. However, even for large values of prior, the P -interval has poor coverage when constructed for P -values around genome-wide significance levels (e.g. P -value < 1.5 × 10 −7). On the other hand, for large P -values the coverage of P -intervals becomes greater than the nominal (1 − α)% value. This is a consequence of the fact that P -interval's width depends on the magnitude of P -values and as P -values become larger, the width of the interval increases as well. For example, given the prior variance, the width of P -intervals and the Bayesian intervals coincides at P -value = 0.446 (hence, the Bayesian intervals are wider than P -intervals at values smaller than 0.446). The P -interval around = 0.446 is: 0.147 ≤ P ≤ 0.995, while the Bayesian interval is 0.110 ≤ P ≤ 0.958.

---

### Single-cell analysis reveals inflammatory interactions driving macular degeneration [^2c3ab50c]. Nature Communications (2023). High credibility.

Automated cluster characterization via manifold-intrinsic diffusion condensation

While identification of pathogenic cellular states is critical, biologists are more interested in what defines these populations. Most manifold-learning methods visualize or cluster populations of interest, requiring further expensive computation to characterize cell populations and discover differentially expressed genes. As our approach continuously condenses the transcriptomic profiles of single cells to local cluster centroids in manifold space, at any iteration, the transcriptomic states of the condensed data can be extracted at no additional computational cost. To enhance this convergence to centroids we implement our diffusion condensation process with an α -decay kernel (Supplementary Fig. 2C). This kernel more strongly thresholds the conversion of distances to affinities, closely resembling the box kernel, which accurately computes cluster centroids over the course of main point merges. When diffusion condensation merges two cells together at a particular iteration, the newly formed point lies close to the centroid of the original two cells in transcriptomic space. Under specific conditions, the new point is exactly the cluster centroid as delineated in the Proposition below. First, we define the α -decay kernel as:

The standard Gaussian kernel function as shown in equation (1) has an α of 2. The default α -decay kernel meanwhile uses a much higher value (default in our implementation is 40), which converts close distances into affinities much more stringently (Supplementary Fig. 2C). As α increases to infinity, this kernel function converges almost completely to the box kernel. With this kernel, we are ready to state a set of conditions under which the diffusion condensation process can be easily characterized.

Proposition 1

Assume there exists a unique global minimum non-zero distance δ i between points x a, x b at each iteration i, with the next pair of points at distance at least δ i + τ i with 0 < τ i. Note that x a, x b could have multiplicity greater than 1, representing clusters of size > 1. Then set the bandwidth to ϵ i: = δ i + τ i /2 at each iteration of the condensation process. For a large enough α, the diffusion condensation process will maintain two invariants for the first N − 1 steps:
The number of points will be N − i;
Unique points will be located at the centroid of their cluster.

---

### Identifying domains of applicability of machine learning models for materials science [^9c5f816f]. Nature Communications (2020). High credibility.

A predictive ML model is then a functionaiming to minimize the expected error (also called prediction risk)measured by some non-negative loss function l that quantifies the cost incurred by predicting the actual property value y with f (x). Examples for loss functions are the squared error, the absolute error, and, for non-zero properties, the relative error. Here P denotes some fixed probability distribution that captures how candidate materials are assumed to be sampled from the materials class (this concept, while commonly assumed in ML, is an unnecessary restriction for high-throughput screening as we discuss in more detail below). Since the true prediction risk is impossible to compute directly without perfect knowledge of the investigated materials class, models are evaluated by the test error (or empirical risk)defined as the average of the individual errors (losses) e i (f) = l (f (x i), y i) on some test set of m reference data points. The samples in this test set are drawn independently and identically distributed according to P and are also independent of the model — which means in practice that it is a random subset of all available reference data that has been withheld from the ML algorithm. In order to reduce the variance of this estimate, a common strategy is cross-validation, where this process is repeated multiple times based on partitioning the data into a number of non-overlapping "folds" and then to use each of these folds as test sets and the remaining data as a training set to fit the model.

This test error properly estimates the model performance globally over the whole representation space X (weighted by the distribution P used to generate the test points). This is an appropriate evaluation metric for selecting a model that is required to work well on average for arbitrary new input materials that are sampled according to the same distribution P. This is, however, not the condition of high-throughput screening. Here, rather than being presented with random inputs, we can decide which candidate materials to screen next. This observation leads to the central idea enabled by the DA analysis proposed in this work: if the employed model is particularly applicable in a specific subdomain of the materials class, and if that subdomain has a simple and interpretable shape that permits to generate new materials from it, then we can directly focus the screening there.

---

### Incorporating economic evidence in clinical guidelines: a framework from the clinical guidelines committee of the American College of Physicians [^e53f1cf5]. Annals of Internal Medicine (2025). High credibility.

GRADE Evidence-to-Decision summary of judgments — A sample blank framework outlines judgment scale options, including "No clinically meaningful," "Small," "Medium," "Large," "Varies," "Uncertain," and "No included studies." Type of recommendation categories listed are "Strong recommendation against the intervention," "Conditional recommendation against the intervention," "Conditional recommendation for either the intervention or the comparator," "Conditional recommendation for the intervention," and "Strong recommendation for the intervention." The figure clarifies "GRADE = Grading of Recommendations Assessment, Development and Evaluation" and notes it was "Prepared with GRADEpro (https://gradepro.org) and modified by the authors."

---

### Subsampling scaling [^8e2e9a81]. Nature Communications (2017). Medium credibility.

As each event is observed independently, the probability of X sub = s is the sum over probabilities of observing clusters of X = s + k events, where k denotes the missed events and s the sampled ones (binomial sampling):

This equation holds for any discrete P (s) defined on, the set of non-negative integers. To infer P (s) from P sub (s), we develop in the following a novel 'subsampling scaling' that allows to parcel out the changes in P (s) originating from spatial subsampling. A correct scaling ansatz collapses the P sub (s) for any sampling probability p.

In the following, we focus on subsampling from two specific families of distributions that are of particular importance in the context of neuroscience, namely exponential distributions P (s) = C λ e − λs with λ > 0, and power laws P (s) = C γ s − γ with γ > 1. These two families are known to show different behaviours under subsampling:
For exponential distributions, P (s) and P sub (s) belong to the same class of distributions, only their parameters change under subsampling. Notably, this result generalizes to positive and negative binomial distributions, which include Poisson distributions.
Power-laws or scale-free distributions, despite their name, are not invariant under subsampling. Namely, if P (s) follows a power-law distribution, then P sub (s) is not a power law but only approaching it in the limit of large cluster size (s →∞).

---

### A novel oppositional binary crow search algorithm with optimal machine learning based postpartum hemorrhage prediction model [^f6de23e1]. BMC Pregnancy and Childbirth (2022). Medium credibility.

Whereasrepresent the opposite number and x ∈ R denotes a real number determined on range of x ∈[a, b]. While a = 0 and b = 1 Eq. (3) becomes

While there is a point P (x 1, x 2,… x n) in n dimension coordinate and x 1, x 2,…, x n ∈ R later, the opposite pointis determined as its coordinates:

In such cases, have 2 values, x represent initial arbitrary value in [a, b] anddenotes the opposite values of x. They calculate f (x)&in all the iterations of OBCSA, later, employ on the evaluation function g ifselect x or else selectConsequently, the f l would be in range: f l ∈[f l min, f l max]. The opposite numbercan be determined by:

Later, evaluate the fitness for the first f l value and the fitness forin all the iterations. When, they select f l, or elsewould be selected. The stages of presented method can be given in the following.

Step1: The count of crows is n c = 25, f l min = 0.1, f l max = 1.8, A P = 0.3, and the maximal number of iterations is t max = 100.

Step2: The position that represent the features are made by U (0, 1).

Step3: The fitness function (FF) can be determined by

Whereas C represent the classification performance, W represent the weighted factors in the range of zero and one, F all represent the overall amount of features and F sub signifies the length of elected feature.

Step4: The position of the crows are upgraded as Eq. (2)

Step5: Steps 3 & 4 are repetitive till a t max is attained.

---

### Quantification of network structural dissimilarities [^3cf663e0]. Nature Communications (2017). Medium credibility.

Identifying and quantifying dissimilarities among graphs is a fundamental and challenging problem of practical importance in many fields of science. Current methods of network comparison are limited to extract only partial information or are computationally very demanding. Here we propose an efficient and precise measure for network comparison, which is based on quantifying differences among distance probability distributions extracted from the networks. Extensive experiments on synthetic and real-world networks show that this measure returns non-zero values only when the graphs are non-isomorphic. Most importantly, the measure proposed here can identify and quantify structural topological differences that have a practical impact on the information flow through the network, such as the presence or absence of critical links that connect or disconnect connected components.

---

### Detecting the ultra low dimensionality of real networks [^542f35b2]. Nature Communications (2022). High credibility.

We evaluated the performance of our method by testing it on model networks generated using themodel. Model networks were produced for specific values of γ, β, and D, and taken as test networks, meaning that an ensemble of random synthetic surrogates was generated for each of them, including the estimation of hidden degrees from observed degrees in the test network. The inferred dimensionality, D *, of the test networks was then used to generate confusion matrices, defined as the probability of predicting D * in a network generated with dimension D. To quantify the goodness of our method, we computed confusion matrices for different types of topologies. Specifically, we tested networks with different degree heterogeneities by varying the exponent γ, and for each of them we inferred the dimension of networks within two different intervals of β: one corresponding to the high clustering regime centered around β = 2.5 D, and the other to the low clustering limit in the neighborhood of β = 1.5 D, more details can be found in subsection "Calculation of confusion matrices" in Methods. Confusion matrices for these experiments are shown in Fig. 3. An inference method is considered a good method when the confusion matrices are close to the identity matrix. As shown, the predictions were very good, only generating mild confusion with contiguous values of D for low values of β, γ and D, as expected. This fact is consistent with the tendency of the paired density edge curves C t, C S, C p to converge for low values of β and γ, as shown in Fig. 2.

Fig. 3
Confusion matrices.

The left column shows results for β = 1.5 D and different values of γ. This choice of β corresponds to the small-world phase even if γ > 3. The right column shows the same for β = 2.5 D. In this case, networks with γ > 3 are large-worlds. The color in each D − D * box indicates the probability that the predicted dimension is D * when the dimension used to generate the synthetic network is D. Each panel is evaluated with approximately 5 × 10 3 networks, more details in subsection "Calculation of confusion matrices" in Methods.

---

### Statistical considerations in the evaluation of continuous biomarkers [^c7960d51]. Journal of Nuclear Medicine (2021). Medium credibility.

Discovery of biomarkers has been steadily increasing over the past decade. Although a plethora of biomarkers has been reported in the biomedical literature, few have been sufficiently validated for broader clinical applications. One particular challenge that may have hindered the adoption of biomarkers into practice is the lack of reproducible biomarker cut points. In this article, we attempt to identify some common statistical issues related to biomarker cut point identification and provide guidance on proper evaluation, interpretation, and validation of such cut points. First, we illustrate how discretization of a continuous biomarker using sample percentiles results in significant information loss and should be avoided. Second, we review the popular "minimal- P -value" approach for cut point identification and show that this method results in highly unstable P values and unduly increases the chance of significant findings when the biomarker is not associated with outcome. Third, we critically review a common analysis strategy by which the selected biomarker cut point is used to categorize patients into different risk categories and then the difference in survival curves among these risk groups in the same dataset is claimed as the evidence supporting the biomarker's prognostic strength. We show that this method yields an exaggerated P value and overestimates the prognostic impact of the biomarker. We illustrate that the degree of the optimistic bias increases with the number of variables being considered in a risk model. Finally, we discuss methods to appropriately ascertain the additional prognostic contribution of the new biomarker in disease settings where standard prognostic factors already exist. Throughout the article, we use real examples in oncology to highlight relevant methodologic issues, and when appropriate, we use simulations to illustrate more abstract statistical concepts.

---

### Foundations of the minimal clinically important difference for imaging [^72f785d9]. The Journal of Rheumatology (2001). Low credibility.

This article develops a generic conceptual framework for defining and validating the concept of minimal clinically important difference. We propose 3 approaches. The first uses statistical descriptions of the population ("distribution based"), the second relies on experts ("opinion based"), and a third is based on sequential hypothesis formation and testing ("predictive/data driven based"). The first 2 approaches serve as proxies for the third, which is an experimentally driven approach, asking such questions as "What carries the least penalty?" or "What imparts the greatest gain?" As an experimental approach, it has the expected drawbacks, including the need for greater resources, and the need to tolerate trial and error en route, compared to the other 2 models.

---

### Guidelines for the appropriate use of bedside general and cardiac ultrasonography in the evaluation of critically ill patients-part I: general ultrasonography [^d58fab9c]. Critical Care Medicine (2015). Medium credibility.

RAND voting lower-consensus paths — < 50% within 1 integer: If " < 50% of panelists voted within 1 integer," then " ≥ 50% achieved at 1 integer" yields "Some Consensus," while "Other responses or disagreement" lead to "No Consensus"; the bottom labels associate "Conditioned/weak Recommendation" with Some Consensus and "NO Recommendation" with No Consensus, and "Disagreement: if ≥ 30% of panelists voted outside the zone of the median."

---

### Guidelines for the appropriate use of bedside general and cardiac ultrasonography in the evaluation of critically ill patients-part II: cardiac ultrasonography [^177de668]. Critical Care Medicine (2016). Medium credibility.

Cardiac ultrasonography — RAND algorithm for consensus and recommendation grading defines entry by either "All panelists gave same number" or the "% of panelists voting in relation to median score." When " ≥ 50% of panelists voted within 1 integer," users "Look at the range of achieving ≥ 80% of panelists," with categories based on " ≥ 80% achieved at 1 integer," " ≥ 80% achieved at 2 integers," or " ≥ 80% achieved at 3 integers." Branches also include " < 50% of panelists voted within 1 integer" and "Other responses or disagreement?," and labeled outputs include "Strong Recommendation."

---

### Modulation of antibiotic effects on microbial communities by resource competition [^07627903]. Nature Communications (2023). High credibility.

Antibiotic treatment can promote coexistence in resource competition regimes involving generalists

To interrogate how antibiotic activity affects species coexistence, we first applied the coexistence conditions in Fig. 1 to analyze the minimal scenario of two species competing for two resources. We considered four qualitatively distinct types of communities: two specialist consumers (each with only one nonzero consumption rate) with no niche overlap, one generalist (with two nonzero consumption rates) and one specialist, a pair of generalists with preference for distinct resources, and a pair of generalists with preference for the same resource (Fig. 2a). Naively, one might expect that increasing the death rate of a species would generally decrease the probability that it can persist in a community, and hence decrease the proportion of supply rates inside the convex hull, i.e. the size of the coexistence region. For two generalists with preference for distinct resources, as the ratio of their enzyme budgets changed from 1 due to antibiotic perturbation, the coexistence region size indeed decreased (Fig. 2a, red). However, for the trivial case of two specialists, there is no competition and hence the two species coexisted regardless of death rates (Fig. 2a, purple). For a generalist and a specialist, decreasing the enzyme budget of the generalist increased the coexistence region size (Fig. 2a, green), indicating that antibiotic activity targeting a generalist can promote coexistence. For two generalists with preference for the same resource, coexistence region size can exhibit non-monotonic dependence on enzyme budgets (Fig. 2a, yellow).

---

### Universal scaling in real dimension [^39479e3b]. Nature Communications (2024). High credibility.

The concept of universality has shaped our understanding of many-body physics, but is mostly limited to homogenous systems. Here, we present a study of universality on a non-homogeneous graph, the long-range diluted graph (LRDG). Its scaling theory is controlled by a single parameter, the spectral dimension d s, which plays the role of the relevant parameter on complex geometries. The graph under consideration allows us to tune the value of the spectral dimension continuously also to noninteger values and to find the universal exponents as continuous functions of the dimension. By means of extensive numerical simulations, we probe the scaling exponents of a simple instance of < mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:mi > O < /mml:mi > < mml:mrow > < mml:mo > (< /mml:mo > < mml:mrow > < mml:mi > N < /mml:mi > < /mml:mrow > < mml:mo >) < /mml:mo > < /mml:mrow > < /mml:math > symmetric models on the LRDG showing quantitative agreement with the theoretical prediction of universal scaling in real dimensions.

---

### Infectious Diseases Society of America guidelines on the treatment and management of patients with COVID-19 (September 2022) [^4d49f44b]. Clinical Infectious Diseases (2024). High credibility.

Convalescent plasma — hospitalized immunocompromised patients: Among immunocompromised patients hospitalized with COVID-19, the IDSA guideline panel suggests against the routine use of COVID-19 convalescent plasma (Conditional recommendation, very low certainty of evidence). Remark: Patients, particularly those who do not qualify for other treatments, who place a higher value on the uncertain mortality reduction and a lower value on the potential adverse effects of convalescent plasma would reasonably select convalescent plasma.

---

### Infectious Diseases Society of America guidelines on the treatment and management of patients with COVID-19 (September 2022) [^b531c121]. Clinical Infectious Diseases (2024). High credibility.

IDSA COVID-19 guidelines — convalescent plasma: Section last reviewed and updated on 2/22/2023. Last literature search conducted 1/31/2023. Recommendation 13 (UPDATED 2/22/2023): Among immunocompetent patients hospitalized with COVID-19, the IDSA guideline panel recommends against COVID-19 convalescent plasma. (Strong recommendation, Moderate certainty of evidence). Recommendation 14 (NEW 2/22/2023): Among immunocompromised patients hospitalized with COVID-19, the IDSA guideline panel suggests against the routine use of COVID-19 convalescent plasma. (Conditional recommendation, very low certainty of evidence). Patients, particularly those who do not qualify for other treatments, who place a higher value on the uncertain mortality reduction and a lower value on the potential adverse effects of convalescent plasma would reasonably select convalescent plasma.

---

### Artificial intelligence-derived photoplethysmography age as a digital biomarker for cardiovascular health [^0e30c165]. Communications Medicine (2025). Medium credibility.

The technical details of the Dist Loss are described below, and further pseudocode is available in Supplementary Fig. S2. Dist Loss comprises two main components: the sample-wise MAE and the distributional MAE. The sample-wise MAE is equivalent to the conventional MAE loss and computes the absolute error between each prediction and its corresponding ground truth, ensuring individual sample accuracy. The distributional MAE, on the other hand, quantifies the discrepancy between the distribution of model predictions and the true label distribution. It is optimized by minimizing the MAE between a pseudo-label sequence (constructed based on the label distribution) and a pseudo-prediction sequence (obtained by sorting the model predictions).

Specifically, we first employ kernel density estimation (KDE)to obtain a smooth approximation of the label distribution across the dataset. For each label y i, a probability p i is estimated to reflect its overall frequency. For a batch of size B, the expected frequency of y i is given by n i = B ⋅ p i. Since n i is generally non-integer, we take its floor value ⌊ n i ⌋. To ensure that the total frequency matches the batch size B, we compute the residual, where L denotes the number of unique labels. An auxiliary sequenceis then defined aswhich distributes the residual across the labels. The adjusted frequency for each label is given by, ensuring that. Using these adjusted frequencies, we construct the pseudo-label sequencethat reflects the theoretical label distribution. For each position j in the batch (1 ≤ j ≤ B), the pseudo-label is determined bywhere j = 1,…, B indexes the position in the sequence. Essentially, this procedure replicates each label y i according to its adjusted frequency. For instance, if the label set is (40, 50, 60) with adjusted frequencies (1, 2, 3), the resulting pseudo-label sequence is (40, 50, 50, 60, 60, 60). The pseudo-prediction sequenceis obtained by sorting the model predictions in the batch. To ensure differentiability, we employ the fast differentiable sorting method described in ref.

---

### Universality of quantum phase transitions in the integer and fractional quantum hall regimes [^2f227a21]. Nature Communications (2024). High credibility.

Introduction

The quantum Hall (QH) effect, observed in a two-dimensional electron gas subject to a perpendicular magnetic field, realizes multiple quantum phase transitions (QPT) between distinct insulating topological states. The magnetic field B quenches the electronic kinetic energy into disorder-broadened discrete Landau energy levels (LL). All electronic single-particle states are localized, barring those at a specific critical energy E c near the center of each LL, which are extended –. When the Fermi energy lies between the extended states of two successive LLs, the system is in a distinct topological phase characterized by a quantized value of Hall resistance R x y and vanishingly small longitudinal resistance R x x. As the Fermi energy approaches E c, the localization length ξ characterizing the single-particle states diverges as ξ ~ ∣ E − E c ∣ − γ while the slowest time-scale diverges as τ ~ ξ z ~ ∣ E − E c ∣ − z γ. The exponent γ governs the critical divergence of the localization length as the filling fraction or magnetic field approach the critical values and z governs the divergence of the coherence length with decreasing temperatures. From the finite-size scaling theory, Here, ν = n h / e B, n is the areal charge-carrier density, h is the Planck constant, e is the electronic charge, and T is the temperature. One additionally defines the scaling exponent κ = 1/ z γ – that governs this temperature dependence of the slope of R x y as well as the width of the R x x peak at the transition. The values of these three critical exponents (of which only two are independent) have been argued to be universal, with γ ≈ 2.3, κ ≈ 0.42, and z = 1 for all IQH transitions.

---

### Inconsistency between overall and subgroup analyses [^eed221ea]. General Psychiatry (2022). Medium credibility.

In table 3, we use numerical examples to show that if p 1 ≠ p 2 and d 1 d 2 ≠0, all combinations of 1–9 and 13 in table 2 may occur.

Table 3
Numerical examples of Δ 0, Δ 1 and Δ when p 1 ≠ p 2 and d 1 d 2 ≠0

The following theorem gives a more general sufficient condition of consistency than the first three cases discussed above.

Theorem: given Δ 0 and Δ 1, for any p 1 and p 2 between 0 and 1, there always exists a p between 0 and 1 such that Δ = Δ 0 p +Δ 1 (1− p) if and only if p 1 = p 2 or d 1 d 2 ≤ 0.

The proof of this theorem is available on request. Note that d 1 d 2 = 0 implies d 1 d 2 ≤ 0.

Unfortunately, if we are only given the information that p 1 ≠ p 2 and d 1 d 2 > 0, we cannot determine whether the inconsistency will happen. For example, combinations 1 and 3 satisfy the condition of p 1 ≠ p 2 and d 1 d 2 > 0. In combination 1, the overall difference is consistent with the subgroup differences, while it is not in combination 3.

---

### Incorporating economic evidence in clinical guidelines: a framework from the clinical guidelines committee of the American College of Physicians [^27eb44ad]. Annals of Internal Medicine (2025). High credibility.

American College of Physicians economic evidence — resource utilization is defined as the cost of resources required to provide or consume health care services or products to achieve health outcomes in the United States; data sources include medical expenditures for health care interventions and services based on applicability and availability in the United States and the cost of medications paid by insurance and prices set by manufacturers when selling to wholesalers, with sources including national databases and published non-industry-conducted analyses.

---

### Evaluating molecular biomarkers for the early detection of lung cancer: when is a biomarker ready for clinical use? An official American Thoracic Society policy statement [^4009df9d]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

American Thoracic Society policy statement — analytic performance characteristics and result types are defined. Limit of detection is "The lowest assay level at which the presence of the analyte is detected with reliability in repeated measurement." Linearity is "The ability to provide results directly proportional to the amount of analyte in the test sample within a given measuring range." Measurement accuracy is "The closeness of agreement between a measurement result and an accepted reference value; an aggregate of trueness and precision." Performance around the cutoff is "The measurement accuracy of an assay at biomarker levels near the threshold chosen to distinguish a positive and negative result for the intended use of the test." Precision is "The closeness of agreement of replicate test results under stipulated conditions," with repeatability defined as "Precision when replicate measurements are taken under the same conditions (within a run)" and reproducibility as "Precision when one of the conditions being varied is the laboratory for an in vitro diagnostic measurement." A qualitative result is "A biomarker result consisting of a set number of possible responses (often two)," whereas a quantitative result is "A biomarker result that is numerical in amount or level of a physical quantity," and semiquantitative denotes "Results of a test that fall into an approximate range of values." Signature refers to "Multiple variables combined to provide a single result," and medical tests are "Results of a clinical, imaging, or laboratory-based assay that are used alone or with other information to help assess a subject's health condition of interest, or target condition."

---

### Scalable spatiotemporal prediction with Bayesian neural fields [^fea8687d]. Nature Communications (2024). High credibility.

Prediction queries

We can approximate the posterior (13) using a set of samples, which may be obtained from either MAP ensemble estimation or stochastic variational inference (by sampling from the ensemble of M variational distributions). We can then approximate the posterior-predictive distribution(which marginalizes out the parameters Θ) of Y (r *) at a novel field index r ✱ = (s *, t *) by a mixture model with M equally weighted components:Equipped with Eq. (23), we can directly compute predictive probabilities of events { Y (r *) ≤ y }, predictive probability densities { Y (r *) = y }, or conditional expectationsfor a probe function. Prediction intervals around Y (r *) are estimated by computing the α -quantile y α (r *), which satisfiesFor example, the median estimate is y 0.50 (s *, t *) and 95% prediction interval is [y 0.025 (s *, t *), y 0.975 (s *, t *)]. The quantiles (24) are estimated numerically using Chandrupatla's root finding algorithmon the cumulative distribution function of the mixture (23).

Temporal seasonal features

Including seasonal features (c.f. Eq. (8)), where possible, is often essential for accurate prediction. Example periodic multiples p for datasets with a variety of time units and seasonal components are listed below (Y = Yearly; Q = Quarterly; Mo = Monthly; W = Weekly; D = Daily; H; Hourly; Mi = Minutely; S = Secondly):
Q: Y = 4
Mo: Q = 3, Y = 12
W: Mo = 4.35, Q = 13.045, Y = 52.18
D: W = 7, Mo = 30.44, Q = 91.32, Y = 365.25
H: D = 24, W = 168, Mo = 730.5, Q = 2191.5, Y = 8766
Mi: H = 60, D = 1440, W = 10080, Mo = 43830, Q = 131490, Y = 525960
S: Mi = 60, H = 3600, D = 86400, W = 604800, Mo = 2629800, Q = 7889400, Y = 31557600

---

### Evolutionary dynamics of behavioral motivations for cooperation [^4f366bb1]. Nature Communications (2025). High credibility.

Fig. 6
Evolution of behavioral motivation.

a The theoretical prediction of the evolutionary direction of behavioral motivations for (b / c) < (b / c) *. The evolution results in individuals transitioning away from undemanding philanthropists (region II) and demanding aspirationalists (region IV), and towards demanding philanthropists (region I) and undemanding aspirationalists (region III), as indicated by the arrows. This transition leads to a decrease in the abundance of cooperation x C. The blue (respectively red) regions represent the behavioral motivation that contributes to a larger abundance of cooperation than defection (respectively defection than cooperation). b The Monte Carlo simulations show the evolutionary trajectories of behavioral motivations for (b / c) < (b / c) * using 300 simulations in random regular networks. Each simulation starts from a monomorphic population with behavioral motivations (α, λ) in one of (0.75,0.05), (0.25,0.05), (0.25,− 0.05), and (0.75,− 0.05), represented by open black dots, and undergoes 5 × 10 7 motivation updating steps, where interactions and action updates repeat for T = 100 rounds before each motivation updating. During each behavioral motivation update, with a probability of 0.01, the imitated behavioral motivation is subject to a random fluctuation in need α (randomly sampled from the range [− 0.1,0.1]) and motivation intensity λ (randomly sampled from the range [− 0.01,0.01]). Each thin red line represents a resulting trajectory (i.e. the average behavioral motivation of the population), and each open red dot represents an ending behavioral motivation. The thick red line represents the linear regression of all final behavioral motivations, which is highly consistent with the evolutionary direction predicted analytically in (a). c The cooperation abundance throughout the evolutionary process (b / c) < (b / c) * by simulations. The highlighted line represents the average cooperation abundance over the 300 simulations. d The theoretical prediction of the evolutionary direction of behavioral motivations for (b / c) > (b / c) * shows that individuals evolve towards undemanding philanthropists (region II) and demanding aspirationalists (region IV). e The evolutionary trajectories of behavioral motivations for (b / c) > (b / c) * by simulations. f The cooperation abundance throughout the evolutionary process for (b / c) > (b / c) * by simulations. Parameter values: N = 100, d = 6, which gives (b / c) * ≈ 6.7, b = 2, c = 1 (bc) and c = 0.2 (ef).

---

### Estimating global, regional, and national daily and cumulative infections with SARS-CoV-2 through nov 14, 2021: a statistical analysis [^5284c7ec]. Lancet (2022). Excellent credibility.

R effective estimation in the past

Using daily infections, we directly estimated R effective in the past by location and day, where R effective at time t is:

The assumptions required for this estimation are the duration from infection to being infectious and the period of infectiousness, collectively represented as θ. We used ranges of 3–5 days for both assumptions to generate estimates of R effective in the past. These estimates are useful for identifying the effect of different non-pharmaceutical interventions on transmission in different settings. An R effective lower than 1·0 indicates that the epidemic is shrinking, whereas an R effective higher than 1·0 indicates that the epidemic is growing.

We compared R effective to an estimate of total immunity in the population of location l at time t (presented as weekly averages), where this value is calculated as:

The proportion of the population effectively vaccinated is a function of doses administered and brand-specific efficacy and is discounted for existing natural immunity at the time of delivery.

Role of the funding source

The funders of the study had no role in the study design, data collection, data analysis, data interpretation, or the writing of the report.

---

### Guidelines for the appropriate use of bedside general and cardiac ultrasonography in the evaluation of critically ill patients-part I: general ultrasonography [^4f4fa27b]. Critical Care Medicine (2015). Medium credibility.

RAND voting consensus thresholds — ≥ 50% within 1 integer: Under "Degrees of Consensus & Grades of recommendation," when " ≥ 50% of panelists voted within 1 integer," panelists then "Look at the range of achieving ≥ 80% of panelists"; " ≥ 80% achieved at 1 integer," " ≥ 80% achieved at 2 integers," and " ≥ 80% achieved at 3 integers" correspond to "Perfect Consensus," "Very Good Consensus," and "Good Consensus," respectively, which align with the displayed "Strong Recommendation."

---

### Digital measurement of SARS-CoV-2 transmission risk from 7 million contacts [^62c6b9de]. Nature (2024). Excellent credibility.

Statistical modelling of the per-exposure-window probability of transmission

In reality, a given individual who reported a positive test was infected either by the background, or in their first recorded window, or in their second recorded window, and so on, but which of these was actually the case is unknown. Hence we modelled the process in terms of risk parameters, shared between individuals, that are to be estimated. We developed a statistical model for the separate contributions to each individual's overall risk from each of their exposure windows and from background risk. Specifically, we modelled the probability of individual i not reporting a positive test during the observation interval aswhere B i is the probability of background transmission (followed by reporting of a positive test) and P t (i 's n th window) is the probability of transmission during the i 's n th window (followed by reporting of a positive test). The justification for this form is that if an individual does not report a positive test, this implies that they were not infected by the background (with subsequent reporting) and were not infected during their first window (with subsequent reporting) and not during their second window, and so on. The probabilities for each of these events not happening should thus be multiplied to give the overall probability for none of them happening. We modelled B i as 1 – (1 – b i) β, defining b i as the sum, over the 14 days following i 's notification, of the weekly smoothed mean daily fraction of geographically matched, not-recently-notified app users that reported a positive test (β is the associated regression coefficient for this term). For small values of b i the background risk is simply rescaled by factor β — that is, B i ≈ βb i; for larger values of b i the functional form accounts for saturation of risk. We modelled P t (i 's n th window) as depending only on the risk score recorded by the app for i 's n th window. We binned risk scores into eight bins, defining a single independent P t parameter for each bin, such that the expression above could be rewritten as

---

### Quantifying the unknown impact of segmentation uncertainty on image-based simulations [^50ae5640]. Nature Communications (2021). High credibility.

EQUIPS then probes the segmentation uncertainty at three probability values: μ − σ, μ, and μ + σ, which we will call the standard segmentations (Fig. 2 h). Here, μ is the mean, 50.0 percentile, and σ is the standard deviation away from the mean, 15.9 and 84.1 percentiles at μ − σ and μ + σ, respectively. We chose these particular percentile values to quickly model the uncertainty distribution in a physics quantity using a characteristic distribution. However, any percentile from the probability map could be used to generate an image segmentation representing that probability value.

For each standard segmentation, we perform a physics simulation to predict the physics quantities (Fig. 2 i). Each inset in Fig. 2 i highlights a region of the domain geometry that undergoes significant alterations. These geometry changes impact the physics quantity behavior and, as a result, these changes manifest in the physics quantity uncertainty distribution. For example, in the μ − σ case, the middle inset shows a material region that is disconnected from the surrounding region and therefore has low heat flux. In contrast, the μ + σ case shows a material region that is fully connected to its neighbors and therefore has a much higher heat flux.

Throughout this work, we focus on a Normal (Gaussian) distribution as the characteristic distribution to rapidly approximate a physics quantity uncertainty distribution using the fewest possible simulations. Only the three standard segmentation simulations are necessary to specify this characteristic distribution. The physics quantity evaluated using the 50.0 percentile segmentation provides the distribution mean, while the 15.9 and 84.1 percentile segmentations provide the standard deviation. The curve in Fig. 2 j is the CDF of the characteristic distribution estimate calculated using this method while the red, black, and green points are the standard segmentation values. This characteristic CDF estimates the uncertainty distribution of a physics quantity as a result of segmentation uncertainty. If the characteristic distribution estimate fits the physics quantity data points well, then the calculation of additional percentile segmentations is likely unnecessary. However, if the characteristic distribution is poor (i.e. the unknown distribution is non-Normal), then additional percentile segmentations are required to capture the underlying physics quantity uncertainty distribution.

In the following discussion, we present three exemplar problems demonstrating the propagation of segmentation uncertainty from image segmentation through physics simulations to physics quantities. In each of these exemplars, simulation conditions and model parameters are held constant to draw attention to the impact that segmentation uncertainty has on the uncertainty distribution of physics quantities.

---

### Finding gene network topologies for given biological function with recurrent neural network [^4ce0b75d]. Nature Communications (2021). High credibility.

Results

Idea demonstration with adaptation

We first demonstrate the basic idea and the implementation of our method with adaptation, a simple, ubiquitous, and well-studied cellular function. In this task, the output node should sense the change of the input stimulus and then return to its pre-stimuli level even the stimulus persists (Fig. 1b). Regulatory networks (with no more than three nodes) for this function have been exhaustively studied. Only two genesplus one input signal I are considered here for simplicity. Here, represents the synthesis rate of the genes.

The functioncontains information about gene-gene interactions, which is usually written in explicit formulas but is now simulated by a small feed-forward NN (Fig. 1a). This feed-forward NN uses the current value ofandas its input and generates f as its output, thereby computes. So, this is a kind of NN-based auto-regressor. By defining f to be non-negative (between 0 and 1), and with the explicit liner degradation terms, our formulation automatically prevents g from diverging. (We simply set, since degradation in reality can also be accounted by diagonal terms in f). Time evolution of the regulatory system then corresponds to iteration of this NN block, yielding an RNN model. Note that while a full-fledged RNN passes hidden-layer information from each time point to the next, in our case, only observablesare passed from t to t + dt.

The Euclidian distance between model trajectory and the target response curveis used as Loss function for training. Here, we require the output node to carry out the adaptation function (Fig. 1b). The other node has no functional constraints and can play a regulatory role. Therefore. With a step-like input signal I (t), the target response curveshould in general be pulse-like — having a fast response phase and a slower recover phase. Any curve with this kind of shape can serve as the training target, and that used in Fig. 1 is simply defined as the sum of two exponentials. (See Fig. S 1a–c for another case). Training converges quickly, yielding perfect adaptation (Fig. 1b) — that is, a negligible adaptation error (difference between the pre-stimulus and the fully adapted g 1 levels) as well as high sensitivity (response peak).

---

### Net risk reclassification p values: valid or misleading? [^b023a91d]. Journal of the National Cancer Institute (2014). Low credibility.

Background

The Net Reclassification Index (NRI) and its P value are used to make conclusions about improvements in prediction performance gained by adding a set of biomarkers to an existing risk prediction model. Although proposed only 5 years ago, the NRI has gained enormous traction in the risk prediction literature. Concerns have recently been raised about the statistical validity of the NRI.

Methods

Using a population dataset of 10000 individuals with an event rate of 10.2%, in which four biomarkers have no predictive ability, we repeatedly simulated studies and calculated the chance that the NRI statistic provides a positive statistically significant result. Subjects for training data (n = 420) and test data (n = 420 or 840) were randomly selected from the population, and corresponding NRI statistics and P values were calculated. For comparison, the change in the area under the receiver operating characteristic curve and likelihood ratio statistics were calculated.

Results

We found that rates of false-positive conclusions based on the NRI statistic were unacceptably high, being 63.0% in the training datasets and 18.8% to 34.4% in the test datasets. False-positive conclusions were rare when using the change in the area under the curve and occurred at the expected rate of approximately 5.0% with the likelihood ratio statistic.

Conclusions

Conclusions about biomarker performance that are based primarily on a statistically significant NRI statistic should be treated with skepticism. Use of NRI P values in scientific reporting should be halted.

---

### Value computations underpin flexible emotion expression [^02b2f7f6]. Communications Psychology (2025). Medium credibility.

We ran 100 chains in parallel for 25,000 samples after a burn-in of 25,000, thinning the samples by a factor of 10. This yielded a total of 250,000 samples to construct the posterior distribution over parameter estimates. We additionally implemented a probabilistic migration step, α = 0.0005, with every MCMC step in place of the differential evolution to improve chain-mixing and convergence towards the high probability density region of the posterior distribution of parameters. The migration step cycles the positions of a subset of chains (N migrate uniformly sampled from the total number of chains, i.e. 100) such that the positions of chains {i, i + 1,… j-1, j} were compared against {i + 1, i + 2,…, j, i} and evaluated based on the Metropolis-Hastings algorithm.

Since parameters such as response threshold and non-decision times were theoretically constrained to be positive, we fitted log-transformed parameters that were subsequently exponentiated to obtain the true diffusion model parameter. we assumed that participant-level parameters were normally distributed with respect to a set of hyperparameters: population mean and standard deviations and set priors over the population means to be a random normal distribution, N(0,1), and over the population standard deviations to be a half-cauchy distribution, HC(0,1). To model differences across the three conditions, we initialized two additional sets of hyperparameters. One set of these hyperparameters captured the difference between population means of the Expression and Self-Report condition, while the other set captured any additional differences between the population means of the Reputation and Expression condition. These population-level differences in conditions were similarly assumed to be drawn from a prior normal distribution N(0,1) which instantiates the null hypothesis that computational model parameters do not differ between the Self-Report, Expression, and Reputation conditions. Variances across participants were assumed to be homogenous and thus only a single population standard deviation was estimated across the three conditions.

---

### Orbitofrontal signals for two-component choice options comply with indifference curves of revealed preference theory [^c5665421]. Nature Communications (2019). High credibility.

To visualize bundle distinction and choice prediction, we performed linear discriminant analysis (LDA). Using the neuronal responses, LDA discriminated bundles numerically and graphically across ICs (P < 0.0001, Wilcoxon rank-sum test against the shuffled data; Fig. 4e, f; dots vs. triangles), but not along ICs (P > 0.08; colors; pairwise bundle comparisons; Supplementary Tables 11–14). This differential discrimination pattern was also seen with entirely unselected neuronal responses (Supplementary Fig. 23d). Thus, LDA confirmed the SVM results, including bundle indiscriminability within ICs.

Thus, the decoders using scalar neuronal signals accurately distinguished vectorial bundles and predicted choice across ICs from neuronal responses, thus confirming the validity of the neuronal revealed preference code. Given this discriminatory accuracy, the decoders found no distinctions between bundles located on same ICs during choice over zero bundle and choice between two nonzero bundles.

Arrow's Weak Axiom of Revealed Preference (WARP)

Arrow's WARP defines a necessary condition for optimal choice and utility maximization. An option that is revealed preferred to all other options in a given option set should remain revealed preferred when the option set is reduced to a smaller subset by removing one or more of the alternative options. In general compliance with Luce's stochastic choice theory, we asked whether the same bundle that is revealed preferred to all other options within a set of three nonzero bundles { x, y, z } (choice P > 0.33) would remain revealed preferred within a subset of two nonzero bundles { x, y } (choice P > 0.5). This requirement was satisfied with convex ICs (Fig. 5a, b) and concave ICs (Supplementary Fig. 24a, b).

---

### Clinical utility of polygenic risk scores for embryo selection: a points to consider statement of the American College of Medical Genetics and genomics (ACMG) [^9ccd284a]. Genetics in Medicine (2024). High credibility.

PGT-P results communication — Figure 3 illustrates using 3 components (percentile rank, relative risk, and absolute risk): families (a) and (b) have embryos at rank extremes while family (c) shows a broader range; two families display different relative risk ranges, with family x values 0.9, 1.0, 1.1, 1.2 and family y values 0.6, 0.9, 1.1, 1.4; example absolute risk increases span 1.0% - 1.4% versus 10.0% - 14.0% for diseases differing by 10x population prevalence.

---

### Polymer collapse in miscible good solvents is a generic phenomenon driven by preferential adsorption [^101b82e5]. Nature Communications (2014). Medium credibility.

Discussion

We now develop a generic microscopic picture of the polymer conformation in mixed solvents. The compact globular structure is driven by the preferential attraction of cosolvent to the polymer. At low cosolvent concentration x c →0, c molecules can bind to two distinctly far monomers inducing bridges that initiate the collapsing process. At high concentrations, when a large number of c molecules is added (x c →1) they decorate the polymer and allow for the extended coil structure. In Fig. 3, we show simulation snapshots during polymer collapse for N l = 100.

It is apparent that the polymer collapse is initiated by several patches along the backbone. A significant Ω-type loop is visible at t = 300τ (Fig. 3b) before finally collapsing into a compact globule (Fig. 3e). From Fig. 3, it is also evident that one can distinguish two types of cosolvent molecules among those that decorate the polymer. A fraction φ B forms bridges between two (distinctly far) monomers and a fraction φ binds to one monomer only. In the collapsed state, one expects to see an increased value of φ B. On the other hand when the polymer re-opens, bridging should vanish φ B →0. In Fig. 4a, we show φ B as a function of x c for N l = 100. As expected, the data (red stars) show a distinct hump between 0.1 < x c < 0.5 consistent with the collapsed conformation observed in Fig. 1. Figure 4a also shows that the strength of the effective negative excluded volume (green diamonds), extracted from the inverse variation of the gyration radius −≈[{ R g / R g (x c = 0)} −3 −1], is proportional to φ B.

---

### A multi-step model of Parkinson's disease pathogenesis [^e2cef97a]. Movement Disorders (2021). Medium credibility.

Box 1
Armitage and Doll Multistep Model – Heuristic Argument*

If disease development depends on one step, incidence in a given year will be proportional to the chance of undergoing that step:If two‐steps are required, then incidence is the product of the chance of undergoing the first step by age t and the rate of undergoing the second step:And for n‐steps:Taking log of both sides returns the equation for a straight line:The slope is one less than the number of steps required to develop the disease, and the intercept represents the combined probability of undergoing these steps.

*see Webster 2019 for full derivation.

A multistep model of pathogenesis could account for many of the epidemiological observations made in PD. These include the variability in the expression of disease and age of onset in carriers of disease‐causing mutations, and the multiple environmental and genetic associations that confer a risk of developing PD. A multistep model could also explain the phenotypic variability seen in PD, if it is assumed that at least some steps apply to specific neuronal populations rather than the nervous system as a whole. Furthermore, the predictions arising from such a model can be used to test specific hypotheses about basic observations in PD, such as whether the higher incidence and prevalence of PD in males observed in most parts of the world relates to differential environmental exposures by sex.

---

### Quantifying the unknown impact of segmentation uncertainty on image-based simulations [^9ef4f637]. Nature Communications (2021). High credibility.

To help further illustrate this critical result, the region between the (μ + σ)- and (μ − σ)-percentile segmentation ratio curves are shaded dark gray and the region between the 10- and 90-percentile segmentation curves is shaded light gray. At the ratio peak, the behavior is intuitive (i.e. monotonic) with the shaded regions enveloping all their respective curves. However, later in time, the ratio gradually becomes increasingly non-monotonic with percentile segmentation. At t = 7.8 s the 10- and 90-percentile segmentation ratios fail to bound other percentile segmentation ratios and ultimately intersect each other moments later. Moreover, each ratio becomes progressively intertwined with others, such that it becomes difficult to predict how a new percentile segmentation ratio calculation would fall on this graph.

The δ τ curves portrayed in Fig. 5 h emphasize the caution required in bounding segmentation uncertainty using only a few simulations. In the first two exemplars, the results are monotonic with increasing percentile segmentations, and the approximation of the uncertainty distribution using a CDF is representative. However, that does not have to be the case with more complicated models. For example, simply using (μ ± σ)-percentile segmentations to bound the segmentation uncertainty not only fails to encompass all of the intermediate percentile values, it also fails to capture the mean behavior. While this does not invalidate our approach to segmentation uncertainty quantification, it does urge caution in blindly performing simulations for only the standard segmentations for a new exemplar without first checking more intermediate segmentations.

---

### Transient power-law behaviour following induction distinguishes between competing models of stochastic gene expression [^e57f79c6]. Nature Communications (2025). High credibility.

Figure 2 A–C shows the probability-probability (P-P) plots comparing cumulative distribution functions of the effective telegraph and the N -state models for the 500 rate parameter sets with the largest WD of those sampled. Very similar distributions fall close to the dashed red line (representing identical distributions), the greater the deviations, the more disagreement between the distributions. For each of the N = 3, 4, 5 state models, we see minor deviations from this line, indicating broad agreement between the compared distributions. As the number of states increases, we see greater, but not significantly so, deviations as the bulge around this line broadens. Overall these P-P plots show that even for the most statistically distinct distributions, the difference between these distributions is not very large. This suggests that in many cases the telegraph model provides an excellent approximation to the N -state models with N > 2.

To obtain a deeper insight into the deviations, we categorised the N = 3, 4, 5 state rate parameter sets according to the shape and statistical properties of their mRNA count distributions. All sampled distribution shapes fell into the following four groups: shape I — unimodal with Fano factor < 2; shape II — unimodal with Fano factor ≥ 2; shape III — bimodal with one zero and one non-zero mode; shape IV — bimodal with two non-zero modes. Note that the single mode of both shapes I and II can be either at zero or at a non-zero value. A typical distribution of each shape is seen in Fig. 2 D–G, along with the WD between the 5-state and the effective telegraph distribution. A comparison of the WD of these four shapes suggests that the quality of the effective telegraph model fits decreases with increasing shape number, being best for shape I and worst for shape IV.

---

### Role of apolipoprotein B in the clinical management of cardiovascular risk in adults: an expert clinical consensus from the National Lipid Association [^24b00f90]. Journal of Clinical Lipidology (2024). High credibility.

Correlation and discordance of apoB, non-HDL-C, and LDL-C — population data show high correlations yet wide variability at the individual level. It is well-established that there exists discordance in LDL-C and non-HDL-C for any given value of apoB, and United Kingdom data report correlations of ≥ 0.95 between apoB and non-HDL-C at the population level with a slightly lower degree of correlation between apoB and LDL-C, yet wide variation in apoB for any given non-HDL-C or LDL-C value. In NHANES 2005–2016 untreated adults, for a non-HDL-C of 100 mg/dL (~10th percentile), the apoB range covering 95% of values was 52–78 mg/dL (~5–30th percentile), and for a non-HDL-C of 160 mg/dL (~70–80th percentile), 95% of apoB values were 88–112 mg/dL (~40–80th percentile), with similar findings for LDL-C compared to apoB. Conditions such as hypertriglyceridemia, obesity, and insulin resistance can yield lower LDL-C but still elevated apoB levels.

---

### What are small, medium and large effect sizes for exercise treatments of tendinopathy? A systematic review and meta-analysis [^6092a8e2]. BMJ Open Sport & Exercise Medicine (2023). High credibility.

Description of effect size distributions

From the initial 1454 outcomes extracted, a total of 38 outliers were removed from the analysis with a lower bound threshold of –0.82 (6 effect sizes below) and an upper bound threshold of 7.0 (32 effect sizes above). Across all outcomes and tendinopathy locations, direct calculation of the 0.25 (small), 0.5 (medium) and 0.75 quantiles (large) from the complete empirical data returned the following SMD pre values: 0.37, 0.77 and 1.31, respectively. Application of the meta-analysis model across the data with borrowing of information across studies resulted in similar but shrunken estimates (0.25 quantile 0.5 = 0.34 (95% CrI: 0.31 to 0.37); 0.5 quantile 0.5 = 0.73 (95% CrI: 0.70 to 0.77) and 0.75-quantile 0.5 = 1.21 (95% CrI: 1.17 to 1.27)). A forest plot of effect sizes illustrating effect sizes across studies is presented in online supplemental file 12. Sensitivity analyses checking the potential influences of study type and study quality are presented in online supplemental file 13. No evidence was obtained of greater effect sizes with quasi-experimental designs or with studies identified as high risk of bias.

Analyses of effect size distributions across the different tendinopathy locations are illustrated in figure 2 with numerical values presented in table 3. Analyses were pooled across all outcome domains as a means to compare tendinopathy locations with the largest amount of data possible. Analysis of the modelled small, medium and large thresholds showed considerable overlap in small and medium thresholds across all tendinopathy locations (0.25 quantile 0.5 ranged from 0.28 to 0.38; 0.5 quantile 0.5 ranged from 0.70 to 0.82). However, greater divergence was identified for large threshold estimates, with the greatest values estimated for the elbow (0.75 quantile 0.5 ranged from 1.18 to 1.49).

Figure 2
Effect size distributions across tendinopathy locations with identification of small, medium and large thresholds. black curve represents density plot of empirical effect size distribution. Diamonds with intervals represent small, medium and large thresholds with credible intervals (black: all outcomes; red: tendinopathy specific). RCRSP, rotator cuff related shoulder pain.

---

### Advancing value-based medicine: why integrating functional outcomes with clinical measures is critical to our health care future [^00922699]. Journal of Occupational and Environmental Medicine (2017). Medium credibility.

University of California WorkStrong program — UC launched a program in 2012 to reduce disability costs, with UC Los Angeles reporting 4181 employees with multiple workers' compensation claims (15,944 injuries), including 968 (23%) with five or more claims and 223 (5%) with 10 or more claims; participation studies indicate improved overall function alongside reduced disability costs, and Figure 1 presents actual versus projected workers' compensation claims 31 months after graduation.

---

### Practical guidance for the evaluation and management of drug hypersensitivity: specific drugs [^45c8244a]. The Journal of Allergy and Clinical Immunology: In Practice (2020). High credibility.

Valproic acid oral desensitization protocol — over 2-day intervals, escalating daily doses are scheduled across Interval (d) steps 1–2, 3–4, 5–6, 7–8, 9–10, 11–12, 13–14, 15–16, 17–18, 19–20, 21–22, 23–24, 25–26, 27–28, and 29–30 with corresponding Daily dose (mg) values of 0.05, 0.1, 0.25, 0.5, 0.75, 1, 2.5, 5, 10, 25, 50, 100, 200, 400, and 800; formulation transitions at later steps specify 1 tablet, 1 tablet twice a day, and 2 tablets twice a day, and solutions are prepared to concentrations of 0.1 mg/mL (Solution A), 5 mg/mL (Solution B), and 200 mg/mL oral solution with 200 mg tablets (Solution C/tablet details).

---

### Anomalous elastic properties across the γ to α volume collapse in cerium [^c571fc01]. Nature Communications (2017). Medium credibility.

The adiabatic bulk modulus decreases in the γ-phase for all temperatures until it reaches its minimum at the VC transition and starts rising again continuously in the α-phase. With higher temperature, the minimum moves to a higher pressure and becomes deeper. Since B T = 0 at the critical point, and B S = B T C P / C V it follows that B S should fall to zero at the critical point just as well since C P and C V are nonzero. Our lowest observed value is B S ~ 6 GPa for p = 1.55 GPa at 481 K which could mean that our temperature was just barely different from the critical temperature. It is possible to fit the pressure dependence of B S near the transition to a power law B S (P) ~ | P − P VC | α. The solid lines in Fig. 3a show the resulting fits to the data. The values for P VC and α are listed in Table 1 for a pressure range of about 0.4 GPa below (−) and above (+) the transition. However, except for the critical temperature, the transition already takes place at P < P VC. This was observed previously by X-ray diffraction for B T and ultrasonic measurements for B S at room temperature which found α − = 0.46, P VC − = 0.83 GPaand α − = 0.42, P VC − = 0.92 GPa, respectively. At the critical temperature (481 K) we find α − = 0.26 and α + = 0.28, basically equal within the experimental uncertainty. An exponent of 0.5 would indicate Gaussian fluctuationsassociated with electron–phonon coupling, an exponent of zero is expected in the mean-field theory. Our value of α = 0.27 ± 0.01 is the only one taken approximately on the critical isotherm which allows a straight forward interpretation. All other values were obtained at lower temperatures for which the descent of the bulk modulus towards zero is interrupted by the transition and thus the measured exponents are not truly "critical". Additionally, the further the temperature is from T C the larger is the gap in the transition pressures P VC − and P VC + which we list in Table 1 as Δ P. This value can serve as a measure of hysteresis that disappears approaching the critical temperature. Table 1 also provides a comparison with previously published results and includes values for fits to the data of refs. that were obtained within the current effort. With the exception of the 481 K isotherm, all exponents measured on the lower pressure side are larger than the ones from the higher-pressure side which one would expect similarly for the behavior of a van der Waals liquid/gas near the critical point.

---

### A Bayesian mixture model for predicting the COVID-19 related mortality in the United States [^24c047b0]. The American Journal of Tropical Medicine and Hygiene (2021). Medium credibility.

Specifically, let the mortality at day t be comprised as, where y i (t) represents the number of deaths at day t attributed to a surge or sub-epidemic indexed by i = 1,…, K, K ≥ 1 is the number of sub-epidemics empirically identified, and E (y i [t]) = μ i (t). The trajectory μ i (t) for surge i can be modeled by a homogeneous Gaussian function μ i (t) = M i *ϕ(t; t peak, i, σ i), where M i is the overall mortality, t peak, i is the time to peak, and σ i represents the width for surge i. The mortality curve for the whole pandemic E (y [t]) = μ(t) is then decomposed as:Letbe the overall mortality of the pandemic, thenis the proportion of all deaths attributed to surge i, with M i = M π i.

Therefore, we proposed the following mixture curve model for modeling the trajectory of daily COVID-19 mortality:where M is the overall mortality and ϕ(t; t peak, i σ i) represents the part of the curve related to some underlying factor(s), π i is the proportion of total deaths that are attributed to such factor(s) indexed by i, and K is the number of sub-curves comprising the mixtures. The number of parameters identifying the curve is 3K: M, π 1…, π K −1, (t peak, 1 σ 1),…, (t peak, K σ K). The ϕ(t; t peak, i σ i) here is the Gaussian density function, defined by the location parameter t peak, i, which represents the time from the first death to apex, and the scale parameter σ i, which represents the spread of the curve. We propose a Bayesian approach for estimating the curve parameters and subsequently any other statistics of interest, with values for the parameters being drawn from their posterior distribution condition on the observed data. We assume the distribution for the observed mortality data y (t) at time t to be negative binomial:where N is the state's population size. A negative binomial distribution function is used for Y (t), as it is appropriate for count data, with. Bayesian Markov Chain Monte Carlo methods are used to make draws from the posterior distribution of the unknown parameters and derive future projections. Weakly informative prior distributions were used for all model parameters(see).

---

### Non-equilibrium critical scaling and universality in a quantum simulator [^a55212f1]. Nature Communications (2025). High credibility.

Fig. 4
Unscaled (a) and scaled (b) fluctuations after a double quench.

a We plot the unscaled fluctuations along y direction at the predicted critical points for system sizes of N = 10–50 ions. The second quench is applied when the fluctuations following the first quench reach their maxima and the time t 2 is counted after the second quench. b We apply the same scaling collapse technique as for the single quench to find the best scaling exponents (α 2, ζ 2) for the experimental data. See Supplementary Fig. 4 for numerical simulations of the data. We observe that the critical fluctuations do not monotonically grow for increasing system sizes, as would be expected from the scaling relations. This effect can be attributed to the imperfect switching time between the first and second quench; a nearly perfect collapse can be reproduced numerically using the precise switch times (SI Sec. VII.D). We also report exponents found by a power-law fit of the maximum fluctuations which agree more closely with the analytical prediction (Inset b.). While determining the experimental critical exponents, we have excluded 50 ion data (gray) [see main text for details]. The error bars of the experimental data are statistical fluctuation around the mean value.

Experimental decoherence causes the observed fluctuations to be damped for both single and double quenches. We see that the unscaled 50 ion fluctuations after the double quench are significantly damped (Fig. 3 a). The major sources of decoherence, which scale with the system size, remain within acceptable thresholds for system sizes N < 50, but these errors start to dominate for N ≥ 50 ("Methods"). This effect is more adverse for the double-quench sequence than the single-quench, since the former involves longer evolution under two quenches. For completeness, we have included all the 50 ion data in Fig. 4 a, c, but excluded it in determining the best collapse exponent. Fitting the maximum amplitudes of the fluctuations toyields exponent α 2 = 0.69(9), with tighter error bounds (Inset Fig. 4 c). Errors in identifying the peak fluctuation result in erroneous switch time between the two quenches, contributing to imperfect exponents. This effect can be reproduced in the simulation with exact experimental parameters, and correction for such errors in further simulations results in exponents that are well in agreement with the analytically predicted non-equilibrium values (SI Sec. VII.D).

---

### 2022 AHA / ACC key data elements and definitions for cardiovascular and noncardiovascular complications of COVID-19: a report of the American college of cardiology / American Heart Association task force on clinical data standards [^fbac12da]. Journal of the American College of Cardiology (2022). High credibility.

COVID-19 symptom status — this data element captures the "Presence or absence of symptoms attributable to acute COVID-19," with permissible values "Symptomatic," "Asymptomatic," and "Unknown," and the note "See list of possible symptoms in Appendix 6."

---

### "Binary" and "non-binary" detection tasks: are current performance measures optimal? [^8d7e974c]. Academic Radiology (2007). Low credibility.

We have observed that a very large fraction of responses for several detection tasks during the performance of observer studies are in the extreme ranges of lower than 11% or higher than 89% regardless of the actual presence or absence of the abnormality in question or its subjectively rated "subtleness". This observation raises questions regarding the validity and appropriateness of using multicategory rating scales for such detection tasks. Monte Carlo simulation of binary and multicategory ratings for these tasks demonstrate that the use of the former (binary) often results in a less biased and more precise summary index and hence may lead to a higher statistical power for determining differences between modalities.

---

### Nonlinear delay differential equations and their application to modeling biological network motifs [^f37098d5]. Nature Communications (2021). High credibility.

Methods

Analytics and numerical simulation

Analytics were in general performed by hand, and checked for validity using Mathematica. Numerical simulations were run in Matlab using the dde23 delay differential equation solver for DDEs and ode45 for ODEs. Simulating activators as repressors with n < 0 technically fails when x is identically zero (Eq. (5)), since that would imply division by zero, but the limit as x goes to zero causes the regulation term to be zero, which is the same result as assumed by our notation. An initial value of exactly zero for x can thus lead to a divide-by-zero error in simulations, and so initial conditions of exactly zero were not used, as that case is an uninteresting fixed point for activators in any case. Note also that the consitutive case for Eq. (5) is degenerate, in that n = 0, α ≠ 0 is equivalent to n ≠ 0, α = 0 with α 0 → α 0 + α /2.

Phase plot simulations and analysis

For autoregulation phase plots, simulations were run with 100 constant-history initial conditions spread logarithmically between 10 −4 and 2 η and run from T = 0 to T = 100(γ + 1). Solutions were considered stable if for all 100 simulations the maximum absolute value of the discrete derivative in the last three-quarters of the simulation time was less than 0.1. Stable solutions were sub-categorized as bistable if a histogram of final values over all 100 solutions had more than 1 peak. Solutions were considered oscillatory if the average Fourier transform of the last three-quarters of the simulation time for all 100 solutions had more than zero peaks with amplitude (square root of power) greater than 100. Solutions were considered spiral if this oscillation condition held for the first one-quarter of the simulation time only. For two-component loops, initial conditions were used that ranged between 0 and, for equal X and Y and for apposing X and Y. Bistability was determined as for autoregulation, and a cutoff of 0.05 was used to determine "low" values. All simulation histories were constant except where indicated in Supplementary Fig. 7. Specific parameter values and simulation details are given in the figures and/or made explicit in the MATLAB code in Supplementary Data 1.

---

### Clinical policy: critical issues in the evaluation of adult patients presenting to the emergency department with acute blunt trauma [^bc848602]. Annals of Emergency Medicine (2024). High credibility.

Translation of classes of evidence to recommendation levels — Level A recommendations are generally accepted principles for patient care that reflect a high degree of scientific certainty (eg, based on evidence from one or more Class of Evidence I or multiple Class of Evidence II studies that demonstrate consistent effects or estimates). Level B recommendations are recommendations for patient care that may identify a particular strategy or range of strategies that reflect moderate scientific certainty (eg, based on evidence from one or more Class of Evidence II studies or multiple Class of Evidence III studies that demonstrate consistent effects or estimates). Level C recommendations are recommendations for patient care that are based on evidence from Class of Evidence III studies or, in the absence of adequate published literature, based on expert consensus, and when consensus recommendations are made, "consensus" is placed in parentheses at the end of the recommendation. Recommendations stemming from a body of evidence may be downgraded due to consistency of results, the uncertainty of effect magnitude, and publication bias, and clinically oriented statistics (eg, likelihood ratios, number needed to treat) are presented to help apply results to individual patients and to patients with extremes of risk.

---

### 2022 AHA / ACC key data elements and definitions for cardiovascular and noncardiovascular complications of COVID-19: a report of the American college of cardiology / American Heart Association task force on clinical data standards [^d172498f]. Journal of the American College of Cardiology (2022). High credibility.

COVID-19 data standards — physical examination respiratory and anthropometric measures define respiratory rate as a measurement describing the rate of breathing measured within a unit time (typically 1 min), permitting numeric cycles/min or unknown; height is the vertical measurement or distance from the base, or bottom, of the patient, to the top of the patient, permitting numeric cm or unknown; weight at encounter is the vertical force exerted by a mass of the patient as a result of gravity, permitting numeric kg or unknown; body mass index is used to indicate body fat based on the ratio of weight to height, with permissible numeric kg/m^2 or unknown.

---

### 2022 AHA / ACC key data elements and definitions for cardiovascular and noncardiovascular complications of COVID-19: a report of the American college of cardiology / American Heart Association task force on clinical data standards [^7c00ac0e]. Journal of the American College of Cardiology (2022). High credibility.

Maximum fraction of inspired oxygen (FiO2) is defined as the maximum molar fraction of oxygen in an inhaled gas, with permissible value type Numeric.

---

### ACOEM practice guidelines: elbow disorders [^4e95c510]. Journal of Occupational and Environmental Medicine (2013). Medium credibility.

ACOEM elbow disorders — strength-of-evidence criteria define levels and thresholds as follows: levels are labeled "A = Strong evidence base," "B = Moderate evidence base," "C = Limited evidence base," and "I = Insufficient evidence†"; A requires "Two or more high-quality studies*," B requires "At least one high-quality study or multiple moderate-quality studies*," C requires "At least one study of moderate quality," and I applies when "Evidence is insufficient or irreconcilable." Study quality scoring specifies "High-quality studies are scored 8.0 to 11.0 points. Moderate-quality studies are scored 4.0 to 7.5 points." The guideline notes "Insufficient evidence recommendations are, by definition, consensus-based guidance."

---

### Use of clinical trial characteristics to estimate costs of new drug development [^945e3485]. JAMA Network Open (2025). High credibility.

Importance

Despite their importance to patients, health, and industry, the magnitude of investments in drug research and development (R&D) remain nebulous. New policies require more granular and transparent R&D cost estimates to better balance incentives for innovation and returns to developers.

Objective

To estimate per-drug R&D costs using a novel, reproduceable approach and to describe firm-level R&D costs per discrete unit of R&D activity (1 patient-month).

Design, Setting, and Participants

This economic evaluation used cross-sectional data to estimate 2014 to 2019 costs per patient-month. Costs per patient-month were calculated using data from 268 US publicly traded drug developers, contributing 1311 firm-year observations, that were highest ranked by assets or market capitalization, after exclusions. Per-drug costs were calculated from all R&D activity through approval for a cohort of 38 new drugs approved by the US Food and Drug Administration in 2019. Data were analyzed from January 2022 to July 2024.

Exposure

R&D activity, measured in terms of clinical trial patient-months.

Main Outcomes and Measures

This study used a 2-step approach to estimate R&D costs, first allocating firm-year-level total R&D spending across similarly aggregated patient-months, and then aggregating these incremental costs to estimate drug-level R&D costs per new drug.

Results

Among 268 developers assessed, 20 large firms accounted for 80.8% of all patient-months and had 27.4% lower mean and 26.7% lower median costs per patient-month compared with other firms. Each 1% increase in patient-months was associated with a 0.9% increase in R&D costs. R&D costs per new drug were highly skewed, with a lower median (IQR), at $708 million ($247 million to $1.42 billion) than mean (SD), at $1.31 ($1.92) billion, after adjusting for the cost of capital and discontinued products. Without these adjustments, direct costs per new drug were a median (IQR) of $150 ($67.6-$453) million and a mean (SD) $369 of ($684) million. While estimated R&D costs varied in sensitivity analyses, mean costs were always substantially greater than median costs.

Conclusions and Relevance

This economic evaluation found median per-drug R&D costs toward the lower end of the range from prior studies, with a mean closer to the middle of the existing range despite the broad scope of included costs. These findings suggest parallel development across indications, adjustment for discontinued products, and a small number of expensive development programs are particularly important drivers of R&D costs.

---

### Assessing the influence of climate on wintertime SARS-CoV-2 outbreaks [^cdf29cbb]. Nature Communications (2021). High credibility.

SIRS model

Our R 0 estimates are incorporated into an SIRS model where R 0 (t) = β (t) D. D is the mean infectious period (set at 5 days) and β (t) is the contact rate. The SIRS model is directly dependent on β (t) and is given bywhere S is the number of susceptibles, I is the number of infecteds, and N is the population size. N = S + I + R, where R is the number of individuals in the recovered category.

We initialize the model on the first day cases are observed. In order to capture different possible trajectories, we initialize varying the proportion infected on the first day. Over a finite range, models initialized with different infected proportions are able to track observed cases to a scaling constant, i.e. the reporting rate (Fig. 1 a, b). We tune the range of starting proportion infected such that the reporting rate stays between 1 and 100%, though results for specific trajectories are shown for 3 and 10% in Fig. 1 a, b, reflecting prior estimates on reporting rates.

---

### Model parameters and outbreak control for SARS [^7c2ce87a]. Emerging Infectious Diseases (2004). Low credibility.

Table 2
The median and the interquartile range (IQR) of the distribution of the basic reproductive number (R 0) of SARS for Toronto, Hong Kong, and Singapore obtained from our uncertainty analysis

Figure 2
Empiric (dots) and stretched exponential estimated probability density function Prob(R 0) = a exp[–(R 0 /b) c] (solid line) of R 0 for the cases of Toronto (a = 0.186, b = 0.803, c = 0.957, after control measures had been implemented), Hong Kong (a = 0.281, b = 1.312, c = 0.858), and Singapore (a = 0.213, b = 1.466, c = 0.883) obtained from our uncertainty analysis. The distribution for the case of perfect isolation (l = 0, a = 0.369, b = 0.473, c = 0.756) is shown as a reference.

Figure 3
(β, l) parameter space when R 0 < 1 obtained from the uncertainty analysis (black dots). The deterministic (β, l) level curve when R 0 = 1 is shown in by the dotted white line. All other parameters in equation 1 were fixed to their baseline values (Table 1). l = 0 denotes perfect isolation; l = 1 denotes no isolation.

---

### 2016 ACC / AHA clinical performance and quality measures for adults with atrial fibrillation or atrial flutter: a report of the American college of cardiology / American Heart Association task force on performance measures [^da0355b5]. Circulation: Cardiovascular Quality and Outcomes (2016). Medium credibility.

2014 ACC/AHA/HRS Guidelines for the Management of Patients With Atrial Fibrillation — antithrombotic therapy recommendations state that in patients with AF, antithrombotic therapy should be individualized based on shared decision-making after discussion of the absolute and RRs of stroke and bleeding, and the patient's values and preferences (Class I, Level of Evidence: C), and selection of antithrombotic therapy should be based on the risk of thromboembolism irrespective of whether the AF pattern is paroxysmal, persistent, or permanent (Class I, Level of Evidence: B). In patients with nonvalvular AF, the CHA2DS2-VASc score is recommended for assessment of stroke risk (Class I, Level of Evidence: B). For patients with AF who have mechanical heart valves, warfarin is recommended and the target international normalized ratio (INR) intensity (2.0 to 3.0 or 2.5 to 3.5) should be based on the type and location of the prosthesis (Class I, Level of Evidence: C). For patients with nonvalvular AF with prior stroke, TIA, or a CHA2DS2-VASc score of 2 or greater, oral anticoagulants are recommended, with options including warfarin (INR 2.0 to 3.0), dabigatran, rivaroxaban, or apixaban. Among patients treated with warfarin, the INR should be determined at least weekly during initiation of antithrombotic therapy and at least monthly when anticoagulation (INR in range) is stable (Class I, Level of Evidence: A). For patients with nonvalvular AF unable to maintain a therapeutic INR level with warfarin, use of a direct thrombin or factor Xa inhibitor (dabigatran, rivaroxaban, or apixaban) is recommended (Class I, Level of Evidence: C). Re-evaluation of the need for and choice of antithrombotic therapy at periodic intervals is recommended to reassess stroke and bleeding risks (Class I, Level of Evidence: C), and for patients with atrial flutter, antithrombotic therapy is recommended according to the same risk profile used for AF (Class I, Level of Evidence: C).

---

### A framework for evaluating clinical artificial intelligence systems without ground-truth annotations [^f3b9d969]. Nature Communications (2024). High credibility.

Evaluate classifier

After training g ϕ, we evaluated it on a held-out set of data comprising data points with ground-truth labels (from both class 0 and class 1) (Fig. 1 b, Step 5). The intuition here is that a classifier which can successfully distinguish between these two classes, by performing well on the held-out set of data, is indicative that the training data and the corresponding ground-truth labels are relatively reliable. Since the data points from class 1 are known to be correct (due to our use of ground-truth labels), then a highly-performing classifier would suggest that the class 0 pseudo-labels of the remaining data points are likely to be correct. In short, this step quantifies how plausible it is that the sampled unlabelled data points belong to class 0.

As presented, this approach determines how plausible it is that the sampled set of data points in some probability interval, s ∈ (s 1, s 2], belongs to class 0. It is entirely possible, however, that a fraction of these sampled data points belong to the opposite class (e.g. class 1). We refer to this mixture of data points from each class as class contamination. We hypothesised (and indeed showed) that the degree of this class contamination increases as the probability output, s, by an AI system steers away from the extremes (s ≈ 0 and s ≈ 1). To quantify the extent of this contamination, however, we also had to determine how plausible it was that the sampled set of data points belong to class 1, as we outline next.

---

### A novel scale for describing corneal staining [^3c9bde94]. Clinical Ophthalmology (2018). Low credibility.

In order to describe the staining as fully as possible, it was decided to follow the same multifactorial approach as the BHVI grading scale and describe the same three distinct factors of the staining appearance, separately across each of the five zones:
the type of staining;
the extent (area) that the staining is spread across; and
the depth of the staining.

Consideration was given to expanding the scale to improve sensitivity and create continuous data points that, if normally distributed, would facilitate parametric analysis.

The type of staining was allocated a continuous integer scale, from 0 to 100, with anchors as described in Table 1 and illustrated in Figure 2.

The extent of staining was also allocated a continuous integer scale, from 0 to 100, to represent the percentage area of the individual zone that contains corneal staining. One important feature of the extent grade is that it does not represent the area of all the punctate dots pulled together, but rather it represents the spread of the staining across the zone (Figure 3). For example, if punctate stain was evident across the entire central zone, then the extent within that zone would be represented by 100, despite the staining dots being small and spread apart from each other.

The depth of corneal staining was already a part of the BHVI grading scale in the form of a 0–4 integer scale with distinct descriptors. Given the limited options available to describe the depth of staining in terms of the penetration of fluorescein into the cornea, this scale was determined to be sufficient for grading the depth of staining and was adopted as the basis for the CORE staining scale (Table 2).

---

### Achilles pain, stiffness, and muscle power deficits: achilles tendinitis [^b09e3298]. The Journal of Orthopaedic and Sports Physical Therapy (2010). Medium credibility.

Statement of intent — this guideline is not intended to be a standard of clinical care and parameters should be considered guidelines only; ultimate judgment must be based on the patient's data, available options, and the patient's values, expectations, and preferences, and we suggest the rationale for significant departures from accepted guidelines be documented in the patient's medical records at the time the relevant clinical decision is made.

---

### 2016 ACC / AHA clinical performance and quality measures for adults with atrial fibrillation or atrial flutter: a report of the American college of cardiology / American Heart Association task force on performance measures [^d67c8d3b]. Circulation: Cardiovascular Quality and Outcomes (2016). Medium credibility.

2014 ACC/AHA/HRS guidelines — antithrombotic therapy and INR monitoring state that in patients with AF, antithrombotic therapy should be individualized based on shared decision-making after discussion of the absolute and RRs of stroke and bleeding, and the patient's values and preferences (Class I, Level of Evidence: C). Selection of antithrombotic therapy should be based on the risk of thromboembolism irrespective of whether the AF pattern is paroxysmal, persistent, or permanent (Class I, Level of Evidence: B). In patients with nonvalvular AF, the CHA2DS2-VASc score is recommended for assessment of stroke risk (Class I, Level of Evidence: B). For patients with AF who have mechanical heart valves, warfarin is recommended and the target international normalized ratio (INR) intensity (2.0 to 3.0 or 2.5 to 3.5) should be based on the type and location of the prosthesis (Class I, Level of Evidence: B). For patients with nonvalvular AF with prior stroke, transient ischemic attack (TIA), or a CHA2DS2-VASc score of 2 or greater, oral anticoagulants are recommended; options include: warfarin (INR 2.0 to 3.0) (Class I, Level of Evidence: A); dabigatran (Class I, Level of Evidence: B); rivaroxaban (Class I, Level of Evidence: B); or apixaban (Class I, Level of Evidence: B). Among patients treated with warfarin, the INR should be determined at least weekly during initiation of antithrombotic therapy and at least monthly when anticoagulation (INR range) is stable (Class I, Level of Evidence: A). For patients with nonvalvular AF unable to maintain a therapeutic INR level with warfarin, use of a direct thrombin or factor Xa inhibitor (dabigatran, rivaroxaban, or apixaban) is recommended (Class I, Level of Evidence: C). Re-evaluation of the need for and choice of antithrombotic therapy at periodic intervals is recommended to reassess stroke and bleeding risks (Class I, Level of Evidence: C). For patients with atrial flutter, antithrombotic therapy is recommended according to the same risk profile used for AF (Class I, Level of Evidence: C).

---

### An official American Thoracic Society workshop report: developing performance measures from clinical practice guidelines [^369e6d46]. Annals of the American Thoracic Society (2014). Medium credibility.

Agency for Healthcare Research and Quality performance measure attributes — Desirable features of performance measures include relevance to stakeholders and addressing important aspects of health, with evidence of a need for the measure; evidence should be explicitly stated, results should be reproducible and truly measure what they purport to measure, and specifications should explicitly define the numerator and denominator with understandable data collection requirements; necessary data sources should be available within the measurement timeframe and data collection costs justified by potential improvement in care or health.

---

### Defining the time-limited trial for patients with critical illness: an official American Thoracic Society workshop report [^d28d917f]. Annals of the American Thoracic Society (2024). High credibility.

Time-limited trials — framing to promote collaboration outlines in Table 3 that they "Should Be" "Framed by uncertainty," "Transparent," "Individualized and adaptable," "Iterative," "Oriented to patients' goals and priorities," "Trials of high-quality, standard-of-care therapies," and "A collaborative process between patients, surrogates, and clinicians," and "Should Not Be" "Framed by notions of "poor prognosis"," "Coercive," "Prescriptive or binding," "Time pressured or finite," "Oriented to one specific outcome," "Lower quality care," or "Determined exclusively by clinicians."

---

### Altered assembly paths mitigate interference among paralogous complexes [^c43c1bc1]. Nature Communications (2024). High credibility.

Phase 2: Enumerating all leaf node assignments of each binary tree structure.

A naïve approach to generate all leaf node assignments of a binary tree structure is to exhaustively permuteintegers and assign each permuted integer to a leaf node in the order of the regular expression (from left to right). This approach yields multiple permutations corresponding to the same equivalent assignment. If the left and right branches of a subtree have identical structures, then two distinct permutations are equivalent by swapping the leaf nodes under the two branches. An accurate algorithm should avoid this type of over-counting and report a list of distinct assembly histories respecting a binary tree structure. Ais the regular expression of a binary tree structure. We denote an ordered partition ofdistinct integers, whereis the number of components, and each componentconsists of distinct integers from. Initially, constructand a list.concentrates all theintegers in the root node. The following procedures were repeated until all elements inhascomponents:
Select awith.
Find the first component inconsisting of more than one integer. Denote it as.
Identify the nodeincorresponding to component. By definitionis an internal node and has two children.
Identify the subtreesandof the left and right branches under. Denote the numbers of leaf nodes inandto beandrespectively, where.
Enumerate allcombinations of splitintegers into two groups withandmembers, respectively. 51 If subtreesandhave different structures, then each split in step 5 induces a unique refined partition of component. Duplicateand replace componentwith two componentsandrespecting the split. The resulting partition hascomponents: 52 If subtreesandhave identical structures, then only half of thesplits are unique since the splits are equivalent by swapping the left and right branches of the tree. Thus, repeat 5.1 only on half of the unique splits.
Removefrom.

---

### Age-related macular degeneration preferred practice pattern ® [^4da5ee23]. Ophthalmology (2025). High credibility.

Regarding quality improvement for age-related macular degeneration, more specifically with respect to socioeconomic considerations, AAO 2025 guidelines recommend to recognize that the considerable burden of disease associated with AMD, as well as the public health benefits of prevention, are highlighted in analyses conducted by the AREDS authors. This research, published in 2003, estimated that 8 million Americans aged 55 and older are at high risk for developing advanced AMD. If these persons received AREDS-formulation supplements, it was estimated that approximately 300,000 would avoid advanced AMD and any associated vision loss over a 5-year period. The Salisbury Eye Study reported that VA loss adversely affected activities of daily living, which subsequently increased mortality risk in older adults. Further calculations estimated that treating AMD with anti-VEGF agents saves 1 to 2 years of life. More cost-effectiveness studies on the use of anti-VEGF therapies have demonstrated that they are highly cost-effective over prior therapies such as PDT. The off-label use of intravitreal bevacizumab was suggested to represent a highly cost-effective, off-label option for management of neovascular AMD compared with the higher cost of ranibizumab. Others have investigated the cost utility of various treatments for AMD. One analysis using CATT trial data found that bevacizumab with PRN dosing offered considerably greater value than ranibizumab in the treatment of neovascular AMD among patients 80 and older. Another analysis using CATT and MARINA data evaluated the relative 10-year cost-effectiveness of bevacizumab and ranibizumab in 65-year-old patients with neovascular AMD. This study estimated the cost utility of bevacizumab treatment (relative to no treatment) at approximately $2,700 per quality-adjusted life year (QALY for monthly dosing) and $3,300 per QALY (for PRN dosing). In contrast, the cost-effectiveness of ranibizumab was estimated as $63, 300/QALY for monthly dosing and $18,600 per QALY for PRN dosing. Wholesale prices of anti-VEGF medications range from $50 to $1,950 per dose, depending on the medication. The use of personalized anti-VEGF treatment guided by OCT has resulted in savings for the U.S. government of $9 billion and $22 billion for patients with neovascular AMD, respectively, in a study comparing patient and Medicare savings. After the FDA approval of the ranibizumab ocular implant, Brown et al evaluated the cost-effectiveness of this new treatment. They found that although the implant seemed to compare favorably at $21,825 with two ranibizumab 100-mg/ml fills at 1 year compared with $18,405 for 11.8 injections of ranibizumab 0.5 mg over that same time period, this benefit did not seem to extend to 5- and 12-year time points. Both treatments were cost-effective compared with no treatment, however. One study developed a drug-pricing model using the Medicare average sales price for bevacizumab, ranibizumab, and aflibercept 2 mg, wholesale acquisition costs of currently available ranibizumab biosimilars, and postulated prices for bevacizumab and aflibercept biosimilars. Results from this model indicated increased costs from a bevacizumab biosimilar and cost reductions from ranibizumab and aflibercept biosimilars. Medicare Part B average sales prices for ranibizumab 0.3 mg, ranibizumab 0.5 mg, and aflibercept 2 mg were $776, $1, 292, and $1, 806, respectively, as of October 2022. The respective wholesale acquisition costs for biosimilar versions of the two dosages of ranibizumab were $816 and $1,130 to $1, 360. The model predicted that a bevacizumab biosimilar priced at $500 would increase Medicare Part B costs by $457 million annually, and one priced at $900 would increase costs by $897 million annually. However, switching from ranibizumab and aflibercept 2 mg to their biosimilars could lead to total cost savings of $132 million for Medicare. Since the FDA approved pegcetacoplan for GA, the cost-effectiveness of this new treatment was evaluated. Based on 2022 Medicare reimbursement data, the authors of the study found that the cost of the two treatment frequencies varied. The cost per area of delaying GA for 2 years with monthly treatment was $87, 300/mm2, compared with EOM treatment, which cost $49, 200/mm2. The costs for extrafoveal GA had greater utility, with costs of $53, 900/mm2 for those treated monthly compared with $32, 100/mm2 in the EOM group. Their model also predicted that 95% atrophy was delayed by 2.5 years in patients treated monthly compared with 2.1 years in the EOM group.

---

### MRNAs, proteins and the emerging principles of gene expression control [^2d1f7bae]. Nature Reviews: Genetics (2020). High credibility.

Gene expression involves transcription, translation and the turnover of mRNAs and proteins. The degree to which protein abundances scale with mRNA levels and the implications in cases where this dependency breaks down remain an intensely debated topic. Here we review recent mRNA-protein correlation studies in the light of the quantitative parameters of the gene expression pathway, contextual confounders and buffering mechanisms. Although protein and mRNA levels typically show reasonable correlation, we describe how transcriptomics and proteomics provide useful non-redundant readouts. Integrating both types of data can reveal exciting biology and is an essential step in refining our understanding of the principles of gene expression control.

---