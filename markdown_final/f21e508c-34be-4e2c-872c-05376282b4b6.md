# Medical Question & Answer

**Sample ID**: f21e508c-34be-4e2c-872c-05376282b4b6
**Dataset Index**: 521

---

## Question

Are these AI tools FDA-approved for clinical use?

---

## Answer

> Let's see… What do we have here? The user is asking whether these AI tools are FDA-approved for clinical use. Let's break this down step-by-step. First, I need to think about what "these AI tools" refers to and whether the question targets a specific set or the broader landscape of AI-enabled medical devices. Then, I should verify the FDA's current authorization count and the regulatory pathways used. Next, I will examine the quality and transparency of evidence supporting these authorizations. After that, I need to assess generalizability, bias, and post-market surveillance gaps. Finally, I will synthesize a clear answer and outline practical implications for clinicians considering adoption.

> Let me first confirm the scope. The phrase "these AI tools" is ambiguous, but given the context of FDA regulation and clinical deployment, I will interpret this as referring to AI-enabled medical devices and Software as a Medical Device (SaMD) intended for diagnosis, triage, or clinical decision support, which are regulated by the FDA under device authorities rather than drug authorities, and are distinct from research-only or administrative tools [^113v4vXd] [^111pmz3k].

> I need to check the regulatory status. The FDA has authorized approximately 950–1,016 AI/ML-enabled medical devices for clinical use as of late 2024, with the vast majority cleared via the 510(k) pathway and a smaller number via De Novo or PMA, indicating that many such tools are indeed FDA-cleared or approved for marketing in the United States [^111pmz3k] [^114ShQBE].

> Hold on, let's not jump to conclusions about what "FDA-approved" means here. I should confirm the evidentiary basis. Most AI devices take the 510(k) route, which often relies on substantial equivalence to predicate devices and does not routinely require prospective randomized trials; in fact, only about 1.6–2.4% of cleared AI devices report RCT data, and many summaries lack key performance metrics or demographic reporting, underscoring a transparency and evidence-quality gap at clearance [^116a1ohq] [^111pmz3k] [^114Swx14].

> Wait, let me verify generalizability concerns. Many devices are trained and validated on retrospective, single-center, or enriched datasets, with limited sex- and age-specific subgroup reporting, raising questions about performance across diverse populations and settings. This is compounded by the fact that some devices explicitly reported no clinical performance studies at all in their public summaries, which complicates external validation and local implementation decisions [^116ZQX8h] [^115DnnhR] [^113Xkujn].

> I should double-check post-market surveillance. The FDA's MAUDE and recall data show hundreds of adverse events and dozens of recalls among AI/ML devices, yet the current passive reporting system is widely viewed as insufficient for detecting performance drift or algorithmic failure at scale, reinforcing the need for active, standardized post-deployment monitoring and lifecycle management frameworks [^116a1ohq] [^115xQMCv] [^115qW5j6].

> Let me consider clinical utility and workflow integration. FDA clearance establishes reasonable assurance of safety and effectiveness for the stated indication but does not guarantee improved patient outcomes or net clinical benefit. Multiple analyses highlight that many cleared devices lack demonstrated outcome improvements, and intended-use statements often position AI as triage or second-read support rather than autonomous decision-makers, which has implications for liability, explainability, and clinician oversight [^115bVewZ] [^116HLesR] [^111MfZ2e].

> But wait, what if the question is about a specific tool or vendor? I should confirm that device-specific status can be verified in the FDA's AI/ML device database and that clinicians should review the 510(k) summary, labeling, and any available post-market evidence before adoption. Local validation against the institution's population and IT infrastructure remains essential even for FDA-cleared tools [^113zjQi3] [^112SXHZy].

> In summary, yes — hundreds of AI-enabled medical devices are FDA-cleared or approved for clinical use in the United States, primarily via 510(k), yet many were authorized with limited prospective evidence and incomplete reporting, so clearance should be viewed as a regulatory starting point rather than definitive proof of clinical utility. Safe, effective deployment requires attention to intended use, local validation, bias assessment, and ongoing performance monitoring [^111pmz3k] [^116a1ohq] [^116PREqQ].

---

Yes, many AI tools are **FDA-approved for clinical use** [^113v4vXd], with over 950 AI/ML-enabled medical devices authorized as of 2024 [^111pmz3k]. Most are cleared via the 510(k) pathway [^111TAPQQ], which requires **substantial equivalence** to existing devices but not necessarily new clinical trials [^111pmz3k]. Only a small fraction have prospective or randomized evidence [^114xRAWk], and **post-market surveillance** is limited [^115xQMCv], raising concerns about real-world performance and bias [^116a1ohq]. Clinicians should verify FDA status, intended use, and local validation before adoption [^112SXHZy].

---

## FDA approval pathways for AI tools

The FDA regulates AI tools as medical devices under three primary pathways:

| **Pathway** | **Description** | **Requirements** |
|-|-|-|
| 510(k) Premarket Notification | For devices substantially equivalent to a legally marketed predicate device | - Demonstrate substantial equivalence <br/> - Most common pathway for AI tools [^111TAPQQ] |
| De Novo Classification | For novel devices without a predicate | - Rigorous review <br/> - Establish special controls [^notfound] |
| Premarket Approval (PMA) | For high-risk devices requiring stringent evidence | - Extensive clinical evidence <br/> - Typically not required for most AI tools [^111pmz3k] |

---

Most AI tools are cleared through the **510(k) pathway** [^111pmz3k], which does not require new clinical trials if substantial equivalence is demonstrated [^111TAPQQ].

---

## Number and types of FDA-approved AI tools

As of 2024, the FDA has authorized **over 950 AI/ML-enabled medical devices** [^111pmz3k], spanning multiple specialties:

- **Radiology**: 692 devices (76.6%) — most common specialty [^114Swx14].
- **Cardiovascular**: 91 devices (10.1%) [^114Swx14].
- **Neurology**: 29 devices (3.2%) [^114Swx14].

These devices perform functions such as **detection, triage, quantification, diagnosis, and prediction** [^114jMyUe], primarily in imaging and diagnostics [^112ZxazP].

---

## Clinical evidence supporting FDA approval

The clinical evidence supporting FDA-approved AI tools varies widely:

- **Clinical performance studies**: Reported for approximately half of devices; one-quarter explicitly stated no such studies were conducted [^116ZQX8h].
- **Study design**: Most studies are retrospective; only 2% are randomized clinical trials (RCTs) [^115DnnhR].
- **Demographic reporting**: Limited sex- and age-specific data, raising concerns about generalizability and bias [^116ZQX8h] [^112t6jHB].

---

## Limitations and concerns

Several **limitations and concerns** have been identified regarding FDA-approved AI tools:

- **Generalizability**: Many devices lack evidence of performance across diverse populations and settings [^1128a4B3] [^115DnnhR].
- **Bias**: Underreporting of demographic data and lack of standardized reporting may exacerbate algorithmic bias and health disparities [^112t6jHB] [^115c2DWG].
- **Post-market surveillance**: Limited post-market monitoring and adverse event reporting hinder assessment of real-world performance [^115xQMCv] [^115qW5j6].

---

## Clinical outcomes and effectiveness

Evidence regarding **clinical outcomes and effectiveness** is limited:

- **RCTs**: Only 1.6% of devices have RCT evidence supporting their clinical utility [^116a1ohq].
- **Patient outcomes**: Few studies report patient-centered outcomes, such as improved diagnosis or treatment [^115bVewZ].
- **Real-world performance**: Ongoing monitoring is essential to ensure continued effectiveness and safety [^1128a4B3] [^113v4vXd].

---

## Regulatory challenges and future directions

The FDA faces several **regulatory challenges** with AI tools:

- **Adaptive algorithms**: Developing frameworks for continuously learning AI systems [^111NCUbL].
- **Harmonization**: Coordinating international regulatory standards [^114aysvh].
- **Transparency**: Improving reporting standards and transparency in AI development and validation [^115c2DWG].

Future directions include **enhanced post-market surveillance**, standardized reporting, and collaboration with international regulatory bodies to ensure safe and effective AI deployment [^116PREqQ] [^114aysvh].

---

## Practical considerations for clinicians

Clinicians considering AI tools should:

- **Verify FDA status**: Confirm the device has appropriate FDA clearance or approval [^117By5K7].
- **Review intended use**: Ensure the device is used within its approved indications [^111KWLfH].
- **Assess local validation**: Evaluate performance in the local clinical setting and population [^112SXHZy].
- **Monitor performance**: Implement ongoing monitoring and quality assurance processes [^1128a4B3].

---

Many AI tools are **FDA-approved for clinical use**, primarily cleared through the 510(k) pathway [^111pmz3k]. However, the evidence base is heterogeneous, with limited prospective or RCT data [^114xRAWk], and post-market surveillance remains limited [^115xQMCv]. Clinicians should carefully verify FDA status, intended use, and local validation before adopting AI tools [^112SXHZy].

---

## References

### Trends in clinical validation and usage of US food and drug administration-cleared artificial intelligence algorithms for medical imaging [^113Xkujn]. Clinical Radiology (2023). Medium credibility.

Aim

To examine the current landscape of US Food and Drug Administration (FDA)-approved artificial intelligence (AI) medical imaging devices and identify trends in clinical validation strategy.

Materials and Methods

A retrospective study was conducted that analysed data extracted from the American College of Radiology (ACR) Data Science Institute AI Central database as of November 2021 to identify trends in FDA clearance of AI products related to medical imaging. Product and clinical validation information of each device was gathered from their respective public 510(k) summary or de novo request submission, depending on their type of authorisation.

Results

Overall, the database included a total of 151 AI algorithms that were cleared by the FDA between 2008 and November 2021. Out of the 151 FDA summaries reviewed, 97 (64.2%) reported the use of clinical data to validate their device, with six (4%) revealing study participant demographics, and eight (5.3%) reporting the specifications of the machines used. A total of 51 (33.8%) AI devices characterised their clinical data as multicentre, three (2%) as single-centre, and the remaining 97 (64.2%) did not specify. The ground truth used for clinical validation was specified in 78 (51.6%) FDA summaries.

Conclusion

A wide breadth of AI algorithms has been developed for medical imaging. Most of the FDA summaries of the devices mention their use of clinical data and patient cases for device validation; however, few devices revealed the patient demographics or machine specifications used in their clinical studies, which may lead some consumers to question their external validation.

---

### How machine learning is embedded to support clinician decision making: an analysis of FDA-approved medical devices [^11238EGR]. BMJ Health & Care Informatics (2021). High credibility.

Finally, there are important questions about responsibility for ML device provided information and the extent to which clinicians should be able to rely on it. While exploration of these questions exceeds the scope of this article, models of use that require clinicians to double check ML devices results, may be less helpful than devices whose output can be acted on. As ML devices become more common there needs to be clearly articulated guidelines on the division of labour between clinician and ML devices, especially in terms of who is responsible for which decisions and under what circumstances. In addition to the configuration of tasks between clinician and ML devices, how devices work and communicate with clinicians is crucial and requires further study. The ability of ML devices to explain decisions through presentation of information, such as marking suspected cancers on images or using explainable AI techniqueswill impact how clinicians will assess and make decisions based on ML device provided information.

Limitations

There are several limitations. First, it was not possible to directly search FDA approval databases, the primary source of approvals. Second, the reporting in approvals varied considerably with nearly one third of included approvals not describing ML utilisation. Indeed, all disagreements on device selection occurred where evidence had to be sought from the manufacturer's website and non-peer reviewed sources, where one reviewer located key information the other did not. Consequently, it is possible that some devices may have been missed. Nevertheless, the review provides useful insights in the absence of capability to systematically search primary sources. Our analysis focused on intended use as described in approvals, rather than actual use in the real world, which may differ. Finally, the focus on medical devices limits the review to ML algorithms approved by the FDA. Nevertheless, our methods to examine the stage of human information processing automated and level of autonomy can be applied to examine clinician interaction with the vast majority of ML CDS which are not regulated as medical devices. Indeed, there is an urgent need to ensure ML based CDS are implemented safely and effectively in clinical settings.

---

### An FDA guide on indications for use and device reporting of artificial intelligence-enabled devices: significance for pediatric use [^111KWLfH]. Journal of the American College of Radiology (2023). Medium credibility.

Radiology has been a pioneer in adopting artificial intelligence (AI)-enabled devices into the clinic. However, initial clinical experience has identified concerns of inconsistent device performance across different patient populations. Medical devices, including those using AI, are cleared by the FDA for their specific indications for use (IFUs). IFU describes the disease or condition the device will diagnose or treat, including a description of the intended patient population. Performance data evaluated during the premarket submission support the IFU and include the intended patient population. Understanding the IFUs of a given device is thus critical to ensuring that the device is used properly and performs as expected. When devices do not perform as expected or malfunction, medical device reporting is an important way to provide feedback about the device to the manufacturer, the FDA, and other users. This article describes the ways to retrieve the IFU and performance data information as well as the FDA medical device reporting systems for unexpected performance discrepancy. It is crucial that imaging professionals, including radiologists, know how to access and use these tools to improve the informed use of medical devices for patients of all ages.

---

### Benefit-risk reporting for FDA-cleared artificial intelligence-enabled medical devices [^116a1ohq]. JAMA Health Forum (2025). High credibility.

Importance

Devices enabled by artificial intelligence (AI) and machine learning (ML) are increasingly used in clinical settings, but there are concerns regarding benefit-risk assessment and surveillance by the US Food and Drug Administration (FDA).

Objective

To characterize pre- and postmarket efficacy, safety, and risk assessment reporting for FDA-cleared AI/ML devices.

Design and Setting

This was a cross-sectional study using linked data from FDA decision summaries and approvals databases, the FDA Manufacturer and User Facility Device Experience Database, and the FDA Medical Device Recalls Database for all AI/ML devices cleared by the FDA from September 1995 to July 2023. Data were analyzed from October to November 2024.

Main Outcomes and Measures

AI/ML reporting of study design, data availability, efficacy, safety, bias assessments, adverse events, device recalls, and risk classification.

Results

The analysis included data for all 691 AI/ML devices that received FDA clearance through 2023, with 254 (36.8%) cleared in or after 2021. Device summaries often failed to report study designs (323 [46.7%]), training sample size (368 [53.3%]), and/or demographic information (660 [95.5%]). Only 6 devices (1.6%) reported data from randomized clinical trials and 53 (7.7%) from prospective studies. Few premarket summaries contained data published in peer-reviewed journals (272 [39.4%]) or provided statistical or clinical performance, including sensitivity (166 [24.0%]), specificity (152 [22.0%]), and/or patient outcomes (3 [< 1%]). Some devices reported safety assessments (195 [28.2%]), adherence to international safety standards (344 [49.8%]), and/or risks to health (42 [6.1%]). In all, 489 adverse events were reported involving 36 (5.2%) devices, including 458 malfunctions, 30 injuries, and 1 death. A total of 40 devices (5.8%) were recalled 113 times, primarily due to software issues.

Conclusions and Relevance

This cross-sectional study suggests that despite increasing clearance of AI/ML devices, standardized efficacy, safety, and risk assessment by the FDA are lacking. Dedicated regulatory pathways and postmarket surveillance of AI/ML safety events may address these challenges.

---

### Commercialization of medical artificial intelligence technologies: challenges and opportunities [^113y9bYS]. NPJ Digital Medicine (2025). Medium credibility.

Obtaining regulatory approval

Once a product has been developed, regulatory approval is required for broad clinical use. This may be challenging as different jurisdictions often have varying AI regulations. In the US, medical products/devices require FDA clearance/approval for clinical use. However, obtaining FDA clearance for AI technologies, often through the 510(k) pathway, can be a significant hurdle requiring extensive data reviews. Within the 510(k) pathway, investigators need to demonstrate that their technology is substantially equivalent to another legally US-marketed device. Therefore, seeking support from scientists, consultants, and administrators with expertise in regulatory approval, often available at major academic institutions, is critical. Early consultation with the FDA's Q-Submission Program is highly encouraged, allowing investigators to receive feedback from FDA review teams during product development. Importantly, regulatory frameworks should be considered at product conception and revisited throughout development, with approval at the end if the process has been adequately followed.

Reimbursement pathways

Most AI tools lack dedicated billing codes and therefore, investigators must work with relevant stakeholders to develop reimbursement pathways to achieve financial sustainability. Given that billing codes are often well-established, modifying them to include AI technologies may present significant challenges requiring buy-in from multiple stakeholders, including public and private insurers, clinicians, and patients. Therefore, early engagement with these groups, often with the support of professional organizations, is essential for successful AI deployment. Importantly, given the complexity of this process, often entangled with the requirement of extensive engagement with stakeholders in different sectors (payers, providers, patients, etc.), it is critical to seek guidance from experienced mentors and actively collaborate with key decision-makers to achieve success.

---

### Artificial intelligence for breast ultrasound: expert panel narrative review [^112ikorZ]. AJR: American Journal of Roentgenology (2024). Medium credibility.

Breast ultrasound is used in a wide variety of clinical scenarios, including both diagnostic and screening applications. Limitations of ultrasound, however, include its low specificity and, for automated breast ultrasound screening, the time necessary to review whole-breast ultrasound images. As of this writing, four AI tools that are approved or cleared by the FDA address these limitations. Current tools, which are intended to provide decision support for lesion classification and/or detection, have been shown to increase specificity among nonspecialists and to decrease interpretation times. Potential future applications include triage of patients with palpable masses in low-resource settings, preoperative prediction of axillary lymph node metastasis, and preoperative prediction of neoadjuvant chemotherapy response. Challenges in the development and clinical deployment of AI for ultrasound include the limited availability of curated training datasets compared with mammography, the high variability in ultrasound image acquisition due to equipment- and operator-related factors (which may limit algorithm generalizability), and the lack of postimplementation evaluation studies. Furthermore, current AI tools for lesion classification were developed based on 2D data, but diagnostic accuracy could potentially be improved if multimodal ultrasound data were used, such as color Doppler, elastography, cine clips, and 3D imaging.

---

### Is the FDA regulation of cardiology AI devices supporting cardiovascular innovation: a scoping review [^116WCfp4]. Heart (2025). Medium credibility.

Background

Artificial intelligence (AI) and machine learning (ML) have shown immense potential in cardiology, leveraging data-driven insights to enhance diagnosis, treatment planning and patient care. This study presents a comprehensive evaluation of US Food and Drug Administration (FDA)-approved AI/ML devices in cardiology, analysing trends in clinical applications, regulatory pathways and evidence transparency.

Methods

FDA clearance summaries from the AI/ML medical device database were reviewed to identify cardiology-specific applications. Devices were categorised using the descriptive, diagnostic, predictive and prescriptive framework. Regulatory pathways, AI technologies and validation data were critically assessed.

Results

Of 1016 FDA-approved AI/ML devices, 277 (27.3%) had cardiology applications, predominantly for imaging (65.3%) and diagnostics (64.3%). Predictive and prescriptive tools constituted only 5.4% and 0.7%, respectively. Most devices (97.1%) were cleared via the 510(k) pathway, with 58.0% at risk of predicate creep. Quality of clinical evidence was limited, with only 3.2% of devices supported by high-quality trials. The type of AI technology was often underreported (58.8%).

Conclusion

While AI/ML technologies are reshaping cardiology, regulatory challenges and reporting transparency impede their optimal use. Strengthened regulatory frameworks, improved trial design and robust post-market surveillance are essential to ensure safety, efficacy and equity in the deployment of AI tools in cardiology.

---

### Clinical validation of an AI-based pathology tool for scoring of metabolic dysfunction-associated steatohepatitis [^111Zm84E]. Nature Medicine (2025). Excellent credibility.

The rapidly evolving field of AI offers a promising avenue to address these challenges. AI has demonstrated notable advancements in numerous medical disciplines, with a marked increase in CE-marked (a standard for European health, safety, performance and environmental requirements) and FDA-approved in vitro diagnostics for AI-based medical devices and algorithms from 2015 to 2020 (ref.), including the FDA authorization for an AI product in digital pathology in 2021 for Paige Prostate. However, the field of quantitative pathology in MASH therapeutic development still awaits a tool that is scalable, reproducible and validated. Recently, we described the development and verification of the AIM-MASH (AI-based measurement of nonalcoholic steatohepatitis) AI-based clinical trial tool. In this previous body of work, the algorithm was developed and verified for accuracy compared to a panel of manual readers to confirm that the tool was ready to be locked. As proof of concept, the algorithm alone (without pathologist review) was also retrospectively deployed on the ATLAS clinical trial dataset to demonstrate the utility of the tool. The work presented here represents extensive, multisite analytical and clinical validation of the algorithm alone and as an assist to MASH pathologists, as it would be used prospectively in a clinical trial, with each histologic component score being assessed individually and as a part of histologic-based composite inclusion criteria and endpoint determination. This validation study, the largest known of its kind, included approximately 13,000 independent reads for over 1,400 biopsies across four completed, global MASH clinical trials with various drug mechanisms of action. The study was performed across multiple sites and included samples with extensive variation in disease activity as well as biopsy, staining and scanning quality. Multiple prospectively collected pathologist reads per case (in which readers were either unassisted (independent manual readers; IMR) or assisted by AI) were collected from MASH expert pathologists, including reads from an independent 'gold-standard' consensus group or ground truth (GT). These reads were used to externally and robustly test both the algorithm alone and as used as an aid to pathologists (Fig. 1a) in representative trial settings. This extensive collection of AIM-MASH validation studies and analyses was designed in partnership with the FDA, the EMA and multiple experts from academia and drug development over several years of collaborative work. The aim was to demonstrate the tool's ability to provide a reliable, efficient solution for pathologists to address the urgent unmet need for accurate, standardized, clinical trial enrollment and histologic endpoint assessments, paving the way for more streamlined MASH drug approval pathways.

---

### The need for a system view to regulate artificial intelligence / machine learning-based software as medical device [^111Kx6DT]. NPJ Digital Medicine (2020). Medium credibility.

Artificial intelligence (AI) and Machine learning (ML) systems in medicine are poised to significantly improve health care, for example, by offering earlier diagnoses of diseases or recommending optimally individualized treatment plans. However, the emergence of AI/ML in medicine also creates challenges, which regulators must pay attention to. Which medical AI/ML-based products should be reviewed by regulators? What evidence should be required to permit marketing for AI/ML-based software as a medical device (SaMD)? How can we ensure the safety and effectiveness of AI/ML-based SaMD that may change over time as they are applied to new data? The U.S. Food and Drug Administration (FDA), for example, has recently proposed a discussion paper to address some of these issues. But it misses an important point: we argue that regulators like the FDA need to widen their scope from evaluating medical AI/ML-based products to assessing systems. This shift in perspective — from a product view to a system view — is central to maximizing the safety and efficacy of AI/ML in health care, but it also poses significant challenges for agencies like the FDA who are used to regulating products, not systems. We offer several suggestions for regulators to make this challenging but important transition.

---

### Machine learning in point-of-care testing: innovations, challenges, and opportunities [^1143TN5j]. Nature Communications (2025). High credibility.

Challenges and future directions

Regulatory challenges

One of the foremost challenges to integrating AI and ML into POCT is navigating the complex regulatory landscape. Regulatory bodies like the FDA and the European Medicines Agency (EMA) have stringent requirements to ensure the safety and efficacy of AI/ML-enabled medical devices –. The FDA categorizes software used for medical purposes under the "software as a medical device" (SaMD) framework. Within this framework, AI is defined as "a device or product that can identify, analyze, and use big data and large complex data sets from a variety of sources". By 2023, the FDA approved nearly 700 AI/ML-enabled medical devices, with a rapid increase in approvals since 2016. This rapid growth can be attributed to significant advancements in AI algorithms over this period and the increased accessibility of AI-related hardware and software. While further details on the FDA-approved AI/ML-enabled medical devices can be found in previous reports, this subsection will primarily focus on the challenges and limitations of current practices and future steps to address these barriers.

The majority (96.6%) of AI/ML-enabled medical devices have received FDA approval through a Premarket Notification 510(k) process, which demonstrates that a new product is substantially equivalent to a previously approved device (predicate). While this process reliably ensures the device's safety and effectiveness without the need for exhaustive clinical trials, the evolving nature of continuously improving AI models imposes concerns about its suitability for AI/ML-enabled medical devices. For instance, as AI algorithms continuously improve, at what point do the changes become significant enough that the device can no longer be considered equivalent to its approved version? What criteria should be used to assess the significance of these changes in the context of regulatory approvals? Addressing these questions is essential to prevent additional regulatory burdens on healthcare companies developing AI/ML-enabled medical devices.

---

### Generalizability of FDA-approved AI-enabled medical devices for clinical use [^114quFwy]. JAMA Network Open (2025). High credibility.

Figure 2.
Number of Artificial Intelligence–Enabled Medical Devices by Specialty, Along With Details on the Design of Their Clinical Performance Studies

The discriminatory performance of AI-enabled medical devices, as reported in clinical performance studies, was typically expressed in terms of sensitivity, specificity, or AUC. However, these metrics were available for only a minority of the devices, often without a clear description of the study cohort in which the performance was assessed. Sensitivity, specificity, and AUC were reported for 183 devices (36.2%), 176 devices (34.9%), and 82 devices (16.2%) of 505 devices that included clinical studies (Figure 3). The pair-plot in Figure 3 illustrates the pairwise associations between these 3 metrics, providing a visual overview of how they correlate with one another. A large proportion of the AI-enabled devices demonstrated high discriminatory performance, with scores exceeding 0.80 on these metrices. The off-diagonal elements of the pair-plot (represented as bubble plots) capture the scatter associations between each pair of metrics. In particular, comparisons between AUC and either sensitivity or specificity shed light on the interplay between predictive accuracy and the device's ability to correctly identify true positives (sensitivity) and true negatives (specificity). Device clustering toward the upper-right corner of the plot indicate a well-balanced performance, with both high sensitivity and specificity. Devices with high AUC tended to show high sensitivity, implying that these devices are effective at identifying true positives. It is noteworthy that some outliers exist — devices that exhibit high AUC values but demonstrate very low sensitivity or specificity. These outliers correspond to applications where the device produces either high rate of false negatives or false positives, highlighting a potential issue in detecting true positives or negatives despite an overall high AUC.

Figure 3.
Performance Distribution and Associations Among AUC, Sensitivity, and Specificity Metrics for AI-Enabled Medical Devices in Clinical Performance Studies

AUC indicates area under the curve. The diagonal plots display the distribution of each individual metric (AUC, sensitivity, and specificity), while the off-diagonal scatter plots show the pairwise relationships between these metrics. Only 3 recalled devices had available sensitivity and specificity values, but none of them had AUC values.

---

### The state of artificial intelligence-based FDA-approved medical devices and algorithms: an online database [^113SkmXE]. NPJ Digital Medicine (2020). Medium credibility.

As studies about AI's role in medicine show, its use cases and companies developing such technologies have been skyrocketing, regulatory bodies, such as the US Food & Drugs Administration (FDA) and the European Medicine Agency (EMA), have tried to tackle its regulation and implementation. As the FDA has shown leadership regarding the adoption of AI/ML-based medical technologies, with a specific framework for AI/ML-based algorithms, we chose it as an example for further analysis.

Because of the high-risk nature of these medical devices and the unknown consequences of using AI/ML for medical decision-making and data analysis, the FDA has stringent regulatory requirements for medical device licensing. Developers of AI/ML-based medical devices and algorithms have to go through rigorous processes that are time and resource consuming. This can be considered pivotal as a barrier for the introduction of AI/ML in medicine.

Before medical hardware or software is legally made available in the US market, the parent company has to submit it to the FDA for evaluation. For medically oriented AI/ML-based algorithms, the regulatory body has three levels of clearance, namely, 510(k), premarket approvaland the de novo pathway, each of which needs specific criteria to be fulfilled in order to be granted (Table 1).

Table 1
Descriptions of the types of FDA approvals for AI/ML-based medical technologies.

For the development and marketing of medical algorithms, the FDA's stringent regulatory requirements pose important challenges to the companies developing them. Before, every new product had to go through the regulatory process. However, as companies update their algorithms on a much shorter time scale, namely in days, the FDA has realized that this process might become impossible to maintain. Therefore, the FDA started to consider "a total product lifecycle-based regulatory framework for these technologies that would allow for modifications to be made from real-world learning and adaptation, while still ensuring that the safety and effectiveness of the software as a medical device is maintained".

---

### Artificial intelligence (AI) for lung nodules, from theSpecial series on AI applications [^112iwQLD]. AJR: American Journal of Roentgenology (2022). Medium credibility.

Interest in artificial intelligence (AI) applications for lung nodules continues to grow among radiologists, particularly with the expanding eligibility criteria and clinical utilization of lung cancer screening CT. AI has been heavily investigated for detecting and characterizing lung nodules and for guiding prognostic assessment. AI tools have also been used for image postprocessing (e.g., rib suppression on radiography or vessel suppression on CT) and for noninterpretive aspects of reporting and workflow, including management of nodule follow-up. Despite growing interest in and rapid development of AI tools and FDA approval of AI tools for pulmonary nodule evaluation, integration into clinical practice has been limited. Challenges to clinical adoption have included concerns about generalizability, regulatory issues, technical hurdles in implementation, and human skepticism. Further validation of AI tools for clinical use and demonstration of benefit in terms of patient-oriented outcomes also are needed. This article provides an overview of potential applications of AI tools in the imaging evaluation of lung nodules and discusses the challenges faced by practices interested in clinical implementation of such tools.

---

### How machine learning is embedded to support clinician decision making: an analysis of FDA-approved medical devices [^111f2xM5]. BMJ Health & Care Informatics (2021). High credibility.

Introduction

Artificial intelligence (AI), technologies undertake recognition, reasoning or learning tasks typically associated with human intelligence, such as detecting disease in an image, diagnosis and recommending treatments, have the potential to improve healthcare delivery and patient outcomes. Machine learning (ML) refers more specifically to AI methods that can learn from data. The current resurgence in ML is largely driven by developments in deep learning methods, which are based on neural networks. Despite the expanding research literature, relatively little is known about how ML algorithms are embedded in working clinical decision support (CDS).

CDS that diagnoses or treats human disease automate clinical tasks otherwise done by clinicians. Importantly, CDS operates within a human–technology system, and clinicians can elect to ignore CDS advice and perform those tasks manually. Clinician interaction with ML-based CDS influences how they work and make decisions which in turn affects care quality and patient safety.

Alongside intended benefits, ML poses new risks that require specific attention. A fundamental challenge is that ML-based CDS may not generalise well beyond the data on which they are trained. Even for restricted tasks like image interpretation, ML algorithms can make erroneous diagnoses because of differences in the training and real-world populations, including new 'edge' cases, as well as differences in image capture workflows. Therefore, clinicians will need to use ML-based CDS within the bounds of their design, monitor performance and intervene when it fails. Clinician interaction with CDS is thus a critical point where the limitations of ML algorithms are either mitigated or translated into harmful patient safety events.

One way to study the interaction between clinicians and ML-based CDS is to consider medical devices. In the USA, software, including CDS that is intended to diagnose, cure, mitigate, treat or prevent disease in humans, are considered medical devicesand subject to regulation. Increasing numbers of devices that embody ML algorithms are being approved by the US Food and Drug Administration (FDA). Approval requires compliance with standards, as well as evaluation of device safety and efficacy. Regulators provide public access to approvals and selected documentation. Therefore, medical devices provide a useful sample for studying how ML algorithms are embedded into CDS for clinical use and how manufacturers intend clinicians to interact with them.

---

### Clinical trials informed framework for real world clinical implementation and deployment of artificial intelligence applications [^112fJCiQ]. NPJ Digital Medicine (2025). Medium credibility.

Discussion

Using a clinical trials framework for healthcare AI provides a pragmatic, structured, stepwise approach to evaluating and scaling novel AI solutions in care delivery. This framework emphasizes patient safety, efficacy and real-world applicability. By mirroring the rigorous processes of traditional clinical trials, this framework offers a robust path to validate AI tools comprehensively, ensuring these technologies benefit diverse populations without introducing unintended risks. Also, this approach addresses the unique challenges of healthcare AI, including regulatory variability, ethical considerations, model drift, and data generalizability, while emphasizing continuous monitoring to sustain model integrity over time. Other healthcare-focused frameworks such as SPIRIT-AI/CONSORT-AI focus more on reporting standards or regulatory guidance (such as HTI-1).

In the US, while external clinical decision support may be considered a medical deviceand potentially be subject to formal FDA review, healthcare organizations can deploy in-house AI models without FDA certification, allowing for significant flexibility in internal clinical use. The need for rigorous and often prolonged evaluation of external solutions subsequently limits immediate market availability. This regulatory flexibility contrasts with requirements in other jurisdictions, where most clinical AI tools must be certified before use. Addressing such regulatory variation is essential for ensuring the framework's applicability across global healthcare settings, balancing flexibility for internal use with structured validation for external deployment.

This framework may be less applicable for AI applications in broader healthcare settings, such as public health or community health programs, where direct clinical workflow integration is not always feasible or necessary. In addition, the clinical trials approach to AI-based healthcare technologies may not be applicable for small- to medium-sized healthcare organizations which may implement these tools once they have already reached the ongoing monitoring stage. Analogous to traditional bench or clinical research, these AI clinical trials are more likely to occur at larger academic medical centers, as they require resources, financial investment, and AI-specific technical expertise. However, while large academic medical centers are likely to lead these efforts, it is crucial that the lessons learned from these initiatives are shared across all healthcare communities, including community healthcare centers and safety net hospitals. By disseminating knowledge and best practices, we can ensure that all populations benefit from safe, effective, and equitable AI solutions.

---

### New horizons in artificial intelligence in the healthcare of older people [^111n1nqC]. Age and Ageing (2023). Medium credibility.

Testing & validation

As with any clinical intervention, it is important that AI tools undergo external validation and assessment of clinical utility in their target population. AI tools should clearly demonstrate added value compared with the existing standard of care. However, a recent systematic review of 65 RCTs evaluating AI prediction tools showed that nearly 40% of tools had no clinical benefit compared with standard care. In the 17 RCTs that had a low risk of bias, machine learning tools showed no benefit over traditional statistical approaches. In geriatric medicine specifically, a systematic review of machine learning technologies developed for LTCs in older adults identified significant heterogeneity in how such technologies are evaluated, a lack of clinically meaningful outcome measures and a lack of testing against clinical gold standards or real-time data. It is important to note that even if an AI algorithm is approved as a medical device by regulatory agencies such as the Food and Drug Administration (FDA) in the USA, the Medicines and Healthcare products Regulatory Agency in the UK or the European Medicines Agency in the EU, it may not be beneficial to patient care.

To ensure that AI tools are robust, there is a need for standardisation of the reporting and clinical assessment of such technologies. Goldsack et al. have developed a three-stage framework for the evaluation of monitoring technologies, which includes 'verification' of sensor outputs, 'analytical validation' of physiological metrics produced from sensor outputs and 'clinical validation' of the technology. For AI based prediction models, a 20 question framework was published in 2020 to assess the transparency, replicability, ethics and effectiveness of AI based prediction tools. A reporting guideline and risk of bias assessment tool specific to AI diagnostic and predictive models is also now in development (the TRIPOD-AI and PROBAST-AI tools, respectively). Importantly, AI tools for older people should be assessed according to outcomes identified as important by older people themselves, including factors such as autonomy and control, loneliness and isolation, and activities of daily living.

---

### FDA-regulated AI algorithms: trends, strengths, and gaps of validation studies [^113wugRw]. Academic Radiology (2022). Medium credibility.

Rationale and Objectives

To assess key trends, strengths, and gaps in validation studies of the Food and Drug Administration (FDA)-regulated imaging-based artificial intelligence/machine learning (AI/ML) algorithms.

Materials and Methods

We audited publicly available details of regulated AI/ML algorithms in imaging from 2008 until April 2021. We reviewed 127 regulated software (118 AI/ML) to classify information related to their parent company, subspecialty, body area and specific anatomy type, imaging modality, date of FDA clearance, indications for use, target pathology (such as trauma) and findings (such as fracture), technique (CAD triage, CAD detection and/or characterization, CAD acquisition or improvement, and image processing/quantification), product performance, presence, type, strength and availability of clinical validation data. Pertaining to validation data, where available, we recorded the number of patients or studies included, sensitivity, specificity, accuracy, and/or receiver operating characteristic area under the curve, along with information on ground-truthing of use-cases. Data were analyzed with pivot tables and charts for descriptive statistics and trends.

Results

We noted an increasing number of FDA-regulated AI/ML from 2008 to 2021. Seventeen (17/118) regulated AI/ML algorithms posted no validation claims or data. Just 9/118 reviewed AI/ML algorithms had a validation dataset sizes of over 1000 patients. The most common type of AI/ML included image processing/quantification (IPQ; n = 59/118), and triage (CADt; n = 27/118). Brain, breast, and lungs dominated the targeted body regions of interest.

Conclusion

Insufficient public information on validation datasets in several FDA-regulated AI/ML algorithms makes it difficult to justify clinical applications since their generalizability and presence of bias cannot be inferred.

---

### How AI is used in FDA-authorized medical devices: a taxonomy across 1, 016 authorizations [^116GABto]. NPJ Digital Medicine (2025). Medium credibility.

Our analysis provides complementary insights to other recent reviews of FDA-authorized devices that have examined aspects such as predicate networks, marketing, explainability, regulatory pathways, and clinical validation. Each of these aspects are important for evaluating individual devices and the FDA regulatory process as a whole. Our taxonomy helps address the fundamental question — what are these devices designed to do — but a subsequent critical question is — how well do they perform for these applications. To this end, recent analyses have called for improved reporting of the clinical validation performed for FDA authorization, and a greater use of prospective studies overall. Our analysis helps inform these discussions by linking device function to the question of what constitutes appropriate validation for a given use case.

Rather than treating AI as a homogeneous technology in medicine, it is essential to understand current use cases and evolving trends. We created an organized taxonomy of AI/ML-enabled medical devices by reviewing over one thousand FDA authorizations. This taxonomy and accompanying database are intended to provide utility for both clinical and research purposes, such as helping clinicians identify use cases relevant to their practice and aiding researchers in formulating novel AI applications. Our analysis of the database confirms some expected patterns, such as a high proportion of quantitative imaging devices, but also highlights notable recent shifts, such as increases in higher-risk use cases and signal-based devices. While LLM-based generative models are not yet represented, more than one hundred devices leverage AI for medical data generation, including image denoising ("super-resolution") and synthetic data creation. To facilitate further exploration, we have developed an interactive website to host the curated database. Our taxonomy is designed not only to capture the current landscape but also to be extendable as new devices receive authorization. Continued monitoring of AI-enabled medical devices will be crucial for healthcare professionals, policymakers, and AI developers to fully realize AI's translational potential in medicine.

---

### Generalizability of FDA-approved AI-enabled medical devices for clinical use [^111TAPQQ]. JAMA Network Open (2025). High credibility.

In accordance with the entirety of FDA-approved medical devices, most approved AI-enabled medical devices received approval via the 510(k) regulatory pathway. This pathway streamlines the approval process by mandating manufacturers to demonstrate that a new device shares technological characteristics and indications with legally authorized devices, thereby establishing substantial equivalence without the compulsory need for clinical testing. Furthermore, current FDA regulations do not prohibit voluntarily recalled devices from serving as predicates for future device authorizations.

Given these considerations, the reliance on the 510(k) pathway raises significant safety concerns, particularly for high-risk (class I) medical devices. This issue is especially pertinent for implantable devices, which, due to their direct impact on individual patient health and safety, should be subject to more stringent approval requirements. However, the FDA database indicates that all implantable AI-enabled medical devices received approval via the less rigorous 510(k) pathway. Of note, a substantial proportion of implantable AI-enabled medical devices (2 of 6 devices [33.3%]) were later recalled by their manufacturers. This recall rate is comparable to trends observed across all FDA-approved medical devices, where most recalled devices underwent the 510(k) approval process. Notably, 19% of class I recalls across all medical devices involved implantable devices, potentially highlighting that substantial equivalence may be an inadequate marker of safety and efficacy.

---

### Generalizability of FDA-approved AI-enabled medical devices for clinical use [^115DnnhR]. JAMA Network Open (2025). High credibility.

Discussion

In our cross-sectional study of the most comprehensive publicly available source of information on AI-enabled medical devices for clinical use, we analyzed a diverse range of such devices that have received FDA approval. At the time of regulatory approval, clinical performance studies had been reported for half of the devices, while one-quarter explicitly stated that none had been conducted. Most of the clinical performance studies were retrospective in design, and only 2% involved randomized clinical trials. The availability of discriminatory performance metrices was limited. Among the clinical studies, less than one-third included sex-specific data, and only one-fourth addressed age-related subgroups.

Clinical Evaluation of Devices

Ensuring that AI models are trained on comprehensive and representative datasets is essential to avoid biases and ensure reliable outcomes in real-world applications. AI models are highly dependent on the data on which they are trained. If the training data are not representative of the broader population (eg, in terms of demographics, disease prevalence, or clinical settings), the AI's performance may not perform equally well in different patient populations or environments. Without robust evidence of generalizability, the effectiveness and safety of these devices may be compromised when used outside of the controlled conditions in which they were initially validated. In addition, AI-enabled devices may perform differently in varied clinical settings due to differences in equipment, protocols, and health care practices. Models trained in one setting may not be effectively applied to another setting without appropriate retraining or adaptation. An AI device developed and validated in one country or region might not perform equally well in another with a different patient population, potentially impacting the generalizability of AI models on a global level. Therefore, the clinical evaluation and validation of AI-enabled medical devices is crucial and remains challenging.

---

### How AI is used in FDA-authorized medical devices: a taxonomy across 1, 016 authorizations [^112ZxazP]. NPJ Digital Medicine (2025). Medium credibility.

We reviewed 1016 FDA authorizations of AI/ML-enabled medical devices to develop a taxonomy capturing key variations in clinical and AI-related features. Quantitative image analysis remains the most common application, but its relative proportion has declined recently. Over 100 devices leverage AI for data generation, though none yet involve LLMs. Our taxonomy clarifies current AI usage in medical devices and provides a foundation for tracking developments as applications evolve.

---

### Biomarkers as drug development tools: discovery, validation, qualification and use [^111VT4fy]. Nature Reviews: Rheumatology (2018). Medium credibility.

The 21st Century Cures Act, approved in the USA in December 2016, has encouraged the establishment of the national Precision Medicine Initiative and the augmentation of efforts to address disease prevention, diagnosis and treatment on the basis of a molecular understanding of disease. The Act adopts into law the formal process, developed by the FDA, of qualification of drug development tools, including biomarkers and clinical outcome assessments, to increase the efficiency of clinical trials and encourage an era of molecular medicine. The FDA and European Medicines Agency (EMA) have developed similar processes for the qualification of biomarkers intended for use as companion diagnostics or for development and regulatory approval of a drug or therapeutic. Biomarkers that are used exclusively for the diagnosis, monitoring or stratification of patients in clinical trials are not subject to regulatory approval, although their qualification can facilitate the conduct of a trial. In this Review, the salient features of biomarker discovery, analytical validation, clinical qualification and utilization are described in order to provide an understanding of the process of biomarker development and, through this understanding, convey an appreciation of their potential advantages and limitations.

---

### A scoping review of reporting gaps in FDA-approved AI medical devices [^111hLdhf]. NPJ Digital Medicine (2024). Medium credibility.

Methods

Based on the intended use, indications for use, and associated risks, the FDA broadly classifies devices as class I (low-risk), class II (medium-risk), and class III (high-risk). Class I and II devices are often cleared through the 510(k) pathway in which applicants submit a premarket notification to demonstrate safety and effectiveness and/or substantial equivalence of their proposed device to a predicate device. Class III (high-risk) devices are defined as those that support human life, prevent impairment of health, or pose potential unreasonable health risk(s). Evaluation of this class of devices follows the FDA's most rigorous device approval pathway known as premarket approval (PMA).

The Food, Drug, and Cosmetic Act subparagraph 520(h)(1)(A) requires that a document, known as a Summary of Safety and Effectiveness Data (SSED) be made publicly available following FDA approval. SSEDs are authored by applicants using a publicly accessible template provided by the FDA. The document intends to provide a balanced summary of the evidence for the approval or denial of the application for FDA approval. To be approved, it is required that the probable benefits of using a device outweigh the probable risks. The studies highlighted in the SSED should provide reasonable evidence of safety and effectiveness.

Thus, we conducted a scoping review of AI-as-a-medical-device approved by the FDA between 1995 and 2023, using FDA Summary of Safety and Effectiveness Data (SSED). This scoping review was conducted in accordance with the Preferred Reporting Items for Systematic Reviews and Meta-Analyses Extension for Scoping Reviews (PRISMA-ScR) guidelines. A completed PRISMA-ScR checklist is included in Supplementary Table 1. A protocol was not registered.

We included all SSEDs of FDA-approved AI/ML-enabled medical devices between 1995 and 2023 made publicly available via. Each SSED was reviewed by an expert in computer science, medicine, or academic clinical research who identified, extracted, and entered relevant variables of interest (Supplementary Table 2). Data was then computed into a Microsoft Excel spreadsheet. Counts and proportions of each variable were generated using Microsoft Excel. The spreadsheet and analysis worksheet on Microsoft Excel have been made available publicly via.

---

### Using artificial intelligence to study atherosclerosis from computed tomography imaging: a state-of-the-art review of the current literature [^115dgVAj]. Atherosclerosis (2024). Medium credibility.

5 AI in the assessment of atherosclerosis: challenges and opportunities

Besides the enormous progress in the integration of AI-based approaches to study atherosclerosis, there are remaining challenges which need to be addressed. In general, the application of AI in atherosclerotic diseases is still in the sphere of a "black box" concept which might prevent clinicians from incorporating these tools into clinical workflows. While the usage of AI tools in comprehensible segmentation tasks has been applied more frequently, complex models for risk prediction are still viewed with restraint. To improve this perception, further challenges need to be tackled in the first instance. They can be divided into remaining challenges regarding the input and the output data (Fig. 4). In the following, these will be outlined separately for the application of AI-based tools to assess coronary plaques.

Fig. 4
What we understand, what we are able to address with AI, and what is missing.

Regarding the input discrepancies, both data quantity and data quality need to be reviewed. In terms of data quantity, there is a striking need for large-scale multicenter studies for the evaluation of AI-based models. This is due to two main reasons: First, it is crucial to incorporate a reproducible and representative group of the whole population, and not to neglect a specific gender or an ethical minority. This type of "ethical responsibility" (84) can only be addressed through large-scale multicenter studies and will ultimately lead to a reduction of existing biases for those complex predictions. Secondly, only data externally validated and further tested in prospective controlled trials will have the prognostic power to pave the way for a standardized risk classification tool. As with pharmaceutical products, some AI-algorithms are classified as a 'medical device' and are subjected to regulations by the Food and Drug Administration (FDA) of the European Medical Agency (EMA). Sometimes, FDA or EMA-approved algorithms pass through a verification of performance in clinical trials. However, in most cases for EMA approval and in some cases for FDA approval this step is skipped.

---

### Virtual biopsy: just an AI software or a medical procedure? [^116aHE3y]. Journal of Nuclear Medicine (2022). Medium credibility.

Current Food and Drug Administration–cleared software tools are focused largely on providing decision support and decision automation within a diagnostic framework that continues to rely on physical biopsy.

On the one end of the diagnostic cascade, for instance, there are Food and Drug Administration–cleared solutions that aim to optimize the prostate MRI workflow through custom AI-augmented software before the extraction of a physical biopsy, such as Quantib. On the other end of the cascade, there are Food and Drug Administration–approved AI tools to detect prostate cancer after the biopsy has been performed and the tissue sample is available for analysis, such as Paige. In both cases, the software can be understood as a diagnostic tool or decision support system. They do not, however, directly replace the physical biopsy itself. Instead, they might indirectly influence management by providing evidence that a physical biopsy can safely be avoided. A medical procedure in the context of a virtual biopsy would refer to a software tool that provides a diagnosis of a histologic type without the need to perform an invasive biopsy. Whether this becomes possible depends on the density and extractability of the information that serves as input to these software tools. If imaging and clinical data were to contain a sufficient level of information that can effectively be used by machine learning techniques to accurately classify disease, then patient management without the need for a physical biopsy could become a reality. Because a virtual biopsy is not restricted to the actual target of a physical biopsy, patient management might even be improved in the future.

Defining and attaining the required level and reliability of information that is accessible through virtual biopsy to obtain stakeholder acceptance remain the central hurdle on the path toward virtual biopsies. As Penzkofer et al. pointed out, "a change in indication from a 'radiological diagnosis support tool' to 'clinical decision-making tool' would need to undergo rigorous testing against clinically valid endpoints, such as the presence of clinically significant lesions in positive and negative cases". This implies that what is ultimately required are large-scale prospective randomized controlled trials providing evidence that virtual biopsies can serve as substitutes for real biopsies. In addition, assessments of health-economic effects (in terms of potential cost reductions) and of quality-of-life changes (in terms of avoidance of complications associated with a physical biopsy) that will result from switching to virtual biopsies are needed. Then, stakeholders, such as patients, individual physicians, and guideline-issuing professional societies, as well as regulatory bodies, might begin to adopt virtual biopsies as medical procedures with management-determining results.

---

### Artificial intelligence and its effect on radiology residency education: current challenges, opportunities, and future directions [^1162FAsz]. Journal of the American College of Radiology (2025). Medium credibility.

Artificial intelligence has become an impressive force manifesting itself in the radiology field, improving workflows, and influencing clinical decision-making. With this increasing presence, a closer look at how residents can be properly exposed to this technology is needed. Within this paper, we aim to discuss the three pillars central to a trainee's experience including education on AI, AI-Education tools, and clinical implementation of AI. An already overcrowded clinical residency curricula makes little room for a thorough AI education; the challenge of which may be overcome through longitudinal distinct educational tracks during residency or external courses offered through a variety of societies. In addition to teaching the fundamentals of AI, programs which offer education tools utilizing AI will improve on antiquated clinical curricula. These education tools are a growing field in research and industry offering a variety of unique opportunities to promote active inquiry, improved comprehension and overall clinical competence. The near 700 FDA-approved AI clinical tools almost guarantees that residents will be exposed to this technology which may have mixed effects on education, although more research needs to be done to further elucidate this challenge. Ethical considerations, including algorithmic bias, liability, and post-deployment monitoring, highlight the need for structured instruction and mentorship. As AI continues to evolve, residency programs must prioritize evidence-based, adaptable curricula to prepare future radiologists to critically assess, utilize, and contribute to AI advancements, ensuring that these tools complement rather than undermine clinical expertise.

---

### Regulating the AI-enabled ecosystem for human therapeutics [^114aysvh]. Communications Medicine (2025). Medium credibility.

Harmonization of AI regulations

AI is becoming a notable priority for many countries, and governments globally are increasingly recognizing the strategic importance of AI for economic growth, national security, and technological innovation. There is a risk that different countries will establish separate, incompatible regulations to govern AI-related biomedical research. This could create barriers to simultaneously bringing new AI-enabled human therapeutics to patients. Rather than competing, the approach taken by the International Conference on Harmonization of the Technical Requirements for Pharmaceuticals for Human Use (ICH) should be considered. ICH is where industry and regulators have come together to coordinate technical requirements worldwide to ensure that safe, effective, and high-quality medicines are developed and registered efficiently. With the rapid integration of AI in drug discovery, development, and regulatory processes, there is a critical need for a harmonized global regulatory framework. This international effort would aim to create a unified regulatory approach that supports innovation while maintaining high safety and efficacy standards for healthcare technologies. This new body, either as a stand-alone organization or an adjunct to ICH, would provide consistent guidelines, fostering international collaboration. This initiative should aim to bridge gaps between different regulatory environments and streamline the adoption of AI innovations in human medicines, ultimately enhancing global health outcomes. Some progress has already been made, with the FDA, MHRA, and Health Canada collaborating to harmonize the regulatory framework for AI and machine learning-enabled medical devices. This collaboration has developed guiding principles and new transparency guidelines to ensure these devices are safe, effective, and trustworthy. Key initiatives include real-time device performance monitoring, managing AI-driven modifications, and ensuring clear communication about the devices' use and performance. Thus, the focus should be on coordination and cooperation between regulators, not competition.

---

### A scoping review of reporting gaps in FDA-approved AI medical devices [^114W9csm]. NPJ Digital Medicine (2024). Medium credibility.

Results

Distribution of device approval across clinical specialties

A total of 692 SSEDs of FDA-approved AI-enabled medical devices/software were analyzed. There was a steady increase in annual FDA approvals for AI-enabled medical devices with a mean of 7 between 1995 and 2015, increasing to 139 approvals in 2022 (Fig. 1). The regulatory class of each device included in the study was determined and categorized according to the United States Food and Drug Administration (FDA) classification system. Only 2 (0.3%) of the devices belonged to the regulatory Class III (devices posing the highest risk), while the vast majority (99.7%) of the devices belonged to Class II (devices whose safety and underlying technology are well understood and therefore considered to be lower risk).

---

### Artificial intelligence in breast cancer screening: evaluation of FDA device regulation and future recommendations [^115bVewZ]. JAMA Internal Medicine (2022). High credibility.

Importance

Contemporary approaches to artificial intelligence (AI) based on deep learning have generated interest in the application of AI to breast cancer screening (BCS). The US Food and Drug Administration (FDA) has approved several next-generation AI products indicated for BCS in recent years; however, questions regarding their accuracy, appropriate use, and clinical utility remain.

Objectives

To describe the current FDA regulatory process for AI products, summarize the evidence used to support FDA clearance and approval of AI products indicated for BCS, consider the advantages and limitations of current regulatory approaches, and suggest ways to improve the current system.

Evidence Review

Premarket notifications and other publicly available documents used for FDA clearance and approval of AI products indicated for BCS from January 1, 2017, to December 31, 2021.

Findings

Nine AI products indicated for BCS for identification of suggestive lesions and mammogram triage were included. Most of the products had been cleared through the 510(k) pathway, and all clearances were based on previously collected retrospective data; 6 products used multicenter designs; 7 products used enriched data; and 4 lacked details on whether products were externally validated. Test performance measures, including sensitivity, specificity, and area under the curve, were the main outcomes reported. Most of the devices used tissue biopsy as the criterion standard for BCS accuracy evaluation. Other clinical outcome measures, including cancer stage at diagnosis and interval cancer detection, were not reported for any of the devices.

Conclusions and Relevance

The findings of this review suggest important gaps in reporting of data sources, data set type, validation approach, and clinical utility assessment. As AI-assisted reading becomes more widespread in BCS and other radiologic examinations, strengthened FDA evidentiary regulatory standards, development of postmarketing surveillance, a focus on clinically meaningful outcomes, and stakeholder engagement will be critical for ensuring the safety and efficacy of these products.

---

### Clinical trials informed framework for real world clinical implementation and deployment of artificial intelligence applications [^115r2m3v]. NPJ Digital Medicine (2025). Medium credibility.

Guidance on implementation approach of solutions for pragmatic deployment in this space are arguably nascent; therefore, we advocate for a structured approach that is still rapid enough to impact clinical change in a reasonable timeframe but measured enough to yield defined outcomes and allow for scalability. Others have previously discussed practical concerns around scaling AI in healthcareand an approach to AI healthcare product development. Here, we offer a framework for healthcare organizations to implement AI technologies safely and with impact, beyond scientific research, using in-house developed tools or vendor-based solutions.

Clinical trials framework

US Food and Drug Administration (FDA) clinical research has four phases that include: Phase 1: understanding safety/drug dosage in 20–100 healthy individuals or individuals with the disease; Phase 2: measuring efficacy and identifying side effects in hundreds of individuals with the disease; Phase 3: larger scale efficacy/benefit and monitoring of adverse reactions in 300–3000 individuals, often relative to standard of care; and Phase 4: post-FDA approval post-market monitoring of safety and efficacy in thousands of individuals. We propose a clinical trials informed framework for AI implementation in healthcare systems at scale, which mirrors the four phases of FDA-regulated clinical trials — safety, efficacy, effectiveness, and post-deployment monitoring — to systematically address critical concerns such as regulatory compliance, patient safety, and model validation. This approach ensures that AI solutions undergo rigorous validation at each stage, creating a foundation for safe and effective clinical integration. By following these stages, healthcare AI can transition more seamlessly from research settings into routine practice, minimizing risks while maximizing patient outcomes. This is a similar approach that the American Medical Informatics Association, the national organization of informatics professionals, researchers, and other members in the US, has taken with case studies of AI.

For this perspective, though we recognize that the term AI encompasses a wide range of systems, we focus on AI solutions based on machine learning or large language models, though these concepts may apply to other AI systems such as computer-interpretable clinical guidelinesand argumentation for medical decision-making. We also recognize that these principles may be most relevant in the US, which has different AI regulations and higher complexity of clinical administrative burden.

Our clinical trials informed framework to AI solution deployment in healthcare is intended for AI solutions that are not necessarily medical devices with four phases (Table 1) for implementation, not regulation. As these AI solutions are neither medical devices nor drugs, this framework includes pragmatic consideration of clinical workflows required in the informatics realm.

Table 1
Clinical Trials Framework for Artificial Intelligence Applications

---

### Technical and clinical validation of commercial automated volumetric MRI tools for dementia diagnosis-a systematic review [^1163Z9zH]. Neuroradiology (2021). Medium credibility.

Previously published reviews covering quantitative radiological tools have either focused purely on AI-driven image analysis software for broader radiology or only covered a limited number of tools available on the market focused on neuropsychiatry. In recent years, there has been a considerable rise in companies providing both AI and non-AI-based automated quantitative analysis methods: 12 of the 17 identified in this study are less than 3 years old. This growth recently prompted the FDA to produce an "action plan" for AI/machine learning-based software as a medical device. In this paper, they outline plans to update current regulatory frameworks, strengthen the harmonized development of "good machine learning practice", support a patient-centred approach and, most relevant to this review, support the development of methods for evaluating and improving machine learning algorithms and promote real-world performance studies, in other words, technical and clinical validation. The ECLAIR guidelines were also published very recently aiming to provide guidance and informed decision-making when evaluating commercial AI solutions in radiology before purchase.

---

### Generalizability of FDA-approved AI-enabled medical devices for clinical use [^1152jVtd]. JAMA Network Open (2025). High credibility.

Limitations

The findings of this analysis should be interpreted in light of several limitations. First, we were only able to evaluate AI-enabled medical devices that received FDA approval for clinical use, and we could not perform any comparative analysis with AI-enabled devices that did not receive such approval. Second, we evaluated and characterized only AI-enabled devices for clinical use that had been approved by the FDA but not other jurisdictions. Therefore, the findings of this analysis may not be applicable outside the FDA regulatory framework. Third, we did not search for published information related to the clinical evaluations of the devices in sources other than the FDA website. Additional information may have been provided to the FDA during the evaluation process, but not all of it was ultimately made publicly available. A broader search among other resources might have allowed us to identify additional publicly available reports describing the clinical evaluation of these tools. Our focus was limited to the information explicitly provided in the public domain through the FDA, whereas linking and justifying the contribution of any clinical evaluation report to the FDA approval of the corresponding AI-enabled medical devices would have been challenging. Fourth, in this analysis, we considered any AI-enabled medical device listed on the FDA website. The definition of AI encompasses a broad range of software algorithms with varying functions, which may have influenced the granularity of the data used in this analysis and, subsequently, the results. Fifth, due to the nature of the devices themselves, some submissions may not necessitate a clinical performance study to demonstrate substantial equivalence, as is the case for devices that generate work lists and link scans in Picture Archiving and Communication System. Sixth, we did not retrieve information on the nature and quality of the references standard used in the clinical performance studies against which the AI-enabled medical devices were evaluated. This information might be relevant for interpreting performance data, especially in cases with imperfect reference standards. Future research should aim to address this limitation.

---

### Benefit-risk reporting for FDA-cleared artificial intelligence-enabled medical devices [^111pmz3k]. JAMA Health Forum (2025). High credibility.

Introduction

Medical devices enabled by artificial intelligence (AI) and machine learning (ML) are increasingly available for diagnosis and management in clinical areas including cancer, cardiology, and neurology. Both the US Food and Drug Administration (FDA) and European Union (EU) grant authorization or European Conformity marks, respectively, to allow marketing of medical devices, including AI/ML devices. As of August 7, 2024, a total of 950 AI/ML devices have been granted clearance or approval by the FDA for clinical use, with 108 and 107 cleared in 2023 and 2024, respectively.

The FDA and EU classify medical devices, including AI/ML devices, according to risk. In the US, the FDA uses 3 clearance pathways for devices. (1) De novo premarket review is intended for new low- to moderate-risk (classes I or II) devices not previously classified. (2) The 510(k) clearance is intended for devices demonstrating substantial equivalence to a legally marketed predicate device. However, the 510(k) process has limitations because it does not require independent clinical evidence for every new device, relying instead on comparisons to predicate devices, which may themselves have been cleared without rigorous clinical testing, potentially allowing incremental risks to accumulate over generations of devices.(3) The Premarket Approval (PMA) pathway is the most stringent and is typically required for devices posing a substantial risk of injury or illness (class III). Most AI/ML devices are designated class II (moderate) risk and are regulated through the 510(k) pathway, which has fewer safety reporting and postmarket surveillance requirements than the PMA pathway. Additionally, one-third of AI/ML devices cleared through the 510(k) pathway originate from non-AI/ML devices.

---

### A scoping review of reporting gaps in FDA-approved AI medical devices [^112t6jHB]. NPJ Digital Medicine (2024). Medium credibility.

Machine learning and artificial intelligence (AI/ML) models in healthcare may exacerbate health biases. Regulatory oversight is critical in evaluating the safety and effectiveness of AI/ML devices in clinical settings. We conducted a scoping review on the 692 FDA-approved AI/ML-enabled medical devices approved from 1995–2023 to examine transparency, safety reporting, and sociodemographic representation. Only 3.6% of approvals reported race/ethnicity, 99.1% provided no socioeconomic data. 81.6% did not report the age of study subjects. Only 46.1% provided comprehensive detailed results of performance studies; only 1.9% included a link to a scientific publication with safety and efficacy data. Only 9.0% contained a prospective study for post-market surveillance. Despite the growing number of market-approved medical devices, our data shows that FDA reporting data remains inconsistent. Demographic and socioeconomic characteristics are underreported, exacerbating the risk of algorithmic bias and health disparity.

---

### FDA perspective on the regulation of artificial intelligence in health care and biomedicine [^113v4vXd]. JAMA (2025). Excellent credibility.

Importance

Advances in artificial intelligence (AI) must be matched by efforts to better understand and evaluate how AI performs across health care and biomedicine as well as develop appropriate regulatory frameworks. This Special Communication reviews the history of the US Food and Drug Administration's (FDA) regulation of AI; presents potential uses of AI in medical product development, clinical research, and clinical care; and presents concepts that merit consideration as the regulatory system adapts to AI's unique challenges.

Observations

The FDA has authorized almost 1000 AI-enabled medical devices and has received hundreds of regulatory submissions for drugs that used AI in their discovery and development. Health AI regulation needs to be coordinated across all regulated industries, the US government, and with international organizations. Regulators will need to advance flexible mechanisms to keep up with the pace of change in AI across biomedicine and health care. Sponsors need to be transparent about and regulators need proficiency in evaluating the use of AI in premarket development. A life cycle management approach incorporating recurrent local postmarket performance monitoring should be central to health AI development. Special mechanisms to evaluate large language models and their uses are needed. Approaches are necessary to balance the needs of the entire spectrum of health ecosystem interests, from large firms to start-ups. The evaluation and regulatory system will need to focus on patient health outcomes to balance the use of AI for financial optimization for developers, payers, and health systems.

Conclusions and Relevance

Strong oversight by the FDA protects the long-term success of industries by focusing on evaluation to advance regulated technologies that improve health. The FDA will continue to play a central role in ensuring safe, effective, and trustworthy AI tools to improve the lives of patients and clinicians alike. However, all involved entities will need to attend to AI with the rigor this transformative technology merits.

---

### Regulating the AI-enabled ecosystem for human therapeutics [^117F1pnt]. Communications Medicine (2025). Medium credibility.

Introduction

Developing new human medicines is typically estimated to take approximately 10 to 15 years and costs around $2.6 billion due to the extensive preclinical research, clinical trials, and regulatory requirements necessary to ensure safety and efficacy. Utilizing artificial intelligence (AI) tools and techniques, new medicines could be discovered, developed, manufactured, and delivered to patients in a shorter time frame and at a fraction of the current cost. However, using these new methodologies will necessitate robust regulatory oversight with appropriate guardrails to protect human health. In this Perspective, we address the potential challenges when regulating the use of AI-driven technologies for the discovery and development of new human medicines while ensuring they are high quality, efficacious, and safe. We also discuss the current processes being adopted by regulatory agencies around the world.

---

### Artificial intelligence tool development: what clinicians need to know? [^114oX19m]. BMC Medicine (2025). Medium credibility.

Registration

The registration of tools with relevant authorities is typically carried out by the manufacturer, developer or the organisation responsible for the AI tool. In many cases, this involves collaboration between technical and regulatory teams within the organisation to ensure compliance with the regulatory requirements of the target market. Tools registration with relevant authorities such as medical device authorities could increase the likelihood of successful implementation and deployment in the real world. The evaluation criteria differ across countries, which may include an effectiveness trial. AI tools in the UK are classified as medical devices and therefore require Medicines and Healthcare products Regulatory Agency (MHRA) approval bearing the "United Kingdom Conformity Assessed" (UKCA) logo. However, AI tools in Europe are regulated by the EU Medical Device Regulation (EU MDR) and bearing the "Conformité Européenne" logo to be marketed in Europe. In the USA, AI tools are regulated by the Food & Drug Administration (FDA). FDA classifies AI tools based on their risk level and intended use following pathways such as 510(k) premarket notification, De Novo classification or premarket approval. Many AI tools are categorised as Software as a Medical Device and must meet rigorous criteria for safety, effectiveness and transparency including Good Machine Learning Practices. Post-market surveillance is often required to monitor real-world performance while labelling must clearly define intended use, performance metrics and limitations. European AI Act prohibits AI systems that collect sensitive personal information that causes discrimination, manipulates human behaviour or exploits vulnerabilities of certain groups of people at all social places. The core principle is that AI "… should be a human-centric technology. It should serve as a tool for people, with the ultimate aim of increasing human well-being". Clinicians play a vital role in evaluating AI tools' suitability for their practice and ensuring their safe and effective use.

---

### A proposal for developing a platform that evaluates algorithmic equity and accuracy [^116stggo]. BMJ Health & Care Informatics (2022). High credibility.

Of course, even a dataset that fairly represents all members of a targeted patient population is not very useful if it is inaccurate in other respects. A dataset that includes a representative sample of African-Americans, for instance, will be of limited value if the algorithm derived from that dataset is not validated with a second, external dataset. For example, when a machine learning approach was used to evaluate risk factors for Clostridium difficile infection, testing the algorithms in two different institutions found that the top 10 risk factors and top 10 protective factors were quite different between hospitals.

Likewise, an algorithm that takes into account socioeconomic status may fall short if it is derived solely from retrospective analysis based on data that is not representative of the population to whom it will be applied. For example, randomised controlled trials (RCTs), which are the gold standard on which to base decisions about the effectiveness of any intervention, often do not enrol fully representative populations due to numerous inclusion and exclusion criteria. Carefully designed and well-executed analyses of 'real-world' datasets can supplement and expand the insights that can be derived from RCT data, especially in the creation of clinical decision support tools. The expectation that an algorithm will perform well on a local health system level today, requires evaluation of performance that incorporates the diversity of the current local population.

This highlights the importance of differentiating between algorithms that are supported by retrospective versus prospective research. There are hundreds of retrospective AI studies that have been mislabeled clinical trials, but in a recent review of the literature, we found only five RCTs that examined the value of machine learning and AI in patient care, and nine non-RCT prospective studies. In light of these shortcoming, many healthcare providers hoping to implement algorithms with substantive evidence often turn to the US Food and Drug Administration (FDA) for guidance, working on the assumption that AI-enhanced software that has received FDA approval are more trustworthy and clinically proven to be safe and effective in patient care. Analysis of 130 FDA-approved AI devices suggests that the agency may not be able to perform an evaluation that guarantees the granularity that might be sought by local users. Wu et al have found:

---

### A unifying force for the realization of medical AI [^113EvuMx]. NPJ Digital Medicine (2022). Medium credibility.

These challenges must be addressed before AI can be used safely in clinical practice. Thankfully, similar barriers have been overcome in other domains of medicine and their solutions codified into regulation. For example, there is a growing recognition that ongoing performance assessment of a deployed AI model is key to combating dataset shift, a concept that follows the principles of continued monitoring of post-market surveillance required by the FDA. There are numerous regulatory resources (Supplementary Table 1)to address software, medical AI, and change modifications –. Much additional work is needed though, with the prevailing FDA regulations (Supplementary Table 1) or ISO governance approaches (Supplementary Table 2) dispersed across over 25 guidanceor standard documents, respectively.

One key question is whether applying regulatory paradigms can supplement the more traditional strength/weaknesses approach pursued in research. We have reconstructed examples where the addition or regulatory principles resulted in documented improvements (Supplementary Table 3). Briefly, the IBM Watson Content Analytics had a poorly described intended use; however, subsequent publications clearly communicate value propositions in regulatory terms (Supplementary Table 3). Google's AI-screening for diabetic retinopathy is an example where the lack of instructions for use was responsible for key performance issues (e.g. operating the device in a dark room). Notably, the lack of regulatory aspects was in direct contradiction to simultaneously published regulatory comments from the FDA and (notably) google itself — emphasizing the importance of regulatory consistencies (Supplementary Table 3). In other words, we can reconstruct that two of the most drastic AI fiascoes entailed inconsistencies in communication that resulted in miscommunication between AI and healthcare experts. Other examples include documented improvements in objectivity and reproducibility when tailoring performance measures to the specific target population. Notably, adoption of the algorithm based on the target population -matched (as a mitigation strategy) enabled overcoming a biomarker challenge in ovarian cancer screening previously flagged as a public health concern (Supplementary Table 3). These examples illustrate that regulatory concepts are consequential and hold clinical value beyond a vantage point in a research publication.

---

### A general framework for governing marketed AI / ML medical devices [^115qW5j6]. NPJ Digital Medicine (2025). Medium credibility.

This project represents the first systematic assessment of the US Food and Drug Administration's postmarket surveillance of legally marketed artificial intelligence and machine learning based medical devices. We focus on the Manufacturer and User Facility Device Experience database-the FDA's central tool for tracking the safety of marketed AI/ML devices. In particular, we evaluate the data pertaining to adverse events associated with approximately 950 medical devices incorporating AI/ML functions for devices approved between 2010 through 2023, and we find that the existing system is insufficient for properly assessing the safety and effectiveness of AI/ML devices. In particular, we make three contributions: (1) characterize the adverse event reports for such devices, (2) examine the ways in which the existing FDA adverse reporting system for medical devices falls short, and (3) suggest changes FDA might consider in its approach to adverse event reporting for devices incorporating AI/ML functions.

---

### US FDA approval of pediatric artificial intelligence and machine learning-enabled medical devices [^116wStgp]. JAMA Pediatrics (2025). High credibility.

Discussion

Despite rapid growth in the availability of AI/ML-enabled devices in recent years, only a small number have been authorized for pediatric use. Among devices labeled for pediatric patients, few device manufacturers disclosed information in regulatory documents on whether algorithm validation was performed in pediatric cohorts and only 18.7% explicitly described validation using datasets that included children. The current regulatory framework may expose children to off-label use, differential performance of algorithms, and safety risks. Additionally, the lack of standardized reporting of pediatric device characteristics precludes informed decision-making by health care clinicians on appropriate device use. Study limitations include reliance on a database curated from publicly available FDA information, exclusion of devices potentially missing from the FDA's website, and inability to account for device specifications that may be published by manufacturers elsewhere.

Pediatric AI/ML-enabled devices should be validated using representative datasets and should include complete and standard documentation on pediatric testing and authorization. Such changes will require cooperation across regulatory and industry stakeholders with a commitment to safe, equitable, and effective AI/ML development for children.

---

### Generalizability of FDA-approved AI-enabled medical devices for clinical use [^116PREqQ]. JAMA Network Open (2025). High credibility.

Future Perspectives

In the US, strong FDA oversight helps maintain public trust in regulated AI-driven health technologies. For long-term success, industry, academia, and regulatory bodies must work together to address irresponsible practices, prevent misleading claims, and refine tools for assessing AI's safety and effectiveness. In Europe, unlike the FDA-approved AI devices, there is no dedicated public list of AI-enabled medical devices with Conformité Européenne (CE) marking. While national registries or databases, such as European Database on Medical Devices may provide information on CE-marked devices, the European Database on Medical Devices is not yet fully publicly accessible, whereas a centralized, public list specifically for AI-enabled medical devices remains unavailable. Along these lines, the European Union's Artificial Intelligence Act introduced a regulatory framework to ensure the safe, transparent, and ethical use of AI. As AI systems become increasingly integrated into medical devices, there will be the need for harmonization between the requirements of Medical Device Regulation and Artificial Intelligence Act. AI-enabled medical devices will need to comply with both frameworks, ensuring safety, efficacy, and ethical use of AI in medical applications.

To keep pace with advancements and clinical applications, regulatory bodies must continuously adapt their frameworks to uphold the clinical safety and effectiveness of AI in health care through adequate clinical evaluation. Priorities should include fostering transparency and accountability for AI-enabled medical devices alongside rigorous clinical evaluation to ensure their effectiveness and generalizability. Standardized methods for the postmarket monitoring of AI-enabled devices will be relevant to detect and address unexpected changes in performance after clinical deployment.

The future of AI-enabled medical devices holds the promise of transforming health care by making it more efficient, personalized, and accessible. Achieving this vision will require sustained collaboration among technology companies, clinicians, and regulators to ensure the safe and ethical application of AI in clinical practice. The medical community should play a central role in this process, which requires them to receive adequate training to effectively monitor the performance of AI-enabled medical devices in real-world settings on an ongoing basis.

---

### Lessons for local oversight of AI in medicine from the regulation of clinical laboratory testing [^1122pAwN]. NPJ Digital Medicine (2024). Medium credibility.

One notable way in which some AICDS tools will differ from common laboratory tests is in their potential for very broad indications for use. This difference is foreshadowed by recent advances in generative AI including large language models (LLMs). Such tools, and potentially someday truly general AI tools, could be useful for extremely broad indications for decision support. Such broad indications would present a challenge for current FDA review that requires defining a narrow indication for use. Relevant correlates in laboratory testing include untargeted drug screens and genome sequencing; although not as broad, for such tests many clinical results will have never been seen before and therefore not explicitly validated. While this breadth of use increases the complexity of tool validation and monitoring, the same overarching principles for ensuring quality apply. With a clear definition of the clinical indication(s), a medical AI director with sufficient understanding of the methods, inputs and outputs, the potential risks for errors, and the potential impact of errors on downstream clinical decision-making, could define a set of policies and procedures to reasonably ensure safety and efficacy. These would include performing a sufficient validation of key performance metrics and defining requirements for evaluating the appropriateness of inputs (i.e. is this a scenario for which the model has been sufficiently trained), bounds for outputs (i.e. scenarios for which the model's prediction is sufficiently confident), and monitoring for changes in model performance.

---

### AI-based diabetes care: risk prediction models and implementation concerns [^116UuiM2]. NPJ Digital Medicine (2024). Medium credibility.

Introduction

With the urgent need to address the increasing incidence and prevalence of diabetes globally, promising new applications of artificial intelligence (AI) for this chronic disease have arisen — development of predictive models, risk stratification, evaluation of novel risk predictors, and therapeutic management.

Thus far, most FDA-approved AI tools have been designed for early intervention and treatment management. Several of these tools are currently used in clinical diabetes care. For early intervention, in 2018 the FDA approved the autonomous AI system Digital Diagnostics, which was found to have high diagnostic accuracy in recognizing diabetes retinopathy in retinal screening images. In the same year, the FDA approved the Guardian Connect System, which uses AI technology to interpret biomedical data and predict a hypoglycemic attack an hour in advance. Since then, the FDA has also approved AI technologies that assist with optimizing insulin dosing and therapy for patients.

---

### FDA approval of artificial intelligence and machine learning devices in radiology: a systematic review [^116d4SFG]. JAMA Network Open (2025). High credibility.

Introduction

Artificial intelligence and machine learning (AI/ML) increasingly support clinical decision-making, particularly in radiology. The number of AI-enabled tools cleared by the US Food and Drug Administration (FDA) continues to rise. However, evidence about clinical generalizability is lacking. We focus on FDA-regulated AI devices in radiology, analyzing premarket submissions with emphasis on testing. We demonstrate that device testing gaps underscore the need for clinical oversight.

---

### Artificial intelligence and machine learning in cancer imaging [^111j3vzR]. Communications Medicine (2022). Medium credibility.

Healthcare and regulatory systems

There are significant perceived values of using AI solutions in healthcareat every stage of the clinical workflow. In radiology, this means improvements to the patient diagnostic pathway, from the appropriateness of imaging requeststo how actionable findings in radiological reports are followed up. The full potential of these improvements are not yet realised as there remain significant barriers to implementation.

From 2021, the new EU Medical Device Regulations has been enforced, mandating deeper scrutiny of software as a medical device (SaMD). Certification is given in accordance with how the software is used and applied within the clinical workflow. The majority of AI software in imaging are being certified as a decision-support tool, that is to say it should not be used on its own in for clinical or patient management. It is also worth considering whether the software is intended to be use by radiologists at primary reporting, or only after the initial primary report is issued as a second read. In the current commercial landscape, there are multitudes of software tools that are cleared by regulators but have not been adopted into healthcare systems.

AI products may continue to evolve after initial release through continuous training. Many products have found their way into the marketplace without being independently tested, despite obtaining CE labelling or FDA clearance. As such, a new FDA framework has been proposed to ensure the safety and effectiveness of AI tools. The FDA has introduced a predetermined change control plan in premarket submissions. This plan includes anticipated modification (SaMD pre-specifications) and the associated methodology used to implement these controlled changes (algorithm change protocol). The FDA expects a commitment from manufacturers on transparency and real-world performance monitoring, as well as updates on changes implemented as part of the approved pre-specifications and the algorithm change protocol.

Once the product or software has been validated as a certified medical device, a Data Protection Impact Assessment process must be initiated, usually at the local level, to safeguard data privacy — in Europe, this means compliance with the General Data Protection Regulations (GDPR). At the same time, a Solution Architecture Review should also be undertaken to carefully examine the possible IT architecture for implementation. Local rules must also be adhered to with regards to patient data use and storage, since each country can vary in the interpretation of the GDPR. Privacy concerns and the need for a rational and coherent digital infrastructure has been referred to as 'the inconvenient truth' in medical AI.

---

### Establishing a validation infrastructure for imaging-based artificial intelligence algorithms before clinical implementation [^112SXHZy]. Journal of the American College of Radiology (2024). Medium credibility.

With promising artificial intelligence (AI) algorithms receiving FDA clearance, the potential impact of these models on clinical outcomes must be evaluated locally before their integration into routine workflows. Robust validation infrastructures are pivotal to inspecting the accuracy and generalizability of these deep learning algorithms to ensure both patient safety and health equity. Protected health information concerns, intellectual property rights, and diverse requirements of models impede the development of rigorous external validation infrastructures. The authors propose various suggestions for addressing the challenges associated with the development of efficient, customizable, and cost-effective infrastructures for the external validation of AI models at large medical centers and institutions. The authors present comprehensive steps to establish an AI inferencing infrastructure outside clinical systems to examine the local performance of AI algorithms before health practice or systemwide implementation and promote an evidence-based approach for adopting AI models that can enhance radiology workflows and improve patient outcomes.

---

### Regulating the AI-enabled ecosystem for human therapeutics [^113JctMs]. Communications Medicine (2025). Medium credibility.

Recent advances in artificial intelligence (AI) tools and techniques can revolutionize the discovery, development, manufacturing, and delivery of new medicines by reducing the time and cost involved. The biopharmaceutical industry is a highly regulated sector where robust regulatory oversight is essential to ensure human therapeutics' quality, efficacy, and safety. This Perspective examines the challenges of regulating AI-driven technologies in drug discovery and development. As AI is anticipated to play an unprecedented role in transforming drug development, the critical question is not whether to regulate these advancements but how to do so effectively. Here, we evaluate current global drug regulatory practices, discuss gaps and unknowns, and provide recommendations on addressing these issues to facilitate effective regulations of AI-driven innovation in the biopharmaceutical industry.

---

### Regulatory aspects of the use of artificial intelligence medical software [^116Hg6vi]. Seminars in Radiation Oncology (2022). Medium credibility.

The rapidly evolving scenario of Artificial intelligence (AI) in medicine comes with new regulatory challenges, including certification, ownership, and control of data sharing, privacy protection, and accountability. The Medical Physicists (MPs) are traditionally responsible for ensuring the safety and quality of technology implementation in diagnostic and therapeutic settings. As such, they are also expected to contribute to the introduction of AI medical devices in routine clinical practice. Specifically, the MPs will play a stakeholder role for AI tools procurement, acceptance testing, commissioning, and quality assurance to confirm the claimed performances in relation to the medical device's intended use. Moreover, MPs who act as co-creators of such AI tools, will play a pivotal role in product requirements definition, data collection and annotation, clinical evaluation, support for regulatory pathways and marketing through scientific congresses and scientific publications. As AI software differs from the traditional (hardware) medical device that the MP is used to introduce in clinical settings, there is a need to acquire new competencies in the field of AI and its regulatory aspects. The purpose of this paper is to provide MPs with practical guidelines on regulatory aspects of AI medical devices, in the European and in the US landscape.

---

### A guideline for open-source tools to make medical imaging data ready for artificial intelligence applications: a society of imaging informatics in medicine (SIIM) survey [^112a2hZ2]. Journal of Imaging Informatics in Medicine (2024). Medium credibility.

In recent years, the role of Artificial Intelligence (AI) in medical imaging has become increasingly prominent, with the majority of AI applications approved by the FDA being in imaging and radiology in 2023. The surge in AI model development to tackle clinical challenges underscores the necessity for preparing high-quality medical imaging data. Proper data preparation is crucial as it fosters the creation of standardized and reproducible AI models while minimizing biases. Data curation transforms raw data into a valuable, organized, and dependable resource and is a fundamental process to the success of machine learning and analytical projects. Considering the plethora of available tools for data curation in different stages, it is crucial to stay informed about the most relevant tools within specific research areas. In the current work, we propose a descriptive outline for different steps of data curation while we furnish compilations of tools collected from a survey applied among members of the Society of Imaging Informatics (SIIM) for each of these stages. This collection has the potential to enhance the decision-making process for researchers as they select the most appropriate tool for their specific tasks.

---

### FDA-reviewed artificial intelligence-enabled products applicable to emergency medicine [^1119pxVV]. The American Journal of Emergency Medicine (2025). Medium credibility.

Objective

To identify and assess artificial intelligence (AI)-enabled products reviewed by the U.S. Food and Drug Administration (FDA) that are potentially applicable to emergency medicine (EM).

Methods

The FDA AI-enabled products website was accessed to identify all marketed products as of March 2024. Board-certified EM physicians analyzed all products for applicability to EM practice. Inclusion criteria included products used by EM physicians directly or non-EM physicians participating directly in the evaluation and management of patients in an acute care setting. The Clinical and Economic Review (ICER) Evidence Rating Matrix was used to rate the net health benefit of applicable products.

Results

A total of 882 AI-enabled products have been reviewed by the FDA from 1995 to 2024. There were 272 products that were updates of prior products that were excluded, leaving 610 unique products. Products were most commonly evaluated by Radiology (454/610), Cardiovascular (59/610), and Neurology (25/610) panels. We found 154 (25%) products applicable to EM that were approved through Radiology (121/154), Cardiovascular (24/154), Neurology (5/154), Anesthesiology (3/154), and Ophthalmology (1/154) panels. There were 30 products that were rated as having a comparable or incremental net health benefit with moderate certainty (a C+ rating).

Conclusion

An increasing number of AI-enabled products are available and regulated by the FDA. We have identified 154 that are applicable to EM, primarily related to assisting with diagnosis on various imaging modalities. There remain many opportunities for EM to assist in product reviews and meaningful translation of products into clinical practice.

---

### Generalizability of FDA-approved AI-enabled medical devices for clinical use [^117TditD]. JAMA Network Open (2025). High credibility.

Information Summarized on Device Level

All information was extracted from online sources and documents downloaded from the FDA website. We collated the following data for each AI-enabled medical device as derived from the FDA website and the attached summary reports for each tool: device and company name, country of origin, type of 510(k) submission program, identification number of the device, medical specialty domain, 510(k) review panel specialty, office responsible for premarket review, device classification name providing keywords or sentences about the theme of the device, date of application received, decision date, and final outcome of regulatory evaluation.

We recorded information regarding eligibility for and performance of third-party review, regulatory class of the device, submission type, Good Manufacturing Practice exemption availability of summary malfunction reporting, implant status of the device, life-sustain/support nature of device, specified target area of the device, and Digital Imaging and Communications in Medicine adherence. We also summarized details on recalls of devices (date of recall, recall class, FDA recall posting date, availability of validation summary, availability of FDA decision summary, type of use (prescription vs over-the-counter use), submission number of potential primary predicate device, and physical state (software only or software and hardware).

In addition, we recorded whether clinical performance studies were performed and reported on the summary page provided by the FDA. For each of the studies reporting the development and/or clinical performance of the device, we recorded the study design (retrospective, prospective nonrandomized, prospective randomized trials), the number of participating sites, countries, and sample size. We summarized whether baseline demographics were provided for the overall study population and in addition for sex- and age-specific subgroups. Finally, we summarized available information related to the diagnostic or predictive performance of the AI-enabled medical device in terms of sensitivity, specificity, and discriminatory ability (area under the curve [AUC]).

---

### A scoping review of reporting gaps in FDA-approved AI medical devices [^116TWyFU]. NPJ Digital Medicine (2024). Medium credibility.

Introduction

To date, the FDA has approved 950 medical devices driven by artificial intelligence and machine learning (AI/ML) for potential use in clinical settings. Most recently, the FDA has launched the Medical Device Development Tools (MDDT) program, which aims to "facilitate device development, timely evaluation of medical devices and promote innovation", with the Apple Watch being the first approved device for this regulatory process. As AI/ML studies begin to translate to clinical environments, it is crucial that end users can evaluate the applicability of devices to their unique clinical settings and assess sources of bias and risk.

One definition of algorithmic bias in the context of AI/ML health systems is instances when an algorithm amplifies inequities and results in poor healthcare outcomes. Some defined sub-categories of algorithmic bias are listed in Box 1.

Despite the rise in awareness of algorithmic bias and its potential implications on the generalizability of AI/ML models, there is a paucity of standardized data reporting by regulatory bodies including the FDA that provide reliable and consistent information on the development, testing, and training of algorithms for clinical use. This limits accurate analysis and evaluation of algorithmic performance, particularly in the context of under-represented research groups such as ethnic minorities, children, maternal health patients, patients with rare diseases, and those from lower socioeconomic strata. Deploying devices that cannot be transparently evaluated by end users may increase health disparity and is particularly relevant in the context of emerging clinical trials and real-world deployment. To date, there has been limited review of this published data.

Here, we investigate AI-as-medical-device Food and Drug Administration (FDA) approvals from 1995–2023 to examine the contents, consistency, and transparency in FDA reporting of market-approved devices with a focus on bias. We focus our study on the limited published FDA data and associated papers, in the format of a scoping review.

Box 1 Categories of algorithmic bias

---

### US FDA approval of pediatric artificial intelligence and machine learning-enabled medical devices [^113wa1Ge]. JAMA Pediatrics (2025). High credibility.

Medical technologies for children have historically lagged behind those for adults. Artificial intelligence–enabled and machine learning–enabled (AI/ML) medical devices represent a rapidly growing product type with unique requirements for extension to pediatric patients. Specifically, device algorithms require validation with datasets that include children to ensure effective application in the intended population. However, while the US Food and Drug Administration (FDA) stipulates that AI/ML algorithms undergo clinical validation, there are no requirements for manufacturers to specify whether testing included pediatric individuals or for device labels to present standard information on the age of patients for which a device was developed. Accordingly, this cross-sectional study aimed to quantify the availability of AI/ML-enabled devices authorized for children and to assess reporting of algorithm validation in the pediatric population.

---

### Generalizability of FDA-approved AI-enabled medical devices for clinical use [^114Swx14]. JAMA Network Open (2025). High credibility.

Importance

The primary objective of any newly developed medical device using artificial intelligence (AI) is to ensure its safe and effective use in broader clinical practice.

Objective

To evaluate key characteristics of AI-enabled medical devices approved by the US Food and Drug Administration (FDA) that are relevant to their clinical generalizability and are reported in the public domain.

Design, Setting, and Participants

This cross-sectional study collected information on all AI-enabled medical devices that received FDA approval and were listed on the FDA website as of August 31, 2024.

Main Outcomes and Measures

For each AI-enabled medical device, detailed information and key characteristics relevant for the generalizability of the devices at the time of approval were summarized, specifically examining clinical evaluation aspects, such as the presence and design of clinical performance studies, availability of discriminatory performance metrics, and age- and sex-specific data.

Results

In total, 903 FDA-approved AI-enabled medical devices were analyzed, most of which became available in the last decade. The devices primarily related to the specialties of radiology (692 devices [76.6.%]), cardiovascular medicine (91 devices [10.1%]), and neurology (29 devices [3.2%]). Most devices were software only (664 devices [73.5%]), and only 6 devices (0.7%) were implantable. Detailed descriptions of development were absent from most publicly provided summaries. Clinical performance studies were reported for 505 devices (55.9%), while 218 devices (24.1%) explicitly stated no performance studies were conducted. Retrospective study designs were most common (193 studies [38.2%]), with only 41 studies (8.1%) being prospective and 12 studies (2.4%) randomized. Discriminatory performance metrics were reported in 200 of the available summaries (sensitivity: 183 devices [36.2%]; specificity: 176 devices [34.9%]; area under the curve: 82 devices [16.2%]). Among clinical studies, less than one-third provided sex-specific data (145 studies [28.7%]), and only 117 studies (23.2%) addressed age-related subgroups.

Conclusions and Relevance

In this cross-sectional study, clinical performance studies at the time of approval were reported for approximately half of AI-enabled medical devices, yet the information was often insufficient for a comprehensive assessment of their clinical generalizability, emphasizing the need for ongoing monitoring and regular re-evaluation to identify and address unexpected performance changes during broader use.

---

### Artificial intelligence and machine learning in clinical development: a translational perspective [^112q3YRz]. NPJ Digital Medicine (2019). Medium credibility.

Introduction

Clinical drug development has remained relatively unchanged for the last 30 years. This is due, in part, to uncertainties in regulatory requirements, risk aversion, and skepticism about rapidly emerging, yet largely unproven, technologies (such as machine learning, and wireless health monitoring devices and sensors), and the lack of relevant actionable biomedical data sources and advanced analytics to generate hypotheses that could motivate the development of innovative diagnostics and therapies. Testing new biomedical treatments for safety and efficacy will also require new strategies, since it has been shown that existing therapies often only work for a small number of indicated individuals. The application of emerging digital technologies, such as next-generation sequencing, though, have increased both our understanding of disease mechanisms in larger pool of patients and the potential for developing personalized therapies. For example, the majority of the new molecular entities approved by the U.S. FDA in recent years were designed to target specific aberrations implicated in disease initiation and maintenance — a hallmark of precision medicine — which aims to tailor interventions based on individual characteristics of patients. In this light, an emerging strategy based on co-developing precision diagnostics and therapeutic agents as companion diagnostics for example may produce highly effective drugs with clinical outcomes that greatly exceed standard therapies.

---

### Generalizability of FDA-approved AI-enabled medical devices for clinical use [^114FVSc1]. JAMA Network Open (2025). High credibility.

Introduction

The advent of artificial intelligence (AI) leads to an ever-increasing number of medical devices that incorporate AI software applications for clinical use. These devices aim to facilitate and improve diagnostic work-up, provide risk estimation, and advance treatment outcomes. The rapid surge in harnessing the power of AI to accelerate medical research and exploit its capacity when addressing specific clinical challenges has led to the regulatory approval of numerous applications. Despite the promising potential and broad applications, transparent information regarding key characteristics and outcomes related to the clinical evaluation of these devices at the time of regulatory approval is not well established.,

Nonetheless, the US Food and Drug Administration (FDA) maintains a publicly accessible list of AI-enabled medical devices approved for clinical use, providing detailed information for each device. These devices are subjected to different evaluation pathways according to several reports. For instance, during the period 2015 to 2020, the FDA approved 222 AI medical device software products via the 510(k) pathway, which relies on demonstrating equivalence to existing devices. Conversely, submitted devices that lack substantially equivalent existing devices on the market must undergo the more stringent de novo or premarket approval process. Although the primary goal of every newly developed AI-enabled medical device is to ensure safe use in clinical practice, it has been suggested that dedicated clinical evaluations are both limited in scope and often have significant shortcomings.

Against this background, we aimed to summarize and outline key characteristics of AI-enabled medical devices that had received FDA approval for clinical use across various medical fields and to assess publicly available characteristics that support a thorough assessment of their clinical generalizability at the time of approval.

---

### Current clinical applications of artificial intelligence in radiology and their best supporting evidence [^111RjR8Y]. Journal of the American College of Radiology (2020). Medium credibility.

Purpose

Despite tremendous gains from deep learning and the promise of artificial intelligence (AI) in medicine to improve diagnosis and save costs, there exists a large translational gap to implement and use AI products in real-world clinical situations. Adoption of standards such as Transparent Reporting of a Multivariable Prediction Model for Individual Prognosis or Diagnosis, Consolidated Standards of Reporting Trials, and the Checklist for Artificial Intelligence in Medical Imaging is increasing to improve the peer-review process and reporting of AI tools. However, no such standards exist for product-level review.

Methods

A review of clinical trials showed a paucity of evidence for radiology AI products; thus, the authors developed a 10-question assessment tool for reviewing AI products with an emphasis on their validation and result dissemination. The assessment tool was applied to commercial and open-source algorithms used for diagnosis to extract evidence on the clinical utility of the tools.

Results

There is limited technical information on methodologies for FDA-approved algorithms compared with open-source products, likely because of intellectual property concerns. Furthermore, FDA-approved products use much smaller data sets compared with open-source AI tools, because the terms of use of public data sets are limited to academic and noncommercial entities, which precludes their use in commercial products.

Conclusions

Overall, this study reveals a broad spectrum of maturity and clinical use of AI products, but a large gap exists in exploring actual performance of AI tools in clinical practice.

---

### Growth in FDA-approved artificial intelligence devices in plastic surgery: a key look into the future [^117MfUE5]. Aesthetic Surgery Journal (2024). Medium credibility.

Plastic surgery has recently seen a significant rise in the adoption of artificial intelligence (AI) and machine learning (ML), aligning with broader trends across the medical field. These technologies aim to enhance surgical efficiency, optimize treatment planning, predict postsurgical aesthetic results, streamline patient management, and improve overall surgical decision-making. However, the integration of AI/ML into clinical practice presents ethical and practical challenges, including concerns about transparency, data bias, and security. The regulation and approval of AI/ML technologies, much like other medical devices, remain complex and continuously evolving. The US FDA has been proactive in addressing these challenges, developing specific frameworks for AI- and/or ML-based devices. As of now, the FDA has approved 950 AI-enabled medical devices, with the majority of approvals occurring in recent years. Although previous studies have examined the growth of AI-enabled devices across medical specialties, there has been limited focus on their growth in plastic surgery, or on their overall impact in this constantly changing field.

Herein, the first study of its kind, we identified 6 FDA-approved AI/ML-based devices in plastic surgery, approved between 2016 and 2024. These devices, categorized as general/plastic surgery–specific by the FDA, primarily focus on intraoperative support, aiming to reduce complications, improve efficiency, and lower costs for payers and providers. Innovations include tools for estimating blood loss through "smart sponges", minimally invasive robotic tools, and high-resolution intraoperative medical imaging (Table 1).

Table 1.
Overview of currently FDA-approved AI-enabled medical devices and software for use in plastic and reconstructive surgery

For these 6 FDA-approved medical devices, we identified 6 corporations responsible for advancing the respective AI-enabled innovation in plastic and reconstructive surgery. We gathered data from Pitchbook (Seattle, WA), Google Patents (Alphabet, Inc. Mountain View, CA), andto determine the number of patents, clinical trials, capital raised, and company status for each of the corporations responsible for these innovations (Table 1). These companies show diverse financial approaches: 2 are publicly listed, 2 have been acquired, 1 is a private start-up, and 1 recently filed for bankruptcy. Patent and clinical trial activity varied widely, depending on the parent company's portfolio, size, and longevity. These differences highlight the range of innovation and development stages within this sector.

---

### Artificial intelligence (AI) for fracture diagnosis: an overview of current products and considerations for clinical adoption, from theSpecial series on AI applications [^113zUopt]. AJR: American Journal of Roentgenology (2022). Medium credibility.

Fractures are common injuries that can be difficult to diagnose, with missed fractures accounting for most misdiagnoses in the emergency department. Artificial intelligence (AI) and, specifically, deep learning have shown a strong ability to accurately detect fractures and augment the performance of radiologists in proof-of-concept research settings. Although the number of real-world AI products available for clinical use continues to increase, guidance for practicing radiologists in the adoption of this new technology is limited. This review describes how AI and deep learning algorithms can help radiologists to better diagnose fractures. The article also provides an overview of commercially available U.S. FDA-cleared AI tools for fracture detection as well as considerations for the clinical adoption of these tools by radiology practices.

---

### The state of artificial intelligence-based FDA-approved medical devices and algorithms: an online database [^115yZr9o]. NPJ Digital Medicine (2020). Medium credibility.

At the beginning of the artificial intelligence (AI)/machine learning (ML) era, the expectations are high, and experts foresee that AI/ML shows potential for diagnosing, managing and treating a wide variety of medical conditions. However, the obstacles for implementation of AI/ML in daily clinical practice are numerous, especially regarding the regulation of these technologies. Therefore, we provide an insight into the currently available AI/ML-based medical devices and algorithms that have been approved by the US Food & Drugs Administration (FDA). We aimed to raise awareness of the importance of regulatory bodies, clearly stating whether a medical device is AI/ML based or not. Cross-checking and validating all approvals, we identified 64 AI/ML based, FDA approved medical devices and algorithms. Out of those, only 29 (45%) mentioned any AI/ML-related expressions in the official FDA announcement. The majority (85.9%) was approved by the FDA with a 510(k) clearance, while 8 (12.5%) received de novo pathway clearance and one (1.6%) premarket approval (PMA) clearance. Most of these technologies, notably 30 (46.9%), 16 (25.0%), and 10 (15.6%) were developed for the fields of Radiology, Cardiology and Internal Medicine/General Practice respectively. We have launched the first comprehensive and open access database of strictly AI/ML-based medical technologies that have been approved by the FDA. The database will be constantly updated.

---

### How AI is used in FDA-authorized medical devices: a taxonomy across 1, 016 authorizations [^114ShQBE]. NPJ Digital Medicine (2025). Medium credibility.

Over one thousand artificial intelligence (AI)/machine learning (ML)-enabled medical devices have been authorized by the US Food and Drug Administration (FDA). Given the vast range of potential applications of AI in clinical care – and the increasing focus on translating AI into routine practice, it is critical to understand what types of AI devices are currently authorized for clinical use, and how these use cases are evolving over time. The FDA's current classification systems for medical devices, including product codes and device classes, provide a broad characterization that do not fully capture key dimensions of AI use. Recent work has provided insights into subsets of devices –, but a comprehensive characterization of FDA-authorized AI-enabled devices is currently lacking.

We reviewed all 1016 authorizations of AI/ML-enabled medical devices listed by the FDA as of December 20, 2024, and created a taxonomy to describe and quantify major axes of variation in current devices (Fig. 1). The taxonomy has three core factors. The first factor is the core clinical function of the device, describing the general medical role or purpose it serves in patient care. The second factor is the AI function, capturing how the device uses AI to assist with the clinical function. The third factor is the data type used as input to perform the AI function. By reviewing public authorization summaries for current devices, we categorized devices into different classes for each of these factors. As a given device can have multiple authorizations over time for updated versions, we additionally grouped the 1016 authorizations into a list of unique devices through a semi-automated matching process (see Methods). We primarily report results at the device level rather than the authorization level to enable a more representative view of the current device distribution.

---

### Digital health technology-specific risks for medical malpractice liability [^113zjQi3]. NPJ Digital Medicine (2022). Medium credibility.

AI-enabled technologies for clinical decision support (CDS) tools

More than 130 different AI-based CDS tools were approved by the Food and Drug Administration (FDA) in the last 5 years for use across a number of different therapeutic areas. Additionally, the FDA has recently created a searchable database of AI-enabled medical devices, although not all such devices in this database are CDS. Healthcare professionals are increasingly asked to consider the data that such CDS tools provide as part of the clinical workflow. But should clinicians always follow the direction of the CDS tool? And if not, are they liable if they ignore the advice and this is later shown to have resulted in harm? Often clinical decisions are made holistically based on the best interests of the patient, and thus decision-making logic can be difficult to fully capture in medical records. These are the types of questions potentially relevant to malpractice risk that clinicians must now consider.

---

### Artificial intelligence tools in clinical neuroradiology: essential medico-legal aspects [^117By5K7]. Neuroradiology (2023). Medium credibility.

Regulation of AI-based medical devices

Key Points:

- Obtaining a CE mark (in the European Union) or FDA clearance/approval (in the United States) is a prerequisite for legally marketing AI-based medical devices and using them on patients in clinical routine.

- Marketing authorization for medical devices represents conformity assessment with regulatory standards and technical safety but does not necessarily demonstrate clinical usefulness.

- Close monitoring of AI-based tools used in clinical practice is important as part of postmarket surveillance.

Some AI tools used in clinical care are classified as medical devices. In order to be marketed and used on patients, any medical device, including software, has to comply with the local medical device regulations. For example, the regulatory framework in the European Union leads to CE marking and, in the USA, to clearance or approval by the Food and Drug Administration (FDA). While some countries outside the EU and the USA accept either CE marking or FDA marketing authorization, additional frameworks exist in other countries, e.g. in Japan through the Pharmaceuticals and Medical Devices Agency (PMDA). Following Brexit, the UK's Medicines and Healthcare Products Regulatory Agency (MHRA) has taken over responsibilities pertaining to the regulation of medical devices, which were previously handled by the EU. In consequence, the United Kingdom Conformity Assessment (UKCA) mark has been created for medical devices being marketed in Great Britain with a transition period until 30 June 2023. As regulatory clearance is a prerequisite for market entry, complying with different local regulatory frameworks is a complex and labor-intensive task for companies. Of note, Switzerland, for instance, recently decided to accept FDA-cleared or FDA-approved medical devices, thus lowering the regulatory burden for market entry.

---

### Artificial intelligence in cardiovascular clinical trials [^1165xkLr]. Journal of the American College of Cardiology (2024). Medium credibility.

REGULATORY PRIORITIES AND GUIDANCE ON AI IN CLINICAL TRIALS

Since 1995, the FDA has received > 300 submissions for drugs and biologic products with AI components and > 700 submissions for AI-enabled medical devices. The drug and biologic application submissions using AI traverse the landscape of drug development from drug discovery to postmarket safety surveillance and cut across a range of therapeutic areas, including CV disease. The diverse uses of AI in these submissions highlight the need for careful regulatory assessment of both benefits and risks and underscore the importance of adopting a risk-based approach commensurate with the level of risk posed by the specific context of use. For any specific AI application in drug development, model risk calculations will be determined by model influence and the decision consequence on the basis of the context of use. For example, high-risk models may require more evidence of credibility than low-risk models, and the regulatory approach may differ accordingly.

As with any innovation, AI creates opportunities and new and unique challenges. To meet these challenges, the FDA has accelerated its efforts to create an agile regulatory ecosystem that can facilitate innovation and adoption while ensuring public safety and guarding against potential risks. For example, in 2021, the FDA, together with Health Canada and the United Kingdom's Medicines and Healthcare Products Regulatory Agency, jointly identified 10 guiding principles informing Good Machine Learning Practice for medical devices that are AI enabled. These principles include the importance of bias mitigation by ensuring that clinical study participants and data sets are representative of the intended patient group using the device, the performance of the human AI team, and postapproval monitoring of performance. In March 2024, the FDA's medical products centers published a joint report describing 4 major areas of focus in regulating AI (Table 1). These areas are collaboration with key stakeholders, support for innovation, development of standards and best practices, and research related to monitoring AI performance for bias and inequity.

---

### Artificial intelligence to revolutionize IBD clinical trials: a comprehensive review [^113gjsxZ]. Therapeutic Advances in Gastroenterology (2025). Medium credibility.

AI in clinical trials is still relatively novel, and regulatory frameworks remain under development. There is currently no universally accepted standard for validating AI tools in clinical trials, making it difficult to guarantee their reliability and reproducibility. Integrating AI into clinical trials necessitates significant adjustments to established workflows. Clinicians and trial staff often lack the technical expertise needed to effectively operate and interpret AI tools. This requires extensive training and collaboration with data scientists. Additionally, resistance to AI adoption may arise from stakeholders skeptical about the accuracy of these systems or concerned about job displacement. The development and implementation of AI systems in clinical trials can be resource-intensive, with high initial investment requirements. Running advanced AI algorithms demands substantial computing infrastructure, which may not be available in all clinical trial settings. Additionally, continuous updates, validation, and monitoring of AI systems are necessary to ensure their ongoing accuracy and relevance, contributing to long-term costs. AI models also require rigorous validation to ensure they can be applied effectively across diverse trial populations. However, many models are tested on limited datasets, raising concerns about their generalizability. Differences in data sources, trial protocols, and patient demographics can lead to inconsistent results when applying the same AI model across multiple settings.

While AI holds tremendous potential to transform clinical trials, its implementation faces numerous challenges. Overcoming these obstacles requires a multidisciplinary approach that involves collaboration between researchers, clinicians, data scientists, ethicists, and regulators. Key strategies include enhancing data standardization, ensuring transparency in AI systems, establishing clear regulatory guidelines, and fostering education and collaboration among stakeholders. Addressing these challenges will enable AI to become a powerful tool for making clinical trials more efficient, equitable, and effective. The FDA's guidance provides a comprehensive framework for the responsible use of AI in drug and biological product development, emphasizing the need for transparency, validation, and patient-centric approaches. A recent report from World Health Organization also highlights the ethics and recommendations for governance of AI in health care. As the field continues to evolve, ongoing collaboration between researchers, regulators, and industry stakeholders will be essential to use AI's full potential in IBD research.

---

### How AI is used in FDA-authorized medical devices: a taxonomy across 1, 016 authorizations [^114jMyUe]. NPJ Digital Medicine (2025). Medium credibility.

Fig. 2
Subclasses of AI function in FDA-authorized medical devices.

a Illustration of the nine subclasses. b Distribution of AI function subclasses across FDA-authorized devices. Counts indicate the number of devices with the AI function subclass.

Across the 653 Analysis devices, 427 (65%; 58% of all devices) were subclassified as performing quantification/feature localization. Triage was the next most common Analysis subclass with 84 devices (12.9% of Analysis devices; 11.4% of all devices). The remaining subclass counts were as follows: 47 (7.2/6.4%) diagnosis, 45 (6.9/6.1%) detection, 40 (6.1/5.4%) detection/diagnosis, and 11 (1.7/1.5%) predictive. The proportion of devices in each Analysis subclass varied by data type and clinical function (Fig. 2). Image-based devices have a relatively higher percentage of quantification/feature localization and triage, whereas signal-based devices have a relatively higher percentage of diagnosis, detection/diagnosis, and predictive.

For generation devices, we identified three current subclasses: image enhancement, acquisition guidance, and synthetic data generation. Image enhancement includes tasks such as image denoising and AI-based reconstruction. Devices that use AI for acquisition guidance assist the clinical user in acquiring the appropriate data, such as indicating whether a body part or acquisition device is properly positioned. These devices do not explicitly generate the primary data used for downstream analysis but instead assist in properly capturing the data. Finally, synthetic data generation uses AI to generate a new data element altogether, such as synthesizing a CT medical image from an MRI or synthesizing a particular ECG bandwidth lead from existing leads.

Of the 106 Generation devices, 84 (79.2%; 11.4% of all devices) perform image enhancement. Acquisition guidance and synthetic data generation included 17 (16.0/2.3%) and eight (7.5/1.1%) devices, respectively. Images were the data type for 105 of the 106 Generation devices, with the remaining device performing synthetic data generation using signals.

---

### Proceedings of the clinical microbiology open 2024: artificial intelligence applications in clinical microbiology [^111NCUbL]. Journal of Clinical Microbiology (2025). Medium credibility.

FUTURE REGULATORY CONSIDERATIONS FOR AI

There are numerous uncertainties regarding the regulatory management of AI technologies, particularly in comparison to other diagnostic tests currently approved by the Food and Drug Administration (FDA). In 2023, the FDA released guidance recommending the inclusion of a Predetermined Change Control Plan (PCCP) in submissions for AI and machine learning (AI/ML) devices. This guidance seeks to streamline the process by which manufacturers update their AI/ML devices while maintaining regulatory oversight. Within the PCCP, the manufacturers are required to pre-specify anticipated changes to the AI/ML software, such as algorithm modifications or retraining with new data. The modification protocol should outline the methods and procedures for implementing these changes, which may involve data management, software validation, and performance testing. Additionally, manufacturers must assess the potential impact of these changes on device performance, supported by scientific evidence demonstrating that the device's safety and effectiveness will remain intact post-modification. If both the 510(k) submission and PCCP are approved, manufacturers are permitted to implement the pre-specified changes without requiring additional FDA approval, provided all conditions outlined in the PCCP are adhered to.

In a collaborative publication from various FDA centers, including the Center for Biologics Evaluation and Research (CBER), Center for Drug Evaluation and Research (CDER), Center for Devices and Radiological Health (CDRH), and Office of Combination Products (OCP), the FDA outlined its approach to the development and regulation of AI technologies across the medical product lifecycle. Key strategies include the following:

Collaborating with developers, patient advocacy groups, academia, global regulators, and other stakeholders to establish a regulatory framework that prioritizes patient safety and equity.
Monitoring trends to identify gaps and opportunities for the integration of AI in the medical product lifecycle, supporting regulatory science to evaluate AI algorithms, and understanding their resilience to changing inputs and conditions. This includes issuing guidance on these areas of focus.
Developing and refining criteria for the safe, ethical, and responsible use of AI in healthcare. The FDA aims to promote best practices for real-world performance monitoring, which may include ensuring the data used to train and validate AI models is fit for purpose and creating frameworks for continuous monitoring and risk mitigation.

---

### Randomized clinical trials of machine learning interventions in health care: a systematic review [^114xRAWk]. JAMA Network Open (2022). High credibility.

Discussion

This systematic review found a lack of RCTs for medical machine learning interventions and highlighted the need for additional well-designed, transparent, and inclusive RCTs for machine learning interventions to promote use in the clinic. There is growing concern that new machine learning models are being released after preliminary validation studies without follow-through on their ability to formally show superiority in a gold standard RCT. Of note, there are currently 343 US Food and Drug Administration (FDA)–approved medical AI or machine learning interventions. Our finding of 41 medical machine learning RCTs suggests that most FDA-approved machine learning–enabled medical devices are approved without efficacy demonstrated in an RCT. This finding is likely explained, in part, by the lower burden of evidence required for AI or machine learning algorithm clearance (often classified by the FDA as software as a medical device) compared with pharmaceutical drugs. To our knowledge, this review is the first rigorous attempt at quantifying this gap.

Prior work has shown a lack of prospective testing in this field but has not rigorously assessed the quantity of RCTs using a PROSPERO-registered method or tie-breaking arbitrationor has analyzed the testing of technologies only related to imaging data. In addition, these studies did not explore study adherence to CONSORT-AI standards or assess the inclusivity of underrepresented minority groups and women in the study populations. Finally, the scope of our review compared with prior work differed; our work specifically focused on the use of clinical AI or machine learning interventions that were used as investigational arms in RCTs. We excluded RCTs that used traditional statistical models and RCTs in which AI or machine learning was included in the study protocol but was not part of the randomized intervention. In this way, we highlighted RCTs that directly compared AI or machine learning with standard of care and RCTs that were designed to demonstrate a high level of evidence for clinical utility. A comparison of the trials included in this study with prior work is available in eAppendix 4 in the Supplement.

---

### Preventing unrestricted and unmonitored AI experimentation in healthcare through transparency and accountability [^1138dDC7]. NPJ Digital Medicine (2025). Medium credibility.

Evolution and oversight of AI in healthcare

Previously, the integration of algorithms into health information systems focused on developing AI models to address specific clinical questions and workflow inefficiencies, such as Clinical Decision Support Systems (CDSS) for evidence-based recommendations, predictive analytics for forecasting patient outcomes, and automating complex coding and billing processes. These AI tools, built from limited subsets of clinical data extracted from medical records, were typically accessible only to healthcare professionals and billing specialists. Research involving conventional AI that falls under the Food and Drug Administration (FDA) definition of Software as a Medical Device is subject to oversight, primarily through FDA and IRB supervision, as the device undergoes testing and validation. Commercial generative AI systems, such as those that use large language models (LLMs), were not initially designed as medical devices and therefore typically lack regulatory oversight throughout their development — from inception to validation. Classified as Software of Unknown Provenance (SOUP), these systems often do not disclose critical details about their data sources, development methodologies, or validation processes, making it challenging to assess and manage biases that may emerge at various stages of the model lifecycle. While AI systems not designed for medical purposes are not inherently subject to FDA or IRB regulation, concerns arise when these tools are repurposed for patient-facing or clinical decision-making tasks. Such applications may fall under FDA oversight if the AI is used for diagnosis, treatment, or disease prevention. Although the FDA applies discretionary enforcement policies for lower-risk software, where efficacy evidence may be minimal, the deployment of AI in high-risk contexts — such as interpreting medical data or guiding treatment — without thorough clinical evaluation introduces significant potential risks. Ensuring rigorous, lifecycle-based oversight for AI used in clinical applications is essential to safeguarding patient safety.

---

### Artificial intelligence and machine learning for cardiovascular computed tomography (CCT): a white paper of the Society of Cardiovascular Computed Tomography (SCCT) [^1154YRbz]. Journal of Cardiovascular Computed Tomography (2024). High credibility.

Cardiovascular CT AI/ML clinical deployment and regulation — pace, oversight, and FDA pathways: AI/ML research is developing rapidly, but clinical deployment is not following anywhere near the same pace, and around the world a variety of regulatory and legal issues must be considered; it is sometimes not clear whether and to what extent AI/ML is embedded within a software that is being used in clinical practice, due diligence and audit before installation and implementation of an AI/ML tool are good practice, and the term AI/ML has sometimes been used to refer to an automated task rather than true AI/ML and this needs to be addressed; AI tools that make autonomous decisions without physician support may be considered a medical device with additional associated regulations in some countries, whereas AI tools which provide information like a "second read" may be considered advisory and subject to lower levels of regulation; in the United States the Food and Drug Administration clearance of AI/ML tools may involve premarket clearance, the de novo classification or premarket approval, for most AI/ML tools for cardiovascular CT the route to FDA approval is premarket clearance (510(k)) based on comparison to a previously approved device, some AI/ML for cardiovascular CT may also receive market authorization through the de novo pathway, FDA clearance does not necessarily mean that an AI/ML software tool is useful in clinical practice, and most FDA cleared AI/ML tools were evaluated using retrospective test data.

---

### Digital twins, synthetic patient data, and in-silico trials: can they empower paediatric clinical trials? [^112UV9J2]. The Lancet: Digital Health (2025). High credibility.

Regulations for the use of digital data

In the USA, the Food and Drug Administration (FDA) regulates and approves digital health technologies for clinical use, and digital health is considered a medical device. Digital health is regulated based on its intended use in the diagnosis, treatment, cure, mitigation, or prevention of disease or condition. The FDA has enforcement discretion, which means that it does not comprehensively regulate low-risk medical devices or technologies. The FDA has implemented a Digital Health Center of Excellence, whose goal is to empower stakeholders to advance health care by fostering responsible and high-quality digital health innovation, including building partnerships, increasing awareness, advancing best practices, and innovating regulatory approaches to provide efficient and the least burdensome supervision. The FDA advocates for in-silico modelling and simulation and acknowledges the potential of computational modelling to enhance the regulatory assessment process. The FDA has also issued guidance on the use of computational modelling in medical device development. In Europe, the Artificial Intelligence Act was passed by the EU parliament on March 13, 2024, and the law came into force on Aug 1, 2024. The new law is applicable to the developers and deployers of general-purpose AI systems, to AI systems including AI enabled digital health tools, and to high-risk AI systems, including digital health tools in a staggered timeline. The Artificial Intelligence Act is a legislation with a broad scope compared to single-sector legislation (vertical legislation), which, for the health-care domain, are principally the Medical Device Regulation and In-Vitro Diagnostic Medical Devices Regulation, both specific to the EU. These two regulations will continue to apply to health-care AI alongside the Artificial Intelligence Act. Effective and safe innovations in the use of virtual patient data for clinical trials require active collaboration and partnership of AI and computational experts, researchers, health-care providers, health-care institutions, and regulatory bodies.

For more on the Artificial Intelligence Act see

For more on the implementation timeline see

---

### Artificial intelligence and machine learning in clinical development: a translational perspective [^116vUjdt]. NPJ Digital Medicine (2019). Medium credibility.

Discussion

MIT Workshops discussed new pathways set up by regulatory agencies for evaluation and adoption of AI and ML in clinical development. For example, in 2016, the 21 st Century Cures Act was signed into law, a significant bipartisan legislative achievement aimed at accelerating the discovery, development, and delivery of new cures and treatments was highlighted. FDA's current strategic policy places emphasis on leveraging innovation, advancing digital health technologies, and developing next-generation analytical approaches to improve health care, broaden access, and advance public health goals. SaMD and digital health pathways for regulatory approvals for AI, ML, and computer vision algorithms have been set up at FDA. To date, FDA has cleared or approved several AI/ML-based SaMD. Typically, these have only included algorithms that are locked prior to marketing. For example, FDA has approved diagnostics company IDx's ML-based software system for autonomous detection of diabetic retinopathy. In addition, Viz.ai's software, which uses a ML techniques to scan Computed Tomography images for indicators associated with stroke, also obtained the regulatory approval. Other software systems listed included automated detection of atrial fibrillation and coronary calcification scores. The FDA is also considering the ability of AI/ML-based SaMD for continuously learning and adaptive algorithms that have the potential to adapt and optimize device performance in real time to improve health care for patients in its regulatory framework. More broadly, FDA is using significance of information provided by SaMD to health care decision such as treat or diagnose, drive clinical or inform clinical management as key determinations for regulatory strategies. Pharmaceutical, biotechnology, and startup community participating in the workshops has recognized the potential of AI and ML in the development of personalized medicines and generating evidence motivating new products (summarized in Box 1). As examples, Johnson & Johnson Innovation's life sciences "JLABS" newco incubator currently includes ML startups, such as Analytics 4 Life, A2A Pharmaceuticals, Envisagenics, among others. Precision medicine company GNS Healthcare and Genentech aim to collaboratively discover and validate new oncology drugs and patient response markers. Pfizer and Novartis are each working with IBM Watson Health to facilitate immuno-oncological research and development. Glaxosmithkline recently announced the creation of "Accelerating Therapeutics for Opportunities in Medicine (ATOM)" Consortium to accelerate drug discovery process using ML tools. These early initiatives are either set up as in-house research groups or as public–private partnerships to engender cross-functional teams.

---

### A general framework for governing marketed AI / ML medical devices [^115xQMCv]. NPJ Digital Medicine (2025). Medium credibility.

This project represents the first systematic assessment of the US Food and Drug Administration's postmarket surveillance of legally marketed artificial intelligence and machine learning based medical devices. We focus on the Manufacturer and User Facility Device Experience database — the FDA's central tool for tracking the safety of marketed AI/ML devices. In particular, we evaluate the data pertaining to adverse events associated with approximately 950 medical devices incorporating AI/ML functions for devices approved between 2010 through 2023, and we find that the existing system is insufficient for properly assessing the safety and effectiveness of AI/ML devices. In particular, we make three contributions: (1) characterize the adverse event reports for such devices, (2) examine the ways in which the existing FDA adverse reporting system for medical devices falls short, and (3) suggest changes FDA might consider in its approach to adverse event reporting for devices incorporating AI/ML functions.

---

### FDA approval of artificial intelligence and machine learning devices in radiology: a systematic review [^1129VdYT]. JAMA Network Open (2025). High credibility.

Discussion

Since 2016, the number of FDA-regulated AI/ML devices has sharply risen, with 97% of all devices cleared via the 510(k) pathway. While efficient, the 510(k) pathway does not require independent clinical data demonstrating performance or safety. Ours is the first study to focus on FDA-regulated radiology devices, showing that clinical testing remains uncommon, even as device approvals accelerate.

Today, most AI/ML devices are used in conjunction with a human, yet only 56 were tested with any human operator. Most have not been validated against defined clinical or performance end points. In a study of radiologists interpreting chest radiographs with AI assistance, high performers maintained strong performance, while low performers did not necessarily improve. Such heterogeneity in performance raises questions about clinical generalizability.

The FDA has historically placed responsibility for device implementation on manufacturers and end users. The agency has released draft guidelines, such as the Total Product Life Cycle framework (TPLC), which provides more structured oversight. However, these guidelines are not consistently applicable.

The European Union has adopted postmarket monitoring frameworks for AI devices. For medical devices, manufacturers may use existing monitoring systems developed for non-AI technologies. In contrast, the FDA's TPLC offers a novel framework that tracks devices from conception to implementation with manufacturer accountability. Radiology AI device approvals have declined in the European Union under evolving regulations, coinciding with a rise in peer-reviewed studies on clinical outcomes.

While regulatory frameworks traditionally focus on safety, end users must consider utility. Institutional governance is needed to determine whether a device has undergone adequate testing and confirm that it functions as intended. Clinical studies and frameworks such as Health Technology Assessment adopted in Europe could also help address this evidence gap by evaluating clinical and economic value postapproval.

This study has limitations. Our analysis was constrained by variability in the completeness of FDA summary documents, potential underreporting of testing details, and exclusion of tools not subject to FDA clearance, such as those in the Health AI Register.

---

### Artificial intelligence for drug development… [^114oCkET]. FDA (2025). Medium credibility.

What role is AI playing in drug development. FDA recognizes the increased use of AI throughout the drug product life cycle and across a range of therapeutic areas. In fact, CDER has seen a significant increase in the number of drug application submissions using AI components over the past few years. These submissions traverse the drug product life cycle, which includes nonclinical, clinical, postmarketing, and manufacturing phases. Additionally, AI is increasingly integrated in areas where CDER is actively engaged, including Digital Health Technologies, and Real-World Data analytics. What is CDER's perspective on the use of AI in drug development. CDER is committed to ensuring that drugs are safe and effective while facilitating innovations in their development. FDA published a draft guidance in 2025 titled, "Considerations for the Use of Artificial Intelligence to Support Regulatory Decision Making for Drug and Biological Products.

" This guidance provides recommendations to industry on the use of AI to produce information or data intended to support regulatory decision-making regarding safety, effectiveness, or quality for drugs. on AI use in drug development; CDER's experience with over 500 submissions with AI components from 2016 to 2023; and a hybrid public workshop for interested parties held on August 6, 2024 to discuss the guiding principles for the responsible use of AI in the development of safe and effective drug and biological products. These activities also helped informed a recent publication titled: "Artificial Intelligence and Medical Products: How CBER, CDER, CDRH, and OCP are Working Together, " published in March 2024, which describes how FDA's medical product Centers plan to align their efforts to advance the responsible use of AI for medical products.

This entails building regulatory approaches that, to the extent feasible, can be applied across various medical products and uses within the health care delivery system. AI will undoubtedly play a critical role in the drug development life cycle and CDER plans to continue developing and adopting a risk-based regulatory framework that promotes innovation and protects patient safety. The advent of novel AI applications, such as generative AI and large language models, is likely to expand opportunities for use of AI within CDER, including by non-technical staff. This will require increased education and coordination to enhance AI knowledge among CDER staff members. There has also been a rapid increase in CDER regulatory submissions incorporating AI, and the scope and impact of AI use in drug development are expanding.

---

### Commercialization of medical artificial intelligence technologies: challenges and opportunities [^111cPkc5]. NPJ Digital Medicine (2025). Medium credibility.

Opportunities and learnings from the successful commercialization of medical AI technologies

Several companies have successfully commercialized their medical AI technologies, including a US company founded in 2016 focused on neurological care and now expanding to other areas, including cardiovascular, trauma, and oncologic care. To date, they have developed several FDA-cleared AI algorithms that analyze medical imaging data across various domains, including CT scans, electrocardiograms, and echocardiograms. These tools support clinicians with respect to diagnosis and treatment decision-making for various pathologies, including cerebral aneurysm, intracranial hemorrhage, ischemic stroke, pulmonary embolism, aortic dissection, and AAA, among others. The company successfully built a suite of commercial products, generating revenue over $40 million in 2024.

There were several key factors for their success. First, they assembled a multidisciplinary team of clinicians/engineers who developed robust AI algorithms demonstrated through multiple peer-reviewed publications –. In this company, academics worked closely with industry partners and followed standardized approaches for medical device development in a quality management system from the outset, with market and regulatory considerations built in early on through International Organization for Standardization (ISO) certifications. A preliminary health technology assessment demonstrated that such AI tools for ischemic stroke detection could save $11 million/year in the UK by identifying strokes that otherwise could have been missed. Resultingly, the company obtained over $290 million in funding from investors to build and scale their products. Using these funds, they developed and refined high-quality AI tools considering regulatory and quality requirements at each step of the process, leading to US Food and Drug Administration (FDA) clearance for their products. Finally, they worked with the US Centers for Medicare and Medicaid Services to establish new billing codes with a view towards clinical guideline integration, including the use of an AI-based system for ischemic stroke detection at a reimbursement rate of $1040 per patient. This pathway to reimbursement and coverage is a complex and variable process that typically takes many years and may fail. Therefore, persistence, mentorship, and collaboration are critical to achieve success in translating novel technologies to routine clinical settings.

This company's path can guide investigators like Chiu and colleagues in bringing their AI innovations to market. More broadly, to date, the FDA has approved over 1000 AI medical devices. Therefore, there is a clear opportunity and precedent to obtain regulatory approval for and commercialize medical AI technologies.

---

### Road map for clinicians to develop and evaluate AI predictive models to inform clinical decision-making [^114CiZJG]. BMJ Health & Care Informatics (2023). High credibility.

Stage 7: licensing the AI predictive model

In the UK, AI-based tools are classified as medical devices and therefore need the Medicines and Healthcare products Regulatory Agency (MHRA) approval. Before Brexit, approved tools required either the 'United Kingdom Conformity Assessed' (UKCA) or 'Conformité Européenne' logo to be marketed in Europe. However, from July 2023, only tools with the UKCA logo will be allowed to be marketed in the UK. In Europe, AI-based software and tools are regulated by the EU Medical Device Regulation (EU MDR), whereas in the USA, AI-based tools are regulated by the Food and Drug Administration.

To licence an AI predictive tool in the UK, the MHRA must ensure that it complies with certain 'conformity assessment' standards, described by the National Institute for Health and Care Excellence (NICE) in 2018 and updated in 2021. It is worth mentioning that NICE framework is designed for AI tools with fixed algorithms (ie, no change over time) rather than AI tools with adaptive algorithms (ie, continually and automatically change); the latter are covered by separate standards (including principle 7 of the code of conduct for data-driven health and care technology). Higher-risk AI tools are classified as those that either target vulnerable patient populations, have serious consequences with errors or system failure, are used solely by patients without healthcare professionals' support or require a change in clinical workflow. For EU-approved tools, the tool should comply with the general safety and performance requirements stated by the EU MDR. Clinicians should be aware of the appropriate approvals that need to be obtained, especially with the growing adoption of these tools.

---

### Artificial intelligence in cardiovascular clinical trials [^113J8Rvt]. Journal of the American College of Cardiology (2024). Medium credibility.

The regulatory process for AI tools targeted to research rather than patient care, or a combination of the 2, is distinct from the 510(k), de novo, or premarket approval medical device review but follows similar principles. The FDA Drug Development Tool (DDT) and Medical Device Development Tool (MDDT) programs specifically assess AI tools for use in research rather than clinical care. The DDT program from the Center for Drug Evaluation and Research (CDER) and the Center for Biological Evaluation and Research (CBER) qualifies methods, materials, and measures that have the potential to facilitate drug development. Examples of non-AI DDTs include biomarkers for clinical trial enrichment, clinical outcome assessments to evaluate clinical benefit, and animal models. DDT qualification is not required for applying a tool in clinical trials. Nonetheless, it may streamline the review of subsequent regulatory submissions by avoiding the need for the FDA to reconsider and reconfirm the tool's validity in each drug program to which it is applied. DDTs are evaluated for a specific context of use, including recognition of a tool's limitations and the contexts in which its application is inappropriate. The Innovative Science and Technology Approaches for New Drugs (ISTAND) Pilot Program accepts submissions for DDTs focused on AI and digital health technology (DHT). The MDDT program at the Center for Devices and Radiological Health (CDRH) is similar to the DDT program but focuses on medical device research and regulation. These programs do not replace the traditional dialogue between investigators or sponsors and regulators to review clinical trial methodology in advance.

---

### Generalizability of FDA-approved AI-enabled medical devices for clinical use [^1128a4B3]. JAMA Network Open (2025). High credibility.

Unlike conventional software and devices, AI-enabled medical software and devices require continuous monitoring and evaluation, even when trained or adapted using datasets from the same center. This is due to AI's susceptibility to data shifts, which may arise from domain changes. These shifts can include population shifts (eg, changes in population characteristics within the same region over time) or acquisition shifts (eg, changes in acquisition instruments or updates to their versions). Comprehensive validation, ongoing monitoring following the approval for clinical use, and potentially the adaptation of AI models to ensure they remain effective and reliable across various settings and populations are critical to ensure the clinical generalizability of AI-enabled medical devices.

Our evaluation found that most AI-enabled medical devices with information available in the public domain and approved by the FDA demonstrated high discriminatory performance, attesting the potential effectiveness in diagnostic settings where the identification of true positives is crucial. However, balancing sensitivity and specificity in the clinical setting is vitally important for applications that prioritize minimizing either false positives or false negatives. Additionally, given the novelty of AI technologies in the medical field, it remains largely uncertain whether reliance solely on procedural regulations — rather than outcomes-focused regulations — during the development of AI-enabled medical devices can ensure meaningful clinical outcomes, as is the case with pharmaceutical products. The absence of outcome data poses a challenge in comparing different algorithms and complicates decision-making as to which systems to implement in clinical practice. Furthermore, due to the complexity and rapid advancements in AI, the proposed developmental procedures can quickly become outdated.

Recent guidelines aim to enhance reporting standards for the early-stage clinical evaluation of AI-driven decision-support systems. These guidelines cover a range of factors, including the intended use of the AI system, the patient population studied, and the influence of human factors on how the systems were used and performed. Notwithstanding, these guidelines provide only a limited framework for the external validation of AI-enabled tools and significant obstacles remain for their reliable deployment, especially since the development is frequently based on a relatively narrow range of patient demographics. Accordingly, the generalizability of data leading to approval of AI-enabled medical devices remains uncertain due to lack of information on how well these tools perform across diverse patient populations, clinical settings, and medical conditions.

---

### Machine learning in precision diabetes care and cardiovascular risk prediction [^114ipqBm]. Cardiovascular Diabetology (2023). Medium credibility.

Navigating the regulatory framework

While clinical decision support (CDS) tools used to assist with electronic patient records, administrative tasks, medical device data, and models aimed at supporting a healthy lifestyle fall outside the FDA's definition of a "device", most diagnostic and predictive clinical models and CDS are regulated under the " Software as Medicine Device (SaMD) " umbrella and categorized as class I (low-risk), II (moderate-risk), III (high-risk). In the US, the FDA mandates that class III devices generally undergo a longer and more in-depth review process, known as pre-market authorization. However, for lower or intermediate-risk devices (class I and II), alternative pathways exist. The 510(k) pathway requires manufacturers to show that the risk presented by their device is no greater than that of a substantially equivalent predicate device, whereas the de novo pathway is designed for class I or II devices without predicates. While built to accelerate the regulatory process, the latter two pathways have been criticized on certain occasions for facilitating the clearance of devices based on faulty predicates. The regulatory process is different in Europe, where lowest risk devices (class I) are the responsibility of the manufacturer, whereas class II and III devices are processed in a decentralized way through private "Notified Bodies". Of the 124 AI/ML-based devices approved in the USA and Europe between 2015 and 2020, 80 were first approved in Europe. The European Union's General Data Protection Regulation (GDPR) further lists explainability as a requisite for any medical AI application. Despite the above, there is no clear consensus as to whether regulatory bodies should require RCT-level evidence to support the effectiveness and safety of their proposed AI tools, even though recent studies have demonstrated the feasibility of testing the net clinical benefit of AI-based ECG and echocardiographic models in the context of pragmatic RCTs.

---

### Generalizability of FDA-approved AI-enabled medical devices for clinical use [^116ZQX8h]. JAMA Network Open (2025). High credibility.

Key Points

Question

To what extent are the key characteristics of medical devices approved by the US Food and Drug Administration (FDA) that use artificial intelligence (AI) ensuring clinical generalizability publicly known and reported?

Findings

This cross-sectional study of 903 devices found that at the time of regulatory approval, clinical performance studies were reported for approximately half of the analyzed devices, while one-quarter explicitly stated that no such studies had been conducted. Less than one-third of the clinical evaluations provided sex-specific data, and only one-fourth addressed age-related subgroups.

Meaning

These findings suggest that available information on the FDA website was frequently insufficient for a thorough assessment of the devices' clinical generalizability, highlighting the importance of ongoing monitoring and regular re-evaluation to identify and address unexpected changes in performance during broader clinical use.

---

### FDA launches agency-wide AI tool to optimize performance for… [^116dg7m4]. FDA (2025). Medium credibility.

- **For Immediate Release**: The U. S. Food and Drug Administration today launched Elsa, a generative Artificial Intelligence tool designed to help employees — from scientific reviewers to investigators — work more efficiently. This innovative tool modernizes agency functions and leverages AI capabilities to better serve the American people. "Following a very successful pilot program with FDA's scientific reviewers, I set an aggressive timeline to scale AI agency-wide by June 30, ". said FDA Commissioner Marty Makary, M. D. M. P. H. "Today's rollout of Elsa is ahead of schedule and under budget, thanks to the collaboration of our in-house experts across the centers. " Built within a high-security GovCloud environment, Elsa offers a secure platform for FDA employees to access internal documents while ensuring all information remains within the agency.

The models do not train on data submitted by regulated industry, safeguarding the sensitive research and data handled by FDA staff. "Today marks the dawn of the AI era at the FDA with the release of Elsa, AI is no longer a distant promise but a dynamic force enhancing and optimizing the performance and potential of every employee, " said FDA Chief AI Officer Jeremy Walsh. "As we learn how employees are using the tool, our development team will be able to add capabilities and grow with the needs of employees and the agency". The agency is already using Elsa to accelerate clinical protocol reviews, shorten the time needed for scientific evaluations, and identify high-priority inspection targets. Elsa is a large language model–powered AI tool designed to assist with reading, writing, and summarizing. It can summarize adverse events to support safety profile assessments, perform faster label comparisons, and generate code to help develop databases for nonclinical applications.

These are just a few examples of how Elsa will be used across the enterprise to improve operational efficiency. The introduction of Elsa is the initial step in the FDA's overall AI journey. As the tool matures, the agency has plans to integrate more AI in different processes, such as data processing and generative-AI functions to further support the FDA's mission. Prioritizing efficiency and responsibility, the FDA launched Elsa ahead of schedule using an all-center approach. Leaders and technologists across the agency collaborated, demonstrating the FDA's ability to transform its operations through AI. Media: HHS Request for Comment 202–690–6343 Consumer: 888-INFO-FDA.

---

### Evaluating consumer and clinical sleep technologies: an American Academy of Sleep Medicine update [^115D3kMB]. Journal of Clinical Sleep Medicine (2021). High credibility.

American Academy of Sleep Medicine update — evaluation considerations emphasize that sensors and AI/ML/DL have driven a progression from consumer to transitional and clinical grade technologies, that clarification on how to evaluate these rapidly evolving tools is needed, that traditional validation studies may take too long for real-time assessments, and that novel validation processes may help integration into clinical care; the document proposes items for clinicians to consider: awareness of FDA terms, defining sleep term definitions across devices/apps, and defining populations.

---

### Progress and challenges of artificial intelligence in lung cancer clinical translation [^112Q6BwF]. NPJ Precision Oncology (2025). Medium credibility.

Approved AI devices in lung cancer

Before AI algorithms can be implemented in clinical settings, official approval is required. The pace of AI development challenges the appropriate regulatory frameworks and requires more staffs to efficiently process submissions. This process involves more stringent clinical trials and validation testing than what is usually presented in academic publications. FDA categorizes these AI medical devices according to the level of potential risk posed to individual patients. Many AI devices in oncology fall under Class II (moderate risk) for which randomized controlled clinical trials are not typically required. To integrate AI product into widespread use, well-controlled clinical studies are necessary to show that the product's benefits outweigh its risks. Also, most AI products performed well in predefined tasks like detection but lack generalizability across different patient populations requires validation. Consequently, only a small proportion of AI algorithms are eligible to be deployed in clinical settings. Among the approved AI applications in lung cancer, they mainly focus on lung nodule detection, diagnosis, and radiotherapy planning, with all these algorithms being imaging-based. (Table 2) Multi-party collaboration is needed to optimize and adjust regulatory frameworks and processes, improve AI development, validation, and documentation standards, address challenges of advanced and evolving AI, and strengthen full lifecycle management and post-market surveillance.

Table 2
FDA approved AI devices in lung cancer

---

### Artificial intelligence and machine learning in cancer imaging [^114XcKCn]. Communications Medicine (2022). Medium credibility.

Future radiological workforce

Appropriate training is required to allow users to judge whether an AI tool is fit for purpose before adoption into clinical practice, which would require radiologists to understand the principles of AI and how AI algorithms should be properly validated.

There are pressures that are encouraging the premature introduction of AI tools into clinical workflows. Firstly, there is a workforce crisis with a shortage of radiologists in many countries. In the UK, about 10% of radiologist vacancies are unfilled. Secondly, there is a marked increase in global imaging demand and workload. In the UK, the CT and MRI workload has been rising by ~10% each year. Thirdly, there is a relentless drive to improve workflow efficiency, by improving image procedure turnaround time without compromising diagnostic accuracy. Finally, AI is seen as a tool to support repetitive tasks (e.g. sequential tumour size measurement, or cancer screening), that are time-consuming and relatively uninteresting for radiologists to undertake.

Empowering radiologists to judge the performance of AI algorithms would require changes in medical school and radiology curricula to include an understanding of the terms and main methodology of AI/ML; the requisite development, training, testing and validation of algorithms; basic statistics relevant to AI/ML; and the challenges of data requirements. Such empowerment will also necessitate educating radiologists in how they can meaningfully and rigorously test the performance of AI algorithms within their own clinical practice.

The future of AI and ML applications in radiology will be reliant upon the education of stakeholders including medical students, trainee radiologists, qualified radiologists, other doctors, radiographers, computer scientists, data scientists and data engineers collaboratively to solve clinically relevant problems. This multidisciplinary dialogue is necessary and critical to the development of clinically relevant and technically accomplished AI tools to address the unmet needs in oncology. There is a clear need for more multidisciplinary AI meetings and conferences to encourage interactions between all stakeholders, both at the local level, as well as at the national and international level.

Box 2 Important factors for the selection of AI into a health system

Criteria and benchmarks
CE labelling
FDA clearance
UKCA marking

Incentives and motivations
Targeting a common disease
Potential for the AI algorithm to be developed into products that generate revenue
Attracting better or new payors
Formulation of fair value proposition between stakeholders or partners
Latitude to create/share own business model
AI tool Infrastructure fits with existing informatic systems
The AI tool can be assimilated into the clinical workflow

---

### Regulatory evaluation of AI uses for medical practices… [^112VdGG9]. FDA (2024). Medium credibility.

The FDA's Center for Devices and Radiological Health has a clear regulatory approach for many types of artificial intelligence -enabled devices, but new clinical indications or new types of AI require novel assessment paradigms to determine safety and effectiveness. AI models intended for rule-out and triage have different practical applications and regulatory implications compared with models intended to help clinicians improve their diagnostic accuracy. Although most AI-enabled devices currently on the market are diagnostic, new devices that are designed for prognosis, treatment response prediction, risk assessment, therapy, improved image acquisition, and multi-class classification require different assessment metrics and reference standards. The use of natural language processing and large language models in the development or the operation of medical devices gives rise to new questions in device assessment.

New types of AI that combine multiple types of data sources demand research on questions about data harmonization and missingness. Projects
- Assessment of Video-based Detection AI for Endoscopy
- Validate a Computer Aided Triage Device Time-saving Evaluation Tool in a Clinical Workflow with Multiple CADt Devices and/or Disease Conditions
- Multi-omics Prediction of Metastatic Breast Cancer Progression and Drug Response
- Evaluating Large Language Models in the Generation of Radiology Reports. Resources
- **QuCAD**: Software to Evaluate Wait-Time-Saving Benefits of Computer-Aided Triage and Notification Devices Based on Queueing Theory | FDA Catalog of Regulatory Science Tools, 2023.
- Coroller et al. Methodology for Good Machine Learning with Multi-Omics Data. Clinical Pharmacology & Therapeutics, 2023.
- Thompson YLE, Levine G, Chen W, Sahiner B, Li Q, Petrick N, Samuelson F, "Wait-time-saving analysis and clinical effectiveness of computer-aided triage and notification devices based on queueing theory, ". Proceedings SPIE Medical Imaging; 120350Q. For more information, email OSEL_AI@fda. hhs. gov.

---

### A policy framework for leveraging generative AI to address enduring challenges in clinical trials [^116Voe4U]. NPJ Digital Medicine (2025). Medium credibility.

Developing and implementing application-specific language models

All three approaches — fine-tuning, RAG, and purpose-built small language models — require high-quality data to realize their potential. Much of this data is already publicly available, including regulatory documents, scientific literature, published trial protocols, and methodological guidance, including studies that assess hundreds of published trials for methodological errors. Such materials provide a strong foundation for developing initial ASLMs capable of improving trial design. While regulatory bodies like the FDA and EMA possess valuable additional datasets — including comprehensive records of investigational new drug applications, approvals, and critically, both successful and unsuccessful trial protocols — industry and academic institutions are often the primary drivers of innovation in trial design and could take a leading role in ASLM development. We envision a collaborative ecosystem where industry associations, academic centers, and regulatory bodies work in parallel to develop and implement ASLMs, each contributing their unique strengths: industry's practical experience and incentives to reduce costly protocol flaws, academia's methodological expertise, and regulators' expertise in oversight as well as comprehensive trove of relevant data sources.

This wealth of data can be leveraged to develop ASLMs capable of significantly improving clinical trial design in several key areas. First, many clinical trials suffer from preventable design flaws, resulting in wasted resources and delayed drug development. Regulators uniquely possess both unsuccessful applications and their subsequent successful versions, providing a rich dataset for improvement. Clinical trial ASLMs could be trained on this data to identify common pitfalls, suggest optimal trial designs for specific therapeutic areas, and predict the likelihood of regulatory approval for new protocols. This could dramatically improve the efficiency of the drug development process, reducing the time and cost required to bring new treatments to patients (Fig. 1).

Second, clinical trials have long struggled with underrepresentation of certain demographic groups, leading to questions about the generalizability of results. Regulatory bodies have access to demographic data from all trial applications, providing a comprehensive view of representation across different therapeutic areas and trial phases. Clinical trial ASLMs trained on such data could analyze proposed inclusion and exclusion criteria, flagging potential biases and suggesting modifications to enhance diversity based on successful precedents. This could lead to more inclusive trials that better reflect the populations that will ultimately use the treatments, improving both the ethical standing and scientific validity of clinical research.

---

### How AI is used in FDA-authorized medical devices: a taxonomy across 1, 016 authorizations [^117RYPoC]. NPJ Digital Medicine (2025). Medium credibility.

Along with a common lack of clarity regarding the form of AI/ML, it was unclear where AI/ML was used altogether in some authorization summaries. The public-facing authorization summary contains only a snapshot of the device compared to the full submission available to the FDA. As such, we used the FDA's published list of authorized AI/ML devices rather than creating a separate list curated from authorization summaries, which may omit devices that describe AI/ML in the full submission but not the summary. This ambiguity regarding AI/ML use was particularly common for devices that perform many functions, some of which may or may not involve AI/ML. In such cases, we estimated the taxonomy classes based on similar devices, domain expertise, and/or available information from manufacturer websites. As a result, our findings should be interpreted in terms of overall trends rather than precise class counts. The varying level of detail present in the authorization summaries also made it infeasible to classify the form of AI/ML used for each device, so our taxonomy focuses on the end use of the AI/ML algorithm regardless of its form. Our taxonomy also does not capture the full scope of the level of AI involvement for the intended clinical task, such as whether a quantification function is performed fully automatically or with potential clinician input/adjustment, as these nuances are also challenging to reliably determine from the summaries. Another limitation of our study is its reliance on manual curation (see Methods). While we explored various LLMs and prompting strategies for automated classification, these methods proved insufficient given the nuances and variability of the data. Future work could explore whether our curated database can now facilitate fine-tuning and/or in-context learning with LLMs to enable automated classification moving forward. Such approaches may also help overcome limitations in relying on the FDA's published list of AI/ML-enabled devices, which is updated irregularly (in April 2025, the latest authorization listed remains September 27, 2024). Specifically, by comparing authorization summaries of all FDA-authorized devices against those listed as AI/ML-enabled, LLM-assisted strategies combined with rule-based logic could help infer AI/ML usage even when it is not explicitly mentioned. Given that the FDA updates its general databases of authorization summaries weekly, such approaches could facilitate more continuous identification of AI/ML-enabled products and, in turn, updates to our taxonomy database.

---

### How AI is used in FDA-authorized medical devices: a taxonomy across 1, 016 authorizations [^112GDqcj]. NPJ Digital Medicine (2025). Medium credibility.

Beyond Analysis vs. Generation, we subclassified the AI function of each device into more fine-grained categories based on the device outputs and intended use (Fig. 2). For Analysis, we identified six subclasses across current devices: triage, quantification/feature localization, detection, diagnosis, detection/diagnosis, and predictive. Triage devices output a binary prediction for a given exam or time series that serves as a notification for prioritized review by a clinician. Devices classified as quantification/feature localization calculate a quantitative metric from the input data and/or provide positional information of structural features. This was considered a joint category because these functions are often performed in tandem, such as segmenting an anatomical structure on a medical image and then quantifying its volume. Detection devices differ from quantification/feature localization in that they are specifically designed to assist clinicians in finding disease-suspicious regions in the input data, such as highlighting lung nodules on CT images or identifying the time interval of a potentially abnormal heart rhythm in an ECG trace. Devices indicated for diagnosis do not explicitly identify suspicious regions but instead output a score or category across the input data that indicates the likelihood of a specific diagnosis. Some devices assist with both detection and diagnosis, and we retain this as a distinct subclass as it is often considered as such in FDA regulation. Finally, predictive devices were defined as those that generate a score or category to indicate the future risk of an event or disease rather than the current risk.

---

### The clinician-AI interface: intended use and explainability in FDA-cleared AI devices for medical image interpretation [^116HLesR]. NPJ Digital Medicine (2024). Medium credibility.

As applications of AI in medicine continue to expand, there is an increasing focus on integration into clinical practice. An underappreciated aspect of this clinical translation is where the AI fits into the clinical workflow, and in turn, the outputs generated by the AI to facilitate clinician interaction in this workflow. For instance, in the canonical use case of AI for medical image interpretation, the AI could prioritize cases before clinician review or even autonomously interpret the images without clinician review. A related aspect is explainability - does the AI generate outputs to help explain its predictions to clinicians? While many clinical AI workflows and explainability techniques have been proposed, a summative assessment of the current scope in clinical practice is lacking. Here, we evaluate the current state of FDA-cleared AI devices for medical image interpretation assistance in terms of intended clinical use, outputs generated, and types of explainability offered. We create a curated database focused on these aspects of the clinician-AI interface, where we find a high frequency of "triage" devices, notable variability in output characteristics across products, and often limited explainability of AI predictions. Altogether, we aim to increase transparency of the current landscape of the clinician-AI interface and highlight the need to rigorously assess which strategies ultimately lead to the best clinical outcomes.

---

### Systematic review of commercial artificial intelligence tools for the detection and volume quantification in intracerebral hemorrhage [^116RPUKK]. European Radiology (2025). Medium credibility.

Objectives

This systematic review evaluates commercial imaging-based artificial intelligence (AI) software for intracerebral hemorrhage (ICH) detection and quantification.

Materials and Methods

A two-step approach was employed. (1) A systematic review, following PRISMA 2020 guidelines, searched PubMed and the Cochrane Library for studies on commercial AI tools for ICH imaging published between 1996 and March 2025, summarizing study designs, detection performance, and volume quantification metrics. (2) A cross-referencing process identified additional publications by consulting FDA and EUDAMED databases, AIforRadiology.com, and company disclosures through direct contact. Identified software was further evaluated in PubMed and the Cochrane Library to identify associated studies. Companies were contacted to verify publication records, regulatory approvals, validation studies, and clinical utilization.

Results

From 2548 publications, 32 studies (2018–2023) met the inclusion criteria, covering 13 software solutions. Prospective designs were reported in 21.9%, with cohorts ranging from 102 to 58,321 scans. Detection performance demonstrated sensitivities of 68.2–99.7%, specificities of 83–97.7%, and accuracies of 85.3–99.16%. Volume quantification was assessed across seven tools, showing high correlations despite inconsistent metrics. Cross-referencing identified four additional tools lacking published studies. Among 19 tools identified, all were certified for ICH detection, 68.42% (13/19) for hematoma quantification-of these, 47.4% (9/19) had FDA certification only, two were pending approval, and one included hematoma expansion prediction. None disclosed internal validation studies.

Conclusion

Commercial AI tools for ICH focus on detection and triage. Volume quantification tools remain limited, with variable performance and regulatory approval. Standardized protocols and greater transparency in validation are needed to enable meaningful comparisons.

Key Points

Question Commercial AI tools for ICH detection and quantification lack standardized validation and comparative analysis, creating challenges for evaluation, comparison, and clinical integration. Findings Of 19 AI solutions identified, 13 had published studies. All supported ICH detection; six addressed volume quantification but varied in inconsistent designs and performance metrics. Clinical relevance Commercial AI tools for ICH are primarily validated for detection, while volume quantification remains less established. Variability in study designs and metrics limits comparability, underscoring the need for standardization to support clinical adoption.

---

### How AI is used in FDA-authorized medical devices: a taxonomy across 1, 016 authorizations [^113xapWm]. NPJ Digital Medicine (2025). Medium credibility.

Taxonomy assignments

Each device was classified according to four factors: data type, clinical function, AI function, and AI function subclass. The possible classes for each factor were defined through the review process to encompass major axes of variation across the devices, and were also informed by existing categories of medical devices, such as computer-aided detection ("CADe") and computer-aided diagnosis ("CADx"). Data type was assigned based on the core input to the AI algorithm in a mutually exclusive manner and could differ from the underlying data modality in some circumstances as our goal was to characterize how AI is actually being used. For instance, if the device involved a chemical assay but only involved AI to quantify the results based on a picture of the assay acquired using a smartphone camera, the data type would be classified as "Images". Clinical function was assigned based on the primary use case for the device according to the indications for use in the authorization summary. For imaging systems such as MRI machines that could theoretically be used for both diagnostic and interventional tasks, we considered the clinical use to be "Assessment" unless the AI component was specifically designed to assist in interventions. Analogously, we assigned the AI function based on the output of the AI model(s) rather than the overall output(s) of the device. If a device had several AI-enabled components, we assigned the AI function for the device as a union across the individual components (i.e. a device could perform both data generation and analysis using AI).

---

### A scoping review of reporting gaps in FDA-approved AI medical devices [^115c2DWG]. NPJ Digital Medicine (2024). Medium credibility.

Box 2 details some recommendations that may be adopted by the FDA and similar regulatory bodies internationally to reduce the risk of bias and health disparity in AI/ML.

While the FDA's Artificial Intelligence/Machine Learning (AI/ML) Action Plan outlines steps to advance the development and oversight of AI/ML-based medical devices, including initiatives to improve transparency, post-market surveillance, and real-world performance monitoring, our study highlights that there remain several clinically relevant inconsistencies in market approval data that may exacerbate algorithmic biases and health disparity.

Poor data transparency in the approval process limits some of the conclusions that can be reliably drawn in this study and limits quality assessment of post-market performance and real-world effectiveness. The paucity of sociodemographic data provided in the SSEDs begets the question of whether applicants were failing to track sociodemographics or if they were simply failing to report them. The FDA SSED templatedoes stipulate disclosure of risks and outcomes that may be impacted by sex, gender, age, race, and ethnicity. Thus, we can only justifiably assume that based on what is available and accessible, the paucity of the subgroup analysis data results from the failure of applicants to track the sociodemographics rather than an overall failure of the application process to capture the relevant information. However, it is clear that devices are approved despite not having all of this information available, making the case also for the potential failure of enforcement of these metrics before approval. Considering that most companies do not publish their post-market outcome data (only 1.9% have published data available) and are currently not mandated to do so, our findings are limited by what is accessible. This further re-emphasizes the argument for rigorous and more transparent regulation by bodies such as the FDA to protect end consumers and enhance post-market evaluation and the assessment of real-world effectiveness.

---

### The current status and future of FDA-approved artificial intelligence tools in chest radiology in the United States [^113usDmk]. Clinical Radiology (2023). Medium credibility.

Artificial intelligence (AI) is becoming more widespread within radiology. Capabilities that AI algorithms currently provide include detection, segmentation, classification, and quantification of pathological findings. Artificial intelligence software have created challenges for the traditional United States Food and Drug Administration (FDA) approval process for medical devices given their abilities to evolve over time with incremental data input. Currently, there are 190 FDA-approved radiology AI-based software devices, 42 of which pertain specifically to thoracic radiology. The majority of these algorithms are approved for the detection and/or analysis of pulmonary nodules, for monitoring placement of endotracheal tubes and indwelling catheters, for detection of emergent findings, and for assessment of pulmonary parenchyma; however, as technology evolves, there are many other potential applications that can be explored. For example, evaluation of non-idiopathic pulmonary fibrosis interstitial lung diseases, synthesis of imaging, clinical and/or laboratory data to yield comprehensive diagnoses, and survival or prognosis prediction of certain pathologies. With increasing physician and developer engagement, transparency and frequent communication between developers and regulatory agencies, such as the FDA, AI medical devices will be able to provide a critical supplement to patient management and ultimately enhance physicians' ability to improve patient care.

---

### Artificial intelligence tools in clinical neuroradiology: essential medico-legal aspects [^111MfZ2e]. Neuroradiology (2023). Medium credibility.

Two further considerations with implications for the use of AI-based CDS arise from the above-mentioned general principle for assessing medical malpractice liability. First, to be on the safe side, physicians should make sure to always follow the standard of care. In other words, AI currently functions more as a tool in clinical practice to confirm medical decisions rather than a tool that improves care by challenging the standard of care. Second, we must acknowledge that the medical standard of care is constantly changing and depends on the available resources. In consequence, it might even become imperative for certain indications to use AI-based CDS tools and adopt their output in the future if their accuracy outperforms "conventional" decision-making at a certain point.

When assessing the risk of medical malpractice liability, not only for AI-based CDS tools but for any software tool in general, it is also relevant to consider whether the tool itself was applied correctly — i.e. according to its intended use. Deviation from the intended use may likely be considered outside of the standard of care and thus increase the risk of liability for a physician. In cases where an AI-based software tool is a medical device (see below), the intended use has to be stated in the process leading to marketing authorization, such as FDA clearance/approval in the USA and "Conformité Européenne" (CE) marking in the European Union.

---

### Marketing and US food and drug administration clearance of artificial intelligence and machine learning enabled software in and as medical devices: a systematic review [^115dK1Y7]. JAMA Network Open (2023). High credibility.

Introduction

Use of artificial intelligence (AI) in health care is widespread. Natural language processingtechniques are used to understand clinical documentation and the implications of physician documentation. Convolutional neural networks help estimate molecule structure and determine effective pharmacological candidates for disease treatment. In the past decade, multiple machine learning (ML) techniques have been used to follow a similar pattern for image classification issues. In ML, scientists use supervised learning on existing labeled data to train a model and then evaluate its effectiveness in reader studies with human experts, with the eventual aim of these ML models being use as a diagnostic aid.

Medical devices must obtain US Food and Drug Administration (FDA) clearance or approval to be legally sold and used in the US. As devices are developed, approved, and marketed to consumers for use in health care settings, FDA approval provides reasonable assurance of legitimacy and safety to the consumer when considering using a device in such a sensitive field. Health care devices that use AI and ML must adhere to the same process, and once approved, the devices should be marketed accurately to inform consumers that their algorithms are safe and effective for public use.

---

### AI in radiology: legal responsibilities and the car paradox [^113ieNER]. European Journal of Radiology (2024). Medium credibility.

The integration of AI in radiology raises significant legal questions about responsibility for errors. Radiologists fear AI may introduce new legal challenges, despite its potential to enhance diagnostic accuracy. AI tools, even those approved by regulatory bodies like the FDA or CE, are not perfect, posing a risk of failure. The key issue is how AI is implemented: as a stand-alone diagnostic tool or as an aid to radiologists. The latter approach could reduce undesired side effects. However, it's unclear who should be held liable for AI failures, with potential candidates ranging from engineers and radiologists involved in AI development to companies and department heads who integrate these tools into clinical practice. The EU's AI Act, recognizing AI's risks, categorizes applications by risk level, with many radiology-related AI tools considered high risk. Legal precedents in autonomous vehicles offer some guidance on assigning responsibility. Yet, the existing legal challenges in radiology, such as diagnostic errors, persist. AI's potential to improve diagnostics raises questions about the legal implications of not using available AI tools. For instance, an AI tool improving the detection of pediatric fractures could reduce legal risks. This situation parallels innovations like car turn signals, where ignoring available safety enhancements could lead to legal problems. The debate underscores the need for further research and regulation to clarify AI's role in radiology, balancing innovation with legal and ethical considerations.