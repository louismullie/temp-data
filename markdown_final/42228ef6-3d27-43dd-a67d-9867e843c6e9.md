# Medical Question & Answer

**Sample ID**: 42228ef6-3d27-43dd-a67d-9867e843c6e9
**Dataset Index**: 840

---

## Question

For this part of your assignment, write a paragraph on how you will organize and interpret the data you collect. Describe the specific data analysis approach that you will use to address your clinical question. This part of your paper should be one paragraph long.

---

## Answer

> Let's see… What do we have here? The user is asking for a one-paragraph plan on how they will organize and interpret the data they collect and the specific data analysis approach they will use to address their clinical question. Let's break this down step-by-step. First, I need to think about aligning the analysis plan with the clinical question and estimand framework. Then, I should verify data preparation and quality assurance steps. Next, I will examine descriptive analytics and assumption checking. After that, I need to select and justify inferential methods, including handling missing data and confounding. Then, I will consider sensitivity analyses and subgroup assessment. Finally, I should integrate clinical significance, reporting standards, and a concise synthesis that ties back to the original question and estimand [^113qBHvf] [^1111Kpfs] [^116s7B7G].

> Let me first confirm the clinical question and estimand alignment, because if I mis-specify the treatment effect I intend to estimate, downstream analyses and interpretations will be off. I need to ensure the PICO elements are explicit and that the estimand captures intercurrent events relevant to the clinical question, such as rescue medication use or treatment discontinuation, so that the analysis answers what clinicians actually want to know rather than what is merely statistically convenient [^113qBHvf] [^1111Kpfs].

> Wait, let me verify the data preparation and quality assurance plan before any modeling, since garbage in, garbage out is especially consequential in clinical datasets. I should double-check data entry and coding procedures, including double data entry or electronic validation, range checks, and source document verification, and I must document anonymization and security plans to protect patient identifiers and comply with standards for data sharing and reproducibility [^114Nyegh] [^112g4HBs].

> I will now examine the dataset descriptively to understand the study population and identify anomalies. Hold on, I should verify that baseline characteristics are summarized with appropriate measures of central tendency and dispersion, and that categorical variables are presented with clear proportions; I need to check for outliers and assess distributions to anticipate test assumptions, and I should confirm that descriptive tables answer the "who, what, when, where, why, how, and how much" about the cohort [^114QsVXb] [^112wqdri] [^1141Ggw4].

> Next, I should review the analytic strategy and ensure it matches the study design and question. For a randomized trial, I need to confirm intention-to-treat as the primary analysis and pre-specify the statistical analysis plan to avoid post hoc bias; for observational data, I should verify that methods address confounding and selection bias, and I must align the analysis with the data structure and sampling design, such as matched case-control or cohort frameworks [^114pY3fa] [^116s7B7G] [^11538ARe].

> Let me think about missing data, because exclusions can bias results. I should confirm the extent and pattern of missingness, explore mechanisms, and prefer multiple imputation or other principled approaches over complete-case analysis when missingness is not missing completely at random; I need to report dropout rates, compare dropouts with completers, and discuss how missing data could influence conclusions [^113xZXWB] [^114WZTam].

> I need to ensure the choice of inferential statistics respects variable types and distributional assumptions. Hold on, I should verify normality with plots and tests when needed, consider transformations if assumptions are violated, and select parametric versus nonparametric tests accordingly; for time-to-event outcomes, I should confirm whether competing risks are present and, if so, use appropriate methods rather than naïve censoring that could mislead interpretation [^111CqqNc] [^112jSUC3] [^111ZWsAr].

> For effect estimation, I should double-check that point estimates are accompanied by confidence intervals and that I interpret results in light of clinically meaningful thresholds defined a priori. Let me reconsider whether the confidence interval includes the region of clinically important benefit and excludes clinically important harm; if so, and if a meaningful proportion of patients benefit, the intervention is likely clinically important in practice [^112QkvDM] [^114QsVXb].

> But wait, what if the intervention effect varies by subgroup; I should confirm that subgroup analyses are pre-specified and limited in number, and that I apply validity criteria such as consistency across studies, biological plausibility, and whether the subgroup hypothesis was one of a small set specified in advance with directionality, to avoid spurious claims driven by chance [^1117Y6en].

> I will now examine sensitivity analyses to test robustness, including per-protocol analyses where appropriate, alternative missing data strategies, and checks for influential observations; I should confirm that any deviations from the pre-specified plan are justified transparently and that sensitivity results are interpreted alongside the primary analysis to contextualize stability of findings [^116s7B7G] [^114WZTam].

> Next, I should review how to integrate and report results. I need to ensure adherence to reporting standards such as CONSORT for trials and PRISMA for systematic reviews, present study-level characteristics transparently, and avoid selective reporting; for meta-analyses, I should assess heterogeneity, consider fixed versus random effects models based on clinical and methodological similarity rather than heterogeneity tests alone, and grade certainty of evidence using GRADE to communicate trustworthiness [^114EJe7U] [^115JMXTS] [^114Dg6aM].

> Hold on, I should verify that clinical interpretation remains anchored to the original estimand and patient-centered outcomes. I need to confirm that conclusions reflect not only statistical significance but also clinical significance, generalizability to the target population, and the totality of evidence, and I should explicitly state limitations and how they constrain inference and applicability to practice [^112QkvDM] [^115uy5RM].

> Finally, let me synthesize succinctly to ensure internal consistency. The analysis will proceed from a pre-specified plan aligned with a clearly defined clinical question and estimand, through rigorous data cleaning and descriptive profiling, to intention-to-treat–based inference with appropriate handling of missing data and confounding, complemented by sensitivity and subgroup analyses that respect validity criteria, and culminating in transparent, standards-compliant reporting that distinguishes statistical from clinical significance and communicates the certainty and limits of evidence for decision-making [^1111Kpfs] [^116s7B7G] [^114EJe7U].

---

I will organize and interpret data using a **pre-specified statistical analysis plan** aligned with the clinical question and estimand framework [^116s7B7G] [^1111Kpfs], starting with descriptive statistics to characterize the sample and assess data quality [^114QsVXb] [^112wqdri]. For primary analyses, I will use **intention-to-treat** with appropriate regression models and report effect sizes with 95% confidence intervals [^114pY3fa] [^114QsVXb]; for secondary analyses, I will conduct subgroup and sensitivity analyses, and handle missing data with multiple imputation [^1117Y6en] [^114WZTam]. I will assess clinical significance using pre-defined minimal clinically important differences [^112QkvDM] and interpret results in context of prior evidence and patient-centered outcomes [^113fbJFU], ensuring transparent reporting and reproducibility [^112g4HBs].

---

## Data organization and preparation

- **Data cleaning and validation**: I will clean and validate data to ensure accuracy and completeness, checking for missing values, outliers, and inconsistencies [^114Nyegh] [^112wqdri].

- **Descriptive statistics**: I will generate descriptive statistics (means, medians, standard deviations, frequencies) to summarize patient characteristics, interventions, and outcomes, providing a clear overview of the dataset [^114QsVXb] [^1141Ggw4].

- **Data coding and categorization**: I will code categorical variables consistently and categorize continuous variables as appropriate for analysis, ensuring clarity and interpretability [^114Nyegh] [^112wqdri].

---

## Data analysis approach

### Primary analysis

- **Intention-to-treat (ITT) analysis**: I will use ITT analysis as the primary approach for randomized controlled trials, analyzing participants according to their assigned intervention group regardless of adherence, to minimize bias and preserve the benefits of randomization [^1111Kpfs].

- **Regression models**: I will employ appropriate regression models (e.g. linear regression for continuous outcomes, logistic regression for binary outcomes, Cox proportional hazards for time-to-event outcomes) to estimate intervention effects, adjusting for relevant covariates [^112UB4R7].

- **Effect sizes and confidence intervals**: I will report effect sizes with 95% confidence intervals to quantify the magnitude and precision of intervention effects, facilitating clinical interpretation [^114QsVXb].

---

### Secondary analyses

Subgroup analyses will examine **pre-specified subgroups** (e.g. age, sex, comorbidities) to identify potential heterogeneity of treatment effects, guided by clinical relevance and prior hypotheses [^1117Y6en]. Sensitivity analyses will assess robustness by testing different assumptions (e.g. handling of missing data, alternative statistical models) [^113xZXWB]. Missing data will be addressed using multiple imputation or other appropriate methods to reduce bias and improve validity [^114WZTam].

---

## Clinical interpretation

- **Clinical significance**: I will interpret results in terms of clinical significance, considering minimal clinically important differences (MCIDs) and patient-centered outcomes [^112QkvDM].

- **Contextualization**: I will contextualize findings within existing literature and clinical guidelines, assessing generalizability and applicability to clinical practice [^115uy5RM].

- **Limitations and bias**: I will explicitly discuss limitations, potential biases, and uncertainties, ensuring transparent and balanced interpretation [^112QkvDM].

---

## Reporting and dissemination

I will adhere to established reporting guidelines (**CONSORT** for randomized trials, **PRISMA** for systematic reviews) to ensure transparency and completeness [^114EJe7U]. I will provide clear, concise visualizations (tables, figures, forest plots) to enhance understanding and facilitate interpretation [^1152FgyH]. I will ensure reproducibility by documenting all analytical steps, code, and data management procedures [^111nVfHa].

---

## Data analysis approach

| **Step** | **Description** |
|-|-|
| Data organization | - Cleaning <br/> - Validation <br/> - Descriptive statistics <br/> - Coding [^114Nyegh] [^114QsVXb] |
| Primary analysis | - ITT <br/> - Regression models <br/> - Effect sizes <br/> - Confidence intervals [^114pY3fa] [^114QsVXb] |
| Secondary analyses | - Subgroup <br/> - Sensitivity <br/> - Missing data handling [^1117Y6en] [^113xZXWB] |
| Clinical interpretation | - Clinical significance <br/> - Contextualization <br/> - Limitations [^112QkvDM] [^115uy5RM] |
| Reporting | - Guidelines adherence <br/> - Visualizations <br/> - Reproducibility [^114EJe7U] [^1152FgyH] |

---

I will organize and interpret data using a structured, pre-specified approach that integrates rigorous statistical analysis with clinical relevance, ensuring accurate, transparent, and actionable findings [^116s7B7G].

---

## References

### An introduction to clinical trial design [^116fCrN4]. Paediatric Respiratory Reviews (2019). Medium credibility.

Clinicians and other decision makers in healthcare use results from clinical trials to inform practice. Interpretation of clinical trial results can be challenging, as weaknesses in trial design, data collection, analysis or reporting, can compromise the usefulness of results. A good working knowledge of clinical trial design is essential to expertly interpret and determine the validity and generalizability of the results. This manuscript will give a brief overview of clinical trial design including the strengths and limitations of various approaches. The focus will be on confirmatory clinical trials.

---

### Systematic review and meta-analysis: pearls for interpretation, appraisal, and application in clinical practice [^115fAm5w]. The Journal of Allergy and Clinical Immunology: In Practice (2021). Medium credibility.

Systematic reviews (SR) are a category of literature review that presents a comprehensive synthesis and analysis of all available literature evidence addressing a specific clinical question. Meta-analysis (MA) is a quantitative technique that is applied to data collected through SR that provides an estimate of an effect across a larger population. By synthesizing data from a large number of sources SR and MA often provide insights that cannot be obtained from single studies and can aid in clinical decision-making. However, these techniques are subject to important limitations. Both the validity and the usefulness of the results of an SR/MA depend on the methodological rigor used in preparing the review and the quality of included studies. In applying SR/MA to clinical decision-making the reader should be able to assess these features. Here we present an overview of important concepts in understanding SR and MA. We provide a general approach to interpretation and evaluation of an SR/MA, model the use of a critical appraisal tool for SR, and discuss the applications of SR and MA to clinical practice.

---

### Preferred reporting items for systematic review and meta-analyses of individual participant data: the PRISMA-IPD statement [^116tiPLG]. JAMA (2015). Excellent credibility.

Importance

Systematic reviews and meta-analyses of individual participant data (IPD) aim to collect, check, and reanalyze individual-level data from all studies addressing a particular research question and are therefore considered a gold standard approach to evidence synthesis. They are likely to be used with increasing frequency as current initiatives to share clinical trial data gain momentum and may be particularly important in reviewing controversial therapeutic areas.

Objective

To develop PRISMA-IPD as a stand-alone extension to the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) Statement, tailored to the specific requirements of reporting systematic reviews and meta-analyses of IPD. Although developed primarily for reviews of randomized trials, many items will apply in other contexts, including reviews of diagnosis and prognosis.

Design

Development of PRISMA-IPD followed the EQUATOR Network framework guidance and used the existing standard PRISMA Statement as a starting point to draft additional relevant material. A web-based survey informed discussion at an international workshop that included researchers, clinicians, methodologists experienced in conducting systematic reviews and meta-analyses of IPD, and journal editors. The statement was drafted and iterative refinements were made by the project, advisory, and development groups. The PRISMA-IPD Development Group reached agreement on the PRISMA-IPD checklist and flow diagram by consensus.

Findings

Compared with standard PRISMA, the PRISMA-IPD checklist includes 3 new items that address (1) methods of checking the integrity of the IPD (such as pattern of randomization, data consistency, baseline imbalance, and missing data), (2) reporting any important issues that emerge, and (3) exploring variation (such as whether certain types of individual benefit more from the intervention than others). A further additional item was created by reorganization of standard PRISMA items relating to interpreting results. Wording was modified in 23 items to reflect the IPD approach.

Conclusions and Relevance

PRISMA-IPD provides guidelines for reporting systematic reviews and meta-analyses of IPD.

---

### Biostatistics: a toolkit for exploration, validation, and interpretation of clinical data [^111FvKDP]. Journal of Thoracic Oncology (2009). Low credibility.

Biostatistics plays a key role in all phases of clinical research starting from the design to the monitoring, data collection, data analysis, and interpretation of the results. A clear understanding of the statistical framework as it relates to the study hypothesis, reported results, and interpretation is vital for the scientific integrity of the study and its acceptance in the general medical community. In this brief report, we will put in perspective the general analytical framework for exploring and validating prognostic factors using data from large databases.

---

### Assessment, measurement and data collection tools [^112aCarb]. Clinical Rehabilitation (2004). Low credibility.

The terms 'measurement' and 'assessment' are often used interchangeably, especially when referring to the tools used to collect information. This leads to unclear thinking, and often to poor selection of a 'measure' or 'assessment'. This editorial suggests that we should distinguish between the identification of the data needed for some purpose, the methods used to collect data, and the use made of (interpretation of) the data collected. This would focus more attention on the three most important questions to consider when collecting data, whether in day-to-day clinical practice or in research: Why should the data be collected? How should the data be collected? and How should the results be interpreted?

---

### Acquisition, analysis, and sharing of data in 2015 and beyond: a survey of the landscape: a conference report from the American Heart Association data summit 2015 [^111bUaw7]. Journal of the American Heart Association (2015). Low credibility.

Improving the Design and Efficiency of Clinical Trials

The clinical trials enterprise is vast and expensive. For a variety of historical reasons, despite the critical societal importance of clinical trials, most clinical trial data have not been shared broadly. The reasons include concern about maintaining the confidentiality and privacy of trial participants, protection of the intellectual property of companies and inventors, and academic control of ideas and results. As transparency efforts gain traction, it is timely to consider the potential benefits of sharing trial data to improve the clinical trials enterprise itself.

Clinical trials can be fundamentally divided into (1) mechanistic trials, which are intended to explore and understand the mechanisms governing human biology, and (2) pragmatic trials, intended to inform healthcare decision makers. 74 Because of the complexity of the types of questions in each of these categories, there is no single best approach to clinical trials. Instead, the principles of quality lead to efforts to optimize the value and efficiency of clinical trials by designing and operationalizing the trial based on the purpose of the trial.

ClincialTrials.gov has provided a new ability to examine the clinical trials enterprise because the vast majority of trials have a legal requirement for registration prior to enrollment and results reporting within 1 year of ascertainment of the last primary end point. 50 Currently, > 380 trials per week are registered, and initial analysis of the accumulating data reveals a large number of trials with major deficiencies that would make it unlikely that they are answering an important question. We now have the opportunity to treat the clinical trials enterprise as a learning ecosystem in which all previous trials can inform the planning and operationalization of the next trial. 75

Several broad questions could be addressed:
What types of trial designs are best for answering different types of questions?
What are the characteristics of trials that answer the question asked by the trial or that fail to answer the question either because of design or conduct specifications?
Over time, what are the characteristics of trials that answer key clinical questions?

More specific questions could also be addressed:
What entry criteria for particular diseases or issues lead to the best recruitment?
What biomarkers and putative surrogates actually work in the clinical trials setting?
What are the clinical outcomes and natural history of particular disease states, and what event rates can be expected for given entry criteria?
What operational approaches lead to the most efficient trial conduct?
What types of data collection and which data items are most useful for different types of trials in different circumstances?
What analytical methods lead to conclusions that are reliable over time and across different types of trials?

---

### Acquisition, analysis, and sharing of data in 2015 and beyond: a survey of the landscape: a conference report from the American Heart Association data summit 2015 [^112PsT5y]. Journal of the American Heart Association (2015). Low credibility.

What Do Clinical Investigators Want From Big Data?

Clinical investigators who are interested in evaluating the efficacy or safety of specific interventions in small‐ or large‐scale trials generate substantial data that might benefit from merging with each other or with other data sets. 17, 102 Using our current approach, however, we create the data platforms for characterizing patients and events de novo every time we design and carry out a clinical trial. Although informed by past experience, each clinical trialist develops a unique protocol and a unique case report form, and trialists are likely to use meaningfully different approaches to identifying patient phenotypes or clinical events. It has been difficult to develop standardized approaches to describing the symptoms, signs, severity, and other relevant features of common cardiac disorders. 36 Consequently, assuming that appropriate statistical and informatics methods were available to combine data sets, it is not clear whether the application of such methods to existing data sets would yield a useful synthesis. Efforts by the US Food and Drug Administration to standardize the definition and reporting of specific cardiac events (eg, myocardial infarction, stroke, or hospitalization for heart failure) represent an important advance toward this goal, but much work remains to be done. These efforts will be complicated by a simultaneous interest in streamlining the amount of information collected in clinical trials, based on the assumption that we already know what data to collect and how to collect them. Assuming that the data collected could be of higher quality and the merger of data sets could be made interoperable, clinical investigators not only would want to contribute to the creation of such data sets but also would be interested in posing questions that could be answered by such data sets. Nevertheless, it is currently very difficult to interact with the platforms available for merged data sets in a manner that is likely to reward clinical investigators for their time and effort. Regardless of the approach developed, clinical investigators wish strongly to be involved in the ongoing discussions with other stakeholders and trust that the financial resources will be available to enable the development and mining of large data sets to address important clinical questions.

---

### Systematic reviews and meta-analysis of published studies: an overview and best practices [^1136qFp5]. Journal of Thoracic Oncology (2011). Low credibility.

Systematic reviews and meta-analytic approaches are widely used in the clinical arena to integrate outcome data from published studies in a patient population that address a set of related research hypotheses. The credibility of this line of research is dependent on how the studies are chosen, how the data are assembled, and how the results are reported. In this brief report, we provide an overview of the minimum set of reporting requirements for systematic reviews and meta-analyses based on the Preferred Reporting Items of Systematic reviews and Meta-Analyses guidelines. As with any research, following a set of established guidelines is essential for quality and consistency of the findings across studies and for assessment of clinical utility.

---

### A review for clinical outcomes research: hypothesis generation, data strategy, and hypothesis-driven statistical analysis [^115yvHY4]. Surgical Endoscopy (2011). Low credibility.

Study design phase

The most important, and arguably the most difficult, phase of a study is its design phase. In fact, most problems with research studies arise in the very first step in this phase — asking the research question. An improperly framed research question will create difficult problems throughout the following steps of the project. Note that both the design phase and the data preparation phase will comprise the Methods section of a manuscript.

An important issue in framing a research question is: Will it be a descriptive study or an analytical study? A descriptive study is often employed when the research question involves a rare or new occurrence, disease, or procedure, since there is little established knowledge about the topic. The hallmark of a descriptive study is questions that begin with "what", "where", "when", "who", or "how". For example, "Who has the disease in question?" or "What are the common comorbidities of patients with the disease in question?" These are also known as "open-ended" questions, and statistical testing is not applicable since there is no a priori expectation of any particular answer. If the study is descriptive, a statistician will not be necessary.

In contrast, an analytical study requires "close-ended" questions, usually beginning with a verb: "Is/Was… " or "Do/Does… " For example, "Does race or gender affect the mortality of patients with the disease in question?" These studies call for a yes/no answer and statistical testing is applicable.

The difference between open-ended questions for an inquiry in its earliest stage versus closed-ended questions in later stages can be compared to gathering history and physical examination information from a patient. A patient interview begins with open-ended questions ("Tell me about your pain"), but then moves on to closed-ended questions as the matrix of information begins to create a picture of the clinical situation. As this process evolves, the clinician begins to formulate a differential diagnosis list (e.g. "Was it dull? Did you have a fever? Was it in the left lower quadrant?").

---

### Secondary data analysis: techniques for comparing interventions and their limitations [^113fAPLG]. Current Opinion in Urology (2017). Low credibility.

Purpose Of Review

Secondary data analysis has become increasingly common in health services research, specifically comparative effectiveness research. While a comprehensive study of the techniques and methods for secondary data analysis is a wide-ranging topic, we sought to perform a descriptive study of some key methodological issues related to secondary data analyses and to provide a basic summary of techniques to address them.

Recent Findings

In this study, we first address common issues seen in analysis of secondary datasets, and limitations of datasets with respect to bias. We cover some strategies for handling missing or incomplete data and a basic summary of three statistical approaches that can be used to address the problem of bias.

Summary

While it is unrealistic for surgeon scientists to aspire to the depth of knowledge of professional statisticians or data scientists, it is important for researchers and clinicians reading to understand some of the common pitfalls and issues when using secondary data to investigate clinical questions. Ultimately, the choice of analytical technique and the particular data sets used should be dictated by the research question and hypothesis being tested. Transparency about data handling and statistical techniques are vital elements of secondary data analysis.

---

### PROMISE: a real-world clinical-genomic database to address knowledge gaps in prostate cancer [^1146spKo]. Prostate Cancer and Prostatic Diseases (2022). Medium credibility.

Conclusions

Precision medicine holds significant promise for helping to individualize treatment approaches for patients with advanced prostate cancer and to help understand the underlying mechanisms of this heterogeneous disease. The PROMISE consortium is an important new collaboration among leading academic centers in the United States that aims to bridge the gap between clinical and genomic information for patients with metastatic prostate cancer and available genomic data from commercial and institutional assays. The well-annotated, de-identified patient data collected for this consortium will be leveraged to help answer specific questions and guide projects proposed by the consortium members. Genomic data entered into the database will be vetted by a bioinformatician and individual projects approved by the executive committee as part of the consortium structure. This organized approach will help to address important questions at the intersection of clinical and genomic data that are most optimally addressed using a real-world dataset. It is our hope that this approach will eventually help to improve outcomes for patients with advanced prostate cancer.

---

### Missing data: a special challenge in aging research [^113xZXWB]. Journal of the American Geriatrics Society (2009). Low credibility.

Scientific evidence should guide clinical care, but special methodological challenges influence interpretation of the medical literature pertaining to older adults. Missing data, ranging from lack of individual items in questionnaires to complete loss to follow-up, affect the quality of the evidence and are more likely to occur in studies of older adults because older adults have more health and functional problems that interfere with all aspects of data collection than do younger people. The purpose of this article is to promote knowledge about the risks and consequences of missing data in clinical aging research and to provide an organized approach to prevention and management. Although it is almost never possible to achieve complete data capture, efforts to prevent missing data are more effective than analytical "cure". Strategies to prevent missing data include selecting a primary outcome that is easy to determine and devising valid alternate definitions, adapting data collection to the special needs of the target population, pilot testing data collection plans, and monitoring missing data rates during the study and adapting data collection procedures as needed. Key steps in the analysis of missing data include assessing the extent and types of missing data before analysis, exploring potential mechanisms that contributed to the missing data, and using multiple analytical approaches to assess the effect of missing data on the results. Manuscripts should disclose rates of missing data and losses to follow-up, compare dropouts with participants who completed the study, describe how missing data were managed in the analysis phase, and discuss the potential effect of missing data on the conclusions of the study.

---

### Principles of use of biostatistics in research [^112jSUC3]. NeoReviews (2014). Low credibility.

Collecting, analyzing, and interpreting data are essential components of biomedical research and require biostatistics. Doing various statistical tests has been made easy by sophisticated computer software. It is important for the investigator and the interpreting clinician to understand the basics of biostatistics for two reasons. The first is to choose the right statistical test for the computer to perform based on the nature of data derived from one's own research. The second is to understand if an analysis was performed appropriately during review and interpretation of others' research. This article reviews the choice of an appropriate parametric or nonparametric statistical test based on type of variable and distribution of data. Evaluation of diagnostic tests is covered with illustrations and tables.

---

### Trial analysis and interpretation in critical care using the evidential (likelihood) approach: rationale and practical considerations [^1134GnZc]. American Journal of Respiratory and Critical Care Medicine (2025). Medium credibility.

Why We Like the Likelihood Analysis for Clinical Research

What we find attractive about likelihood analysis is that, when we have built the likelihood function, we can ask any question about the relative support for different effect sizes without penalty or consideration of priors. By "without penalty", we mean that LRs and S values retain their interpretation regardless of how many comparisons are made or how many times the data are examined, unlike P values, which require adjustment for multiple testing. However, it is essential to note that, even when likelihood-based results are used with prespecified decision thresholds (e.g. those in formal stopping rules), the risk of erroneous conclusions remains, as it does in any statistical framework. The distinction is that the evidential approach offers flexibility in assessing exploratory evidence without altering the mathematical interpretation of the ratios themselves. For instance, we might compare the likelihood of a clinically meaningful benefit versus harm. In terms of traditional diagnostic test metrics, a positive LR equals sensitivity/(1 − specificity), whereas a negative LR equals (1 − sensitivity)/specificity. This interpretive framework naturally extends to the evaluation of comparative hypotheses after data collection, providing an evidential summary that quantifies the relative compatibility of observed results with the effect sizes of interest. Thus, just as a positive test result might be "10 times more likely in patients with the disease than without", evidential analysis allows us to assert that a treatment effect of "X is 10 times more likely than Y" based on our data.

---

### How to interpret and use a clinical practice guideline or recommendation: users' guides to the medical literature [^115uy5RM]. JAMA (2021). Excellent credibility.

Importance

Clinicians may rely on recommendations from clinical practice guidelines for management of patients.

Observations

A clinical practice guideline is a published statement that includes recommendations that are intended to optimize patient care. In the guideline development process, a panel of experts formulates recommendation questions that guide the retrieval of evidence that is used to inform the recommendations. Typically, methods of guideline development, a summary of the supporting evidence, and a justification of the panel's decisions accompany the recommendations. To use such guidelines optimally, clinicians must understand the implications of the recommendations, assess the trustworthiness of the development process, and evaluate the extent to which the recommendations are applicable to patients in their practice settings. Helpful recommendations are clear and actionable, and explicitly specify whether they are strong or weak, are appropriate for all patients, or depend on individual patients' circumstances and values. Rigorous guidelines and recommendations are informed by appropriately conducted, up-to-date systematic reviews that consider outcomes important to patients. Because judgments are involved in the interpretation of the evidence and the process of moving from evidence to recommendations, useful guidelines consider all relevant factors that have a bearing in a clinical decision and are not influenced by conflicts of interest.

Conclusions and Relevance

In considering a guideline's recommendations, clinicians must decide whether there are important differences between the factors the guideline panel has considered in making recommendations and their own practice setting.

---

### Clinical practice guidelines for hemodialysis adequacy, update 2006 [^112UB4R7]. American Journal of Kidney Diseases (2006). Medium credibility.

Hemodialysis adequacy — data collection and analytic standards state that this will require a methodologically sound foundation with standards that are generalizable, and that future data collection will require assessments using prespecified approaches to data analysis that include confounders (demographics, case-mix, medical therapeutics) in a clinically valid multivariable statistical model; otherwise the ability to ascertain the evidence of contributions of best practices versus guideline achievement becomes a statistical/ logistical impossibility. Because such a consideration is an intense undertaking, it should not be initiated without total commitment to the resources needed to address each of these issues and create the valid models needed to monitor improved care in a meaningful way, and if this is not done, interpretation of partially collected or invalid data would be unable to determine the root cause of changes in clinical outcomes, not be valid across and/or within facilities, and add limited value above the present outcome analyses.

---

### How to perform a systematic review and meta-analysis of diagnostic imaging studies [^116efTrr]. Academic Radiology (2018). Low credibility.

A systematic review is a comprehensive search, critical evaluation, and synthesis of all the relevant studies on a specific (clinical) topic that can be applied to the evaluation of diagnostic and screening imaging studies. It can be a qualitative or a quantitative (meta-analysis) review of available literature. A meta-analysis uses statistical methods to combine and summarize the results of several studies. In this review, a 12-step approach to performing a systematic review (and meta-analysis) is outlined under the four domains: (1) Problem Formulation and Data Acquisition, (2) Quality Appraisal of Eligible Studies, (3) Statistical Analysis of Quantitative Data, and (4) Clinical Interpretation of the Evidence. This review is specifically geared toward the performance of a systematic review and meta-analysis of diagnostic test accuracy (imaging) studies.

---

### Clinical evidence 2030 [^116JaGcd]. Clinical Pharmacology and Therapeutics (2025). Medium credibility.

PRINCIPLE 2: EXISTING DATA AND KNOWLEDGE ARE LEVERAGED TO INFORM THE IDENTIFICATION OF GAPS, GENERATION OF CLINICAL EVIDENCE, AND HEALTHCARE DECISIONS

Clinical evidence generation is planned and guided by purpose, data, knowledge, and expertise. When formulating research questions and designing clinical evidence programs, existing data, information, and knowledge should be leveraged. Currently, this is not always the case, and clinical studies may be planned ignorant of previous study results or learnings from other medicinal products. To enable this informed approach to clinical research, access to data, information and knowledge, including study protocols and results, reports on suspected adverse reactions and the outcome of regulatory assessments should be made publicly available and scrutinized when designing studies. Multi‐stakeholder dialogue at the planning stage will also facilitate access to existing knowledge. In this way, past successes and failures inform identification of gaps and further clinical evidence generation and may avoid unnecessary duplication.

---

### Challenges in applying the results of clinical trials to clinical practice [^112QkvDM]. JAMA Ophthalmology (2016). Medium credibility.

The relevance of clinical trial results to clinical practice hinges on 2 critical questions: Will the results be replicated in one's practice, and Are the results clinically important? The answers to the following 5 questions may help one determine how relevant a study result is to clinical practice. First, have steps been taken to minimize bias (eg, masking, randomization)? Second, is the result likely due to the treatment (vs confounding factors)? Third, is the result unlikely to be due to chance? Fourth, is the study population representative of your patients? Fifth, is the totality of the evidence consistent across studies? To determine if a study result is likely to be clinically important, consider a 3-step approach. In step 1, decide, a priori, what a clinically meaningful difference between 2 treatments would be to define regions of beneficial, harmful, and trivial outcomes. In step 2, determine whether the CIs around the average outcome include the range of beneficial outcomes and lie outside the range of harmful outcomes. In step 3, determine the proportion of patients achieving a clinically meaningful benefit. If the CIs mostly include the range of beneficial outcomes and lie outside the range of clinically harmful outcomes and if a substantial proportion of patients achieve a clinically meaningful benefit, then the intervention is probably clinically important. Application of clinical trial results to clinical practice requires critical analysis of the extant literature as well as good clinical judgment.

---

### A review for clinical outcomes research: hypothesis generation, data strategy, and hypothesis-driven statistical analysis [^1141Ggw4]. Surgical Endoscopy (2011). Low credibility.

Analysis phase

Following the process described below will produce the Results section of the manuscript.

Step 1: univariate descriptive analysis

The univariate descriptive analysis describes the entire study population. It is called "univariate" analysis because the population is described one characteristic at a time: average age, proportion males, race, socioeconomic status, insurance status, location, and so on. This is important so that future readers can determine whether the study applies to their patients. Since this section is solely descriptive, no formal statistical testing is necessary or applicable. An example data table for a univariate analysis is presented in Table 1. A study that is only descriptive would likely end after this stage. Analytical studies will continue on through the next few steps.

Table 1
Example of a univariate/demographics table

IQR interquartile range

---

### Recommendations for reporting machine learning analyses in clinical research [^113eKtUo]. Circulation: Cardiovascular Quality and Outcomes (2020). Medium credibility.

Use of machine learning (ML) in clinical research is growing steadily given the increasing availability of complex clinical data sets. ML presents important advantages in terms of predictive performance and identifying undiscovered subpopulations of patients with specific physiology and prognoses. Despite this popularity, many clinicians and researchers are not yet familiar with evaluating and interpreting ML analyses. Consequently, readers and peer-reviewers alike may either overestimate or underestimate the validity and credibility of an ML-based model. Conversely, ML experts without clinical experience may present details of the analysis that are too granular for a clinical readership to assess. Overwhelming evidence has shown poor reproducibility and reporting of ML models in clinical research suggesting the need for ML analyses to be presented in a clear, concise, and comprehensible manner to facilitate understanding and critical evaluation. We present a recommendation for transparent and structured reporting of ML analysis results specifically directed at clinical researchers. Furthermore, we provide a list of key reporting elements with examples that can be used as a template when preparing and submitting ML-based manuscripts for the same audience.

---

### Acquisition, analysis, and sharing of data in 2015 and beyond: a survey of the landscape: a conference report from the American Heart Association data summit 2015 [^111nVfHa]. Journal of the American Heart Association (2015). Low credibility.

The IOM Report on Responsible Data Sharing contains 4 recommendations. 44 First, the stakeholders in a clinical trial should foster a culture in which data sharing is the expected norm and should be committed to a responsible strategy for this process.

Second, sponsors and investigators should share the various types of clinical trial data no later than the following timelines: Before the trial is initiated, the trial should be registered with ClinicalTrials.gov, or the equivalent in other countries, and should include the protocol, the data sharing plan, and the statistical analysis plan. Within 12 months of study completion in participant follow‐up, (eg, last patient, last visit), a summary of results should be provided in ClinicalTrials.gov and in a lay or public‐level presentation. Within 6 months of publication in a medical or scientific journal, the patient‐level analyzed and deidentified data used in the paper should be made available along with the protocol, the statistical analysis plan, and the analytic code used in the analysis. Continuing on this timeline, within 18 months after trial completion (last patient, last visit), the full analyzable deidentified data set should be made available along with the protocol, the metadata for the full data set, the protocol, the statistical analysis plan, and all analytic code used in publications or summary reports. For trials that are submitted to regulatory agencies for product approval, the full analyzable deidentified data set, protocol, statistical analysis plan, and redacted complete summary report should be made available.

Third, holders of clinical trial data should use data use agreements, designate an independent review panel, and make access to clinical trial data transparent.

Fourth, sponsors and investigators must address the following specific issues:
Infrastructure: There are currently insufficient platforms to store and manage data.
Technological: The current platforms are not consistently discoverable, searchable, and interoperable.
Workforce: Our current clinical trial research staff lacks skills and knowledge to manage operational and technical aspects of data sharing.
Sustainability: The current model costs are borne by a small subset of sponsors, funders, and trialists and are unsustainable.

The IOM report includes the following key summary messages:
Responsible clinical trial data sharing should become the norm.
Stakeholders and institutions need to work together to agree on best practices, standards, and incentives.
Evolution should be guided by empirical data, lessons learned, and best practices.

---

### Getting started in research, redefined: five questions for clinically focused physicians in family medicine [^112umwqr]. Family Medicine and Community Health (2019). Medium credibility.

A question-oriented approach

Typical research generally follows a scientific process of posing questions, collecting and analysing data, and reporting results. Such research characteristically emphasises specific methodologies (appropriately matched to the questions posed), systems-level outcomes and processes, and practice-based network behaviours. It focuses on generalisability and group outcomes as final outcomes. However, attending to this process and addressing these concerns may feel overly daunting to individual family physicians in clinical practice.

Due to their high volume of patient care duties, their daily concentration on presenting concerns and their embeddedness in community settings, these same clinically focused family physicians are well suited to generating insight and innovative solutions to important clinical concerns emerging from practice. Hence, we suggest the adaptation of an alternative approach for family medicine investigators who are focused on issues of clinical immediacy.

---

### Tutorials in clinical research: part III. selecting a research approach to best answer a clinical question [^116CtMRh]. The Laryngoscope (2001). Low credibility.

Objective

This is the third in a series of sequential "Tutorials in Clinical Research". The objectives of this specific report are to enable the reader to rapidly dissect a clinical question or article to efficiently determine what critical mass of information is required to answer the question and what study design is likely to produce the answer.

Study Design

Tutorial.

Methods

The authors met weekly for 3 months exploring clinical problems and systematically recording the logic and procedural pathways from multiple clinical questions to the selection of proper research approaches. The basic elements required to understand the processes of selection were catalogued and field tested, and a report was produced to define and explain these elements.

Results

Fundamental to a research approach is the assembly of subjects and the allocation of exposures. An algorithm leading to the selection of an approach is presented. The report is organized into three parts. The tables serve as a rapid reference section. The initial two-part narrative explains the process of approach selection. The examples section illustrates the application of the selection algorithm.

Conclusions

Selecting the proper research approach has six steps: the question, logic and ethics, identification of variables, data display considerations, original data source considerations, and selection of prototypical approaches for assembly of subjects. Field tests of this approach consistently demonstrated its utility.

---

### Guide to the statistical analysis plan [^116s7B7G]. Paediatric Anaesthesia (2019). Medium credibility.

Biomedical research has been struck with the problem of study findings that are not reproducible. With the advent of large databases and powerful statistical software, it has become easier to find associations and form conclusions from data without forming an a-priori hypothesis. This approach may yield associations without clinical relevance, false positive findings, or biased results due to "fishing" for the desired results. To improve reproducibility, transparency, and validity among clinical trials, the National Institute of Health recently updated its grant application requirements, which mandates registration of clinical trials and submission of the original statistical analysis plan (SAP) along with the research protocol. Many leading journals also require the SAP as part of the submission package. The goal of this article and the companion article detailing the SAP of an actual research study is to provide a practical guide on writing an effective SAP. We describe the what, why, when, where, and who of a SAP, and highlight the key contents of the SAP.

---

### Clinical databases for chest physicians [^112XWBG2]. Chest (2018). Low credibility.

A clinical database is a repository of patient medical and sociodemographic information focused on one or more specific health condition or exposure. Although clinical databases may be used for research purposes, their primary goal is to collect and track patient data for quality improvement, quality assurance, and/or actual clinical management. This article aims to provide an introduction and practical advice on the development of small-scale clinical databases for chest physicians and practice groups. Through example projects, we discuss the pros and cons of available technical platforms, including Microsoft Excel and Access, relational database management systems such as Oracle and PostgreSQL, and Research Electronic Data Capture. We consider approaches to deciding the base unit of data collection, creating consensus around variable definitions, and structuring routine clinical care to complement database aims. We conclude with an overview of regulatory and security considerations for clinical databases.

---

### New horizons in evidence synthesis for older adults [^114VH86s]. Age and Ageing (2023). Medium credibility.

Evidence synthesis, embedded within a systematic review of the literature, is a well-established approach for collating and combining all the relevant information on a particular research question. A robust synthesis can establish the evidence base, which underpins best practice guidance. Such endeavours are frequently used by policymakers and practitioners to inform their decision making. Traditionally, an evidence synthesis of interventions consisted of a meta-analysis of quantitative data comparing two treatment alternatives addressing a specific and focussed clinical question. However, as the methods in the field have evolved, especially in response to the increasingly complex healthcare questions, more advanced evidence synthesis techniques have been developed. These can deal with extended data structures considering more than two treatment alternatives (network meta-analysis) and complex multicomponent interventions. The array of questions capable of being answered has also increased with specific approaches being developed for different evidence types including diagnostic, prognostic and qualitative data. Furthermore, driven by a desire for increasingly up-to-date evidence summaries, living systematic reviews have emerged. All of these methods can potentially have a role in informing older adult healthcare decisions. The aim of this review is to increase awareness and uptake of the increasingly comprehensive array of newer synthesis methods available and highlight their utility for answering clinically relevant questions in the context of older adult research, giving examples of where such techniques have already been effectively applied within the field. Their strengths and limitations are discussed, and we suggest user-friendly software options to implement the methods described.

---

### How to find answers to clinical questions [^114BuGKA]. American Family Physician (2009). Low credibility.

Many barriers exist to finding answers to physicians' clinical questions. Lack of time, resources, and computer skills, as well as physicians' environment and attitudes about problem solving, all contribute to unanswered questions. Making use of computer-based information resources can give physicians a framework for answering questions and keeping their practice consistent with the best available evidence.

---

### Clinical bioinformatician body of knowledge-bioinformatics and software core: a report of the Association for Molecular Pathology [^116btaCJ]. The Journal of Molecular Diagnostics (2025). High credibility.

Practical skills — data visualization — Bioinformaticians must be able to design, generate, and interpret visualizations because pipelines produce complex, high‑dimensional data that are often difficult or impossible to review directly in raw form, and reducing data into fewer outputs may yield opaque or misleading interpretations. Visualizations are often appropriate or required at different phases of the bioinformatics data stream, and bioinformaticians should identify the core question and translate it into an appropriate graph, set of plots, or interactive exploration. Visualization of sequencer or pipeline output can play critical roles during algorithm or pipeline development, during validation to identify assay parameters, limitations, and performance characteristics, as part of quality management during clinical operation, and for troubleshooting and investigating errors or assay failures. Bioinformaticians should be comfortable with at least one system to generate and display basic charts such as bar or line plots, scatter plots, and receiver operating characteristic curves, with ecosystems available in common languages (eg, ggplot in R and matplotlib in Python).

---

### Methodologic issues in systematic reviews and meta-analyses [^111r2fni]. Clinical Orthopaedics and Related Research (2003). Low credibility.

Systematic reviews of original research are increasing in number. Systematic reviews are distinct from narrative reviews because they address a specific clinical question, require a comprehensive literature search, use explicit selection criteria to identify relevant studies, assess the methodologic quality of included studies, explore differences among study results, and either qualitatively or quantitatively synthesize study results. Systematic reviews that quantitatively pool results of more than one study are called meta-analyses. Several organizations are collaboratively involved in producing high quality systematic reviews and meta-analyses. Familiarity with how to do a systematic review and meta-analysis will lead to greater skill in using this type of article. For clinicians, teachers, and investigators, systematic reviews and meta-analyses are useful sources of evidence.

---

### Descriptive statistics: reporting the answers to the 5 basic questions of who, what, why, when, where, and a sixth, so what? [^114QsVXb]. Anesthesia and Analgesia (2017). Low credibility.

Descriptive statistics are specific methods basically used to calculate, describe, and summarize collected research data in a logical, meaningful, and efficient way. Descriptive statistics are reported numerically in the manuscript text and/or in its tables, or graphically in its figures. This basic statistical tutorial discusses a series of fundamental concepts about descriptive statistics and their reporting. The mean, median, and mode are 3 measures of the center or central tendency of a set of data. In addition to a measure of its central tendency (mean, median, or mode), another important characteristic of a research data set is its variability or dispersion (ie, spread). In simplest terms, variability is how much the individual recorded scores or observed values differ from one another. The range, standard deviation, and interquartile range are 3 measures of variability or dispersion. The standard deviation is typically reported for a mean, and the interquartile range for a median. Testing for statistical significance, along with calculating the observed treatment effect (or the strength of the association between an exposure and an outcome), and generating a corresponding confidence interval are 3 tools commonly used by researchers (and their collaborating biostatistician or epidemiologist) to validly make inferences and more generalized conclusions from their collected data and descriptive statistics. A number of journals, including Anesthesia & Analgesia, strongly encourage or require the reporting of pertinent confidence intervals. A confidence interval can be calculated for virtually any variable or outcome measure in an experimental, quasi-experimental, or observational research study design. Generally speaking, in a clinical trial, the confidence interval is the range of values within which the true treatment effect in the population likely resides. In an observational study, the confidence interval is the range of values within which the true strength of the association between the exposure and the outcome (eg, the risk ratio or odds ratio) in the population likely resides. There are many possible ways to graphically display or illustrate different types of data. While there is often latitude as to the choice of format, ultimately, the simplest and most comprehensible format is preferred. Common examples include a histogram, bar chart, line chart or line graph, pie chart, scatterplot, and box-and-whisker plot. Valid and reliable descriptive statistics can answer basic yet important questions about a research data set, namely: "Who, What, Why, When, Where, How, How Much?"

---

### Intention-to-treat analysis in clinical research: basic concepts for clinicians [^114pY3fa]. American Journal of Physical Medicine & Rehabilitation (2024). Medium credibility.

Abstract

This review presents a comprehensive summary and critical evaluation of intention-to-treat analysis, with a particular focus on its application to randomized controlled trials within the field of rehabilitation. Adhering to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses guidelines, we conducted a methodological review that encompassed electronic and manual search strategies to identify relevant studies. Our selection process involved two independent reviewers who initially screened titles and abstracts and subsequently performed full-text screening based on established eligibility criteria. In addition, we included studies from manual searches that were already cataloged within the first author's personal database. The findings are synthesized through a narrative approach, covering fundamental aspects of intention to treat, including its definition, common misconceptions, advantages, disadvantages, and key recommendations. Notably, the health literature offers a variety of definitions for intention to treat, which can lead to misinterpretations and inappropriate application when analyzing randomized controlled trial results, potentially resulting in misleading findings with significant implications for healthcare decision making. Authors should clearly report the specific intention-to-treat definition used in their analysis, provide details on participant dropouts, and explain upon their approach to managing missing data. Adherence to reporting guidelines, such as the Consolidated Standards of Reporting Trials for randomized controlled trials, is essential to standardize intention-to-treat information, ensuring the delivery of accurate and informative results for healthcare decision making.

---

### Engaging stakeholders to improve presentation of patient-reported outcomes data in clinical practice [^1136kszm]. Supportive Care in Cancer (2016). Low credibility.

Purpose

Patient-reported outcomes (PROs) can promote patient-centered care, but previous research has documented interpretation challenges among clinicians and patients. We engaged stakeholders to improve formats for presenting individual-level PRO data (for patient monitoring) and group-level PRO data (for reporting comparative clinical studies).

Methods

In an iterative process, investigators partnered with stakeholder workgroups of clinicians and patients to address previously identified interpretation challenges. Candidate approaches were then tested in semi-structured, one-on-one interviews with cancer patients and clinicians. Interpretation issues addressed included conveying score meaning (i.e., what is good/bad) and directional inconsistency (whether higher scores are better/worse). An additional issue for individual-level PROs was highlighting potentially concerning scores and, for group-level PROs, identifying important between-group differences (clinical, statistical).

Results

One-on-one interviews in a purposive sample of clinicians (n = 40) and patients (n = 39) provided insights regarding approaches to address issues identified. For example, adding descriptive labels to the Y-axis (none, mild, moderate, severe) helps address directional inconsistency and aids interpretation of score meaning. Red circles around concerning data points or a threshold line indicating worse-than-normal scores indicate possibly concerning scores for individual-level PRO data. For group-level PRO data, patients and some clinicians are confused by confidence limits and clinical versus statistical significance, but almost all clinicians want p values displayed.

Conclusions

Variations in interpretation accuracy demonstrate the importance of presenting PRO data in ways that promote understanding and use. In an iterative stakeholder-driven process, we developed improved PRO data presentation formats, which will be evaluated in further research across a large population of patients and clinicians.

---

### Statistics in medicine [^113BuBBS]. Plastic and Reconstructive Surgery (2011). Low credibility.

The scope of biomedical research has expanded rapidly during the past several decades, and statistical analysis has become increasingly necessary to understand the meaning of large and diverse quantities of raw data. As such, a familiarity with this lexicon is essential for critical appraisal of medical literature. This article attempts to provide a practical overview of medical statistics, with an emphasis on the selection, application, and interpretation of specific tests. This includes a brief review of statistical theory and its nomenclature, particularly with regard to the classification of variables. A discussion of descriptive methods for data presentation is then provided, followed by an overview of statistical inference and significance analysis, and detailed treatment of specific statistical tests and guidelines for their interpretation.

---

### Clinical evidence 2030 [^113fbJFU]. Clinical Pharmacology and Therapeutics (2025). Medium credibility.

PRINCIPLE 1: PATIENTS ARE AT THE CENTER OF CLINICAL EVIDENCE GENERATION AND GUIDE EVERY STEP

Clinical evidence is generated for patients' needs and public health. Through their engagement, patients provide critical insight into their medical needs and what really matters to them at every level of healthcare decisions. Clinical evidence generation should revolve around these needs. Patients have been increasingly involved in healthcare decisions, including those related to the evaluation of the benefit–risk of medicines by regulators, where patients bring their personal experience, knowledge, and expertise both on the conditions and the available treatment options, and also on the impact of regulatory decisions on their lives.

Efforts are ongoing to guide the generation, collection, and use of patient experience data to support decisions on the development and benefit–risk evaluation of medicines. To further build on these efforts, multi‐stakeholder collaboration in this field is encouraged.

---

### Proposal for standardized definitions for efficacy end points in adjuvant breast cancer trials: the STEEP system [^112aPyBw]. Journal of Clinical Oncology (2007). Low credibility.

Purpose

Standardized definitions of breast cancer clinical trial end points must be adopted to permit the consistent interpretation and analysis of breast cancer clinical trials and to facilitate cross-trial comparisons and meta-analyses. Standardizing terms will allow for uniformity in data collection across studies, which will optimize clinical trial utility and efficiency. A given end point term (eg, overall survival) used in a breast cancer trial should always encompass the same set of events (eg, death attributable to breast cancer, death attributable to cause other than breast cancer, death from unknown cause), and, in turn, each event within that end point should be commonly defined across end points and studies.

Methods

A panel of experts in breast cancer clinical trials representing medical oncology, biostatistics, and correlative science convened to formulate standard definitions and address the confusion that nonstandard definitions of widely used end point terms for a breast cancer clinical trial can generate. We propose standard definitions for efficacy end points and events in early-stage adjuvant breast cancer clinical trials. In some cases, it is expected that the standard end points may not address a specific trial question, so that modified or customized end points would need to be prospectively defined and consistently used.

Conclusion

The use of the proposed common end point definitions will facilitate interpretation of trial outcomes. This approach may be adopted to develop standard outcome definitions for use in trials involving other cancer sites.

---

### Interpreting results from biomarker-guided clinical trials: avoiding "negative" or "failed" terminology [^116ohPco]. BMC Medicine (2025). Medium credibility.

Background

Well-conducted clinical trials provide data to support decision-making for healthcare policy, guidelines, and clinical practice. In recent years, there has been rapid development in genomics and development of novel therapeutics, which have provided hope for a more personalised or precision approach to healthcare. One main method that has been proposed by which precision medicine could be delivered is using validated biomarkers to enable patient stratification.

Following robust assessment of biomarkers for analytical validity and clinical validity, it is widely accepted that the next key step is for clinical utility to be demonstrated. There have been a multitude of biomarker-guided clinical trial designs reported in the literature to assess for clinical utility of a biomarker for patient stratification (as opposed to a surrogate endpoint), including those using a fixed-sample design or adaptive design.

With the increased complexity of biomarker-guided trial designs, there is potentially a greater chance for misunderstanding and misinterpretation of outcomes. Therefore, education and training are required to enable appropriate interpretation of such trials. In this article, we summarise key principles to consider when evaluating the results of biomarker-guided clinical trials, including how to appropriately interpret results from such trials, and we stress the importance of appropriate terminology and wording to reflect understanding of these trials.

---

### Selection of inferential statistics: an overview [^113iDWHc]. Journal of the American Academy of Child and Adolescent Psychiatry (2002). Low credibility.

This column serves as an introduction to selection of appropriate statistical methods. In the next five columns we will discuss conceptually, and in more depth, these statistical methods. We will use clinical examples and discuss why the author(s) selected a particular statistical method and how the results of the statistical method were interpreted.

---

### Tutorials in clinical research, part VI: descriptive statistics [^111Z8aMm]. The Laryngoscope (2002). Low credibility.

Objectives

This is the sixth in a series of Tutorials in Clinical Research. The objectives of this tutorial are to produce a brief, step-by-step, user-friendly atlas with clinical scenarios and illustrative examples to serve as ready references and, it is hoped, to open an inviting door to understanding statistics.

Study Design

Tutorial.

Methods

The authors met weekly for 6 months discussing clinical research articles and the applied statistics. Liberal use of reference texts and discussion focused on constructing a tutorial that is factual as well as easy to read and understand.

Results

The tutorial is organized into five sections: Overview, Key to Descriptive Summaries, Steps in Summarizing Descriptive Data, Understanding the Summaries of Specific Data Types, and Understanding Scope or Dispersion of Data.

Conclusions

Assessing the validity of medical studies requires a working knowledge of statistics; however, obtaining this knowledge need not be beyond the ability of the busy surgeon. Desire is the only limiting factor. We have tried to construct an accurate, basic, easy-to-read-and-apply introduction to the task of summarizing the characteristics of a single variable. This is known as descriptive statistics.

---

### Effective use of tables and figures in abstracts, presentations, and papers [^1152FgyH]. Respiratory Care (2004). Low credibility.

In some situations, tables, graphs, and figures can present certain types of information (including complicated relationships and sequences of events) more clearly and in less space than the same information would require in sentence form. However, do not use tables, graphs, and figures for small amounts of data that could be conveyed clearly and succinctly in a sentence. Also, do not reiterate in sentences the data that are shown in a table, graph, or figure: the point of creating a table or graph or figure is to eliminate that type of sentence from your manuscript. In building a data table you must balance the necessity that the table be complete with the equally important necessity that it not be too complex. Sometimes it is helpful to break a large table into several smaller ones to allow the reader to identify important information easily, but, conversely, it is a common mistake of novice authors to split up into several tables data that belong in one table. In almost all cases, only one table or graph or figure should be included in an abstract, and then only if it can convey essential information in less space and in a more easily interpretable way than the sentence form. For a poster, in almost all instances you should use only one typeface and one font in a table, graph, or figure. In general, do not use bold, italics, or color unless you are presenting a great deal of data and you need to highlight certain data values and you are certain that using bold, italics, or color will improve readability, which is rare. Do not include identical information in a table and a graph/figure. In reporting a clinical trial you will need to include a patient flow chart that identifies the number of patients initially screened for the study, the number of patients who were excluded (and why) after initial screening or in the final analysis, and how many patients entered, exited early, and completed each arm of the study. A treatment protocol should also be described with a flow chart. In preparing a graph the most common error is to include a line that suggests an unsubstantiated extrapolation between or beyond the data points. In selecting the graph's axes, avoid truncating, enlarging, or compressing the axes in ways that might make the graph confusing or misleading. To prepare clear, accurate, easily interpretable tables, graphs, and figures, rely on the rules described in authoritative guides such as the Council of Science Editors' Scientific Style and Format and the American Medical Association's Manual of Style.

---

### Functional neuroimaging in disorders of consciousness: towards clinical implementation [^115Utv6z]. Brain (2025). Medium credibility.

Implementation of neuroimaging

Up to this point, we have outlined which patients stand to benefit from advanced neuroimaging techniques, when they should be used, and which approaches are most appropriate for answering specific diagnostic and prognostic questions. However, a major barrier to translating these specialized research techniques into widespread clinical practice is the lack of practical knowledge regarding the acquisition, analysis and interpretation of functional neuroimaging data. Successfully integrating advanced imaging techniques from research into clinical settings for DoC patients will require a collaborative effort among clinicians, radiologists, medical staff and scientific researchers. Thus, we have outlined, in Table 1, a series of steps that can be taken to practically implement these techniques by outlining common considerations for neuroimaging set-up, acquisition, analysis and interpretation. In brief, interpreting neuroimaging data requires a nuanced approach. It is important to ensure that imaging data are of high quality, free from artefacts and noise, and correctly preprocessed to account for motion, spatial normalization, and other factors. Clinical teams must also consider the heterogeneity of the DoC population, as variations in brain injury aetiology, extent of damage, and patient-specific factors can influence the neuroimaging results. Results should be interpreted with caution and reported in electronic medical records. Medical teams should review results before conveying them to families of loved ones.

Table 1
Practical recommendations for implementation of neuroimaging as an assessment tool in disorders of consciousness

If centres do not have the personnel to analyse data, the hub and spoke model may be an effective approach to promoting the implementation of advanced neuroimaging techniques in DoC. According to this model, regional centres (spokes) are responsible for collecting neuroimaging data from patients, which are then sent to specialized centres (hubs) for analysis and interpretation. This structure ensures that patients across various regions benefit from advanced imaging technologies. By centralizing the expertise for data analysis and interpretation at the hubs, the model promotes timely assessments, consistent care standards, and collaborative care efforts. This approach may ultimately lead to improved and more efficient utilization of healthcare resources. In clinical practice, similar approaches are commonly used in other contexts. For example, in the field of epilepsy, EEGs are often acquired at regional or local centres for seizure monitoring. These recordings are then sent to specialized epilepsy centres for detailed analysis and interpretation by clinical experts in the field.

---

### How to use a subgroup analysis: users' guide to the medical literature [^1117Y6en]. JAMA (2014). Excellent credibility.

Clinicians, when trying to apply trial results to patient care, need to individualize patient care and, potentially, manage patients based on results of subgroup analyses. Apparently compelling subgroup effects often prove spurious, and guidance is needed to differentiate credible from less credible subgroup claims. We therefore provide 5 criteria to use when assessing the validity of subgroup analyses: (1) Can chance explain the apparent subgroup effect; (2) Is the effect consistent across studies; (3) Was the subgroup hypothesis one of a small number of hypotheses developed a priori with direction specified; (4) Is there strong preexisting biological support; and (5) Is the evidence supporting the effect based on within- or between-study comparisons. The first 4 criteria are applicable to individual studies or systematic reviews, the last only to systematic reviews of multiple studies. These criteria will help clinicians deciding whether to use subgroup analyses to guide their patient care.

---

### Conducting a high-quality systematic review [^114Dg6aM]. The Journal of Rheumatology (2025). Medium credibility.

Systematic reviews (SRs) are a structured means of knowledge synthesis used by a variety of healthcare practitioners to aid in medical decision making. The SR, if conducted rigorously, is considered to be at the top of the hierarchy for research studies. In addition to synthesizing evidence, SRs identify research priorities, address questions that may not be answerable by individual studies, and identify gaps to be addressed in future primary research. There are several steps that need to be taken when developing SRs to provide the best available evidence-the most essential being the assessment of risk of bias (ROB). Several ROB tools have been developed for use according to study design. Increasingly used is the assessment of certainty of evidence using approaches such as those developed by the Grading of Recommendations Assessment, Development, and Evaluation (GRADE) working group. Whereas ROB is assessed for individual studies, the certainty of evidence is assessed for each critical or important outcome across studies. Analysis can be quantitative (metaanalysis) or qualitative (narrative), with the former intended to develop estimates of the effect measure (ie, the statistic that compares collated data), with confidence limits around that estimate. This review will focus on the steps required to develop SRs, from registration of the review protocol to the conduct, analysis, and reporting, with a focus on the assessment of ROB and certainty of evidence to ensure the development of a methodological and rigorous process.

---

### Preparing raw clinical data for publication: guidance for journal editors, authors, and peer reviewers [^112g4HBs]. BMJ (2010). Excellent credibility.

Data preparation guidance

What is the dataset?

For the purposes of this guidance, the dataset is the aggregated collection of patient observations (including sociodemographic and clinical information) used for the purposes of producing the summary statistical findings presented in the main report of the research project, whether previously published or not.

Data are almost always collected at a greater level of detail than are reported in a journal article. For example, each participant in a pain study may complete a pain diary twice a day for 30 days, with the authors reporting "mean post-treatment pain" for one or more groups of participants. Similarly, a quality of life questionnaire may include a large number of questions divided into domains such as physical, mental, emotional, and social wellbeing.

Here we define a dataset as that containing the minimum level of detail necessary to reproduce all numbers reported in the paper. The dataset for the pain trial, for example, might therefore consist of one value per individual for mean post-treatment pain rather than 30 values for pain levels on each day. However, if more detailed, underlying data are available and can be shared then that should also be encouraged, provided the data conform to the same standards — as proposed in this article — as the main dataset. If possible, authors should present all outcomes and variables, regardless of significance.

Anonymisation

A list of 28 patient identifiers has been formulated, based on information aggregated from policy documents and research guidance from major UK and US funding agencies, governmental health departments and statutes, and three internationally recognised publication ethics resources for editors of biomedical journals. This list is provided in the table.

list of potential patient identifiers in datasets

Types of identifying information have been classified as either direct or indirect. Publication of any direct identifiers places individuals in the dataset at risk of being identified. Although none of the indirect identifiers on its own would point to an individual, a dataset with several indirect identifiers, especially those relating to attributes, might do. The consensus of the authors, and working group members acknowledged in the current manuscript, is that a dataset including three or more indirect identifiers should be assessed by an independent researcher or ethics committee to evaluate the risk that individuals might be identifiable. If the risk of identification is considered non-negligible, before publication can proceed approval should be sought from a relevant advisory body (see below). An explicit justification for publication of a dataset with three or more indirect identifiers should be given by the researcher — as an annotation to the dataset and in any accompanying articles. This should include the name of any oversight bodies consulted.

---

### Your results, explained: clarity provided by row percentages versus column percentages [^114YS7zm]. The Annals of Thoracic Surgery (2016). Low credibility.

The overwhelming majority of clinical research papers begin their Results section with a series of tables summarizing the distribution of significant demographics, clinical characteristics, and potential outcome variables within their study cohort. However, the presentation of these tables often leaves the reader confused or wanting different information than what is presented. One particularly vexing decision in the presentation of categoric variables is choosing to present the "row percentage" in preference to the "column percentage". This report provides a guide for clinical scientists, some of whom may not have access to statistical support, to summarize their descriptive results in the manner that provides the most clarity for the reader.

---

### Why estimands are needed to define treatment effects in clinical trials [^1111Kpfs]. BMC Medicine (2023). Medium credibility.

Background

The estimand for a clinical trial is a precise definition of the treatment effect to be estimated. Traditionally, estimates of treatment effects are based on either an ITT analysis or a per-protocol analysis. However, there are important clinical questions which are not addressed by either of these analyses. For example, consider a trial where patients take a rescue medication. The ITT analysis includes data after use of rescue, while the per-protocol analysis excludes these patients altogether. Neither of these analyses addresses the important question of what the treatment effect would have been if patients did not take rescue medication.

Main Text

Trial estimands provide a broader perspective compared to the limitations of ITT and per-protocol analysis. Trial treatment effects depend on how events occurring after treatment initiation such as use of alternative medication or discontinuation of the intervention are included in the definition. These events can be accounted for in different ways, depending on the clinical question of interest.

Conclusion

The estimand framework is an important step forward in improving the clarity and transparency of clinical trials. The centrality of estimands to clinical trials is currently not reflected in methods recommended by the Cochrane group or the CONSORT statement, the current standard for reporting clinical trials in medical journals. We encourage revisions to these guidelines.

---

### Opportunities and challenges for real-world studies on chronic inflammatory joint diseases through data enrichment and collaboration between national registers: the nordic example [^113UomXc]. RMD Open (2018). Low credibility.

A Nordic initiative to facilitate collaboration across enriched rheumatology registers

With largely similar healthcare structures, the existence of national registers on medical and societal outcomes (cancer, hospitalisations, pregnancies, sick leave, other cost reimbursements and so on) and the possibility to link information across registers, the premises for collaborative large-scale register-based research including enriched clinical data are good across the Nordic countries, though not without challenges as those outlined above.

We have initiated a collaboration between the Nordic Rheumatology registers which aims to establish a standing network across Sweden, Denmark, Finland, Norway and Iceland for register research on patients with RA, AS, SpA and PsA. Taking a pragmatic and research-question–based approach, we will exploit various approaches to harmonise (1) primary data collection, (2) data management and (3) analytical protocols, and to address legal, logistic and technical challenges involved in data handling and in data sharing, both with regard to the clinical register data and to linked data from other data sources (figure 2).

The research questions that can be addressed through a collaboration such as ours can largely be divided into two categories: (1) questions that can be addressed using the clinical registers only (eg, a clinical effectiveness study of response to treatment X vs response to treatment Y) and (2) questions that can typically only be addressed using data enriched with data from other sources data (eg, long-term malignancy risks with treatment Z). A logical first step before embarking on specific comparative effectiveness/safety projects is to characterise and compare the patient populations across countriesand to assess the relative uptake of different therapies in each country. Even with the similarities across the Nordic countries, heterogeneity is to be expected. Harmonisation of clinical input data is therefore a prerequisite, both regarding the definition and the collection, as well as of the study protocol. For each specific subproject launched within our collaboration, a prespecified statistical analysis protocol must be agreed on, and exposure, outcome and covariates need to be clearly defined. Besides study-specific definitions and analysis plans, this work will eventually result in a library of 'standard' and generic definitions for data harmonisation regardless of specific research question.

---

### Statistical considerations in meta-analysis [^113Ev9Jr]. Infectious Disease Clinics of North America (2009). Low credibility.

Systematic reviews and meta-analyses have recently emerged as key sources of evidence to inform clinical care, from bedside individualized decisions to policy making. This article discusses the principles behind statistical methods for quantitative evidence synthesis, including meta-analysis and meta-regression. The authors present pertinent concepts in an intuitive and nonmathematical fashion by use of examples from the infectious diseases literature.

---

### SPIRIT 2013 explanation and elaboration: guidance for protocols of clinical trials [^116LBnch]. BMJ (2013). Excellent credibility.

Harms

Item 22: Plans for collecting, assessing, reporting, and managing solicited and spontaneously reported adverse events and other unintended effects of trial interventions or trial conduct

Explanation

Evaluation of harms has a key role in monitoring the condition of participants during a trial and in enabling appropriate management of adverse events. Documentation of trial related adverse events also informs clinical practice and the conduct of ongoing and future studies. We use the term "harms" instead of "safety" to better reflect the negative effects of interventions. An adverse event refers to an untoward occurrence during the trial, which may or may not be causally related to the intervention or other aspects of trial participation. This definition includes unfavourable changes in symptoms, signs, laboratory values, or health conditions. In the context of clinical trials, it can be difficult to attribute causation for a given adverse event. An adverse effect is a type of adverse event that can be attributed to the intervention.

Harms can be specified as primary or secondary outcomes (Item 12) or can be assessed as part of routine monitoring. To the extent possible, distinctions should be made between adverse events that are anticipated versus unanticipated, and solicited versus unsolicited, because expectation can influence the number and perceived severity of recorded events. For example, providing statements in the informed consent process about the possibility of a particular adverse effect or using structured, as opposed to open ended, questionnaires for data collection, can increase the reporting of specific events ("priming"). The timeframe for recording adverse events can also affect the type of data obtained.

The protocol should describe the procedures for and frequency of harms data collection, the overall surveillance timeframe, any instruments to be used, and their validity and reliability, if known. Substantial discrepancies have been observed between protocol specified plans for adverse event collection and reporting, and what is described in final publications. Although trials are often not powered to detect important differences in rates of uncommon adverse events, it is also important to describe plans for data analysis, including formal hypothesis testing or descriptive statistics.

Finally, the protocol should address the reporting of harms to relevant groups (eg, sponsor, research ethics committee/institutional review board, data monitoring committee, regulatory agency), which is an important process that is subject to local regulation. Key considerations include the severity of the adverse event, determination of potential causality, and whether it represents an unexpected or anticipated event. For multicentre studies, procedures and timing should be outlined for central collection, evaluation, and reporting of pooled harms data.

---

### Guidelines for the descriptive presentation and statistical analysis of contact allergy data [^115RMc8Q]. Contact Dermatitis (2004). Low credibility.

The present guidelines aim to support clinical researchers in adequately presenting data on contact allergy, and to use statistical tests appropriate for their data. A description of the mode of selection of patients, and of their relevant demographic details, is an essential prerequisite for the correct interpretation of study results. Proportions and rates, if regarded as estimate of these parameters of a target populations, should normally be supplemented with confidence intervals to address precision. Concordance, i.e., agreement between two ratings in a dependent sample, must be quantified with a chance-corrected measure such as Cohen's kappa coefficient. If the diagnostic quality of an outcome is being assessed, standard measures like sensitivity and specificity, as well as the prevalence-dependent positive and negative predictive values should be calculated. Often, contact allergy to a certain substance depends on several factors. In this situation, depending on the research question, techniques like stratification, standardization or multifactorial analysis should be employed. With increasing complexity of statistical description and analysis, consulting with a biostatistician is often mandatory.

---

### Streamlining cardiovascular clinical trials to improve efficiency and generalisability [^111VSK4o]. Heart (2017). Low credibility.

Controlled trials provide the most valid determination of the efficacy and safety of an intervention, but large cardiovascular clinical trials have become extremely costly and complex, making it difficult to study many important clinical questions. A critical question, and the main objective of this review, is how trials might be simplified while maintaining randomisation to preserve scientific integrity and unbiased efficacy assessments. Experience with alternative approaches is accumulating, specifically with registry-based randomised controlled trials that make use of data already collected. This approach addresses bias concerns while still capitalising on the benefits and efficiencies of a registry. Several completed or ongoing trials illustrate the feasibility of using registry-based controlled trials to answer important questions relevant to daily clinical practice. Randomised trials within healthcare organisation databases may also represent streamlined solutions for some types of investigations, although data quality (endpoint assessment) is likely to be a greater concern in those settings. These approaches are not without challenges, and issues pertaining to informed consent, blinding, data quality and regulatory standards remain to be fully explored. Collaboration among stakeholders is necessary to achieve standards for data management and analysis, to validate large data sources for use in randomised trials, and to re-evaluate ethical standards to encourage research while also ensuring that patients are protected. The rapidly evolving efforts to streamline cardiovascular clinical trials have the potential to lead to major advances in promoting better care and outcomes for patients with cardiovascular disease.

---

### A practical guide to surveys and questionnaires [^112hiFJh]. Otolaryngology — Head and Neck Surgery (2011). Low credibility.

Surveys with questionnaires play a vital role in decision and policy making in society. Within medicine, including otolaryngology, surveys with questionnaires may be the only method for gathering data on rare or unusual events. In addition, questionnaires can be developed and validated to be used as outcome measures in clinical trials and other clinical research architecture. Consequently, it is fundamentally important that such tools be properly developed and validated. Just asking questions that have not gone through rigorous design and development may be misleading and unfair at best; at worst, they can result in under- or overtreatment and unnecessary expense. Furthermore, it is important that consumers of the data produced by these instruments understand the principles of questionnaire design to interpret results in an optimal and meaningful way. This article presents a practical guide for understanding the methodologies of survey and questionnaire design, including the concepts of validity and reliability, how surveys are administered and implemented, and, finally, biases and pitfalls of surveys.

---

### Methodological standards for qualitative and mixed methods patient centered outcomes research [^111kv2Fg]. BMJ (2020). Excellent credibility.

MM-2: Select and justify appropriate mixed methods sampling strategy

Mixed methods research aims to contribute insights and knowledge beyond that obtained from quantitative or qualitative methods only, which should be reflected in the sampling strategies as well as in the design of the study and the research plan. Qualitative and quantitative components can occur simultaneously or sequentially, and researchers must select and justify the most appropriate mixed method sampling strategy and demonstrate that the desired number and type of participants can be achieved with respect to the available time, cost, research team skillset, and resources. Those sampling strategies that are unique to mixed methods (eg, interdependent, independent, and combined) should focus on the depth and breadth of information across research components.

MM-3: Integrate data analysis, data interpretations, and conclusions

Qualitative and quantitative data often are analyzed in isolation, with little thought given to when these analyses should occur or how the analysis, interpretation, and conclusions integrate with one another. There are multiple approaches to integration in the analysis of qualitative and quantitative data (eg, merging, embedding, and connecting). As such, the approach to integration should determine the priority of the qualitative and quantitative components, as well as the temporality with which analysis will take place (eg, sequentially, or concurrently; iterative or otherwise). Either a priori or emergently, where appropriate, researchers should define these characteristics, identify the points of integration, and explain how integrated analyses will proceed with respect to the two components and the selected approach.

---

### A summary of the methods that the national clinical guideline centre uses to produce clinical guidelines for the national institute for health and clinical excellence [^112LzFPy]. Annals of Internal Medicine (2011). Medium credibility.

NICE review question formulation — The areas in scope are developed into specific review questions using the PICO (patient, intervention, comparison, outcome) framework to ensure questions are specific enough for successful literature searches, and questions may address interventions, the accuracy or clinical value of diagnostic tests, or prognostic factors; prognostic evidence may help determine risk categories or subgroups and inform patients.

---

### Institutionalising an evidence-informed approach to guideline development: progress and challenges at the World Health Organization [^111pEXU6]. BMJ Global Health (2018). Medium credibility.

Methods

Study design

This study used a qualitative study design involving primary data collection consisting of semistructured interviews with senior WHO staff. The analytical approach involved identification and interpretation of themes in the qualitative data describing key experiences, perceptions and views about WHO's guideline development reforms. This study did not operate with explicitly defined theoretical frameworks to guide the scope of the study, the specification of the research questions, or support interpretation; instead, the major themes identified in the qualitative data were interpreted in the context of the existing literature on WHO's guideline development process and guideline development more generally.

To identify major themes in the qualitative data, we followed the five-cycle process described by Yin (described in greater detail in the Data analysis section). Yin does not explicitly describe this process as 'thematic analysis'; yet this analytical process involves identification of themes capturing 'something important about the data in relation to the research question', and representing an 'idea or concept that captures and summarises the core point of a coherent and meaningful pattern in the data'. Accordingly, the methodological approach described below resembles Braun & Clark's six phases of thematic analysis: (1) familiarisation with the data, (2) coding, (3) searching for themes, (4) reviewing themes, (5) defining and naming themes and (6) writing up. Moreover, the way our findings have been identified, interpreted and presented under 'Results' is broadly consistent with the qualitative descriptive approach described by Sandelowski. Here, the identified themes present a comprehensive and rich summary of experiences responding to the research objectives. However, Sandelowski proposes the use of content analysis as the main analytical approach in a qualitative descriptive study, a term used interchangeably with thematic analysis, but which many consider to be two distinct ways of approaching qualitative data analysis.

The COnsolidated criteria for REporting Qualitative research (COREQ) was used to report on the characteristics of the research team, study design and data analysis (online supplementary file S1).COREQ was originally developed to promote transparent and comprehensive reporting of qualitative health research involving the use of interviews and/or focus groups to explore preferences and needs of clinicians, healthcare providers, policy-makers and patients. However, its relevance goes beyond research on interactions within healthcare systems since COREQ's items clarify various choices important for understanding the study design, the collection and the interpretation of qualitative data more generally.

---

### American Burn Association (ABA) burn care quality platform (BCQP) and large data set analysis considerations: a practical guide to investigating clinical questions in burns via large data sets [^113ZpVby]. Journal of Burn Care & Research (2024). High credibility.

Burn Care Quality Platform (BCQP) limitations — data quality, interpretation, and inference boundaries are outlined: "First, all data sets should examine the profile of patient centers", as variations among extreme burn sizes may lead to "increased "misassignments" of certain parameters". Despite a data dictionary, "interpretive variance remains problematic between centers", so "work must continue to form consensus definitions" to guide BCQP data entry. For large data set investigations, "the extent of conclusions should be tempered", and while retrospective observational studies can generate hypotheses, "randomized controlled trials are still vital to verify causal effects". Data issues should be fed back: "it is crucial that these inaccuracies be reported back to the BCQP", since "conscientious use of the BCQP can allow for prompt investigation and resolution of each error identified", and "the more responsibly the BCQP is used, the better its fidelity will become".

---

### Author guidelines for conducting systematic reviews and meta-analyses [^112rqE6X]. Knee Surgery, Sports Traumatology, Arthroscopy (2021). Medium credibility.

This article is a guidance how to write systematic reviews (SR's) and meta-analyses (MA) in orthopaedics and which aspects to focus on for transparency, systematicity and readability. Both SR and MA summarise and synthesise the best evidence available on a specific topic. This requires a systematic, structured and transparent process of analysis. The title should be concise, indicate type of review and ideally report the most important finding. Next, the structured abstract (no more than 350 words) should also raise key points and report the overall level of evidence. A relevant clinical question must be defined before the literature search is started. Methodological details such as databases searched, the exact search strategy (including time frame), inclusion/exclusion criteria, method of literature appraisal and statistical analysis must be described briefly. The primary and secondary outcomes should be mentioned. SR's be pre-registered before data extraction, to ensure transparency and the reduction of risk of bias. If registered, registration number should be stated in the abstract and the funding sources. A clear summary of the findings is important including the number of identified studies (depicted in a flowchart) and for meta-analyses a forest plot. The results of the literature appraisal and statistical analyses should be reported precisely. Subsequently, a general interpretation of findings and their significance and relevance to clinical practice should be provided. Clinical implications from the analysis should be drawn carefully and further research questions should be addressed. Finally, a conclusion, based solely on the results of the study is a necessity. Up to ten keywords are requested representing the main content of the article. Most applicable keywords should facilitate finding the manuscript in the databases and therefor considered carefully.

---

### American Society for Parenteral and Enteral Nutrition and Academy of Nutrition and Dietetics: revised 2014 standards of practice and standards of professional performance for registered dietitian nutritionists (competent, proficient, and expert) in nutrition support [^112wLxqc]. Nutrition in Clinical Practice (2014). Medium credibility.

Application of research — Registered Dietitian Nutritionist standards of professional performance outline indicators that emphasize incorporating evidence-based research into practice; integrating into practice knowledge of the research associated with the science, physiology, and pathophysiology of complex disease processes; and integrating best available research/evidence with best practices, clinical, and managerial expertise, and customer values. Indicators specify facilitating the incorporation of population-specific, evidence-based research findings into clinical policies and procedures/protocols, assessing system barriers and facilitators to adoption of evidence-based policies and procedures, incorporating knowledge of cultural diversity when integrating research in a specific patient/client population, and reviewing policies and procedures/protocols and revising as appropriate. The standards include contributing to the development of new knowledge and research in nutrition and dietetics; demonstrating knowledge of the standards of professional and ethical conduct established to guide researchers in protecting the rights, well-being, and dignity in the recruitment and retention of human subjects in clinical research; identifying nutrition support-related research questions for a broad patient/client population and formulating a well-defined research question for a specific patient/client population; and designing a research study protocol to address the clinical research question by identifying the target population, identifying measures to be applied to the clinical research project, proposing a research data analysis plan based on proven statistical methods, obtaining approval, independently, from pertinent governing bodies (eg, Institutional Review Board [IRB]), competing successfully for peer-reviewed grants, and overseeing all steps of research process and ensuring adherence to timeline and budget. The standards also promote research through alliances and collaboration with food and nutrition practitioners and other professionals and organizations by collaborating with research teams to collect and analyze data, advocating for and developing the use of data collection tools, utilizing data analysis software and/or consulting with a statistician to perform appropriate statistical analysis, designing and/or implementing studies with a multidisciplinary team, and partnering with research team members to disseminate research findings through didactic presentations and/or manuscript preparation and submission for publication.

---

### American Burn Association (ABA) burn care quality platform (BCQP) and large data set analysis considerations: a practical guide to investigating clinical questions in burns via large data sets [^114WZTam]. Journal of Burn Care & Research (2024). High credibility.

Large data set statistical considerations in burns — Researchers should develop a statistical analysis plan with independent and outcome variables clearly defined from the BCQP data dictionary and identify approaches for data "missingness", measurement error, and confounders; the exclusion of data based on missing values can cause biased interpretations and multiple imputations may be more robust than complete case analysis; measurement error should be considered with test characteristics and sensitivity analyses proposed a priori; analyzing outcomes en masse (> 20) is likely to yield statistical significance in at least one variable, particularly if a p-value of 0.05 is used; investigators should minimize multiplicity and ensure clinical meaningfulness; and the American Statistical Association advocates for presenting p-values alongside other important statistics, including point estimates and associated confidence intervals.

---

### Clinical evidence 2030 [^113qBHvf]. Clinical Pharmacology and Therapeutics (2025). Medium credibility.

PRINCIPLE 3: EVIDENCE GAPS ARE FORMULATED IN CLEAR RESEARCH QUESTIONS THAT DRIVE THE CHOICE OF EVIDENCE GENERATION

A "research question"‐driven approach, clearly articulating one or several research questions, strengthens clinical evidence generation through optimal study design, particularly if supported by scientific advice that involves different stakeholder voices. This approach should drive the selection of the suitable study method, with some questions best answered through experiments (including clinical trials) and others through observations (including non‐interventional studies).

Scientific advice and regulatory support for research and development (R&D) are key to better evidence generation, and should be strengthened, further rationalized and simplified.

Different clinical research frameworks exist for the formulation of scientific research questions. These include the PICOT framework (Population‐Intervention‐Comparator‐Outcome‐Time), historically used in the field of pharmacoepidemiology research, and the Target Trial Emulation framework, which allows clear specification of the population, intervention, comparator, outcome and time horizon for the research. The increasing understanding and use of the Estimand framework, already used to optimize clinical trial design, complements existing frameworks by stimulating further specification of research questions through considerations of intercurrent events.

---

### Fundamentals of research data and variables: the devil is in the details [^111CqqNc]. Anesthesia and Analgesia (2017). Low credibility.

Designing, conducting, analyzing, reporting, and interpreting the findings of a research study require an understanding of the types and characteristics of data and variables. Descriptive statistics are typically used simply to calculate, describe, and summarize the collected research data in a logical, meaningful, and efficient way. Inferential statistics allow researchers to make a valid estimate of the association between an intervention and the treatment effect in a specific population, based upon their randomly collected, representative sample data. Categorical data can be either dichotomous or polytomous. Dichotomous data have only 2 categories, and thus are considered binary. Polytomous data have more than 2 categories. Unlike dichotomous and polytomous data, ordinal data are rank ordered, typically based on a numerical scale that is comprised of a small set of discrete classes or integers. Continuous data are measured on a continuum and can have any numeric value over this continuous range. Continuous data can be meaningfully divided into smaller and smaller or finer and finer increments, depending upon the precision of the measurement instrument. Interval data are a form of continuous data in which equal intervals represent equal differences in the property being measured. Ratio data are another form of continuous data, which have the same properties as interval data, plus a true definition of an absolute zero point, and the ratios of the values on the measurement scale make sense. The normal (Gaussian) distribution ("bell-shaped curve") is of the most common statistical distributions. Many applied inferential statistical tests are predicated on the assumption that the analyzed data follow a normal distribution. The histogram and the Q-Q plot are 2 graphical methods to assess if a set of data have a normal distribution (display "normality"). The Shapiro-Wilk test and the Kolmogorov-Smirnov test are 2 well-known and historically widely applied quantitative methods to assess for data normality. Parametric statistical tests make certain assumptions about the characteristics and/or parameters of the underlying population distribution upon which the test is based, whereas nonparametric tests make fewer or less rigorous assumptions. If the normality test concludes that the study data deviate significantly from a Gaussian distribution, rather than applying a less robust nonparametric test, the problem can potentially be remedied by judiciously and openly: (1) performing a data transformation of all the data values; or (2) eliminating any obvious data outlier(s).

---

### Methodological standards for meta-analyses and qualitative systematic reviews of cardiac prevention and treatment studies: a scientific statement from the American Heart Association [^115JMXTS]. Circulation (2017). Low credibility.

Meta-analyses are becoming increasingly popular, especially in the fields of cardiovascular disease prevention and treatment. They are often considered to be a reliable source of evidence for making healthcare decisions. Unfortunately, problems among meta-analyses such as the misapplication and misinterpretation of statistical methods and tests are long-standing and widespread. The purposes of this statement are to review key steps in the development of a meta-analysis and to provide recommendations that will be useful for carrying out meta-analyses and for readers and journal editors, who must interpret the findings and gauge methodological quality. To make the statement practical and accessible, detailed descriptions of statistical methods have been omitted. Based on a survey of cardiovascular meta-analyses, published literature on methodology, expert consultation, and consensus among the writing group, key recommendations are provided. Recommendations reinforce several current practices, including protocol registration; comprehensive search strategies; methods for data extraction and abstraction; methods for identifying, measuring, and dealing with heterogeneity; and statistical methods for pooling results. Other practices should be discontinued, including the use of levels of evidence and evidence hierarchies to gauge the value and impact of different study designs (including meta-analyses) and the use of structured tools to assess the quality of studies to be included in a meta-analysis. We also recommend choosing a pooling model for conventional meta-analyses (fixed effect or random effects) on the basis of clinical and methodological similarities among studies to be included, rather than the results of a test for statistical heterogeneity.

---

### Understanding systematic reviews and meta-analyses [formula: see text] [^1157FyFS]. JPEN: Journal of Parenteral and Enteral Nutrition (2017). Low credibility.

Systematic reviews should be distinguished from narrative reviews. In the latter, an editor asks an expert to sum up all of the information that is known about a particular topic. However, the expert is under no constraints regarding what he or she does, or does not, choose to include in the review. As a result, his or her bias can influence the final message. A systematic review, which may or may not be written by experts, typically asks a narrower question, and then answers it using the entirety of the medical literature. The systematic review process includes computer searches to identify the pertinent literature, a statement of the inclusion and exclusion criteria for identified studies, a list of items of interest to extract from each study, a method to assess the quality of each study, a summary of the evidence that has been found (which may or may not involve attempts to combine data), a discussion of the evidence and the limitations of the conclusions, and suggestions for future research efforts. If the data are combined, that process is called meta-analysis. In meta-analysis, an estimate of the reliability of each study is made, and those that appear to be more reliable are weighed more heavily when the data are combined. While systematic reviews depend on a more preplanned method and thus, unlike narrative reviews, contain sections on method, they can be easily read once the reader becomes familiar with the vocabulary.

---

### Right answers, wrong questions in clinical research [^111bAbyf]. Science Translational Medicine (2014). Low credibility.

To ensure that clinical research arrives at the "right" answers to the right questions for patients, studies should be designed to more closely approximate real-world use of therapeutics and devices.

---

### Systematic evaluation of immune regulation and modulation [^1173KHMy]. Journal for Immunotherapy of Cancer (2017). Low credibility.

Cancer immunotherapies are showing promising clinical results in a variety of malignancies. Monitoring the immune as well as the tumor response following these therapies has led to significant advancements in the field. Moreover, the identification and assessment of both predictive and prognostic biomarkers has become a key component to advancing these therapies. Thus, it is critical to develop systematic approaches to monitor the immune response and to interpret the data obtained from these assays. In order to address these issues and make recommendations to the field, the Society for Immunotherapy of Cancer reconvened the Immune Biomarkers Task Force. As a part of this Task Force, Working Group 3 (WG3) consisting of multidisciplinary experts from industry, academia, and government focused on the systematic assessment of immune regulation and modulation. In this review, the tumor microenvironment, microbiome, bone marrow, and adoptively transferred T cells will be used as examples to discuss the type and timing of sample collection. In addition, potential types of measurements, assays, and analyses will be discussed for each sample. Specifically, these recommendations will focus on the unique collection and assay requirements for the analysis of various samples as well as the high-throughput assays to evaluate potential biomarkers.

---

### How to use a clinical practice guideline [^114HYZyK]. The Journal of Urology (2009). Low credibility.

Purpose

Leading organizations increasingly recognize clinical practice guidelines as an important approach in promoting an evidence-based clinical practice of urology. In light of their considerable clinical, economic and medicolegal impact, guideline users should be confident that the guidelines were rigorously developed and address relevant patient questions. In this article we outline a practical approach for critically appraising a clinical practice guideline.

Materials and Methods

We outline a 3-step approach to the assessment of a clinical practice guideline that answers the questions of whether the recommendations are valid, what the recommendations are and whether they will help in the treatment of an individual patient.

Results

To determine the adequacy of a clinical practice guideline, urologists should carefully review the rigor of the development process and its content. Important questions that relate to the validity of a guideline are whether, for specific questions, all important management options and outcomes were considered, and whether there was an explicit and sensible process to identify, select and combine all relevant evidence. Clinical practice guidelines should formally grade the quality of the available evidence for a given clinical question and outline a formal process of how the recommendations were derived. Value judgments made in the guideline development process about the relative importance of the potential benefits and harms of a given health care intervention should be made transparent to the reader. The recommendations made should be practical and should address important clinical issues. Furthermore, their strength should be graded to reflect the underlying uncertainty about the evidence and the values applied in the guideline development process.

Conclusions

The systematic approach presented in this article will allow urologists to critically appraise clinical practice guidelines. Determining the validity of the recommendations, understanding the recommendations and assessing their applicability to patients are 3 fundamental steps toward an evidence-based approach to using clinical practice guidelines.

---

### 'Real-world' observational studies in arrhythmia research: data sources, methodology, and interpretation. A position document from European Heart Rhythm Association (EHRA), endorsed by Heart Rhythm Society (HRS), Asia-Pacific HRS (APHRS), and Latin America HRS (LAHRS) [^114QJPJ7]. Europace (2020). High credibility.

Observational defibrillator studies — model selection, discrimination metrics, and competing risk are outlined as follows: The statistical methods used should match the clinical question, and if the aim is to estimate survival benefit after 5 years a model addressing prediction is needed, whereas if it is sufficient to know the device does 'something', models providing hazard ratio, rate ratio, or odds ratio may suffice; integrated discrimination improvement (IDI) and net reclassification index (NRI) are not valid and the C-index from a Cox model should also not be used to indicate discriminative improvement at specific times; competing risk has no influence on the calculation of hazard ratios but complicates interpretation, and a significant hazard ratio does not ensure long-term prediction such as 5-year cardiovascular mortality, requiring dedicated prediction analysis when that is the goal.

---

### Statistical evaluation of biomedical studies [^111oMybZ]. Osteoarthritis and Cartilage (2010). Low credibility.

The aim of this chapter is to familiarize the reader with the basic information and common statistical analyses used in medical research. The chapter will aid in deciding what type of analyses best fit the study data and how each analysis differs. The chapter was written to be user-friendly from a medical research and statistical consultant perspective.

---

### The American Society of Addiction Medicine clinical practice guideline development methodology [^1111Jv8t]. Journal of Addiction Medicine (2024). High credibility.

Synthesis of evidence — systematic reviews include a qualitative synthesis that is a narrative description of what the evidence shows at a high level, including similarities and differences across studies, key studies that contribute the most, applicability to different populations or settings, and clarifying what is known and gaps in evidence; some systematic reviews may also include a quantitative synthesis (meta-analysis), and the study team determines whether clinical heterogeneity across studies is too great to conduct a meta-analysis and notes that clinical trials lend themselves more readily to meta-analysis than observational studies (with some exceptions); if there are many meaningful differences across studies a meta-analysis could be misleading, whereas assessing results across studies with some small differences can be an advantage, and the team – usually with a statistician with expertise in meta-analysis – can explore heterogeneity statistically and conduct subgroup and sensitivity analyses and even meta-regression analyses for larger bodies of literature; the systematic review team will also note the certainty of evidence supporting each finding using well-established criteria such as those proposed by the GRADE working group.

---

### A meta-analysis of meta-analyses [^111GoszF]. Fertility and Sterility (2011). Low credibility.

Meta-analyses have become an increasingly popular method of drawing conclusions when there are multiple publications addressing a particular topic. While the statistical calculations are straightforward, many of the decisions regarding the research question, populations of interest, and inclusion/exclusion criteria are subjective and have a significant impact on the conclusions. Readers must understand the objective and subjective components of meta-analysis to properly evaluate these types of analyses.

---

### Analysis of the systematic reviews process in reports of network meta-analyses: methodological systematic review [^113XmHsG]. BMJ (2013). Excellent credibility.

Methods

Search strategy

We systematically searched for all published network meta-analyses in the following databases from their dates of inception: Cochrane Database of Systematic Reviews, Database of Abstracts of Reviews of Effects, Medline, and Embase. Search equations were developed for each database and were based partly on search terms used by Song and colleaguesand Salanti and colleagues(web table 1). The equations were based on specific free text words pertaining to network meta-analyses or overviews of reviews. The date of the last search was 12 July 2012. We also screened the references of methodology papers and reviewsand searched for papers citing landmark statistical articles through the Web of Science or SCOPUS database.

Eligibility criteria

All reports of network meta-analyses comparing the clinical efficacy of three or more interventions based on randomised controlled trials were eligible. We excluded reports of adjusted meta-analyses involving indirect comparisons (that is, an open loop network of three interventions)as well as publications of methodology, editorial style reviews or reports, cost effectiveness reviews, reviews based on individual patient data, and reviews not involving human participants. We also excluded reviews not published in English, French, German, Spanish, and Italian.

Selection of relevant reports of network meta-analyses

Two reviewers (AB, LT) independently selected potentially relevant reports of network meta-analyses on the basis of the title and abstract and, if needed, the full text according to prespecified eligibility criteria. Disagreements were discussed to reach consensus.

Data extraction

A standardised form was used to collect all data from the original reports of network meta-analyses and supplementary appendices when available. We collected all data for epidemiological and descriptive characteristics and those pertaining to the key methodological components of the systematic review process. Two reviewers (AB, RS) independently extracted all data from a random sample of 20% of reports. From all reports, two reviewers (AB, RS) independently extracted data for items that involved subjective interpretation. The corresponding items were questions referring to participants, interventions, comparisons, and outcomes; the type of comparator; how reporting bias was assessed; the publication status of selected randomised controlled trials; and whether statistical assumptions (homogeneity, similarity, and consistency) were mentioned. Disagreements were resolved by discussion.

---

### Common errors in statistics and methods [^114yqoDu]. BMJ Paediatrics Open (2024). High credibility.

Quality assessment of studies

Quality assessment should be performed as a quantitative assessment of the studies included in meta-analysis using a set of predefined safeguards to reduce the risks of bias. Quality assessment tools should be used as they were designed to be used, with any user-defined adaptations clearly justified. Any issues of quality or potential risk of bias, which may impact on meta-analysis results, should be briefly described. Quality 'scores' should only be assigned according to scoring systems defined by the tool, and user-defined scores (ie, user-defined thresholds for high, moderate, low quality, etc) should be avoided. While a study with the best 'score' does not ensure high quality, it ensures that predefined safeguards to reduce risks of bias are met.

Lack of grading the certainty of the main results in meta-analysis

It is common practice to accept the combined results of a meta-analysis at face value and categorise them as significant or non-significant without considering the importance of the findings as well as potential biases and flaws in both the individual studies included in the meta-analysis and the overall estimation (particularly heterogeneity, publication bias or small study bias). To be valuable in clinical practice, the results of a meta-analysis should also be evaluated for certainty and quality using methods such as GRADE.

Incomplete info to understand/reproduce the search results in meta-analysis

This is another illustration of the general rule that your paper should have enough information to allow other researchers to duplicate what you did. Some authors simply list terms (and not search engines or detailed search structure) or do not give dates.

---

### A summary of the methods that the national clinical guideline centre uses to produce clinical guidelines for the national institute for health and clinical excellence [^115WA4DU]. Annals of Internal Medicine (2011). Medium credibility.

NICE preferred study designs by question type — The information scientist searches for evidence appropriate to each review question; for effectiveness of an intervention, a randomized, controlled trial is usually the best study design; questions on the clinical value of a diagnostic test are best answered by randomized, controlled trials, while questions on test accuracy are best answered by cross-sectional studies in which the index test and reference standard are performed in the same patients; questions on prognoses are best answered by prospective cohort studies.

---

### Asking good clinical research questions and choosing the right study design [^1179FGFu]. Injury (2010). Low credibility.

Clinicians and researchers seek answers to clinical research questions, primarily by accessing the results of clinical research studies. This paper moves the focus of research enquiry from getting answers to developing good clinical research questions. Using worked examples, the steps involved in refining questions drawn from various sources to create 'answerable' clinical research questions using the 'PICO' principle are described. Issues to consider in prioritising clinical research questions are also identified. Theoretical and practical considerations involved in choosing the right study design for a clinical research question are then discussed using the worked examples. These include:

---

### Statistics review 1: presenting and summarising data [^112wqdri]. Critical Care (2002). Low credibility.

The present review is the first in an ongoing guide to medical statistics, using specific examples from intensive care. The first step in any analysis is to describe and summarize the data. As well as becoming familiar with the data, this is also an opportunity to look for unusually high or low values (outliers), to check the assumptions required for statistical tests, and to decide the best way to categorize the data if this is necessary. In addition to tables and graphs, summary values are a convenient way to summarize large amounts of information. This review introduces some of these measures. It describes and gives examples of qualitative data (unordered and ordered) and quantitative data (discrete and continuous); how these types of data can be represented figuratively; the two important features of a quantitative dataset (location and variability); the measures of location (mean, median and mode); the measures of variability (range, interquartile range, standard deviation and variance); common distributions of clinical data; and simple transformations of positively skewed data.

---

### Telemedicine for ophthalmology information statement-2018 [^113YP4n7]. AAO (2018). Medium credibility.

Decision analytics and artificial intelligence in telehealth — an emerging area noted includes the use of decision analytics and artificial intelligence for the interpretation of images and other patient care data collected electronically. A complete discussion is outside the scope of this document. Future reports will be needed to describe the validation and introduction of these approaches to clinical care, many of which may move the analysis to the point of care.

---

### Considerations of competing risks analysis in cardio-oncology studies: state-of-the-art review [^111ZWsAr]. JACC: CardioOncology (2022). Medium credibility.

A naive approach commonly used in such scenarios is to censor patient follow-up at the occurrence of Z (receipt of heart transplantation) and use standard survival analysis for censored data (such as Kaplan-Meier [KM] analysis). However, as question 5 indicates, this approach is appropriate only if censoring is independent; that is, time to Z is not related to the time to Y. When this assumption is violated, naive censoring could introduce a selection bias. In our specific example C, patients with VADs who have clinical deterioration related to VAD complications may be prioritized to undergo heart transplantation to prolong survival, and thus censoring caused by transplantation is not independent of hazard of death.

Instead, one could consider 2 alternative approaches depending on the study objective. If the objective is to estimate the probability of death with a VAD before undergoing heart transplantation or the probability of death as the first event before heart transplantation (ie, follow the right arrow from question 6), then receipt of a heart transplant should be treated as a competing risk. Otherwise, if the probability of death (Y) in a hypothetical setting without heart transplantation (Z) (ie, the marginal survival) is of interest (ie, follow the left arrow from question 6), then censoring at the occurrence of Z is appropriate only if more advanced methods to handle dependent censoring are applied. These methods require modeling of the censoring process as a function of the observed covariates followed by inverse probability weighting or multiple imputations to adjust for the bias induced by the dependent censoring. We refer readers to several publications for detailed discussions on the implementation of these methods. Note that this approach is valid only when data on the covariates that determine the censoring process are collected and available for analyses. This highlights the importance of a well-defined study question and careful consideration of analytical approach at the design stage, to ensure that adequate information is collected to conduct valid analyses that match the research objective. If the study objective is to estimate marginal survival but a competing risks analysis is performed instead, then the results are not directly interpretable in addressing the study question of interest.

---

### Understanding the impact of medicaid-serving primary care team functioning and clinical context on cancer care treatment quality: implications for addressing structural inequities [^1171s8XV]. JCO Oncology Practice (2023). Medium credibility.

Data Collection

Data collection occurred during three half-day site visits at each practice between 2019 and 2020. Data included direct observation (except in examination rooms)and in-depth and key informant interviews with clinic members, usually in the context of normal workflow, to document how clinic members discussed and organized care relevant to cancer screening, diagnosis, and treatment follow-up. Each clinic included four to seven semistructured interviews with selected physicians, clinic staff, and administrators. Interviews included questions about the perspectives around and experience of delivering care to Medicaid patients; the team's role in cancer care; and communication with cancer teams. Interviews ran 30–60 minutes in length and were digitally recorded and transcribed verbatim. Qualitative data yielded approximately 280 pages, including fieldnotes from practice observation and key informant interviews with 20 physicians and clinic staff members, and transcripts of 14 in-depth interviews. Interview participants received a $25 in US dollars (USD) gift card. Clinics received a $1,000 (USD) incentive.

Analyses

We used an immersion/crystallization analysis approach, guided by two research questions: (1) How does context, team structure, and care delivery processes shape Medicaid-serving primary care teams' performance contributions to cancer care quality? (2) What key primary care within- and between-team processes affect cancer care quality?, An initial codebook was applied that included a focus on care coordination and team-based care (Appendix Table A 1, online only). After applying the codebook, we organized and connected emerging themes, and consulted the literature to refine our conceptualization of teaming and care coordination. During this phase of interpretation, characterized as a dynamic iterative process, we corroborate with external perspectives and identified the MTS cancer care coordination framework. We then recoded selected initial codes (eg, care coordination, oncology care transition, etc) to highlight the cyclical nature between inputs (MTS structure/inputs), mediators (teamwork and care coordination tasks), and outputs (guideline adherence) using conceptual definition for specific MTS components found in the study by Verhoeven et al. Two teams ([J.T. and J.H.] and [D.M.O. and M.D.]) coded all data using Atlas.ti version 8. We assessed confirming/disconfirming evidence until thematic saturation was reached and resolved discrepancies using small group discussion.

---

### Race and ethnicity in pulmonary function test interpretation: an official American Thoracic Society statement [^115FtntJ]. American Journal of Respiratory and Critical Care Medicine (2023). High credibility.

Race and ethnicity in PFT interpretation — clinical improvement and research directions: We must improve PFT interpretation by taking the patient's medical context and social history into consideration in the clinic, acknowledging challenges including lack of validated approaches, and by emphasizing the value of longitudinal data as trending pulmonary function values over time provides clinical insight. The statement stresses the use of the ratio of FEV1 to FVC to identify obstructive ventilatory defects in chronic obstructive pulmonary disease, asthma, and other airway diseases, because it varies minimally by race or ethnicity. It further aims to move beyond a simple statistical description of normal pulmonary function by examining associations with meaningful health outcomes in which we can intervene, which has the potential to remove the need for reference equations; a larger variety of chronic lung diseases must be included. Thoughtfully collecting race and ethnicity data in research remains important to address disparities in lung health, identify modifiable determinants of reduced pulmonary function including those resulting from structural racism, and increase the diversity of participants in studies that include pulmonary function.

---

### The PRISMA statement for reporting systematic reviews and meta-analyses of studies that evaluate healthcare interventions: explanation and elaboration [^114EJe7U]. BMJ (2009). Excellent credibility.

Reporting study-level data also allows the comparison of the main characteristics of the studies included in the review. Authors should present enough detail to allow readers to make their own judgments about the relevance of included studies. Such information also makes it possible for readers to conduct their own subgroup analyses and interpret subgroups, based on study characteristics.

Authors should avoid, whenever possible, assuming information when it is missing from a study report (such as sample size, method of randomisation). Reviewers may contact the original investigators to try to obtain missing information or confirm the data extracted for the systematic review. If this information is not obtained, this should be noted in the report. If information is imputed, the reader should be told how this was done and for which items. Presenting study-level data makes it possible to clearly identify unpublished information obtained from the original researchers and make it available for the public record.

Typically, study-level characteristics are presented as a table as in the example (table 2). Such presentation ensures that all pertinent items are addressed and that missing or unclear information is clearly indicated. Although paper based journals do not generally allow for the quantity of information available in electronic journals or Cochrane reviews, this should not be accepted as an excuse for omission of important aspects of the methods or results of included studies, since these can, if necessary, be shown on a website.

Following the presentation and description of each included study, as discussed above, reviewers usually provide a narrative summary of the studies. Such a summary provides readers with an overview of the included studies. It may, for example, address the languages of the published papers, years of publication, and geographic origins of the included studies.

The PICOS framework is often helpful in reporting the narrative summary indicating, for example, the clinical characteristics and disease severity of the participants and the main features of the intervention and of the comparison group. For non-pharmacological interventions, it may be helpful to specify for each study the key elements of the intervention received by each group. Full details of the interventions in included studies were reported in only three of 25 systematic reviews relevant to general practice.

---

### 'Real-world' observational studies in arrhythmia research: data sources, methodology, and interpretation. A position document from European Heart Rhythm Association (EHRA), endorsed by Heart Rhythm Society (HRS), Asia-Pacific HRS (APHRS), and Latin America HRS (LAHRS) [^11556r5E]. Europace (2020). High credibility.

Instrumental variable analysis and missing data in observational research are summarized as follows: A good instrument is a variable that affects an outcome and is not affected by confounders, with 'Mendelian randomization' noted as the only common clinical example where genes that influence a factor of interest are used instead of directly addressing the factor; missing data are common and most procedures exclude individuals with missing data, which can bias interpretation if patients are simply removed, and techniques exist to include as much information as possible from cases with missing data.

---

### Basics of statistics for primary care research [^116GMpyf]. Family Medicine and Community Health (2019). Medium credibility.

The purpose of this article is to provide an accessible introduction to foundational statistical procedures and present the steps of data analysis to address research questions and meet standards for scientific rigour. It is aimed at individuals new to research with less familiarity with statistics, or anyone interested in reviewing basic statistics. After examining a brief overview of foundational statistical techniques, for example, differences between descriptive and inferential statistics, the article illustrates 10 steps in conducting statistical analysis with examples of each. The following are the general steps for statistical analysis: (1) formulate a hypothesis, (2) select an appropriate statistical test, (3) conduct a power analysis, (4) prepare data for analysis, (5) start with descriptive statistics, (6) check assumptions of tests, (7) run the analysis, (8) examine the statistical model, (9) report the results and (10) evaluate threats to validity of the statistical analysis. Researchers in family medicine and community health can follow specific steps to ensure a systematic and rigorous analysis.

---

### Institutionalising an evidence-informed approach to guideline development: progress and challenges at the World Health Organization [^115dPTmn]. BMJ Global Health (2018). Medium credibility.

Data analysis

The process for identifying major themes in the qualitative data followed the general five-cycle process described by Yin, which represents an iterative approach involving five phases: (1) compiling, (2) disassembling, (3) reassembling, (4) interpreting and (5) concluding. While each step addresses specific aspects of data analysis, we moved back and forth between phases 2 and 4 as part of continuously revisiting the accuracy of initial coding and interpretation of the data. Compilation consisted of transcribing and further deidentifying each audio recording, including removing specific mentions of names, titles and departments, which may indirectly identify the interviewee. Disassembling consisted of open coding where level 1 codes, including in vivo codes, were assigned to words, phrases and larger fragments of each interview transcript. During this phase, relationships between level 1 codes from different interviews were identified and assigned level 2 codes, thereby facilitating an incremental understanding of the major themes from across the interviews. The reassembling phase consisted of bringing level 1 and level 2 codes together to identify themes representing a central concept and/or message that captured recurring patterns observed across the interviews. The codes and patterns deemed most relevant for the study questions were continuously refined through an iterative process and by using the constant comparison method inspired from grounded theory. We observed that the last five to six interviews only introduced a few new codes and no new major themes, which we used as the indication for reaching data saturation. During the interpretive phase, we used the reassembled data to write a narrative around the study questions, while continuously assessing whether revisiting the disassembling and reassembling phases was needed to recompile the data. Field notes and memos documenting observations and reflections during and after the interviews (eg, whether interviewees emphasised particular aspects or the investigator's initial comparisons of issues raised in the interview with those raised in previous interviews) were considered during all stages of the analysis.

---

### SPIRIT 2013 explanation and elaboration: guidance for protocols of clinical trials [^114Nyegh]. BMJ (2013). Excellent credibility.

Data management

Item 19: Plans for data entry, coding, security, and storage, including any related processes to promote data quality (eg, double data entry; range checks for data values). Reference to where details of data management procedures can be found, if not in the protocol

Explanation

Careful planning of data management with appropriate personnel can help to prevent flaws that compromise data validity. The protocol should provide a full description of the data entry and coding processes, along with measures to promote their quality, or provide key elements and a reference to where full information can be found. These details are particularly important for the primary outcome data. The protocol should also document data security measures to prevent unauthorised access to or loss of participant data, as well as plans for data storage (including timeframe) during and after the trial. This information facilitates an assessment of adherence to applicable standards and regulations.

Differences in data entry methods can affect the trial in terms of data accuracy, cost, and efficiency. For example, when compared with paper case report forms, electronic data capture can reduce the time required for data entry, query resolution, and database release by combining data entry with data collection (Item 18a). When data are collected on paper forms, data entry can be performed locally or at a central site. Local data entry can enable fast correction of missing or inaccurate data, while central data entry facilitates blinding (masking), standardisation, and training of a core group of data entry personnel.

Raw, non-numeric data are usually coded for ease of data storage, review, tabulation, and analysis. It is important to define standard coding practices to reduce errors and observer variation. When data entry and coding are performed by different individuals, it is particularly important that the personnel use unambiguous, standardised terminology and abbreviations to avoid misinterpretation.

As with data collection (Item 18a), standard processes are often implemented to improve the accuracy of data entry and coding. Common examples include double data entry; verification that the data are in the proper format (eg, integer) or within an expected range of values; and independent source document verification of a random subset of data to identify missing or apparently erroneous values. Though widely performed to detect data entry errors, the time and costs of independent double data entry from paper forms need to be weighed against the magnitude of reduction in error rates compared to single-data entry.

---

### The American Society of Addiction Medicine clinical practice guideline development methodology [^116QEZCJ]. Journal of Addiction Medicine (2024). High credibility.

Systematic review — background and processes describes that a quality clinical practice guideline should be based on a systematic review of the literature, which helps ensure that all relevant evidence is identified, critiqued, and synthesized using methods that help reduce the likelihood of bias, and is typically conducted prior to initiating work on the CPG by a group separate from the CPG Writing Group. Systematic reviews use well-established methods to search, identify, appraise, and synthesize the literature examining prespecified key questions of interest; they can contain a qualitative synthesis of evidence, a quantitative synthesis of evidence, or a combination of both; a meta-analysis is the quantitative synthesis of evidence and is based on a systematic review, although not all systematic reviews include a meta-analysis; narrative reviews broadly summarize a topic but do not adhere to strict inclusion and exclusion criteria and search parameters nor use explicit criteria to evaluate quality. Broadly speaking, processes include define the question(s) of interest and methods, conduct literature search, identify studies and other sources of information for inclusion, data abstraction, critical appraisal, and synthesis of evidence.

---

### Research methods: beyond the clinical trial [^113ybrYb]. Annals of Emergency Medicine (2003). Low credibility.

Emergency medicine differs from other specialties in that it has few restrictions in terms of pathology or patient group but is instead defined by time, with patients being selected on the basis of urgency. The important research questions generated by emergency medicine are therefore often complex and relate to organizational, economic, or social issues. Clinical trials have a limited role to play in these circumstances, and concepts such as the hierarchy of evidence might be unhelpful if the best methodology is not appropriate to the research question. Emergency medicine researchers therefore need to be prepared to use a wide range of methods, often in combination and often drawing on the social sciences. This article will introduce readers to methods from clinical epidemiology, operational research, health economics, and qualitative research, discussing the value of these approaches and identifying potential pitfalls.

---

### Systematic reviews and meta-analysis for the surgeon scientist [^117J9U3B]. The British Journal of Surgery (2006). Low credibility.

Background

Understanding of data-reporting methods is imperative for correct interpretation of the medical literature as well as for proper performance of future clinical research. Recent developments in biostatistics have greatly changed the types of statistical analyses used and the minimum quality standards that must be maintained.

Method

Different types of review are described, including systematic review with and without meta-analysis. Minimum reporting standards, sources of bias, both quantitative and qualitative, and references are discussed.

Results and Conclusion

Meta-analysis has become a clearly defined technique, with reporting standards for both randomized controlled trials and observational studies. It is assuming a wider role in the surgical literature. Although many sources of bias exist, there are clear reporting standards and readers should be aware of these when studying the literature.

---

### Methods for implementing and reporting patient-reported outcome (PRO) measures of symptomatic adverse events in cancer clinical trials [^114vN7k9]. Clinical Therapeutics (2016). Low credibility.

Purpose

There is increasing interest to use patient-reported outcome (PRO) measures to evaluate symptomatic adverse events (AEs) in cancer treatment trials. However, there are currently no standard recommended approaches for integrating patient-reported AE measures into trials.

Methods

Approaches are identified from previous trials for selecting AEs for solicited patient reporting, administering patient-reported AE measures, and analyzing and reporting results.

Findings

Approaches for integrating patient-reported AE measures into cancer trials generally combine current standard methods for clinician-reported AEs and established best practices for using PRO measures. Specific AEs can be selected for a PRO questionnaire based on common and expected reactions in a given trial context, derived from literature review and qualitative/mixed-methods evaluations and should be the same set administered across all arms of a trial. A mechanism for collecting unsolicited patient-reported AEs will also ideally be included. Patients will preferably report at baseline and at the end of active treatment as well as on a frequent standardized schedule during active treatment, such as weekly from home, with a recall period corresponding to the frequency of reporting (eg, past 7 days). Less frequent reporting may be considered after an initial intensive monitoring period for trials of prolonged treatments and during long-term follow-up. Electronic PRO data collection is preferred. Backup data collection for missed PRO reports is advisable to boost response rates. Analysis can use a combination of approaches to AE and PRO data. If a high proportion of patients is experiencing baseline symptoms, systematic subtraction of these from on-study AEs should be considered to improve reporting of symptoms related to treatment. More granular longitudinal analyses of individual symptoms can also be useful.

Implications

Methods are evolving for integrating patient-reported symptomatic AEs into cancer trials. These methods are expected to further evolve as more data from trials become available.

---

### 'Real-world' observational studies in arrhythmia research: data sources, methodology, and interpretation. A position document from European Heart Rhythm Association (EHRA), endorsed by Heart Rhythm Society (HRS), Asia-Pacific HRS (APHRS), and Latin America HRS (LAHRS) [^115r3VfG]. Europace (2020). High credibility.

Observational ("real world") studies — EHRA position document scope: The paper provides an overview of sources of data for observational studies, reviews sources of bias and confounding, outlines causal inference techniques that are increasingly used, and reviews statistical modeling with focus on risk versus prediction; the final section provides examples of common problems with reporting observational data.

---

### Clinical bioinformatician body of knowledge-clinical laboratory regulation and data security core: a report of the Association for Molecular Pathology [^1177SYZR]. The Journal of Molecular Diagnostics (2025). High credibility.

Clinical laboratory risk assessments — scope and methods are described as follows: Risk assessment is an essential component of computer system validation, laboratory informatics operations, and general clinical laboratory practice, involving identifying vulnerabilities, hazards, and risks to the system or data, measuring their associated impacts, and taking measures to reduce risks to an acceptable level. Information gathering for identification of potential risks and vulnerabilities can be accomplished via document reviews, expert opinions, checklists, diagramming, structured surveys, medical device security questions (National Electrical Manufacturers Association Manufacturer Disclosure Statement for Medical Device Security), and feedback from stakeholders of the system. The evaluation should be directed toward the intended functionality of the system and should consider the specific setting within which the system is deployed, and ideally, the assessment should comprehensively encompass all aspects of the system's processes before its implementation and should be conducted with the participation of members of the workforce and stakeholders. The identified risks are analyzed using qualitative or quantitative techniques to determine their probability of occurrence, severity, and impact; qualitative risk analysis is a subjective approach based on expert judgement rather than numerical values and may use a risk assessment matrix for prioritization, whereas in contrast, quantitative risk analysis uses verifiable data, quantitative tools, and techniques to objectively report risks in numerical terms.

---

### Structured documentation [^116RVTR2]. Otolaryngologic Clinics of North America (2002). Low credibility.

Structured documentation is a concept-oriented method of capturing clinical information that can be shared readily through automated systems. Standards are evolving for controlled vocabularies that are applicable to various aspects of clinical medicine. Vocabulary expressiveness and the significant preparatory work needed to use structured terms are hindrances to the rapid acceptance of these tools; however, the benefits of increased efficiencies, data sharing, and automated triggers may outweigh the cost of these hindrances. Understanding the basic principles of structured documentation and defining the use of the structured data within a practice allows an organization to select a tool that most closely fits its needs.

---

### Harmonized outcome measures for use in degenerative lumbar spondylolisthesis patient registries and clinical practice [^1117Egyt]. Journal of Neurosurgery: Spine (2021). Medium credibility.

Objective

The development of new treatment approaches for degenerative lumbar spondylolisthesis (DLS) has introduced many questions about comparative effectiveness and long-term outcomes. Patient registries collect robust, longitudinal data that could be combined or aggregated to form a national and potentially international research data infrastructure to address these and other research questions. However, linking data across registries is challenging because registries typically define and capture different outcome measures. Variation in outcome measures occurs in clinical practice and other types of research studies as well, limiting the utility of existing data sources for addressing new research questions. The purpose of this project was to develop a minimum set of patient- and clinician-relevant standardized outcome measures that are feasible for collection in DLS registries and clinical practice.

Methods

Nineteen DLS registries, observational studies, and quality improvement efforts were invited to participate and submit outcome measures. A stakeholder panel was organized that included representatives from medical specialty societies, health systems, government agencies, payers, industries, health information technology organizations, and patient advocacy groups. The panel categorized the measures using the Agency for Healthcare Research and Quality's Outcome Measures Framework (OMF), identified a minimum set of outcome measures, and developed standardized definitions through a consensus-based process.

Results

The panel identified and harmonized 57 outcome measures into a minimum set of 10 core outcome measure areas and 6 supplemental outcome measure areas. The measures are organized into the OMF categories of survival, clinical response, events of interest, patient-reported outcomes, and resource utilization.

Conclusions

This effort identified a minimum set of standardized measures that are relevant to patients and clinicians and appropriate for use in DLS registries, other research efforts, and clinical practice. Collection of these measures across registries and clinical practice is an important step for building research data infrastructure, creating learning healthcare systems, and improving patient management and outcomes in DLS.

---

### The PRISMA statement for reporting systematic reviews and meta-analyses of studies that evaluate healthcare interventions: explanation and elaboration [^1153fuTk]. BMJ (2009). Excellent credibility.

Item 5: Protocol and registration

Indicate if a review protocol exists, if and where it can be accessed (such as a web address), and, if available, provide registration information including the registration number.

Example "Methods of the analysis and inclusion criteria were specified in advance and documented in a protocol".

Explanation A protocol is important because it pre-specifies the objectives and methods of the systematic review. For instance, a protocol specifies outcomes of primary interest, how reviewers will extract information about those outcomes, and methods that reviewers might use to quantitatively summarise the outcome data (see item 13). Having a protocol can help restrict the likelihood of biased post hoc decisions in review methods, such as selective outcome reporting. Several sources provide guidance about elements to include in the protocol for a systematic review. For meta-analyses of individual patient-level data, we advise authors to describe whether a protocol was explicitly designed and whether, when, and how participating collaborators endorsed it.

Authors may modify protocols during the research, and readers should not automatically consider such modifications inappropriate. For example, legitimate modifications may extend the period of searches to include older or newer studies, broaden eligibility criteria that proved too narrow, or add analyses if the primary analyses suggest that additional ones are warranted. Authors should, however, describe the modifications and explain their rationale.

Although worthwhile protocol amendments are common, one must consider the effects that protocol modifications may have on the results of a systematic review, especially if the primary outcome is changed. Bias from selective outcome reporting in randomised trials has been well documented. An examination of 47 Cochrane reviews revealed indirect evidence for possible selective reporting bias for systematic reviews. Almost all (n = 43) contained a major change, such as the addition or deletion of outcomes, between the protocol and the full publication. Whether (or to what extent) the changes reflected bias, however, was not clear. For example, it has been rather common not to describe outcomes that were not presented in any of the included studies.

Registration of a systematic review, typically with a protocol and registration number, is not yet common, but some opportunities exist. Registration may possibly reduce the risk of multiple reviews addressing the same question, reduce publication bias, and provide greater transparency when updating systematic reviews. Of note, a survey of systematic reviews indexed in Medline in November 2004 found that reports of protocol use had increased to about 46%from 8% noted in previous surveys. The improvement was due mostly to Cochrane reviews, which, by requirement, have a published protocol.

---

### Expert panel on integrated guidelines for cardiovascular health and risk reduction in children and adolescents: summary report [^1158NsqT]. PES (2012). Medium credibility.

Quality criteria for assessment of individual randomized controlled trials — criteria needed for "Y" selection: Requirements for data collection and analysis include study protocol components where "Protocol described for all intervention components/regimens studied" and "Description of extra or unplanned treatments"; outcomes measurement where "Primary and secondary outcome(s)/end point(s) described" and "Primary and secondary outcomes(s)/end point(s) relevant to the objective"; adequate duration/follow-up with "Duration of intervention sufficient to detect meaningful effect on primary and secondary outcomes" and "Period of follow-up long enough for important outcome(s) to occur"; and statistical analysis elements including "Statistical analyses described", "Appropriate statistical test used and assumptions of test not violated", "Statistics reported with levels of significance and/or confidence intervals", "Intent-to-treat analysis of outcomes", "Adequate adjustment for effects of confounding factors that might have affected the outcomes", "Results/findings address statistical significance", and "Confidence interval or power calculations reported for null findings". Additional considerations include clinical significance where "Results/findings address clinical significance", discussion of findings where "Findings and implications discussed" and "Biases and study limitations identified, including assessment of how well an intervention was delivered", and adverse events reporting with "Safety outcomes/adverse events specifically reported" and "Appropriate sample size and duration to detect safety outcome(s)". For generalizability, "Study population is appropriate to answer research question" and "Intervention can be feasibly conducted in a general practice/routine/community setting", and outcomes where "Outcome(s)/end point(s) are associated with an increase or decrease in cardiovascular disease risk factor(s) or cardiovascular disease risk during childhood or adulthood" and "Outcome(s)/end point(s) can be feasibly measured in a general practice/routine/community setting"; the table notes " Consider all criteria listed; however, not all criteria must be met for a 'Y'.

---

### Analyzing and interpreting data | Field epi manual… [^11538ARe]. CDC (2025). Medium credibility.

At a glance Chapter 8 of The CDC Field Epidemiology Manual About Field investigations are usually conducted to identify the factors that increased a person's risk for a disease or other health outcome. In certain field investigations, identifying the cause is sufficient; if the cause can be eliminated, the problem is solved. In other investigations, the goal is to quantify the association between exposure and the health outcome to guide interventions or advance knowledge. Both types of field investigations require suitable, but not necessarily sophisticated, analytic methods. This chapter describes the strategy for planning an analysis, methods for conducting the analysis, and guidelines for interpreting the results. An analysis plan includes some or most of the content listed below.

Some of the listed elements are more likely to appear in an analysis plan for a protocol-based planned study, but even an outbreak investigation should include the key components in a more abbreviated analysis plan, or at least in a series of table shells. A more specific question — for example, "Which foods were more likely to have been consumed by case-patients than by controls. " — indicates that key variables will be food items and case–control status and that the analysis method will be a two-by-two table for each food. Analytic strategies. Different types of studies are analyzed with different measures and methods. Therefore, the analysis strategy must be consistent with how the data will be collected. For example, data from a simple retrospective cohort study should be analyzed by calculating and comparing attack rates among exposure groups.

Data from a case–control study must be analyzed by comparing exposures among case-patients and controls, and the data must account for matching in the analysis if matching was used in the design. Components of an Analysis Plan
- List of the research questions or hypotheses
- Source of data
- Description of population or groups
- Source of data or data sets, particularly for secondary data analysis or population denominators
- Type of study
- How data will be manipulated
- Data sets to be used or merged. Analyzing Data from a Field Investigation The first two tables usually generated as part of the analysis of data from a field investigation are those that describe clinical features of the case-patients and present the descriptive epidemiology. Because descriptive epidemiology is addressed in Chapter 6, the remainder of this chapter addresses the analytic epidemiology tools used most commonly in field investigations.

---

### Data analysis… [^111MohEd]. ori.hhs.gov (2003). Low credibility.

Data Analysis is the process of systematically applying statistical and/or logical techniques to describe and illustrate, condense and recap, and evaluate data. According to Shamoo and Resnik various analytic procedures "provide a way of drawing inductive inferences from data and distinguishing the signal from the noise present in the data". While data analysis in qualitative research can include statistical procedures, many times analysis becomes an ongoing iterative process where data is continuously collected and analyzed almost simultaneously. Indeed, researchers generally analyze for patterns in observations through the entire data collection phase. The form of the analysis is determined by the specific qualitative approach taken and the form of the data.

assumptions about the population from which the data are drawn. If one uses unconventional norms, it is crucial to clearly state this is being done, and to show how this new and possibly unaccepted method of analysis is being used, as well as how it differs from other more traditional methods. For example, Schroder, Carey, and Vanable juxtapose their identification of new and powerful data analytic solutions developed to count data in the area of HIV contraction risk with a discussion of the limitations of commonly applied methods. If one uses unconventional norms, it is crucial to clearly state this is being done, and to show how this new and possibly unaccepted method of analysis is being used, as well as how it differs from other more traditional methods.

For example, Schroder, Carey, and Vanable juxtapose their identification of new and powerful data analytic solutions developed to count data in the area of HIV contraction risk with a discussion of the limitations of commonly applied methods. Determining significance While the conventional practice is to establish a standard of acceptability for statistical significance, with certain disciplines, it may also be appropriate to discuss whether attaining statistical significance has a true practical meaning, i.e., 'clinical significance'. Jeans defines 'clinical significance' as "the potential for research findings to make a real and important difference to clients or clinical practice, to health status or to any other problem identified as a relevant priority for the discipline".

---

### Step 5-generate and support conclusions… [^117SHF2U]. CDC (2025). Medium credibility.

Overview and Importance Step 5 involves generating evaluation conclusions, which are answers to the evaluation questions supported by the data collected in Step 4. This step involves the following:

- Reviewing the evidence expectations identified previously
- Planning data analysis
- Conducting data analysis
- Interpreting the analytic findings
- Forming recommendations. Planning Data Analysis A data analysis plan includes how and by whom the data would be organized, analyzed, and synthesized. A plan that is feasible, appropriate, and aligns to the evaluation questions and data collection methods identified in earlier steps is more likely to produce findings that will be used by the interest holders. Evaluators will want to collaboratively engage interest holders to obtain agreement on the techniques for analysis and interpreting findings before data collection begins to promote transparency.

Conducting Data Analysis Common steps for quantitative and qualitative analysis are described below. Common key steps for quantitative data analysis:

- Clean and pre-process raw data for accuracy and consistency. Check for missing data, standardize data formats, and make any other corrections needed.
- Import data into a statistical software program such as SAS, STATA, SPSS, R, or Excel.
- Analyze data based on the indicators defined in Step 4.
- Calculate descriptive statistics to describe, summarize, and compare key characteristics about the data. For some indicators, data will need to be stratified or grouped based on variables of interest first.
- Explore distributions and variations within the data. Analyze trends, patterns, and relationships within the data.
- Transcribe recordings from interviews or focus groups and/or enter narrative comments from surveys or field notes into a word processing or qualitative analysis software such as nVivo, Excel, MAXQDA, or ATLAS. ti.
- Review and annotate the data to understand and familiarize yourself with the content. This could involve reading interview transcripts or listening to recordings.
- Organize the data for instance, by date, by data collection type, or by question asked.
- Code the text to identify and label key themes that correspond to your evaluation questions.
- Group text by key themes.
- Review the themes and codes for refinement and to identify any sub-themes that emerge.
- Draw conclusions from the findings.
- More advanced qualitative analysis techniques include the use of multiple coders, calculation of interrater reliability, and within or between-case analysis.

---

### Designing and conducting analytic studies in the field… [^114xJPKs]. CDC (2025). Medium credibility.

- Conduct a small number of in-depth, open-ended interviews. When a likely source is not quickly evident, conducting in-depth, open-ended interviews with a subset of patients or their caregivers can be the best way to identify possible sources. It helps to begin with a semistructured list of questions designed to help the patient recall the events and exposures of every day during the incubation period. In concept, a cohort study, like an experimental study, begins with a group of persons without the disease under study, but with different exposure experiences, and follows them over time to find out whether they experience the disease or health condition of interest. However, in a cohort study, each person's exposure is merely recorded rather than assigned randomly by the investigator. Then the occurrence of disease among persons with different exposures is compared to assess whether the exposures are associated with increased risk for disease.

Cohort studies can be prospective or retrospective. Observational Study Type 2: Case–Control In a case–control study, the investigator must identify a comparison group of control persons who have had similar opportunities for exposure as the case-patients. Case–control studies are commonly performed in field epidemiology when a cohort study is impractical. Table 7. 1 Observational Study Type 3: Case–Case In case–case studies, a group of patients with the same or similar disease serve as a comparison group.
- Thoroughly reflect the source population from which case-patients arose, and
- Provide a good estimate of the level of exposure one would expect from that population. Sometimes the source population is not so obvious, and a case–control study using controls from the general population might be needed to implicate a general exposure.

The investigation can then focus on specific exposures among persons with the general exposure. Conclusion Cohort, case–control, and case–case studies are the types of analytic studies that field epidemiologists use most often. They are best used as mechanisms for evaluating — quantifying and testing — hypotheses identified in earlier phases of the investigation. Cohort studies, which are oriented conceptually from exposure to disease, are appropriate in settings in which an entire population is well-defined and available for enrollment.