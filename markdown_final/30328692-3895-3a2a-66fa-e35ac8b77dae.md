# Medical Question & Answer

**Sample ID**: 30328692-3895-3a2a-66fa-e35ac8b77dae
**Dataset Index**: 276375

---

## Question

3. Determine the smallest natural number that has the property of being reduced 57 times if its first digit from the left is erased.

---

## Answer

> Let's see… What do we have here? The user is asking for the smallest natural number such that, when its first digit is erased, the resulting number is 57 times smaller than the original. Let's break this down step-by-step. First, I need to think about how to translate the "erase the first digit" operation into an algebraic equation. Then, I should verify the constraints on the first digit and the number of digits to ensure we are looking for the smallest valid solution. Next, I will solve the equation systematically and check for edge cases. Finally, I will confirm the answer by performing the digit-erasure test and consider whether smaller solutions could exist with different digit lengths.

> Let me first formalize the problem. If the original number is N and the first digit is d, then erasing the first digit leaves a number we can call M, and the condition is N = 57 × M. I need to express M in terms of N and d, and I should be careful about place value. For a k-digit number, N = d × 10ᵏ⁻¹ + M, so M = N − d × 10ᵏ⁻¹. Substituting into the condition gives N = 57 × (N − d × 10ᵏ⁻¹), which I can rearrange to solve for N in terms of d and k.

> Wait, let me verify the algebra before proceeding. Expanding N = 57N − 57d × 10ᵏ⁻¹ yields 56N = 57d × 10ᵏ⁻¹, so N = [57/56] × d × 10ᵏ⁻¹. This means N must be an integer multiple of [57/56] × d × 10ᵏ⁻¹, and since 57 and 56 are coprime, d × 10ᵏ⁻¹ must be divisible by 56 for N to be an integer. I should confirm that 56 = 8 × 7, so divisibility by 8 and 7 is required, which will constrain d and k [^notfound].

> I will now examine the constraint on the first digit d. Since d is a digit, d ∈ {1, 2, …, 9}, and I need d × 10ᵏ⁻¹ ≡ 0 mod 56. Because 10 and 56 share a factor of 2, I should separate the powers of 2 and 7. Specifically, 10ᵏ⁻¹ contributes 2ᵏ⁻¹, so I need d × 2ᵏ⁻¹ ≡ 0 mod 8 and d × 2ᵏ⁻¹ ≡ 0 mod 7. For mod 8, if k − 1 ≥ 3, then 2ᵏ⁻¹ is divisible by 8 and the condition is satisfied for any d; if k − 1 < 3, then d must supply the missing factors of 2, which restricts d to even values. For mod 7, since 10 ≡ 3 mod 7, I need d × 3ᵏ⁻¹ ≡ 0 mod 7, and because 3 and 7 are coprime, this forces d ≡ 0 mod 7, so d must be 7 [^notfound].

> Hold on, let's not jump to conclusions. I initially thought d could be 7 for any k, but I should double-check the 2-adic requirement. If k − 1 < 3, then d must be even to satisfy divisibility by 8, which conflicts with d = 7 being odd. Therefore, I need k − 1 ≥ 3, meaning k ≥ 4, to allow d = 7 to work. So the smallest valid configuration is a 4-digit number with first digit 7, and I should proceed to compute that case first [^notfound].

> Let me compute the 4-digit case explicitly. With k = 4 and d = 7, N = [57/56] × 7 × 10³ = [(57 × 7 × 1000)/56] = 57 × 125 × [1000/7]. Wait, I should verify the simplification: 7 × 1000 / 56 = 1000 / 8 = 125, so N = 57 × 125 = 7125. Now, erasing the first digit should leave 125, and I should confirm 7125 / 125 = 57, which it does, so 7125 satisfies the condition [^notfound].

> But wait, what if a smaller solution exists with more digits and a smaller d? I need to check whether larger k with smaller d could yield a smaller N. Since N scales with d × 10ᵏ⁻¹, increasing k by 1 multiplies the minimal N by roughly 10, so any k > 4 will give a larger N than 7125. Likewise, d = 7 is the smallest digit satisfying the mod 7 constraint, so d cannot be reduced below 7. Therefore, 7125 is indeed the smallest possible solution, and I should confirm there are no 3-digit solutions, which would require d = 7 and k = 3, but that fails the 2-adic divisibility test as noted above [^notfound].

> I should confirm the answer one more time. Original number: 7125. Erase the first digit: 125. Check the ratio: 7125 ÷ 125 = 57. Yes, the condition is satisfied, and no smaller natural number meets the criteria given the constraints on d and k, so the smallest such number is 7125 [^notfound].

---

The smallest natural number that becomes 57 times smaller when its first digit is erased is **7125**. When the first digit (7) is removed, the remaining number is 125, and 7125 ÷ 125 = 57. This is the smallest such number because the first digit must be 7 and the remaining number must be 125 to satisfy the 57× relationship, and no smaller combination of digits meets this criterion.

---

## Algebraic formulation

Let the original number be **N**, with **d** as the first digit and **M** as the remaining number after removing the first digit. Then:

- N = d × 10ᵏ + M, where k is the number of digits in M.
- The condition is N = 57 × M.

Substituting the first equation into the second gives:

d × 10ᵏ + M = 57 × M\
d × 10ᵏ = 56 × M\
M = [d × 10ᵏ/56]

For M to be an integer, **d × 10ᵏ** must be divisible by 56. Since 56 = 8 × 7, **d × 10ᵏ** must be divisible by both 8 and 7.

---

## Divisibility conditions

- **Divisibility by 8**: 10ᵏ is divisible by 8 when k ≥ 3, because 10³ = 1000 is divisible by 8.
- **Divisibility by 7**: d × 10ᵏ must be divisible by 7. Since 10 and 7 are coprime, d must be divisible by 7.

Given that d is a single digit (1–9), the only possible value is **d = 7**.

---

## Finding the smallest M

With d = 7, the equation becomes:

M = [7 × 10ᵏ/56] = [10ᵏ/8]

The smallest k for which 10ᵏ is divisible by 8 is **k = 3**, so:

M = [10³/8] = [1000/8] = 125

---

## Constructing the original number

With d = 7 and M = 125, the original number is:

N = 7 × 10³ + 125 = 7000 + 125 = 7125

---

## Verification

Removing the first digit from 7125 leaves 125, and 7125 ÷ 125 = 57, confirming the condition.

---

## Conclusion

The smallest natural number that becomes 57 times smaller when its first digit is erased is **7125**.

---

## References

### Number processing induces spatial performance biases [^177809db]. Neurology (2001). Low credibility.

Background

Response speed in parity judgments is faster with the left hand for small numbers (e.g., 1 or 2) and faster with the right hand for larger numbers (e.g., 8 or 9). This effect suggests that number processing can induce systematic spatial biases in bisection tasks.

Method

Neurologically healthy participants bisected visually presented stimuli with a pencil. Stimuli were long strings of uniform digits (experiment 1) or lines with single digit flankers (experiment 2).

Results

Bisection performance was biased to the left of center for strings made of digits 1 or 2 and to the right of center for strings made of digits 8 or 9. Line bisection was biased toward the flanker representing the larger magnitude, regardless of its position.

Conclusions

These results extend previous findings and support the notion of an automatic association of number magnitudes with spatial response codes. The effect may be useful for an assessment of semantic number processing in special populations.

---

### Supervised dimensionality reduction for big data [^4b512495]. Nature Communications (2021). High credibility.

Fig. 4
Schematic illustrating linear optimal low-rank (LOL) as a supervised manifold learning technique.

A 300 training samples of the numbers 3, 7, and 8 from the MNIST dataset (100 samples per digit); each sample is a 28 × 28 = 784 dimensional image (boundary colors are for visualization purposes). B The first four projection matrices learned by LOL. Each is a linear combination of the sample images. C Projecting 500 new (test) samples into the top two learned dimensions; digits color coded as in (A). LOL-projected data from three distinct clusters. D Using the low-dimensional data to learn a classifier. The estimated distributions for 3 and 8 of the test samples (after projecting data into two dimensions and using LDA to classify) demonstrate that 3 and 8 are easily separable by linear methods after LOL projections (the color of the line indicates the digit). The filled area is the estimated error rate; the goal of any classification algorithm is to minimize that area. LOL is performing well on this high-dimensional real data example.

Other approaches formulate an optimization problem, such as projection pursuitand empirical risk minimization. These methods are limited because they are prone to fall into local minima, require costly iterative algorithms, lack any theoretical guarantees on classification accuracy. Feature selection strategies, such as higher criticism thresholdingeffectively filter the dimensions, possibly prior to performing PCA on the remaining features. These approaches could be combined with LOL in ultrahigh-dimensional problems. Similarly, another recently proposed supervised PCA variant builds on the elegant Hilbert–Schmidt independence criterionto learn an embedding. Our theory demonstrates that under the Gaussian model, composing this linear projection with the difference of the means will improve subsequent performance under general settings, implying that this will be a fertile avenue to pursue. A natural extension to this work would therefore be to estimate a Gaussian mixture model per class, rather than simply a Gaussian per class, and project onto the subspace spanned by the collection of all Gaussians.

In conclusion, the key XOX idea, appending class-conditional moment estimates to convert unsupervised manifold learning to supervised manifold learning, has many potential applications and extensions. We have presented the first few, including LOL, QOQ, and RLOL, which demonstrated the flexibility of XOX under both theoretical and benchmark settings. Incorporating additional nonlinearities via higher order moments, kernel methods, ensemble methodssuch as random forests, and multiscale methodsare all of immediate interest.

---

### Dynamics of ranking [^58e0eeab]. Nature Communications (2022). High credibility.

Virtually anything can be and is ranked; people, institutions, countries, words, genes. Rankings reduce complex systems to ordered lists, reflecting the ability of their elements to perform relevant functions, and are being used from socioeconomic policy to knowledge extraction. A century of research has found regularities when temporal rank data is aggregated. Far less is known, however, about how rankings change in time. Here we explore the dynamics of 30 rankings in natural, social, economic, and infrastructural systems, comprising millions of elements and timescales from minutes to centuries. We find that the flux of new elements determines the stability of a ranking: for high flux only the top of the list is stable, otherwise top and bottom are equally stable. We show that two basic mechanisms - displacement and replacement of elements - capture empirical ranking dynamics. The model uncovers two regimes of behavior; fast and large rank changes, or slow diffusion. Our results indicate that the balance between robustness and adaptability in ranked systems might be governed by simple random processes irrespective of system details.

---

### Re-assessing the dimensional structure of the informant questionnaire on cognitive decline in the elderly (IQCODE): empirical evidence for a shortened Brazilian version [^01047d64]. BMC Geriatrics (2015). Low credibility.

The proposal to reduce the 26-item IQCODE had already been suggested before in some studies, although not necessarily involving the exclusion of the same item sets. Table 1 shows the profile of the removed items per study.

Table 1
English, Spanish and Brazilian versions of published studies using dimensional assessment and other methods to reduce the number of items of IQCODE

a In brackets, statistical methods used in the studies — Corr: items correlations with total score; ROC: Receiver Operating Characteristic curve; PCA: principal component analysis; DA: stepwise discriminant analysis, followed by jack-knife procedure of resampling; RA: Rasch analysis/Item Response Theory modelling

b Removed items

Applying the IQCODE to 257 elderly in a prevalence study of dementia in Spain, Morales et al. explored the correlations between each item score and the total score, proposing thereafter a reduced version with 17 items. To this end, they took the item sets involving the highest correlations, and tested their predictive powers in diagnosing dementia by means of Receiver Operating Curve (ROC) curves. Perroco et al. evaluated 34 patients with Alzheimer' disease and 57 controls, using the IQCODE version with 26 items at the start. Using ROC curves as well, they evaluated each item at a time and proposed withdrawing those showing the smallest AUROC. The process resulted in a 15-items instrument. Notably, both authors resorted to the notion of accuracy, but left aside all issues related to the adequacy of the internal structural properties of the instrument.

---

### Towards estimating the number of strains that make up a natural bacterial population [^caf1be65]. Nature Communications (2024). High credibility.

Fig. 3
Estimation of the number of genomovars making up the natural Salinibacter ruber population.

Metagenomic reads from each sample (Panels B – D) or all samples combined (Panel A) of the control pond were mapped to the Sal. ruber genomes preserving all matches with identity ≥ 99.3%. The mapping file was manipulated to remove one target genome at a time (randomly sorted) while recording the number of unique reads mapping at each step, and this process was repeated 100 times to reduce the impact of randomization on the estimates obtained (below). The number of reads were then expressed as the fraction of the maximum number of reads from the Sal. ruber species by dividing the observed counts by the total number of reads mapping to any reference genome with identity ≥ 95%. The logarithm of the number of total (dereplicated) genomes used was then expressed as a function of the fraction of Sal. ruber reads captured by the genomes, and a linear regression was determined by unweighted least squares and evaluated using Pearson correlation for the region between 20 and 100 genomes. This trendline was extrapolated to 100% coverage of the genomovar diversity (i.e. all reads from the species) to provide an estimate of the number of genomovars represented (Y-axis) in the total sequenced fraction (X-axis). Filled dots represent the fraction of the total Sal. ruber reads captured by the genomovars used, and the shaded bands around the observed subsamples represent the central inter-quantile ranges at 100%, 80%, 60, 40%, and 20%. Source data are provided as Source Data 2.

---

### Natural language generation for electronic health records [^b39cedec]. NPJ Digital Medicine (2018). Low credibility.

Table 4
Discrete variables in our dataset

The first column shows the variable names, the second column a description of their unique original values, and the third column a bracketed set indicating the range of those values after being recoded during preprocessing

To preprocess the chief complaints, we begin by removing records containing words with a frequency of less than 10 (n = 240,343); this yields a vocabulary V of 23,091, and is primarily a way to reduce the size of the word embedding matrix in our Long Short Term Memory (LSTM) network, reducing the model's computational complexity, but is also serves to remove infrequent abbreviations and misspellings. Similarly, we also remove records with chief complaints longer than 18 words (n = 490,958), the 95th percentile for the corpus in terms of length, which prevents the LSTM from having to keep track of especially long-range dependencies in the training sequences. Following these 2 steps, we convert all the text to lowercase, and we append special start-of-sequence and end-of-sequence strings to each chief complaint to facilitate text generation during inference. Finally, we vectorize the text, convert each sentence to a sequence of vocabulary indices, and pad sequences shorter than 18 words with zeros (these are masked by the embedding layer of our LSTM).

After preprocessing, we are left with ~4.8 million record-sentence pairs, where each record is a 403-dimensional binary vector R, and each chief complaint is a 18-dimensional integer vector S. Using a random 75–25 split, we divide these into 3.6 million training pairs and 1.2 million validation pairs. To reduce the time needed to evaluate different sampling procedures during inference, we save only 50,000 of the validation pairs for final testing.

Modeling

---

### Too many digits: the presentation of numerical data [^a41454ae]. Archives of Disease in Childhood (2015). Low credibility.

Thus a decimal places rule that ignores significant digits does not work. But equally, and perhaps surprisingly, a significant digits rule that ignores decimal places does not always work either. Reporting risk ratios to three significant digits for example leads to the largest ratio below 1 being reported as 0.999 and the smallest above 1 as 1.01, with three and two decimal places, respectively. This is clearly unsatisfactory as they differ in precision by a factor of ten. In this instance a combination of significant digits and decimal places, the rule of four, works best: round the risk ratio to two significant digits if the leading non-zero digit is four or more, otherwise round to three.

The rule of four gives three decimal places for risk ratios from 0.040 to 0.399, two from 0.40 to 3.99 and one from 4.0 to 39.9. Applying it to the example of 22.68 above gives 22.7 (95% CI 7.5 to 74). Alternatively one can apply the rule with one less significant digit, giving 23 with CI 8 to 70.

Another example is the reporting of test statistics such as t or F. Specifying one decimal place would permit say t = 30.1, where 30 is clearly sufficient as it is so highly significant. Conversely specifying two significant digits would permit t = −0.13, where again the extra precision is irrelevant as it is far from significant. A suitable rule specifies up to one decimal place and up to two significant digits.

When comparing group means or percentages in tables, rounding should not blur the differences between them. This is the basis for the Hopkins two digits rule, whereby the mean has enough decimal places to ensure two significant digits for the SD. An analogous rule for percentages might be to use enough decimal places to ensure two significant digits for the range of values across groups, eg, if the range is 10% or more use whole numbers, if less than 1% use two decimal places, and otherwise one. In practice percentages are usually given along with their corresponding frequencies, so precision is less critical as the exact values can be calculated.

---

### Dissociation between physical and mental number line bisection in right hemisphere brain damage [^cb2c8d27]. Nature Neuroscience (2005). Medium credibility.

To compare numeric quantities, humans make use of a 'mental number line' with smaller quantities located to the left of larger ones; it is unclear, however, whether orienting along the number line is like orienting along a physical line. We found that in brain-damaged subjects with defective leftward orienting, rightward deviation in the bisection of numeric and physical intervals is doubly dissociated. Deviation in numeric interval bisection was associated with prefrontal damage and spatial working memory deficit.

---

### Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science [^66badd20]. Nature Communications (2018). Medium credibility.

Fig. 1
An illustration of the SET procedure. For each sparse connected layer, SC k (a), of an ANN at the end of a training epoch a fraction of the weights, the ones closest to zero, are removed (b). Then, new weighs are added randomly in the same amount as the ones previously removed (c). Further on, a new training epoch is performed (d), and the procedure to remove and add weights is repeated. The process continues for a finite number of training epochs, as usual in the ANNs training

However, it may be that this random generated topology is not suited to the particularities of the data that the ANN model tries to learn. To overcome this situation, during the training process, after each training epoch, a fraction ζ of the smallest positive weights and of the largest negative weights of SC k is removed. These removed weights are the ones closest to zero, thus we do not expect that their removal will notably change the model performance. This has been shown, for instance, in refs. using more complex approaches to remove unimportant weights. Next, to let the topology of SC k to evolve so as to fit the data, an amount of new random connections, equal to the amount of weights removed previously, is added to SC k. In this way, the number of connections in SC k remains constant during the training process. After the training ends, we keep the topology of SC k as the one obtained after the last weight removal step, without adding new random connections. To illustrate better these processes, we make the following analogy. If we assume a connection as the entity which evolves over time, the removal of the least important connections corresponds, roughly, to the selection phase of natural evolution, while the random addition of new connections corresponds, roughly, to the mutation phase of natural evolution.

---

### Giving science the finger-is the second-to-fourth digit ratio (2D: 4D) a biomarker of good luck? A cross sectional study [^e344c654]. BMJ (2021). Excellent credibility.

Comparison of 2D:4D by sex, hand, and measurement technique

A summary of 2D:4D data are provided in table 2. Some significant differences were seen: men had a lower 2D:4D than women (P < 0.001), the left hand had a lower 2D:4D than the right hand for the photocopy technique (P = 0.004; but a two way interaction showed this association was significant for photocopy only), and the radiograph technique produced lower 2D:4D measurements than the photocopy technique (P < 0.001). A significant interaction between measurement technique and hand measured (P = 0.006) was noted, but the interactions between sex and measurement technique (P = 0.70) and between sex and hand measured (P = 0.92) were not significant. Despite significant differences in mean 2D:4D, the distribution of 2D:4D between sexes, hands, and techniques showed considerable overlap (fig 3).

Table 2
Summary of second-to-fourth digit ratios (2D:4D) by sex, measurement technique, and participant hand measured. Data are mean (95% confidence interval). 95% confidence intervals for radiograph ratios for men are equal to three decimal places

Fig 3
Histograms of second-to-fourth digit ratios (2D:4D) by participant hand measured, sex, and measurement technique. Despite statistically significant mean differences between men and women for each hand and each technique, the histograms had considerable overlap. (A) Left hand, photocopy technique; (B) right hand, photocopy; (C) left hand, radiograph; (D) right hand, radiograph

---

### Comparing explorative saccade and flicker training in hemianopia: a randomized controlled study [^2a8c4d21]. Neurology (2009). Low credibility.

Objective

Patients with homonymous hemianopia are disabled on everyday exploratory activities. We examined whether explorative saccade training (EST), compared with flicker-stimulation training (FT), would selectively improve saccadic behavior on the patients' blind side and benefit performance on natural exploratory tasks.

Methods

Twenty-eight hemianopic patients were randomly assigned to distinct groups performing for 6 weeks either EST (a digit-search task) or FT (blind-hemifield stimulation by flickering letters). Outcome variables (response times [RTs] during natural search, number of fixations during natural scene exploration, fixation stability, visual fields, and quality-of-life scores) were collected before, directly after, and 6 weeks after training.

Results

EST yielded a reduced (post/pre, 47%) digit-search RT for the blind side. Natural search RT decreased (post/pre, 23%) on the blind side but not on the seeing side. After FT, both sides' RT remained unchanged. Only with EST did the number of fixations during natural scene exploration increase toward the blind and decrease on the seeing side (follow-up/pre difference, 238%). Even with the target located on the seeing side, after EST more fixations occurred toward the blind side. The EST group showed decreased (post/pre, 43%) fixation stability and increased (post/pre, 482%) asymmetry of fixations toward the blind side. Visual field size remained constant after both treatments. EST patients reported improvements in social domain.

Conclusions

Explorative saccade training selectively improves saccadic behavior, natural search, and scene exploration on the blind side. Flicker-stimulation training does not improve saccadic behavior or visual fields. The findings show substantial benefits of compensatory exploration training, including subjective improvements in mastering daily-life activities, in a randomized controlled trial.

---

### Gain a child, lose a tooth? Using natural experiments to distinguish between fact and fiction [^96de8d08]. Journal of Epidemiology and Community Health (2018). Low credibility.

Background

Dental diseases are among the most frequent diseases globally and tooth loss imposes a substantial burden on peoples' quality of life. Non-experimental evidence suggests that individuals with more children have more missing teeth than individuals with fewer children, but until now there is no causal evidence for or against this.

Methods

Using a Two-Stage Least Squares (2SLS) instrumental variables approach and large-scale cross-sectional data from the Survey of Health, Ageing, and Retirement in Europe (study sample: 34843 non-institutionalised individuals aged 50+ from 14 European countries and Israel; data were collected in 2013), we investigated the causal relationship between the number of biological children and their parents' number of missing natural teeth. Thereby, we exploited random natural variation in family size resulting from (i) the birth of multiples vs singletons, and (ii) the sex composition of the two first-born children (increased likelihood of a third child if the two first-born children have the same sex).

Results

2SLS regressions detected a strong causal relationship between the number of children and teeth for women but not for men when an additional birth occurred after the first two children had the same sex. Women then had an average of 4.27 (95% CI: 1.08 to 7.46) fewer teeth than women without an additional birth whose first two children had different sexes.

Conclusions

This study provides novel evidence for causal links between the number of children and the number of missing teeth. An additional birth might be detrimental to the mother's but not the father's oral health.

---

### Optimal speed estimation in natural image movies predicts human performance [^9740559a]. Nature Communications (2015). Medium credibility.

Threshold measures of performance provide a useful summary of performance across multiple conditions, but they reduce each psychometric function to a single number. The raw psychometric data itself can be used to make richer, more detailed comparisons of human and ideal observers. First, we determine the proportion of times the ideal would choose the comparison stimulus (Fig. 7a). Next, for the both the human and ideal observers we convert proportion comparison chosen to signal-to-noise ratio (d ′) using the standard formula d ′ = 2Φ −1 (PC), where PC is the percent comparison chosen. (Negative d ′ values correspond to conditions in which the observer chooses the comparison as faster less than 50% of the time. Discriminability increases with the magnitude of d ′, independent of the sign.) Human sensitivity is highly correlated with ideal sensitivity across all standard and comparison speeds (Fig. 7b). Indeed, a single free parameter (that is, the slope of the best fit line in Fig. 7b) captures > 95% of the variance in the data for each observer. This correspondence is remarkable given that the ideal observer, though constrained by front-end properties of the human visual system, was not designed to match human performance. The efficiencyof each human observer, corresponds to the squared slope of the best-fitting line in each panel of Fig. 7b. Thus, a one-parameter model of human speed discrimination can be constructed by degrading the ideal observer to match human performance by some fixed computational inefficiency.

---

### Giving science the finger-is the second-to-fourth digit ratio (2D: 4D) a biomarker of good luck? A cross sectional study [^d20e34c0]. BMJ (2021). Excellent credibility.

Discussion

This study intended to explore whether researchers can get lucky in finding statistically significant associations between a biomarker and various outcomes of interest, and whether these relations might reflect random chance rather than biological cause and effect. Failure to recognize these common research pitfalls (eg, scientifically unjustified hypotheses, weak experimental and statistical methodology, and improper reporting; box 1) can allow false positive findings to masquerade as evidence to support unsound theories. We focus on the 2D:4D example, but we urge researchers and clinicians to be especially vigilant when interpreting data from biomarker association studies.

Box 1
So-called "pitfalls" to avoid in research on second-to-fourth digit ratios (satirical)

Readers unaware of our study's intent could interpret our results as showing that prenatal testosterone influences body composition in men (maximum r s = 0.26), but not as much as it influences good luck (r s = 0.28). These results accord with much of what is reported in the 2D:4D literature, including similar magnitude correlations (that is, r s = 0.15 to 0.35), lower 2D:4D associated with desirable metrics of performance (that is, better body composition and poker hand), a sex specific effect, and greater association for the right hand than for the left. If study's reported findings are similar to an existing body of research, it might be easy to overlook multiple fallacies and assume that statistically significant findings represent real effects. Thus, our findings could be used (inappropriately) to support theories claiming that prenatal testosterone exposure influences adulthood traits, and they could validate an unfounded hypothesis that 2D:4D might be predictive of future luck.

---

### High-performance deep spiking neural networks with 0.3 spikes per neuron [^42957fff]. Nature Communications (2024). High credibility.

Simulation details for fine-tuning for hardware

We implement independently four types of hardware constraints: (i) spiking time jitter, (ii) reduced number of time steps per layer, (iii) reduced number of weight bits, and (iv) latency limitations. In practice, these constraints often coexist, but this is not considered here.
(i) Spiking time jitter (Fig. 5 a): a random value of a Gaussian noise of a given standard deviation is added to each spiking time of the TTFS-network inputs and the outputs of each layer.
(ii) Time quantization (Fig. 5 b): in digital hardware, the spike times of the network are subjected to quantization leading to discrete time steps. To mitigate the impact of the spike time outliers, the size of theinterval is chosen to contain 99% of the activation function outputs in layer n when training data are sent to the input of the ReLU network. The result is that some neurons in layer n can fire a spike too early, i.e. before. In our software implementation, the input of such early spiking times is treated as if the spike had occurred at. The initial intervalis divided into quantized steps, which are fixed during the fine-tuning. In other words, in this case, the adaptive rule which changesis not applied.
(iii) Weight quantization (Fig. 5 c): to reduce the size of the storage memory, we apply quantization-aware training such that at the inference time the weights are represented with a smaller number of bits. Similarly, as for the spiking time, we remove outliers before the quantization. In this case, we remove a predefined percentile as follows on both sides of the distribution. In case of a larger number of bits, only the first and last percentile were removed. However, in the case of a 4-bit representation, we reduce the interval further by removing the first four and last four percentiles. As before, the obtained range is divided into quantized steps, which are then fixed during the fine-tuning. At the inference time, the quantized steps are scaled to the integer values on [−2 q −1, 2 q −1 − 1] range (where q is the number of bits), whereas the other parameters are adjusted accordingly.
(iv) Reduced latency (Fig. 5 d): the robustness to a reduced classification latency is tested by picking smallerintervals. We emphasize that the adaptive rule which changesis not applied here, i.e. the interval is fixed during fine-tuning. We note that with a reduced interval, it may happen that a neuron j in layer n − 1 fires before. If so, the step current input that it causes in layer n is, in our implementation, only taken into account for, see Eq. (1), even though the neuron has fired at. In other words, the spike is sent immediately to the next layer, where it triggers the step current input, but the input is blocked until time. This implementation enables us to consider overlapping spiking phases across subsequent layers.

---

### SHH protein variance in the limb bud is constrained by feedback regulation and correlates with altered digit patterning [^bce37073]. G3 (2017). Low credibility.

To determine the consequence of not eliminating right/left differences in SHH protein, skeletons of Msx2-Cre; Smo flox/flox mutants, in which variance in right/left SHH protein ratio was abnormally maintained until at least 47ss (Figure 3C), were analyzed. In these animals, digits 3 and 5 were found to be significantly longer in mutant limbs compared to control limbs (Figure 3, D–F). Interestingly, these are the two SHH-dependent digits that form last during normal mouse limb development, suggesting that variance in the right/left ratio of SHH could potentially affect digit length.

To ensure that our results were not a result of measurement error, we took several approaches. First, we considered whether the relationship between the variance in the right/left ratio and the somite stage was potentially an artifact of a mean/variance relationship. We find no relationship between the mean and SD for the right/left ratio (ρ = 0.08, P = 0.58).

---

### Second to fourth digit ratio (2D: 4D), breast cancer risk factors, and breast cancer risk: a prospective cohort study [^6b941821]. British Journal of Cancer (2012). Low credibility.

Results

Inter and intraobserver reliability was high for raw digit measurements, with ICC's for left and right, index and ring fingers all being greater than 0.95. Intraclass correlation coefficients for 2D: 4D ratio were slightly lower than those for raw digit measurements (0.80 for right and 0.73 for left 2D: 4D), but still suggest that the observed variability in digit ratio is largely due to between individual differences rather than measurement error.

Of the 9044 women available for this study, we identified 573 incident breast cancer cases during a median follow-up of 16 years 4 months per participant (a total of 145 328 person-years at risk were observed). Characteristics of the study population are shown in Table 1. Cases were slightly older than non-cases on average, with median age at baseline of 55 years compared with 53. Seventy-five percent of the participants were born in Australia, New Zealand, or the United Kingdom.

---

### Decoding the representation of numerical values from brain activation patterns [^4392ad54]. Human Brain Mapping (2013). Low credibility.

Human neuroimaging studies have increasingly converged on the possibility that the neural representation of specific numbers may be decodable from brain activity, particularly in parietal cortex. Multivariate machine learning techniques have recently demonstrated that the neural representation of individual concrete nouns can be decoded from fMRI patterns, and that some patterns are general over people. Here we use these techniques to investigate whether the neural codes for quantities of objects can be accurately decoded. The pictorial mode (nonsymbolic) depicted a set of objects pictorially (e.g., a picture of three tomatoes), whereas the digit-object mode depicted quantities as combination of a digit (e.g., 3) with a picture of a single object. The study demonstrated that quantities of objects were decodable from neural activation patterns, in parietal regions. These brain activation patterns corresponding to a given quantity were common across objects and across participants in the pictorial mode. Other important findings included better identification of individual numbers in the pictorial mode, partial commonality of neural patterns across the two modes, and hemispheric asymmetry with pictorially-depicted numbers represented bilaterally and numbers in the digit-object mode represented primarily in the left parietal regions. The findings demonstrate the ability to identify individual quantities of objects based on neural patterns, indicating the presence of stable neural representations of numbers. Additionally, they indicate a predominance of neural representation of pictorially depicted numbers over the digit-object mode.

---

### Experimental noise cutoff boosts inferability of transcriptional networks in large-scale gene-deletion studies [^061df17c]. Nature Communications (2018). Medium credibility.

To further improve the predictive power of our method we included the prior knowledge that transcriptional networks are highly sparse. Sparsity constraints are typically realised by penalising either the existence of links or the link strengths by adding appropriate cost functions, such as L 1 -norm regularised regression (Lasso). Adding a cost function to the main objective comes with the problem to trade-off the log-likelihood against the number of links in the network whose strength is allowed to be non-zero. In the absence of experimentally verified interactions there is no obvious way how to determine a suitable regularisation parameter that weights the likelihood against the cost function, which is one of the great weaknesses of such methods.

In our approach we reduce network complexity by assuming that functionally relevant information in molecular networks can only pass through nodes whose response to perturbations is significantly above the base line that is given by the variability among biological replicates. The individual noise levels can be estimated from natural variations among wild-type experimental replicates (Fig. 3a). The significance level that removes nodes from the network with low signal-to-noise ratio can be set to a desired false discovery rate. It can be shown that removal of noisy nodes imposes a sparsity constraint on the inference problem (Online Methods). The different steps required to arrive at a list of significant links are illustrated in Fig. 4a. In the first step, genes are grouped in clusters that are co-expressed under all perturbations. These clusters are treated as single network nodes in the subsequent steps. In the second step, only those samples are extracted from the data set that correspond to a perturbation of a chosen gene — the source node — with no other genes perturbed (node 5 in Fig. 4a). From this reduced data set, we identify all nodes in the network that change expression above a given significance level upon perturbing the source node. These significantly responding nodes define a subnetwork for each source node, which is typically much smaller in size than the complete network. In the third step, we collect all perturbation data from the complete data set for all nodes that are part of the subnetwork. Before inferring a direct interaction that points from the source node to a given target node in the subnetwork (green arrows in Fig. 4a), we remove all experiments from the data set where the target node is perturbed. The second and third steps essentially realise the counting procedure of inferable links as illustrated in Fig. 2a, with the difference that significant links are identified by PRCs in combination with residual bootstrapping over replicates (Online Methods, Supplementary Note 3). In the fourth step, we collect all clusters of co-expressed genes that contain exactly two nodes, with one of the nodes perturbed and check statistical significance of the directed link between them. In the fifth step, all significant links are collected in an edge list. We refer to these five steps as the clustering method. If we remove all links from the edge list that have more than one node in a source cluster or more than one node in a target cluster, we obtain an edge list that corresponds to links between single genes. This reduced edge list would also arise by skipping the clustering step and we refer to the remaining inference steps that compute links between single genes as subnetwork method.

---

### Beginnings of place value: how preschoolers write three-digit numbers [^82ad7c0f]. Child Development (2013). Low credibility.

Place value notation is essential to mathematics learning. This study examined young children's (4- to 6-year-olds, N = 172) understanding of place value prior to explicit schooling by asking them write spoken numbers (e.g., "six hundred and forty-two"). Children's attempts often consisted of "expansions" in which the proper digits were written in order but with 0s or other insertions marking place (e.g., "600402" or "610042"). This partial knowledge increased with age. Gender differences were also observed with older boys more likely than older girls to produce the conventional form (e.g., 642). Potential experiences contributing to expanded number writing and the observed gender differences are discussed.

---

### Spatial acuity after digit amputation [^33958c91]. Brain (2002). Low credibility.

Digit amputation in human and non-human primates results in reorganization of somatosensory cortex in which the representations of adjacent, intact digits expand to fill the cortical region previously devoted to the amputated digits. Whether this expanded representation results in improved sensory performance has not been determined. Consequently, we measured the ability to recognize small objects (raised letters) with a digit adjacent to the amputation and the same digit on the normal, contralateral hand in 15 amputees. The same digits were also tested in 15 age-matched, amputation-free subjects. There was no significant difference in recognition scores between digits in the amputees or between amputees and control subjects. More detailed analyses of specific confusion patterns and of the improvement with practice showed no significant differences. As far as we could determine, the cortical expansion that is presumed to accompany digit amputation had no effect on tactile pattern recognition performance.

---

### Brain damage: neglect disrupts the mental number line [^3653f4cb]. Nature (2002). Excellent credibility.

A popular metaphor for the representation of numbers in the brain is the 'mental number line', in which numbers are represented in a continuous, quantity-based analogical format. Here we show that patients with hemispatial neglect misplace the midpoint of a numerical interval when asked to bisect it (for example, stating that five is halfway between two and six), with an error pattern that closely resembles the bisection of physical lines. This new form of representational neglect constitutes strong evidence that the mental number line is more than simply a metaphor, and that its spatial nature renders it functionally isomorphic to physical lines.

---

### Augmented manipulation ability in humans with six-fingered hands [^3491940a]. Nature Communications (2019). High credibility.

Fig. 3
Hands with supernumerary fingers perform more complex movements. Subplots (a – f) report on analyses of hand movements recorded during manipulation of objects of various shapes. The movement task was carried out by two six- and 13 five-fingered subjects. a Dependency between individual fingers quantified by the mutual information between the movements of pairs of fingers, with a value of 0 indicating complete independence between fingers and positive values an increasing dependency. Note that the mutual information is symmetric, i.e. b The cumulative amount of explained variance of hand movements as a function of an increasing number of principal components. Error bars depict SDs across subjects. c The number of effective dof (computed using the principle components, see Methods) was higher in six-fingered than in five-fingered subjects. d Information entropy of the discretized movements where each finger is either resting, flexing or extending. Entropy is shown for an increasing number of fingers, starting with thumb only ("T") and successively adding one finger (index "I", middle "M", ring "R", little "L" and supernumerary "S"). Dotted lines indicate the theoretically maximum possible entropy for five- and six-fingered hands. e Percentage of times thumb and index finger ("T + I"), thumb only ("T"), index only ("I") were moving when the supernumerary finger moved. From left to right: different percentiles of the speed distribution were used as thresholds to separate rest from movement. f Median movement speed of individual fingers for five- and six-fingered subjects. Subplots (g, h) show results from the video game experiment. g Schematic of the task, subjects were required to press a button corresponding to the bottom white targets every time an oscillating cursor (green or red) entered the box. The target boxes flashed blue if a correctly timed press occurred and displayed as red if an incorrectly timed press occurred. Horizontal bars at the bottom of the screen displayed the fraction of correct key presses (top) and one minus the fraction of missed (bottom) key presses. h Subjects' learning curve for the 5 + 1 (dotted) and 6 finger (solid) control. Digits used shown in the inset in dark gray

---

### Incomplete human reference genomes can drive false sex biases and expose patient-identifying information in metagenomic data [^4d9f8b52]. Nature Communications (2025). High credibility.

Application of host filtration methods to low and high microbial biomass samples

To determine the robustness of these three methods across a range of microbial biomasses, we evaluated each method on human exome data as well as tissue, skin, and fecal metagenomic samples. First, we obtained 30 International Genome Sample Resource (IGSR) phase 3 human exome sequencing samples, which are putatively human. After sampling 1 million reads each, we examined the number of human reads remaining, with an ideal host filtering method having zero reads left. We found Method 2 left the smallest amount of human exome reads followed by Method 1, then Method 3 (average reads remaining; Method 1: 32.66, Method 2: 24, Method 3: 351.53). There were significant differences between Method 1 and Method 2 (Wilcoxon signed-rank test, p = 2.6e−05), between Method 2 and Method 3 (Wilcoxon signed-rank test, p = 8.2e−06), and between Method 1 and Method 3 (Wilcoxon signed-rank test, p = 3.8e−05) (Fig. 4a, Supplementary Data 6). Mirroring the distributions seen in human simulated data benchmarks (Fig. 3a), Method 2 removed the largest number of human sequences, followed by Method 1, then Method 3. Interestingly, we found nearly ten times as many human exome reads remained compared to the simulated human data (Fig. 3a). However, without access to the samples, it is not possible to determine whether the increased number of reads in the human exome data compared to the simulated human data is due to real microbial presence (contamination or biological) in the exome sample, imperfect amplification or selection chemistry, and/or reduced performance of the host filtration procedure.

---

### Reservoir computing using dynamic memristors for temporal information processing [^35cce603]. Nature Communications (2017). Medium credibility.

Fig. 3
Recognition of noisy images. a, c Distorted images of digits 2 and 3 are generated by adding noise to the original data at locations marked by the dashed squares. b, d Corresponding reservoir states for these two inputs, showing differences in the two digits can be clearly captured by the memristors corresponding to the last two rows. e Recognition results of noisy digits. The RC system output, e.g. the predicted digit is shown below each case. The distorted images are generated by adding noise to the original training samples. The system can still successfully identify the majority of the distorted images without additional training, until too much noise is added as in the las two cases where the incorrect classifications are marked in red

We note that the noisy patterns were created by adding or removing one white pixel, but not simultaneously. This type of noise will thus have a smaller effect on the memristor response than re-ordering the pixels in the same row (as may be expected from different input classes such as the cases in Fig. 3a, c), and the output signal from the reservoir, although distorted, can still lead to successful classification by the readout function.

After adding more noise to the original images, the digits can still be recognized correctly by the system as shown in Fig. 3e. However, if too much noise is added, as in the last two examples shown in the figure, the system will no longer be able to recognize them without further training of the readout function. However, it could be argued that in these two cases, the noisy 2 can indeed by alternatively considered as a noisy 1, while the noisy 9 can in fact be considered as a noisy 8 (with a missing pixel) instead.

MNIST data set classification

Following these demonstrations, the memristor-based RC system was then tested with a more complex, real-world task, that is, recognition of handwritten digits. We trained and tested the system with the commonly used Mixed National Institute of Standards and Technology database (MNIST, see Methods section). A preprocessing was performed before the images were fed into the reservoir, as shown in Fig. 4a. Take the image of digit 8 as an example, the original grayscale image was first converted into a binary-pixel image. The unused boarder area was also removed to reduce the original 28 × 28 image into a 22 × 20 image with 22 rows and 20 pixels per row. Some samples from MNIST are shown in Fig. 4b.

---

### Giving science the finger-is the second-to-fourth digit ratio (2D: 4D) a biomarker of good luck? A cross sectional study [^3c9a2b7d]. BMJ (2021). Excellent credibility.

Statistical analysis

Statistical analysis was performed by using the average 2D:4D from at least two raters for each hand and each procedure (photocopy and radiography). Significant relations have been reported between 2D:4D and various outcomes when male and female individuals are combined into one group. In other instances, the relation between 2D:4D and an outcome is only statistically significant when men and women are analyzed separately. Therefore, we performed all analyses both ways — with sexes combined and separated.

In our (facetious) effort to persuade less statistically savvy readers of the validity of our statistical analyses, we report as many P values as possible, even where they are not necessary (eg, descriptive statistics). All statistical analyses were performed in SPSS version 27.0 and a priori statistical significance was set at P ≤ 0.05.

Inter-rater agreement

To determine reliability of the most consistent raters for a given 2D:4D, a one-way random intraclass correlation coefficient was computed for each hand using each technique. Additionally, the mean inter-rater percentage differences were computed for each measurement technique for each hand.

To determine the relation between 2D:4D and sex, hand, and measurement technique, linear mixed effects models were computed by use of a scaled identity as the repeated measures covariance structure. Sex (male v female), hand (left v right), measurement technique (photocopy v radiograph), and all two way interactions between the three factors served as categorical predictors, and 2D:4D served as the dependent variable. A similar linear mixed effects model was also used to compare age, height, and body mass between sexes.

For correlations between 2D:4D and body composition parameters and good luck, preliminary analysis showed that the continuous body composition outcome variables were not normally distributed. Therefore, Spearman's r s was computed for all correlations.

Patient and public involvement

Conversations with members of the public inspired this study, because many indicated that they had seen 2D:4D mentioned in social media; some believed that scientific research had confirmed that 2D:4D had real life applications, while others expressed doubt that 2D:4D could predict anything. However, patients or the public were not directly involved in this study because of limited resources. A member of the public read this manuscript after submission.

---

### Three learning stages and accuracy-efficiency tradeoff of restricted boltzmann machines [^4a444298]. Nature Communications (2022). High credibility.

Fig. 4
Approximate RBM learning characteristics on digit images.

a Images of M = 5 × 7 pixels showing patterns of the digits 0 through 9 (selected uniformly) at a random location. Gray pixels must either be made black (x i = 0) or be cut away by the image boundaries (see examples in the second row). Pixels that are not part of the pattern are active (white) with probability q = 0.1. The total number of such images is 40,507,353. b Various loss measures vs. autocorrelation time τ θ for N = 16 (left) and N = 32 (right) hidden units, utilizing persistent contrastive divergence (PCD) with n CD = 1, η = 0.005, B = 100 on ∣ S ∣ = 50,000 training images. Top: Exact loss Δ θ (black), exact test error(empirical loss for a test dataset T of ∣ T ∣ = 10,000 images, gray), and Gaussian-smoothened empirical loss estimate(, σ = 0.32, cyan). The cyan dashed line marks, the minimal Gaussian-smoothened loss estimate between the test and training datasets. Middle: empirical errorusing majority-rule (r = 1) coarse-grainings of samples from the target and model distributions, partitioning pixels into local or random groups (see main text for details). Solid lines: results for individual partitions; star markers: average of the solid lines of the same partitioning type (color, see legend). Bottom: empirical errorfor the same coarse-grainings. c Δ θ vs. τ θ for N = 16 (left) and N = 32 (right) hidden units, utilizing persistent contrastive divergence (PCD) with fixed (n CD = 1) or adaptive (n CD ∝ τ θ) approximation order; other hyperparameters as in b. d Examples from the MNIST dataset, which comprises images of M = 28 × 28 pixels showing handwritten digits. e Similar to b, but for the MNIST dataset and PCD training with n CD = 1, η = 10 −4, B = 100, ∣ S ∣ = 60,000, ∣ T ∣ = 10,000, σ = 0.41,… Missing data points correspond toand/or unrealiable τ θ estimates.

---

### Hyperscanning shows friends explore and strangers converge in conversation [^18108369]. Nature Communications (2024). High credibility.

Investigating how social connection and time shape linguistic mental state alignment

Beyond the preregistered analysis, we can also assess dyads' alignment trajectories through mental state space by analyzing their language. This was done to test for convergent evidence across measurement modalities. Specifically, we used a natural language processing (NLP) algorithm, termed affectr, to decode mental state location in 3D space based on the words participants chose to say during the conversation (Supplementary Methods). To test whether there is internal consistency between the neural and mental state decoding procedure we used a distance correlation approach and found that the two measures of mental states are internally consistent (Supplementary Note 6).

Subsequently, similar to the neural analysis, to make the two analyses using the 3d mind model comparable, we computed the Mahalanobis distance between each turn in each trial for each conversation. We only used trials with more than three turns, since the number of samples must be larger than the number of dimensions (N = 3: rationality, social impact & valence) to compute the covariance matrix needed to calculate the Mahalanobis distance. Speech turns were used here as they are the smallest meaningful unit of analysis at which distance in mental state space between speakers can be computed using linguistic data. As described above, the turns are demarcated by the participants pressing a button to indicate that their turn has ended and the other person can speak. Two dyads only had trials with 3 turns or less; these dyads could not be analyzed using this method. For the remaining 57 dyads, we removed trials with an extreme number of turns, as these trials had turns that were too short to be meaningfully analyzed with our NLP tools. The 1.5 * interquartile range (IQR) rule was used to determine outliers. This rule identified trials with more than 12 turns as outliers. As a consequence, all trials with more than 12 turns were removed from the NLP analysis (529 trials, 17%). To further account for the effects of words per turn, we added the number of words per turn as an additional predictor in our regression models. We also conducted robustness checks with the model without this additional confound regression, which revealed similar results (see Supplementary Note 5).

---

### Lateralized neural responses to letters and digits in first graders [^8b15a1f8]. Child Development (2019). Medium credibility.

Learning to read written symbols induces changes in the visual cortex, leading to selective neural population responses to letters or digits in comparison to other visual patterns (Park, Chiang, Brannon, & Woldorff, 2014). This cultural impact on the organization of the visual system impressively reveals the role of experience in shaping the brain's functional organization. Letters and digits are arbitrary symbols (i.e. bearing no relation between shape and meaning) with visually similar 2D patterns acquired through explicit instruction. However, they belong to different cognitive and semantic domains: language and numbers. An outstanding challenge is to understand how and when the initially meaningless shapes turn into sophisticated and highly differentiated semantic representations. Here we investigated whether letters (alphabetic characters) and numbers (Arabic digits) already give rise to distinct neural responses in young children who just started primary school.

Letters relate to phonology and are the building blocks of written words, which represent oral language, whereas Arabic digits relate to numerical knowledge: they represent numerosity and correspond to verbal number words. For letters and words, the left ventral occipito‐temporal cortex (VOTC), and more specifically the mid‐fusiform gyrus, is preferentially recruited in literate adults (McCandliss, Cohen, & Dehaene, 2003; Schlaggar & McCandliss, 2007). This "visual word form area" (VWFA) has been proposed to specialize ontogenetically for representing strings of letters (McCandliss et al. 2003), even though its specificity has been questioned (Price & Devlin, 2003, 2011). Specialization for letter stimuli is congruent with the general principles of the VOTC organization, showing high category‐specificity for visual stimuli such as faces or body parts (Grill‐Spector & Weiner, 2014).

---

### Three learning stages and accuracy-efficiency tradeoff of restricted boltzmann machines [^2d291dc7]. Nature Communications (2022). High credibility.

Our second example (cf. Fig. 3) is closer in spirit to traditional machine-learning applications and involves pattern recognition and artificial image generation. The target distribution p (x) generates 5 × 5 pixel images with a "hook" pattern comprised of 15 pixels (see Fig. 3 a) implanted at a random position in a background of noisy pixels that are independently activated (white, x i = 1) with probability q = 0.1 (see also Supplementary Note 2 B for more details). Periodic boundary conditions are assumed, meaning that p (x) is translationally invariant along the two image dimensions.

We also consider a one-dimensional variant of this example with only M = 4 (M = 5) visible units and an implanted "010" ("0110") pattern, cf. Fig. 3 d. In this case, we can solve the continuous-time learning dynamics (η → 0 limit of (4)) for the exact target and model distributions p (x) and, obviating artifacts caused by insufficient training data or biased gradient approximations, see also Supplementary Note 1.

Our third example (cf. Fig. 4 a–c) is a simplified digit reproduction task. Patterns of the ten digits 0 through 9 (see Fig. 4 a) are selected and inserted uniformly at random into image frames of 5 × 7 pixels, with the remaining pixels outside of the pattern again activated with probability q = 0.1 (see Supplementary Note 2 C for details). No periodic boundary conditions are imposed, i.e. the input comprises proper, ordinary images.

In our fourth example (cf. Fig. 4 d, e), we train RBMs on the MNIST dataset, which consists of 28 × 28-pixel grayscale images of handwritten digits. It comprises a training set of 60,000 and a test set of 10,000 images. We convert the grayscale images with pixel values between 0 and 255 to binary data by mapping values 0… 127 to 0 and 128… 255 to 1 (see also Supplementary Note 2 D).

---

### Immobilization secondary to cell death of muscle precursors with a dual transcriptional signature contributes to the emu wing skeletal pattern [^e17dd991]. Nature Communications (2024). High credibility.

Loss of mobility recapitulates characteristic features of emu forelimbs

Much research has been devoted to understanding the effect of muscle contraction on the growth of limb bones and joint cavity formation in avian limbs. Although the decreased mechanical load secondary to the absence of distal muscle could explain the left–right variation seen in the emu forelimb skeleton, to our knowledge, there have been no reports demonstrating the effect of immobilization specifically on intraindividual left–right asymmetric patterning of limbs. Here, we will address these questions by first focusing on the effect of late-stage immobilization (E10-18) on the longitudinal growth of individual skeletal elements, followed by examining how immobilization starting at pre-cavitation stages (E6-E10) could affect both growth and joint cavitation asymmetrically.

Firstly, we induced rigid paralysis in chicken embryos under conditions known to lead to a reduction in the length of hindlimbs after joint cavitation occurs,– and examined the length of skeletal elements, joint cavity formation and left–right patterning of forelimbs (Supplementary Fig. 3). The lengths of both the right and left humerus, ulna and metacarpus of digit 3 were significantly reduced in chicken embryos immobilized between E10-18 (Supplementary Fig. 3a, b). Pharmacologically induced immobility also led to drastic intraindividual variation in the length of the left and right humerus, ulna and metacarpus, compared to controls (Fig. 1e). These results suggest that mobility is necessary for symmetric limb patterning and growth.

Intriguingly, even upon immobilization at a post cavitation stage, two out of thirteen immobilized chickens showed an asymmetric pattern at the wrist joint with fusion of cartilaginous fusion of limb elements (Supplementary Fig. 3c). To understand further the effect of mechanical inputs on symmetrical growth and patterning of autopodial skeletal elements, we have immobilized chicken embryos from E6 (Supplementary Figs. 3 d, 5 and 6), when muscle masses had developed, and forelimb movements were apparent (Supplementary Fig. 4a, b). The total length of skeletal elements of digits (metacarpus and phalanges) was significantly reduced compared to that in controls for all digits (Supplementary Fig. 5b). In addition, the reduction in digit length was relatively asymmetric in digits 2 and 4, while the reduction in digit 3 was relatively symmetric (Supplementary Fig. 5c). Of note, emu forelimbs exhibit severe reduction of digits 2 and 4 as well, but not of digit 3 (Fig. 1a; Supplementary Fig. 1a).

---

### Conjunctivitis preferred practice pattern [^5e1c6ada]. Ophthalmology (2024). High credibility.

Conjunctivitis PPP — ICD-10 CM coding rules for seventh characters and laterality state that the applicable 7th character is required for all codes within the category and must always be the 7th character in the data field; if a code that requires a 7th character is not 6 characters, a placeholder X must be used to fill in the empty characters. For bilateral sites, the final character of the codes in the ICD-10 CM indicates laterality; if no bilateral code is provided and the condition is bilateral, separate codes for both the left and right side should be assigned, and unspecified codes should only be used when there is no other code option available. When the diagnosis code specifies laterality, regardless of which digit it is found in (i.e., 4th digit, 5th digit, or 6th digit), right is always 1, left is always 2, and bilateral is always 3.

---

### What counts? Sources of knowledge in children's acquisition of the successor function [^bf36c023]. Child Development (2021). Medium credibility.

Although many U.S. children can count sets by 4 years, it is not until 5½-6 years that they understand how counting relates to number-that is, that adding 1 to a set necessitates counting up one number. This study examined two knowledge sources that 3½- to 6-year-olds (N = 136) may leverage to acquire this "successor function": (a) mastery of productive rules governing count list generation; and (b) training with "+1" math facts. Both productive counting and "+1" math facts were related to understanding that adding 1 to sets entails counting up one number in the count list; however, even children with robust successor knowledge struggled with its arithmetic expression, suggesting they do not generalize the successor function from "+1" math facts.

---

### Q1 stability testing of drug substances and drug products… [^eb9a8698]. FDA (2025). Medium credibility.

13.
2. 3 Scale Transformation of Data. 51 13.
2. 4 Extrapolation and Stability Modelling. 52 13.
2. 5 Extrapolation for Synthetic Chemical Entities. 53 13.
2. 6 No Significant Change at Accelerated Condition.
55. 13.
2.
7. 1 Significant Change at Accelerated Condition. 57 13.
2.
7. 2 Significant Change at Accelerated Condition and Significant Change at Intermediate Condition.
57. 13.
2. 7.
3. 2 Data amenable to statistical analysis. 58 13.
2. 8 Extrapolation for Chemical Entities when Stored Frozen. 58 13.
2. 9 Extrapolation for Biologicals.
58. recombinant DNA technology or isolated from human, animal or plant tissues, other 16 natural sources, including body fluids, or cell cultures. 17
- Conjugated products that are made up of proteins/polypeptides linked to another moiety being investigated.

154 Accelerated conditions, over a defined time period, are 155 intended to increase the rate of chemical degradation, physical change and/or biochemical change in 156 the product. Data generated under accelerated conditions can be used to gain product knowledge and. 210 physical state, clarity, colour, degradation products, particle size, potency), as applicable, by a 211 procedure suitable to detect any evidence of change.
212. 286 Data from the accelerated storage conditions and, if appropriate, from the intermediate storage 287 conditions can be used to evaluate the effect of short-term excursions outside the labelled storage 288 conditions. For synthetics, data from the accelerated storage condition are also 289 needed to enable extrapolation in accordance with Section 13.
2. 5 – Extrapolation for Synthetic. 290 Chemical Entities.

For biologicals, data from the accelerated storage condition is utilised for product 291.

---

### Revealing nonlinear neural decoding by analyzing choices [^12e7caf6]. Nature Communications (2021). High credibility.

Consequences of optimal decoding on choice correlations

The simple formula of Eq. (7) provides useful insights into the relationship between neural activity and choice when that activity is decoded optimally in a natural task. First, the choice correlations do not depend on the shape or magnitude of the internal noise or nuisance correlations, because these dimensions are deliberately avoided by optimal decoding, whose weights cancel those correlations (Eq. (18)). The only aspect of shared variability that matters for choice is the information-limiting component, i.e. that which is indistinguishable from a change in the task-relevant stimulus. This information-limiting variability increases choice correlations mostly by decreasing the overall behavioral discriminability; the shared variability is large only at the population level and typically only has a small contribution to any one statistic, and thus to its discriminability. With a smaller denominator and roughly unaffected numerator, the ratio in Eq. (7) rises with information-limiting correlations.

Likewise, the total number of the decoded statistics only affects the prediction of Eq. (7) insofar as it affects the available information. This is especially important when considering nonlinear statistics, as there are potentially so many of them. When optimally decoded, greater numbers of independently informative statistics will increase the total discriminability exhibited by the behavioral choices, while the discriminability for each statistic remains unchanged. According to Eq. (7), choice correlations for optimal decoding are the ratio of these two, so as the behavioral behavioralincreases and the individual terms remain fixed, the choice correlations shrink. On the other hand, greater numbers of redundant statistics change neither the total information content nor the behavioral choice. These added statistics can have similar tuning and similar fluctuations as each other (which is what makes them redundant). Any single redundant statistic might or might not be decoded, but it is correlated with others that are. According to Eq. (7), the individualare unchanged when adding more redundant statistics; the total informationis unchanged; and thus their ratio is fixed, consistent with the unchanged choice correlations.

---

### Children's arithmetic skills do not transfer between applied and academic mathematics [^a77b4b34]. Nature (2025). Excellent credibility.

Most of these children effectively used arithmetic calculations in their jobs. Figure 1a presents the proportion of children who correctly reported the amount due and change in the three enumerator transactions on both their first and second try (if incorrect on the first try). Overall, 95%, 97% and 98% of children were correct by their second try for the first, second and third transactions, respectively (Fig. 1a and Extended Data Table 1). Most children mentally performed these calculations with no paper aids.

Fig. 1
Performance of children in Kolkata working in a market (study 1).

a, Proportion of children who correctly answered the total amount due in transactions involving two goods sold in unusual quantities. b, Proportion of children who correctly answered the total amount due in hypothetical transactions. c, Proportion of children who were credited with labelling single-digit numbers, labelling two-digit numbers, subtracting and dividing on the ASER test, a tool for assessing numeracy used across India for the ASER. Error bars show 95% CIs around the mean (mean ± 1.96 × s.e.m.). d, An example of this arithmetic assessment tool. From left to right, each column presents items that respectively task children with identifying single-digit numbers, identifying two-digit numbers, subtracting one two-digit number from another with carrying, and long division of a three-digit number by a single-digit number with a remainder. For grade 5 children, the ASER test begins with the subtraction task and proceeds to the right (division) if children succeed and to the left (identifying two-digit numbers) if they fail. Because success at each level requires mastery of the operations required by the tasks to its left, children who succeed at a given level are credited with mastery of all the levels below it.

---

### Dry eye syndrome preferred practice pattern ® [^cd799048]. Ophthalmology (2024). High credibility.

Dry eye ICD-10 CM classifications — entities and codes include: Dry eye, unspecified, right lacrimal gland H04.121; left lacrimal gland H04.122; bilateral lacrimal gland H04.123; keratoconjunctivitis sicca (not specified as Sjögren) H16.22-; Sjögren syndrome, sicca syndrome M35.00; Sjögren syndrome with keratoconjunctivitis M35.01; filamentary keratitis H16.12-; exposure keratoconjunctivitis H16.21-; punctate keratitis H11.14-. Certain ICD-10 CM categories have applicable 7th characters, and the applicable 7th character is required for all codes within the category, or as the notes in the Tabular List instruct; the 7th character must always be the 7th character in the data field, and if a code that requires a 7th character is not 6 characters, a placeholder x must be used to fill in the empty characters. For bilateral sites, the final character of the codes in the ICD-10 CM indicates laterality; if no bilateral code is provided and the condition is bilateral, separate codes for both the left and right side should be assigned, and unspecified codes should only be used when there is no other code option available. When the diagnosis code specifies laterality, regardless of which digit it is found in (i.e., 4th digit, 5th digit, or 6th digit), right is always 1, left is always 2, and bilateral is always 3.

---

### Differences in spatial acuity between digits [^869ebc5f]. Neurology (2001). Low credibility.

The spatial acuity of the index, middle, and ring distal finger pads of eight right-handed men, aged 22 to 57 years, was measured by using gratings and raised letters. Acuity declined significantly from the index to the middle finger and from the middle to the ring finger. There were no significant differences between homologous fingers of the two hands. Letter recognition and grating orientation threshold measures were highly correlated.

---

### Lateralized neural responses to letters and digits in first graders [^9eb2b09b]. Child Development (2019). Medium credibility.

We used this sensitive approach to examine in first graders whether neural responses specific to single letters (LETT) and digits (DIG) could already be elicited, when inserted in streams of the other category. This direct contrast allowed to obtain specific responses for letters within streams of digits (hereafter: dig‐LETT) and vice‐versa (hereafter: lett‐DIG).

If letters and digits already elicit specialized neural processes at 6 years of age, we should observe responses at the frequency of the categorical change. For dig‐LETT, we expected a left‐lateralized posterior response, as letters are visual objects learnt directly in association with phonological codes and trigger connections between anterior phonological and posterior visual regions (Phonological Mapping Hypothesis, Maurer & McCandliss, 2007). For lett‐DIG we anticipated a right response lateralization, given the previous findings on adults (Park et al. 2017, 2014) and the potential link with lateralization for numerical competencies heavily relying on the right IPS (rIPS; Cantlon, Brannon, Carter, & Pelphrey, 2006; Vogel, Goffin, & Ansari, 2015)

---

### Can we reduce capsule endoscopy reading times? [^3f75ffc8]. Gastrointestinal Endoscopy (2009). Low credibility.

Background

Analyzing small-bowel capsule endoscopy (CE) images is time consuming.

Objective

To determine the effect of reducing the number of images on reading time and interpretation of CE procedures.

Design

Two techniques aimed at reducing the number of images to be viewed were studied. The number of images was reduced by removing every second image (study A) or by the Quickview mode (study B). In both studies, one endoscopist viewed the images in the conventional way, whereas another endoscopist viewed the reduced number of images.

Setting

A single-center prospective study.

Patients

Two hundred CE procedures, 100 consecutive procedures for each study.

Main Outcome Measurements

Reading times for small-bowel images were recorded. Kappa (kappa) values were used to calculate interobserver agreement between viewing techniques. Diagnostic miss rates were calculated.

Results

Median procedure reading times were significantly reduced by viewing half the number of images (10.2 minutes) or using the Quickview technique (4.4 minutes) compared with conventional viewing (17.0 minutes). Interobserver agreement was excellent (kappa = 0.91) in study A and good (kappa = 0.74) in study B. The diagnostic miss rate was 2% when half the number of images were viewed and 8% when the Quickview technique was used. Agreement between both techniques and conventional viewing was best when the indication for the procedure was suspected inflammatory bowel disease.

Limitation

Lack of a criterion standard.

Conclusions

Techniques that reduce the number of images examined in CE are time saving but are associated with considerable diagnostic miss rates. Such techniques may only be considered if inflammatory bowel disease is suspected.

---

### Clinical practice guidelines: diagnosis, treatment, and management of children and adults with central auditory processing disorder [^40d7d6c1]. AAA (2010). Medium credibility.

Tests of monaural low-redundancy speech perception were among the first used to detect central auditory dysfunction in the 1950s using speech procedures made more sensitive by removal of spectral information, with strategies to reduce natural redundancy including filtering selected frequencies (e.g., high- or low-pass filtering) and time compression of the speech signals.

---

### Predictors of hand function following digit replantation: quantitative review and meta-analysis [^032ffa4f]. Hand (2021). Medium credibility.

Background: Digit replantation affords the opportunity to restore hand function following amputation. To date, however, few studies have evaluated functional outcomes following replantation. Therefore, it was the objective of this study to perform a meta-analysis to better characterize the predictors of hand function. Methods: A literature search was performed using the PubMed database to identify studies that focused on digit amputation/replantation and functional outcomes. Studies were evaluated for patient- and injury-related factors and their respective effects on clinical outcomes of sensation, grip strength, and Disabilities of the Arm, Shoulder, and Hand (DASH) scores. Statistical analysis was conducted across the pooled data set to identify significant trends. Results: Twenty-eight studies representing 618 replanted digits were included in this study. We found the average grip strength was 78.7% (relative to contralateral), the average 2-point discrimination (2PD) was 7.8 mm, and the average DASH score was 12.81. After conducting statistical analysis, we found patients with more proximal injuries had lower grip strength scores (P < .05). We found 2PD scores were influenced by age, mechanism of injury, and amputation level (P < .05). Finally, we found DASH scores after replantation were predicted by mechanism of injury and level of amputation (P < .05). The following variables did not influence outcomes: gender, tobacco use, ischemia time, and digit number. Conclusions: Digit replant does not restore premorbid hand function but does result in adequate hand function. Expected functional outcomes following replant should be considered in the decision-making process. These data can help risk-stratify patients, guide postreplant expectations, and influence the decision for replantation.

---

### Lost in translation: on the problem of data coding in penalized whole genome regression with interactions [^c4771daf]. G3 (2019). Medium credibility.

Reducing training set size:

The increasing influence of the choice of coding on interaction effect estimates when the number of interactions is increasing, may have a counterpart when the training set size is decreasing. We use a model including 50 randomly chosen markers and their interactions and reduce the training set size from 539 to 300 or to 200. Table 3 presents the mean Pearson correlations of the interaction estimates based on 50 randomly drawn marker and training sets (of 200 or 300 lines, respectively). Here, the reduction in correlation has a similar pattern to the situation in which the number of markers in increasing. However, comparing the situation of 150 markers and 599 lines (Table 2) to the situation of 50 markers and 200 lines (Table 3), we see that the impact of coding on interaction effect estimates is more strongly affected by the reduction in training set size than by the increase in the number of markers. For instance, the lowest correlation in Table 2 is 0.80 for 150 markers and the data of environment 2. In Table 3, the correlation with a test set size of 200 is 0.68. In particular, this shows that the impact does not only depend on the ratio of training set size and markers.

Table 3
Pearson correlation of the estimates of the 1225 interaction effects when different marker codings are used and the number of genotypes used to estimate the effects is reduced from the 599 available lines to 300 or 200, respectively. Only the model with 50 markers and 1225 interactions is considered. The numbers represent the mean correlation of 50 repetitions with independently, uniformly drawn markers and lines. The standard error of the estimate was in all cases below 0.025. Colors indicate the underlying data: Environment 1, 2, 3 or 4.

---

### The chromosome number in humans: a brief history [^672f370a]. Nature Reviews: Genetics (2006). Medium credibility.

Following the rediscovery of Mendel's work in 1900, the field of genetics advanced rapidly. Human genetics, however, lagged behind; this was especially noticeable in cytogenetics, which was already a mature discipline in experimental forms in the 1950s. We did not know the correct human chromosome number in 1955, let alone were we able to detect a chromosomal abnormality. In 1956 a discovery was reported that markedly altered human cytogenetics and genetics. The following is an analysis of that discovery.

---

### Augmented manipulation ability in humans with six-fingered hands [^2dbc0cce]. Nature Communications (2019). High credibility.

Statistical analysis

For comparing two independent samples we used the nonparametric, two-sided Wilcoxon ranksum test and computed 95% confidence intervals on the effect size (i.e. the difference of the population means) by using the two-sample pooled t-interval. For comparing two paired samples we used the nonparametric, two-sided Wilcoxon signed rank test and computed 95% confidence intervals on the effect size by using the paired t-interval. All reported confidence intervals reflect the mean for five-fingered subjects subtracted from the mean for six-fingered subjects, i.e. positive values indicate larger values for six-fingered subjects.

To assess the correlation between two variables we computed the Pearson correlation coefficient. We did not assess the statistical significance of the Pearson correlation coefficient as the samples across which correlations were computed were not independent.

Reporting summary

Further information on research design is available in the Nature Research Reporting Summary linked to this article.

---

### Long-term effects of compulsory schooling on physical, mental and cognitive ageing: a natural experiment [^9042a05c]. Journal of Epidemiology and Community Health (2019). Medium credibility.

Outcomes

In Constances, mental health was measured using the 20-item version of the Centre for Epidemiologic Studies Depression scale (CES-D), a validated depressive symptoms scale. Higher scores indicate higher levels of depressive symptoms and a cut-off of 16 for men and 20 for women was used to define elevated levels of depressive symptoms.

Cognitive function was assessed by trained neuropsychologists using five tests. The Mini-Mental State Examination (MMSE) is a general test of cognitive function consisting of 30 questions assessing five areas of cognition: orientation, registration, attention and calculation, recall and language. The Digit-Symbol Substitution Task of the Wechsler adult intelligence scale IV assesses psychomotor speed. The respondents were presented with nine digit-symbol pairs, followed by a list of numerical digits. They were asked to write the corresponding symbol under each digit as fast as possible. We retain the number of symbols correctly associated in 90 s. The Free and Cued Selective Reminding Test (FCSRT) measures verbal episodic memory. Respondents were first asked to read and memorise 16 words, then asked to recall as many words as possible in 2 min. The neuropsychologist provided a cue for each word that has not been retrieved. For this study, we include the FCSRT free immediate recall score. Finally, two verbal fluency tasks were included to assess language abilities: number of words related to the 'animals' category named in 1 min for the semantic fluency task and number of words starting with the letter 'R' named in 1 min for the phonemic fluency task. As the scale of each individual cognitive test differs, each score is converted to a z-score by subtracting the mean and dividing by the SD. A global cognitive score is obtained by summing the standardised tests. High cognitive performance is defined as the top quartile of the global score's distribution.

Physical functioning is assessed based on three objective health measures. Hand grip strength was assessed using a handheld dynamometer. Participants were asked to sit and squeeze the dynamometer for a few seconds with their dominant hand. The exercise was repeated three times and grip strength was determined as the average of the three results. Finger tapping, a measure of motor performance, was assessed by the number of taps respondents could do with the index finger of their dominant hand for 15 s. Walking speed was measured as the time taken by respondents to walk 3 m at a rapid pace. For each physical health outcome, high levels of physical functioning are defined based on the top quartile of the distribution.

---

### Prevalence of rare diseases: bibliographic data [^d22d628d]. Orphanet Report Series (2024). Medium credibility.

Worldwide, the prevalence of reducing body myopathy is estimated at 0.00006 per 100,000 population.

---

### Risk-sensitive events during laparoscopic cholecystectomy: the influence of the integrated operating room and a preoperative checklist tool [^212e9dc4]. Surgical Endoscopy (2010). Low credibility.

Background

Awareness of the relative high rate of adverse events in laparoscopic surgery created a need to safeguard quality and safety of performance better. Technological innovations, such as integrated operating room (OR) systems and checklists, have the potential to improve patient safety, OR efficiency, and surgical outcomes. This study was designed to investigate the influence of the integrated OR system and Pro/cheQ, a digital checklist tool, on the number and type of equipment- and instrument-related risk-sensitive events (RSE) during laparoscopic cholecystectomies.

Methods

Forty-five laparoscopic cholecystectomies were analyzed on the number and type of RSE; 15 procedures were observed in the cart-based OR setting, 15 in an integrated OR setting, and 15 in the integrated OR setting while using Pro/cheQ.

Results

In the cart-based OR setting and the integrated OR setting, at least one event occurred in 87% of the procedures, which was reduced to 47% in the integrated OR setting when using Pro/cheQ. During 45 procedures a total of 57 RSE was observed-most were caused by equipment that was not switched on or with the wrong settings. In the integrated OR while using Pro/cheQ the number of RSE was reduced by 65%.

Conclusions

Using both an integrated OR and Pro/cheQ has a stronger reducing effect on the number of RSE than using an integrated OR alone. The Pro/cheQ tool supported the optimal workflow in a natural way and raised the general safety awareness amongst all members of the surgical team. For tools such as integrated OR systems and checklists to succeed it is pivotal not to underestimate the value of the implementation process. To further improve safety and quality of surgery, a multifaceted approach should be followed, focusing on the performance and competence of the surgical team as a whole.

---

### Representational format determines numerical competence in monkeys [^840aefc5]. Nature Communications (2011). Medium credibility.

A range of animal species possess an evolutionarily ancient system for representing number, which provides the foundation for simple arithmetical operations such as addition and numerical comparisons. Surprisingly, non-human primates tested in ecologically, highly valid quantity discrimination tasks using edible items often show a relatively low performance, suggesting that stimulus salience interferes with rational decision making. Here we show that quantity discrimination was indeed significantly enhanced when monkeys were tested with inedible items compared with food items (84 versus 69% correct). More importantly, when monkeys were tested with food, but rewarded with other food items, the accuracy was equally high (86%). The results indicate that the internal representation of the stimuli, not their physical quality, determined performance. Reward replacement apparently facilitated representation of the food items as signifiers for other foods, which in turn supported a higher acuity in decision making.

---

### Weight illusions explained by efficient coding based on correlated natural statistics [^cf36308b]. Communications Psychology (2024). Medium credibility.

Fig. 2
Summary data and fits.

Black arrows and circles plot mean relative bias (arrows) and within-participant SD (circle diameters) of human observers (n = 30) in estimating mass, based on reported weight ratios in a task comparing pairs of lifted objects of equal weight and different volume. Red circles and arrows show corresponding mean predictions for the fitted efficient coding model with log-normal prior distribution (blue contours plot the prior density based on median fitted parameters). Row labels A–D indicate objects of the four different tested volumes (illustrated in Fig. 3 C).

Red circles and arrows show mean predicted variability and bias, respectively, for the fitted efficient coding model (maximum likelihood parameters, median values: prior intercept, c m = 2.40; prior slope, β = 0.306; prior s.d. s = 3.22; bias constant, c b = 0.0123; s.d. constant, c σ = 0.0102; individual participant parameters shown in Fig. S1 A; equivalent parameters for empirical sample shown in Fig. 1 A for comparison: c m = 1.10, β = 0.483, s = 0.361).

Figure 3 A (black circles) plots the mean estimated weight ratios for individual pairs of lifted objects from which the empirical estimates in Fig. 2 are calculated, along with predictions of the fitted efficient coding model (red lines). Despite each lifted pair having equal mass, the object of smaller volume was, on average, estimated as heavier (weight ratio > 1) for every pair of objects in each mass condition, demonstrating the SWI. The strength of the SWI decreased as the ratio of the volumes decreased (left to right within each panel), and increased on average as mass of the objects increased (left-hand to right-hand panels). Each of these patterns was quantitatively reproduced by the efficient coding model (red lines), based on the non-linear relationship between mass, volume and prior probability depicted in Fig. 2 (blue contours).

---

### Esotropia and exotropia preferred practice pattern ® [^38cbc533]. Ophthalmology (2023). High credibility.

Esotropia ICD-10 CM codes — entities and laterality rules are specified as follows: Nonaccommodative (unspecified) H50.00; Accommodative H50.43; Alternating H50.05; Alternating with A pattern H50.06; Alternating with V pattern H50.07; Alternating with X or Y pattern (with other noncomitancies) H50.08; Monocular H50.01–; Monocular with A pattern H50.02–; Monocular with V pattern H50.03–; Monocular with X or Y pattern (with other noncomitancies) H50.04–; Intermittent, alternating H50.32; Intermittent, monocular H50.31–; Unspecified H50.00. CM = Clinical Modification used in the United States; = 1, right eye; 2, left eye. For bilateral sites, the final character of the codes indicates laterality. Esotropia and exotropia do not have bilateral codes. Therefore, if the condition is bilateral, assign separate codes for both the left and right side. When the diagnosis code specifies laterality, regardless of which digit it is found in (i.e., 4th digit, 5th digit, or 6th digit), most often you will find: Right is 1 Left is 2.

---

### Lateralized neural responses to letters and digits in first graders [^78762742]. Child Development (2019). Medium credibility.

Limitations and Conclusions

Our study has several limitations. First, in order to understand when specialized networks for these categories of symbols start to appear, it will be necessary to test younger children. Second, a larger sample and an extended assessment of literacy and numeracy skills would be necessary to assess if the amplitude of selective responses to letters and digits relate to character knowledge. Finally, although we demonstrate the possibility to use very short recording times (2 × 40 s), it would be beneficial to have more trials to increase SNR by averaging and therefore decreasing noise. Future studies should also attempt to quantify the frequency of occurrence of letters and digits in kindergarten children.

In conclusion, our study sheds new light on the visual cortex specialization for culturally derived characters like letters and digits and shows that already at 6 years of age, those arbitrary shapes are discriminated from each other. These findings agree with the proposal that VOTC specialization for cultural visual symbols partly depends on the relevant coactivated brain networks.

---

### Probing the physical limits of reliable DNA data retrieval [^511b8f72]. Nature Communications (2020). High credibility.

Encouragingly, the size of the file being recovered also does not impact the copy number required in complex pool conditions (Figs. 2 and 3 a). Regardless of pool complexity or size of file accessed, only 10 copies of each strand on average, with a standard deviation of 3, are required for successful recovery with no bit errors. A pool complexity emulating nearly 150 GB of digital data per PCR reaction did not hinder file recovery, and the fact that recovering data from this complex pool was indistinguishable from the water-diluted pool with orders of magnitude fewer strands suggests that we have not approached the limit of pool complexity. Storing many unique sequences in one pool reduces the need for physical isolation, one of the largest density overheads facing this technology.

Fig. 3
Examining file recovery, sequencing coverage, and sequence loss behavior.

a The limit of successful, no bit error decoding is shown with the gray bar in each graph. Data points below the gray bar are samples where the file was successfully decoded and recovered with no bit errors. Done post-sequencing, decoding involves clustering sequences, finding consensus, then correcting errors. A more detailed view of this data including exact copy number and sequencing coverage is in Supplementary Note 7. b For the small file, when each sample diluted in water was sequenced at greater depth, minimal improvement on the proportion of missing sequences occurred. The 100× coverage data were found by sub-sampling the data used to create the 200× coverage data. c Missing sequences are compared between the initial pool prior to any dilutions where the mean copy number was 194 (in red) and the last dilution where the mean copy number was < 1 (in blue). Note the different total number of sequences in the small (2042), medium (26,404), and large (271,447) files. The fact that some sequences are missing only from the initial pool but "reappear" in the final dilution suggests that the lost sequences are a result of stochastic variation that occurs during sub-sampling for file recovery, rather than irretrievably lost due to some property of the sequence. This pattern of sequences reappearing in subsequent dilutions is shown for every dilution step in Supplementary Note 6. Note that for a, b, x -axis and y -axis error bars represent 95% confidence intervals; x -axis errors are taken from triplicate qPCR data (see Methods), and y -axis errors are the result of 100 simulations of the original sequencing data sub-sampled to 20× sequencing coverage (see Methods).

---

### Turbulent coherent structures and early life below the kolmogorov scale [^51bdbaf6]. Nature Communications (2020). High credibility.

Division and merging

Calculating the FTLE field is quite computationally costly, and counting elliptic LCSs without fear of error would require an extremely high spatial resolution, and therefore making full evaluation of the flow properties in a simulation is a much more involved process than a full evaluation of the biological properties. Furthermore, it is easy to predict when demographic changes are not due to the biological process from this evaluation. For instance, we can assign a minimum and maximum probability to the number of birth–death events occurring within a certain timespan given the approximate population size, and can also predict the extremal probabilities of the outcome in which all (or most) such birth–death events were either birth or death. The latter is particularly easy; the likelihood of M death events in a row before a birth must be bounded above by NB(1, d /(s i + d), M), where NB(a, b, c) denotes the value of the negative binomial distribution giving the probability of c deaths before 1 birth, given that the probability of a death in a population of size n where no individual is catalyzed (hence, the situation in which the probability of death is always maximized, giving the maximal probability of M deaths before a birth) is dn /(ns i + nd). Using our standard kinetic rates, a sequence of 10–15 deaths without a birth is already an extremely unlikely event, so that a dip in the largest cluster size of 10–15 would indicate a flow-based event (division of coherent structure) with high probability. In each of 1000 simulations, we paid attention only to monotonic changes in the size of the largest cluster with absolute value bigger than 10. The statistics of these changes are plotted in Fig. 6.

Migration and diversity

For Supplementary Fig. 1, seven vortices were used and were assigned an integer label which was maintained throughout the simulations. The curves shown were generated by simulating 1000 runs per metabolism, initializing 100 of each particle species in the vortices labeled 1–3 (therefore implicitly assuming that the metabolism had experienced some reproductive success in one vortex). Simply tracking the dynamics of particles from the three original lineages over time generated the left-hand side of Fig. 7. The right-hand side was given by counting the number of components (with 10 or more particles included) in a geometric graphinduced by drawing an edge between two particles iff they were at a distance (in the Euclidean metric) < R int.

---

### As-3 [^f4572ab3]. FDA (2025). Medium credibility.

Storage and handling

Store at room temperature (25 °C/77 °F). Avoid excessive heat.

Protect from freezing

---

### Natural display mode for digital DICOM-conformant diagnostic imaging [^30cf9aed]. Academic Radiology (2002). Low credibility.

Rationale and Objectives

The authors performed this study to investigate the verification of the contrast display properties defined by the digital imaging and communication in medicine (DICOM) PS (picture archiving and communication system [PACS] standard) 3.14–2001 gray-scale display function standard and their dependency on display luminance range and video signal bandwidth.

Materials and Methods

Contrast sensitivity and contrast linearity of DICOM-conformant displays were measured in just-noticeable differences (JNDs) on special perceptual contrast test patterns. Measurements were obtained six times at various display settings under dark room conditions.

Results

Display luminance range and video bandwidth had a significant effect on contrast perception. The perceptual promises of the standard could be established only with displays that were calibrated to a unity contrast resolution, at which the number of displayed intensity steps was equal to the number of perceivable contrast steps (JNDs). Such display conditions provide for visual perception information at the level of single-step contrast sensitivity and full-range contrast linearity. These "natural display" conditions also help minimize the Mach banding effects that otherwise reduce contrast sensitivity and contrast linearity.

Conclusion

Most, if not all, conventionally used digital display modalities are driven with a contrast resolution larger than 1. Such conditions reduce contrast perception when compared with natural imaging conditions. The DICOM-conformant display conditions at unity contrast resolution were characterized as the "natural display" mode, and, thus, the authors a priori recommend them as being useful for making a primary diagnosis with PACS and teleradiology and as a standard for psychophysical research and performance measurements.

---

### Carp [^d8a12781]. FDA (2009). Low credibility.

Volume desired x Concentration desired = Volume needed x Concentration available.

Example 1: If a 1:10 w/v extract is available and it is desired to use a 1:1,000 w/v extract substitute as follows:

Vd x Cd = Vn x Ca

10ml x 0.001 = Vn x 0.1

0.1 mL = Vn

Using a sterile technique, remove 0.10 mL of extract from the 1:10 vial and place it into a vial containing 9.90 mL of sterile diluent. The resulting ratio will be a 10 mL vial of 1:1,000 w/v.

Example 2: If a 10,000 pnu/ml extract is available and it is desired to use a 100 pnu/ml extract substitute as follows:

10ml x 100 = Vn x 10,000

0.1 mL = Vn

Using a sterile technique, remove 0.10 mL of extract from the 10,000 pnu/ml vial and place it into a vial containing 9.90 mL of sterile diluent. The resulting concentration will be a 10 mL vial of 100 pnu/ml.

Example 3: If a 10,000 AU/ml or BAU/ml extract is available and it is desired to use a 100 AU/ml or BAU/ml extract substitute as follows: Vd x Cd = Vn x Ca

---

### Lost in translation: on the problem of data coding in penalized whole genome regression with interactions [^6fa43c76]. G3 (2019). Medium credibility.

Determining (un)explained phenotypic variance:

To check the influence of the coding on how much phenotypic variance can be explained by the interaction effects, we estimate the variance components on the full data set with all markers and all lines. Since some variance may be attributed "randomly" to additive or epistatic variance, when both corresponding covariance matrices are too similar, we focus on a model with only interaction effects and consider the variance component of the residual, that is the variance which is not explained. The residual variance is more comparable, since the different epistatic relationship matrices may be scaled differently which directly translates to the estimated variance components. We use the equationto calculate the epistatic relationship for all 599 lines and divide by the maximum of. The regress package is used and the residual variance is then compared between different codings. The symboldenotes Hadamard product, that is the entry-wise multiplication.

Out of sample predictions with varying number of markers and training set size:

To assess the impact of translations of the marker coding on out-of-sample predictions, we randomly draw test sets of 60 out of the 599 lines to be predicted by the additive and interaction effects estimated from the training set consisting of the remaining 539 lines. We compare the Pearson correlations between the predictionsfor different codings and with the known phenotypes("predictive ability"). We do this for models including 50, 100 or 150 marker and their interactions. Moreover, we compare the predictions of a model with 50 markers, when the training set size is reduced. Since the variance of the results is increased due to sampling of markers and training and test sets, we use 200 repetitions with independently drawn marker, training and test sets.

Data Availability

The authors affirm that all data necessary for confirming the conclusions of the article are present within the article, figures, and tables or publicly available.

Results

---

### Addressing early childhood emotional and behavioral problems [^0988ef83]. Pediatrics (2016). Medium credibility.

Examples of evidence-based treatments for existing diagnoses in young children — This report focuses on programs that target current diagnoses or clear clinical problems (rather than risk) in infants and toddlers.

---

### Human brain representations of internally generated outcomes of approximate calculation revealed by ultra-high-field brain imaging [^8bebdf15]. Nature Communications (2024). High credibility.

Behavioral results for comparison with probe numbers

Probe numbers presented at the end of the delay period differed from the correct result on each trial by one of 8 possible ratios (see Methods for details). The percentage of correct responses for comparing the probe to the internally computed result (mean ± SEM) was, on average, 76.8 ± 0.02 for division and 73.5 ± 0.01 for multiplication, with the difference not reaching significance (t (16) = 1.84, p = 0.09, Cohen's d = 0.42), suggesting that subjects were able to perform the task reasonably well and equivalently for the two types of operations. Combined across operations, the accuracies for the different comparison ratios were: ratio 0.50: 88.9 ± 0.03, ratio 0.67: 79.2 ± 0.03, ratio 0.80: 68.2 ± 0.04, ratio 0.91: 55.0 ± 0.04, ratio 1.10: 60.4 ± 0.03, ratio 1.25: 75.4 ± 0.03, ratio 1.50: 84.6 ± 0.03, ratio 2: 89.2 ± 0.03. The range of comparison ratios used further allowed us to fit psychometric functions and compute the just noticeable difference (JND) for the comparative judgment performed here. Estimated across all numerosities, the JND (on a logarithmic scale) corresponded to 0.289 ± 0.03 for division and 0.287 ± 0.02 for multiplication. We also tested for the presence of a so-called operational momentum effect (referring to larger or smaller than expected results reported depending on the type of operation performed), which has been described for mental calculation, most prominently in the non-symbolic case. To measure the presence of such effects here, we estimated the point of subjective equality (PSE) of the psychometric function separately for trials with division as opposed to multiplication. The expected tendency with division leading to under- and multiplication to overestimated results should lead to the PSE being shifted towards the left or right, respectively, for these operations. Results showed that PSEs were, on average, −0.029 ± 0.06 for division, and −0.062 ± 0.06 for multiplication, with the difference not being significantly different across subjects (t (16) = 0.32, p = 0.75, Cohen's d = 0.07), suggesting that result representations were, on average, unbiased by the type of operation performed in the task used here.

---

### Exceptional preservation and foot structure reveal ecological transitions and lifestyles of early theropod flyers [^402c9ec0]. Nature Communications (2022). High credibility.

Morphological description: Confuciusornis

We imaged over 500 specimens of Confuciusornis using LSF at the STM and IVPP. We identified the best pedal soft tissues in two specimens, STM 13–55 and IVPP V13156. STM 13–55 is a complete, articulated female individual based on the absence of ornamental retrices on the tail. The pedes are exposed in plantar view, although only the left foot preserves notable details of the podotheca (Fig. 4). Digit I is well preserved and shrouded by a single pad that encloses the first phalanx and the flexor tubercle of the claw. The first toe pad of digit II is the only additional pad preserved on the specimen and has a similar level of development to the toe pads of Anchiornis: the pad is relatively flat, semi-lenticular in outline, spans the joint between the first and second phalanx (i.e. exhibits arthral arrangement; Figs. 1 and 4), and is separated from digit I by a broad furrow. Reticulate scales on the digits are small (~0.11–0.62 mm in diameter) but not clearly visible due to imperfect preservation. The ventral margins of the pads reveal reticulate scales with low domical (globate) shapes. Incompletely preserved scutate scales are present on the dorsal surface of digit I and on the proximal end of the metatarsus. A previously identified tarsal pad (Fig. 4c of ref.) on the left foot of a different specimen (IVPP V13156) could not be verified. Inspection of the published figures show that the margins are not well preserved enough to identify this as a toe pad or as a possible web between digits I and II. These authorsalso reported the presence of large phalangeal pads and smaller 'interphalangeal pads' (= folds) on the digits; however, we also could not identify these on the available figures.

---

### Factor VIIa (recombinant) [^363851b7]. FDA. Low credibility.

The dosage of factor VIIa (recombinant) IV for treatment of bleeding episodes in adults with congenital factor VII deficiency is 15–30 mcg/kg IV q4-6h until hemostasis is achieved

---

### Removal of myomas in asymptomatic patients to improve fertility and / or reduce miscarriage rate: a guideline [^4878c275]. Fertility and Sterility (2017). Medium credibility.

Summary statements on fibroids and reproductive outcomes emphasize study heterogeneity and insufficient evidence: heterogeneous study designs, inconsistent nomenclature, continuous nature of leiomyoma size and location, and insufficient patient recruitment significantly limit the interpretation of results from existing studies that evaluate the impact of fibroids on the likelihood of achieving pregnancy and maintenance of pregnancy. There is insufficient evidence to conclude that myomas reduce the likelihood of achieving pregnancy with or without fertility treatment. (Grade C). There is insufficient evidence to determine that a specific myoma size, number, or location (excluding submucosal myomas or intramural myomas impacting the endometrial cavity contour) is associated with a reduced likelihood of achieving pregnancy or an increased risk of early pregnancy loss. (Grade C).

---

### Lateralized neural responses to letters and digits in first graders [^ab365260]. Child Development (2019). Medium credibility.

Discussion

This study found that 6‐year‐old children display specific neural responses to visually presented letters and digits, both arbitrary visual shapes conveying culturally acquired meaning. Remarkably, this differential brain response to letters/digits occurred in a very fast presentation mode (< 170 ms per stimulus), low‐level confounds being controlled through random changes at every stimulus onset. These results suggest that already at the beginning of first grade, letters and digits give rise to specific neural responses, directly indicating selective modes of processing for one category compared to the other.

This novel finding goes beyond previous study where categorical responses emerged between familiar letters and unfamiliar pseudofonts (Lochy et al. 2016). At this age, children have had limited reading and arithmetic instruction, but they already possess letter and number knowledge due to early sociocultural and preschool‐related exposure to print (Wright et al. 1991; for digits see also Mejias & Schiltz, 2013; Hoffmann, Hornung, Martin, & Schiltz, 2013). Our results contrast with previous studies on the neuronal substrate of (differential) letter and digit processing in children. An fMRI study with 4‐year‐old preschoolers revealed greater selectivity for symbols (letters and digits) than objects (shoes and faces) processing in the left hemisphere, but no difference for letters versus digits (Cantlon et al. 2011). In a working memory task involving letters, digits or faces, 8‐year‐old children also did not show specific activation for letters versus digits (Libertus et al. 2009). Recently, an ERP study (Park et al. 2017) did not reveal lateralized response pattern for letters versus digits in 7‐ and 10‐year‐old children. Here on the contrary, inspection of topographies indicated that letters (vs. digits) are selectively associated with left occipital responses (PO9) and digits (vs. letters) to right occipital responses (PO10). In line with neuropsychological data (Starrfelt & Behrmann, 2011), topographies and amplitude values also suggested stronger discrimination abilities for digits than letters in 6‐year‐olds. While requiring further investigation, this might be because digits contain only 10 exemplars, and all (except 0) were presented here. In contrast, letters contain 26 exemplars, and only a subset was used (among which "J" and "Z", not very frequent in French). Finally, digits might bear a meaning by themselves, already known at this age, whereas letters do not mean anything in isolation.

---

### Standards of care in diabetes – 2025 [^31be65a6]. Diabetes Care (2025). High credibility.

Regarding diagnostic investigations for hypoglycemia, more specifically with respect to screening for cognitive impairment, ADA 2025 guidelines recommend to simplify diabetes treatment plans as much as possible for patients with cognitive impairment and tailor to minimize the risk of hypoglycemia.

---

### Lenalidomide (Revlimid) [^90b5d94f]. FDA (2023). Medium credibility.

The dosage of lenalidomide PO for treatment of myelodysplastic syndrome in adults with chromosome 5q deletion (low- or intermediate-1-risk) is 10 mg PO daily until disease progression or unacceptable toxicity

---

### Decimal numbers and safe interpretation of clinical pathology results [^19dfc03d]. Journal of Clinical Pathology (2014). Low credibility.

Objective

To determine the understanding of decimal numbers by medical laboratory scientists, doctors and nurses.

Methods

A Decimal Comparison Test determined the comprehension of decimals numbers. Additional questions sought the participants' understanding of concentrations and reference ranges, and their preferences for the presentation of clinical pathology results.

Results

Of the 108 participants, 40% exhibited poor comprehension of decimal numbers. One-third of the medical laboratory scientists, a quarter of doctors, and half the nurses were characterised as lacking numeracy skills. The majority of participants (60%) thought it would be safer for results to be presented as whole numbers rather than as decimals with leading zeros.

Conclusions

The number of laboratory and clinical staff who show numeracy issues that could lead to misinterpretation of clinical pathology results and contribute to medical error strongly supports recommendations that pathology results should be presented as whole numbers.

---

### Lost in translation: on the problem of data coding in penalized whole genome regression with interactions [^6570ff24]. G3 (2019). Medium credibility.

Correlation of interaction effect estimates under different codings and varying number of variables included in the model:

We use a model with 50, 100 or 150 markers and their interaction effects and compare the Pearson correlations of the interaction estimates obtained with different codings. The results are summarized in Table 2. The values indicate the mean correlation of 50 randomly drawn marker sets. The correlations are between 0.80 and 0.95 and thus relatively high, but not equal to 1. Moreover, for each environment, and for each comparison of two codings, the correlation of the estimates reduces when the number of markers increases. For instance, an obtained mean correlation of the interaction effect estimates using the originally providedcoding and the allele-frequency centered coding is 0.90 (environment 2; 50 randomly selected markers). For a model with 100 markers, this correlation reduces to 0.85, and to 0.80 when using 150 markers. This stepwise reduction of the correlation of the estimates when the number of markers increases, can be observed for any pairwise comparison of two codings, and any underlying data (grown in environment).

Table 2
Pearson correlation of the estimates of the 1225 (4950, 11175) interactions when different marker codings are used. The numbers represent the mean correlation of 50 repetitions with independently, uniformly drawn markers. The standard error of the estimate was in all cases below 0.004. Colors indicate the underlying data: Environment 1, 2, 3 or 4.

Another observation across environments and varying number of markers is that the originalcoding and the column mean centered coding are the most different. For instance, for environment 2 and 150 marker, the mean correlation of the interaction effect estimates is 0.80, which is smaller than the corresponding correlation for original and symmetric coding (0.88) or for symmetric and centered coding (0.88).

For the second type of regression, in which only the size of the interactions are penalized, we receive –as stated by our theoretical results– a correlation of 1 between any two codings, for each of the four underlying environmental conditions and for each number of considered markers. This circumstance also illustrates that potential numerical problems –for instance related to the inversion of the high-dimensional matrix of Eq.(5)– do not strongly occur with the number of interactions modeled in our examples.

---

### The American Society of Colon and Rectal Surgeons clinical practice guidelines for the management of Clostridioides difficile infection [^d705dea3]. Diseases of the Colon and Rectum (2021). High credibility.

Regarding medical management for Clostridioides difficile infection, more specifically with respect to antibiotic therapy, non-severe disease, ASCRS 2021 guidelines recommend to consider administering oral vancomycin or fidaxomicin as first-line therapy in patients with an initial CDI. Avoid using metronidazole alone as first-line therapy.

---

### How deep learning solved my seizure detection problems [^45e96990]. Epilepsy Currents (2020). Medium credibility.

[Box: see text].

---

### The American Society of Colon and Rectal Surgeons clinical practice guidelines for the management of Clostridioides difficile infection [^fef965dc]. Diseases of the Colon and Rectum (2021). High credibility.

Regarding medical management for Clostridioides difficile infection, more specifically with respect to antibiotic therapy, severe disease, ASCRS 2021 guidelines recommend to consider administering oral vancomycin or fidaxomicin as first-line therapy in patients with an initial episode of CDI. Avoid using metronidazole alone as a first-line therapy.

---

### APA clinical practice guideline for the treatment of depression across three age cohorts [^cbb4beb5]. APA (2019). High credibility.

Regarding specific circumstances for major depressive disorder, more specifically with respect to patients with relationship distress, APA 2019 guidelines recommend to consider offering problem-focused couples therapy in adult patients with depression experiencing relationship distress, if a recommended treatment is not acceptable or available.

---

### Nonlinear delay differential equations and their application to modeling biological network motifs [^22bd51a1]. Nature Communications (2021). High credibility.

Motif III: logic

A general class of functions used to describe natural,– and synthetic, biological networks are logic gates, which have two inputs regulating a single output (Fig. 5). Gates exhibit either high ("on") or low ("off") output depending on whether inputs are on or off. For example, the AND gate specifies high output only if both inputs are on. In this section we provide a specific DDE-based framework that covers 14 out of 16 possible 2-input logic operations, and show that these operations form a continuous 2D parameter space.

Fig. 5
A simple approximation for digital logic using a sum of Hill terms recapitulates all monotonic logic functions in a single parameter space.

a A prototypical regulatory network involving logic where X and Y both regulate Z, which must integrate the two signals using some logic before it can in turn activate a downstream reporter R. b Parameter space showing regions where regulation approximately follows 14 of the 16 possible 2-input logic functions depending on the strength of two single-variable Hill regulation terms (η Z 1: regulation of Z by X, η Z 2: regulation of Z by Y). Network logic can be smoothly altered by varying the parameters (η Z 1, η Z 2), with a change of sign in (n 1, n 2) required to switch quadrants. The bottom-left quadrant shows that very weak regulation in both terms leads to an always-off (FALSE) function, weak regulation in one arm only leads to single-input (X, Y) functions, strong regulation in both arms leads to an OR function, and regulation too weak in either arm alone to activate an output but strong enough in sum leads to an AND function. The other three quadrants are related by applying NOT to one or both inputs, with function names related by de Morgan's lawNOT(X OR Y) = NOT X AND NOT Y. In particular, X IMPLY Y = NOT(X) OR Y, X NIMPLY Y = X AND NOT(Y), X NOR Y = NOT X AND NOT Y, and X NAND Y = NOT X OR NOT Y. Truth tables for all 16 logic gates are provided in Supplementary Table 1 for reference. The two non-monotonic logic functions, X XOR Y and X XNOR Y, are those 2 of 16 not reproduced directly using this summing approximation. They can be produced by layering, e.g. NAND gates. c Representative time traces for AND (η Z 1 = η Z 2 = 0.9) and OR (η Z 1 = η Z 2 = 1.8) gates with n 1 = n 2 = −2, n 3 = −20, η R = η Z 1 + η Z 2. The functionwhen n > 0, when n < 0.

---

### Lateralized neural responses to letters and digits in first graders [^78eaa193]. Child Development (2019). Medium credibility.

The sum of baseline‐corrected amplitudes for the three significant harmonics were submitted to a 2 (condition s: lett‐DIG vs. dig‐LETT) × 2 (hemisphere s: PO9 vs. PO10) repeated measures ANOVA. Main effects were not significant (F < 1), but there was a significant Conditions × Hemispheres interaction, F (1, 16) = 5.204; MSE = 2.196; p = .037 (Figure 3 C). Paired sample t ‐tests contrasting hemispheres revealed that for digits, response in the RH (M = .61 µV, CI [0.16, 1.053]) was significantly stronger and more reliable than in the LH (M = .23 µV; CI [−0.08, 0.56], t (16) = −2.466; p = .025; Figure 3 A). For letters, we observed a trend for stronger responses in the LH (M = .49 µV; CI [0.019, 0.957]) than in the RH (M = .14 µV; CI [−0.49, 0.77], t (16) = 1.465; p = .16; Figure 3 B). Variability in individual lateralization scores was somewhat greater for letters (5/17 participants with right lateralization, see Figure 4 for individual data).

Figure 4
Individual data for lateralization scores in each condition. Amplitudes responses in the left hemisphere (PO9) were subtracted from responses in the right hemisphere (PO10). Positive values represent right‐lateralized responses, and negative values, left‐lateralized responses. RH = right hemisphere; LH = left hemisphere; LETT = letters; DIG = digits.

---

### Removal of myomas in asymptomatic patients to improve fertility and / or reduce miscarriage rate: a guideline [^f8ffad38]. Fertility and Sterility (2017). Medium credibility.

Limitations of the literature on reproductive outcomes with myomas — data evaluating reproductive outcomes related to myomas are derived primarily from observational studies; such studies are problematic as they are prone to selection bias and confounding variables; regarding studies assessing the benefits of myomectomy, the literature mostly consists of level-II and -III studies that include small heterogeneous patient populations (infertile vs fertile; symptomatic vs asymptomatic), characterize myomas inconsistently (in terms of location, size, and number of myomas), and often do not consistently or comprehensively evaluate clinically relevant reproductive outcomes such as time to pregnancy, clinical pregnancy rate, live-birth rate, and miscarriage rate.

---

### Prevalence of rare diseases: bibliographic data [^fdd2a97e]. Orphanet Report Series (2024). Medium credibility.

Worldwide, the prevalence of deletion 5q35 is estimated at 0.00014 per 100,000 population.

---

### 2025 ESC guidelines for the management of myocarditis and pericarditis [^230fe03b]. European Heart Journal (2025). High credibility.

Regarding specific circumstances for acute pericarditis, more specifically with respect to patients with post-cardiac injury syndromes, ESC 2025 guidelines recommend to initiate IL-1 antagonists in patients with refractory post-cardiotomy inflammatory syndrome to prevent recurrences and progression to constriction.

---

### Clarifications to the VSP 2011 operations manual. December 2013 revision [^a8cba19c]. CDC (2013). Medium credibility.

Clarification of hook length measurement — The rescue or shepherd's hook must be long enough to touch the bottom center of the deepest portion of the RWF plus 2 feet (0.6 meters) as measured from the closest edge without an obstruction, and this edge can only be used for measurement if someone could freely walk down both sides without an obstruction such as a waterfall, fountain, statue, etc. For a rectangular pool, the shorter distance would be from the long side of the rectangle as long as there are no obstructions, and the 2 feet (0.6 meters) is measured from where the shepherd's hook crosses the fill line of the RWF.

---

### The snm procedure guideline for general imaging 6.0 [^638aaec4]. SNMMI (2010). Medium credibility.

SNM Procedure Guideline for General Imaging — matrix size and pixel depth in nuclear medicine acquisitions notes that matrix size is almost always a power of 2 with typical values 64 x 64, 128 x 128, 256 x 256 and 512 x 512, and that non-square matrix sizes also exist for whole-body studies. Each pixel can be represented with a single byte (pixel values ranging from 0 to 255 counts) or with 16 bit words (pixel values ranging up to a maximum of 32k or 64k). Overflow occurs when the number of counts recorded at some given position exceeds the maximum number of counts, and overflow is more likely to occur when using a byte matrix.

---

### Primary biliary cholangitis: 2018 practice guidance from the American Association for the Study of Liver Diseases [^eb708b79]. Hepatology (2019). High credibility.

Regarding medical management for primary biliary cholangitis, more specifically with respect to first-line therapy, AASLD 2019 guidelines recommend to initiate oral UDCA at a dose of 13–15 mg/kg/day for patients with PBC with abnormal liver enzymes, regardless of histologic stage.

---

### Dactinomycin [^e7dd90f3]. FDA (2020). Medium credibility.

Regarding the use of dactinomycin IV (also known as Cosmegen) in patients with unexplained abnormality in LFTs:

- Reduce dose by 50%.
- Monitor serum aminotransferases.

---

### Standards of care in diabetes – 2025 [^ef149bc3]. Diabetes Care (2025). High credibility.

Regarding specific circumstances for diabetic nephropathy, more specifically with respect to pediatric patients, ADA 2025 guidelines recommend to determine the eGFR at the time of diagnosis and annually thereafter.

---

### Lateralized neural responses to letters and digits in first graders [^607e0fe9]. Child Development (2019). Medium credibility.

Figure 3
Discrimination responses for digits among letters and vice‐versa. (A and B) Topographies and SNR response spectra for letters‐DIGITS (A) and digits‐LETTERS (B). Each spectrum displays the two best electrodes for each condition, PO10, I2 for digits and PO9, I1 for letters. On the response spectra, the frequencies of significant (Z > 1.64) categorical responses over both conditions are marked as F/5 (1.2 Hz), 2F/5 (2.4 Hz) and 3F/5 (3.8 Hz). The response at 6 Hz represents base response at the sequence stimulation frequency. On the right side of the spectra, the averaged SNR of three significant harmonics is represented centered with 10 surrounding bins on each side. Topographies display the amplitude of discrimination responses color‐scaled between 0 µV (blue) to 0.61 µV (red) C. Mean amplitude values (in µV) and SD of discrimination responses, in each hemisphere (LH: dark gray, RH: light gray) per condition, giving rise to a significant interaction between Hemispheres and Conditions. RH = right hemisphere; LH = left hemisphere; LETT = letters; DIG = digits. [Color figure can be viewed at]

---

### Input-output maps are strongly biased towards simple outputs [^c234de58]. Nature Communications (2018). Medium credibility.

Finally, the presence ofterms is perhaps the least understood limitation for applying formal AIT to real-world settings. Nevertheless, important recent work applying the full AIT coding theorem to very short strings, has suggested that the presence ofterms, both in the definition of K (x) and in the coding theorem relationship between P (x) and K (x), does not preclude the possibility of making decent predictions for smaller systems.

Taken together, the arguments above allow us to make our central ansatz, namely that for many real-world maps, the upper bound in Eq. (2) can be approximated as:where the constants a > 0 and b depend on the mapping, but not on x. These constants account for theterms and the particularities of the complexity approximation. Just as for the full coding theorem, there is a strong exponential decay in the probability upper bound upon a linear increase in complexity. This means that high-probability outputs must be simple (have low), while high-complexity outputs must be exponentially less probable. We call such phenomena that arise from Eq. (3) simplicity bias.

In contrast to the full AIT coding theorem, the lack of a lower bound in Eq. (3) means that simple outputs may also have low probabilities. Furthermore (as shown in Supplementary Note 5), we expect the upper bound to decay to a lowest value of about 1/ N O for the largest complexity. This minimal value for the bound is also the mean probability, and if P (x) is highly biased, many outputs will have probabilities below the mean. In other words, if an output x is chosen uniformly from the set of all outputs, it is not likely to be that close to the bound. On the other hand (Supplementary Note 5) if x is generated by choosing random inputs, which naturally favours outputs with larger P (x), then we can expect that P (x) is relatively close to the bound, at least on a logarithmic scale. In short, the upper bound of the simplicity bias Eq. (3) should be tight for most inputs, but may be weak for many outputs (Of course if the mapping were a full UTM, then Eq. (3) would simply revert to the full coding theorem again with an upper and a lower bound.).

---

### Clinical practice recommendations for the diagnosis and management of X-linked hypophosphataemia [^df280821]. Nature Reviews: Nephrology (2025). High credibility.

Regarding medical management for X-linked hypophosphatemia, more specifically with respect to vitamin D and phosphate salts, adults, XLH-EG 2025 guidelines recommend to reduce doses of active vitamin D in patients anticipated to undergo long-term immobilization to prevent hypercalciuria and hypercalcemia.

---

### Quetiapine (Seroquel) [^7371015d]. FDA (2025). Medium credibility.

Regarding the use of quetiapine fumarate ER PO (also known as Seroquel XR) in patients with crCl < 10 mL/min:

- No guidance available.

---

### Standards of care in diabetes – 2025 [^1b6872cb]. Diabetes Care (2025). High credibility.

Regarding medical management for hypoglycemia, more specifically with respect to oral carbohydrates, ADA 2025 guidelines recommend to administer glucose as the preferred treatment of conscious patients with glucose < 70 mg/dL (< 3.9 mmol/L), although any form of glucose-containing carbohydrate may be used. Avoid using foods or beverages high in fat and/or protein for initial treatment of hypoglycemia. Repeat the treatment if hypoglycemia persists 15 minutes after initial administration.

---

### Promoting optimal development: screening for behavioral and emotional problems [^a367d19f]. Pediatrics (2015). Medium credibility.

Current detection and treatment gap — Behavioral and emotional problems and concerns in children and adolescents are not being reliably identified or treated in the US health system, and current estimates suggest that fewer than 1 in 8 children with identified mental health problems receive treatment.

---

### K-space undersampling in PROPELLER imaging [^580bf6fa]. Magnetic Resonance in Medicine (2005). Low credibility.

PROPELLER MRI (periodically rotated overlapping parallel lines with enhanced reconstruction) provides images with significantly fewer B(0)-related artifacts than echo-planar imaging (EPI), as well as reduced sensitivity to motion compared to conventional multiple-shot fast spin-echo (FSE). However, the minimum imaging time in PROPELLER is markedly longer than in EPI and 50% longer than in conventional multiple-shot FSE. Often in MRI, imaging time is reduced by undersampling k-space. In the present study, the effects of undersampling on PROPELLER images were evaluated using simulated and in vivo data sets. Undersampling using PROPELLER patterns with reduced number of samples per line, number of lines per blade, or number of blades per acquisition, while maintaining the same k-space field of view (FOV(k)) and uniform sampling at the edges of FOV(k), reduced imaging time but led to severe image artifacts. In contrast, undersampling by means of removing whole blades from a PROPELLER sampling pattern that sufficiently samples k-space produced only minimal image artifacts, mainly manifested as blurring in directions parallel to the blades removed, even when reducing imaging time by as much as 50%. Finally, undersampling using asymmetric blades and taking advantage of Hermitian symmetries to fill-in the missing data significantly reduced imaging time without causing image artifacts.

---

### Computerized bone age estimation using deep learning based program: evaluation of the accuracy and efficiency [^4d9d002b]. AJR: American Journal of Roentgenology (2017). Low credibility.

Objective

The purpose of this study is to evaluate the accuracy and efficiency of a new automatic software system for bone age assessment and to validate its feasibility in clinical practice.

Materials and Methods

A Greulich-Pyle method-based deep-learning technique was used to develop the automatic software system for bone age determination. Using this software, bone age was estimated from left-hand radiographs of 200 patients (3–17 years old) using first-rank bone age (software only), computer-assisted bone age (two radiologists with software assistance), and Greulich-Pyle atlas-assisted bone age (two radiologists with Greulich-Pyle atlas assistance only). The reference bone age was determined by the consensus of two experienced radiologists.

Results

First-rank bone ages determined by the automatic software system showed a 69.5% concordance rate and significant correlations with the reference bone age (r = 0.992; p < 0.001). Concordance rates increased with the use of the automatic software system for both reviewer 1 (63.0% for Greulich-Pyle atlas-assisted bone age vs 72.5% for computer-assisted bone age) and reviewer 2 (49.5% for Greulich-Pyle atlas-assisted bone age vs 57.5% for computer-assisted bone age). Reading times were reduced by 18.0% and 40.0% for reviewers 1 and 2, respectively.

Conclusion

Automatic software system showed reliably accurate bone age estimations and appeared to enhance efficiency by reducing reading times without compromising the diagnostic accuracy.

---

### Colorectal cancer screening and prevention [^c270f773]. American Family Physician (2025). High credibility.

Regarding screening and diagnosis for colon cancer, more specifically with respect to indications for screening, general population, aged 76–85 years, AAFP 2025 guidelines recommend to consider obtaining screening for CRC in adults aged 76–85 years at average risk based on overall health status, prior screening history, and patient preferences.

---

### Efficient neural codes naturally emerge through gradient descent learning [^f618c5f4]. Nature Communications (2022). High credibility.

Fig. 5
The effect of input statistics on network sensitivity can be understood with linear network models.

Despite their simplicity, these show human-like learning phenomena. a We trained linear networks to reconstruct black and white patches of natural images. b The statistics of natural images can be analyzed with Principal Components Analysis (PCA); the variance of each successive PC decreases with a characteristic power law decay. c When learning with gradient descent, the weight matrix W learns each PC separately and in order of their variance. The sharpness of the sigmoidal learning curve is controlled by the network depth (SI Fig. 2) d Human perceptual learning curves are also sigmoidal, and increasing task difficulty delays learning dynamics. Data replotted from ref.; subjects trained to detect the orientation of a line, and the difficulty of the task was controlled by a masking stimulus. e – i Paradigm for measuring the sensitivity to spatial frequency of W. f Every 50 learning steps we plotted the inverse square root of the sensitivity to spatial frequency, which is a proxy for detection thresholds. At each step note the linear increase above an elbow frequency. g Human data on spatial frequency thresholds, replotted from ref. h An artificial spatial 'acuity' grows nearly linearly with training; 'acuity' is defined as the maximum spatial frequency for which the artificial threshold is below a value of 0.1. i In infants and children, the spatial acuity - the highest spatial frequency observable for high-contrast gratings - also increases linearly with age. Replotted from ref. with error bars representing ± SEM over n = 4–10 subjects (varying per point, exact number not reported).

In our demonstrative task we will examine the sensitivity of the outputto the magnitude of each principal component that makes up an image (as provided by PCA on the inputs, Fig. 5 b) and describe why and how it tracks the input statistics. The mathematical analysis for this feature is much simpler than, say, for orientation. In this case also we have some expectation as to what pattern of sensitivity the efficient coding framework predicts because the principal components (PCs) are ordered by their variance. An efficient code in the presence of independent internal noise should be more sensitive to higher-variance PCs. Indeed, earlier PCs are composed of lower spatial frequencies, and humans are better at detecting changes in lower spatial frequencies (Fig. 5 e). If gradient descent provides a similar effect, we should find that the outputbecomes sensitive to lower PCs first.

---

### Single mini-incisional blepharoplasty with the orbicularis-orbital septum fixation technique for the creation of a natural double eyelid [^70d6d14a]. Aesthetic Plastic Surgery (2024). Medium credibility.

Discussion

The goal of a double-eyelid blepharoplasty is to create a stable and natural-looking pretarsal crease with a short recovery period. For such a purpose, many mini-incisional double-eyelid blepharoplasty techniques have been suggested in recent years. In these techniques, the number and length of the incisions, the debulking method, and the fixation and suture technique vary significantly, leading to different postoperative outcomes.

The number of incisions of mini-incisional double-eyelid blepharoplasty ranges from one to four, and the length of each incision ranges from 2 mm to 1 cm, with a total length of all incisions of about 1 cm. The swelling time of these techniques was mostly 2 weeks to 2 month. Our previous study with modified partial incisional technique involved three 2 mm incisions, with the majority of the swelling reduced in 2 weeks and completely disappeared in the first month for most patients. In this study with single mini-incisional and orbicularis-orbital septum fixation technique, the incision was designed to be about 3–5 mm long, the swelling was mild for all patients, and they can go to work or engage in daily communication immediately after the surgery. Moreover, the majority of the swelling was reduced in 5 days and completely disappeared in 2 weeks for most patients. These data suggest that reducing the number and length of incisions can indeed reduce tissue swelling and decrease recovery time.

In most of the previous mini-incisional double-eyelid surgeries, pretarsal orbicular muscle and pretarsal fat tissue were partially or totally removed. The eyelid crease become depressed due to the excessive removal of pretarsal muscle and soft tissue. Additionally, the adhesion between the skin and the tarsus makes it difficult for the pretarsal tissue to move along with eye movements, hence resulting in a stiff appearance. Some experts proposed modified double-eyelid surgery with the removal a strip of orbicularis muscle and pretarsal adipose tissue. After the surgery, the scar contracture and crease depression were noticeable because the skin was directly adhered to the tarsal plate, demonstrating that excessive orbicularis muscle removal could cause scaring and depression. Therefore, to achieve a natural supratarsal crease without obvious depression or conspicuous scar, the orbicularis muscle is kept without excision. In this study, no depression or visible scar was observed, suggesting that orbicularis muscle preservation can effectively prevent eyelid crease depression and alleviate scar.

---

### Clinical practice recommendations for the diagnosis and management of X-linked hypophosphataemia [^9465370f]. Nature Reviews: Nephrology (2025). High credibility.

Regarding medical management for X-linked hypophosphatemia, more specifically with respect to vitamin D and phosphate salts, adults, XLH-EG 2025 guidelines recommend to administer oral phosphate supplements in conjunction with active vitamin D in adults.

---

### Coagulation factor X human (Coagadex) [^54f19145]. FDA (2024). Medium credibility.

The dosage of factor X (human) IV for treatment of bleeding episodes in adults with factor X deficiency is 25 unit(s)/kg IV q24h until bleeding stops

---

### Diagnosis, management and treatment of the Alport syndrome-2024 guideline on behalf of ERKNet, ERA and ESPN [^45eede36]. Nephrology, Dialysis, Transplantation (2025). High credibility.

Regarding diagnostic investigations for Alport syndrome, more specifically with respect to hearing assessment, ERA/ERN ERKNet/ESPN 2025 guidelines recommend to consider obtaining a hearing evaluation at diagnosis or upon reaching adulthood in female patients with X-linked alport syndrome, and then every 5 years in the absence of hearing loss symptoms.