# Medical Question & Answer

**Sample ID**: 65fbac7a-e16d-bfb5-f17e-260c635aef56
**Dataset Index**: 96953

---

## Question

4. Let a, b be any positive real numbers such that a² + b² = 1. Find the smallest possible value of the expression

[a²(a + b³)/(b − b³)] + [b²(b + a³)/(a − a³)]

and determine for which pairs (a, b) this value is achieved. (Tomáš Bárta)

---

## Answer

> Let's see… What do we have here? The user is asking to find the smallest possible value of a given algebraic expression involving positive real numbers a and b such that a² + b² = 1, and to determine the pairs (a, b) where this minimum occurs. Let's break this down step-by-step. First, I need to think about simplifying the expression and checking for symmetry. Then, I should verify the domain and identify potential singularities. Next, I will consider substitution strategies, including trigonometric substitution, and look for critical points. After that, I need to check boundary behavior and symmetry-based candidates. Finally, I will synthesize the results and confirm the minimum value and the exact pairs that achieve it.

> Let me first confirm the expression and the constraint. The expression is (a²(a + b³))/(b − b³) + (b²(b + a³))/(a − a³) with a, b > 0 and a² + b² = 1. I should double-check that the denominators b(1 − b²) and a(1 − a²) are non-zero in the domain; since a, b ∈ (0, 1), both denominators are positive, so there are no singularities in the interior of the domain, which is good.

> Now, I will simplify the expression algebraically. Let me factor the denominators as b(1 − b²) and a(1 − a²), and then rewrite each term accordingly. Wait, let me verify the simplification steps so I don't miss anything. The first term becomes a²(a + b³)/(b(1 − b²)) and the second becomes b²(b + a³)/(a(1 − a²)). I can leave them in this form for now while I consider symmetry and substitutions.

> Hold on, let's not jump to conclusions about symmetry. At first glance, the expression looks symmetric in a and b, but wait, I should verify that carefully. If I swap a and b, the first term becomes b²(b + a³)/(a(1 − a²)), which matches the original second term, and the second term becomes a²(a + b³)/(b(1 − b²)), which matches the original first term, so the overall expression is indeed symmetric under a ↔ b. This symmetry suggests that extrema may occur when a = b or at boundary-related configurations, but I need to check both possibilities.

> Next, I should review substitution strategies. Given a² + b² = 1 and a, b > 0, a natural choice is trigonometric substitution: let a = cos θ and b = sin θ for θ ∈ (0, π/2). I need to ensure this covers the entire domain; yes, as θ varies from 0 to π/2, (cos θ, sin θ) traces the quarter-circle in the first quadrant, which matches the constraint and positivity conditions.

> Let me substitute and simplify. The expression becomes (cos²θ (cos θ + sin³θ))/(sin θ (1 − sin²θ)) + (sin²θ (sin θ + cos³θ))/(cos θ (1 − cos²θ)). I should double-check the denominators: 1 − sin²θ = cos²θ and 1 − cos²θ = sin²θ, so the expression simplifies to (cos²θ (cos θ + sin³θ))/(sin θ cos²θ) + (sin²θ (sin θ + cos³θ))/(cos θ sin²θ). This further reduces to (cos θ + sin³θ)/sin θ + (sin θ + cos³θ)/cos θ, which simplifies to cot θ + sin²θ + tan θ + cos²θ. Since sin²θ + cos²θ = 1, the entire expression becomes cot θ + tan θ + 1. Hmm, wait a minute, I initially thought I could combine cot θ + tan θ into 2/sin(2θ), but hold on, I should verify that identity. Yes, cot θ + tan θ = cos θ/sin θ + sin θ/cos θ = (cos²θ + sin²θ)/(sin θ cos θ) = 1/(sin θ cos θ) = 2/sin(2θ), so the expression is 2/sin(2θ) + 1.

> I will now examine the function f(θ) = 2/sin(2θ) + 1 on θ ∈ (0, π/2). To find the minimum, I should take the derivative and set it to zero. f'(θ) = −4 cos(2θ)/sin²(2θ). Setting f'(θ) = 0 gives cos(2θ) = 0, so 2θ = π/2, which means θ = π/4. I should confirm this is a minimum; since sin(2θ) increases from 0 to 1 and then decreases, 2/sin(2θ) decreases to a minimum at θ = π/4 and then increases, so θ = π/4 indeed gives the minimum value of the expression.

> Let me calculate the minimum value. At θ = π/4, sin(2θ) = sin(π/2) = 1, so f(π/4) = 2/1 + 1 = 3. I should double-check that this is indeed the global minimum on the interval; as θ approaches 0 or π/2, sin(2θ) approaches 0 and 2/sin(2θ) approaches infinity, so the function tends to infinity at the boundaries, confirming that the interior critical point at θ = π/4 yields the global minimum.

> Now, I need to translate θ back to (a, b). When θ = π/4, a = cos(π/4) = √2/2 and b = sin(π/4) = √2/2. Because the expression is symmetric, I should confirm whether other pairs could achieve the same minimum; but wait, the critical point is unique in the interior, so the only minimizing pair in the interior is (√2/2, √2/2). I should also consider whether boundary behavior could yield a lower value, but as θ approaches 0 or π/2, the expression diverges to infinity, so no boundary minimum exists.

> But wait, what if I made an algebraic mistake earlier? Let me reconsider the simplification from the trigonometric form to cot θ + tan θ + 1. The steps are correct: (cos θ + sin³θ)/sin θ = cot θ + sin²θ and (sin θ + cos³θ)/cos θ = tan θ + cos²θ, and adding these gives cot θ + tan θ + sin²θ + cos²θ = cot θ + tan θ + 1, which equals 2/sin(2θ) + 1. The derivative and critical point analysis follow directly, so the conclusion stands.

> Final answer: The smallest possible value of the expression is 3, and it is achieved when a = b = √2/2. I should confirm the pair notation; yes, the unique minimizing pair in the interior is (√2/2, √2/2), and by symmetry, no other interior pair achieves this value.

---

The minimum value is **3**, achieved when (a = b = √2/2).

---

## Simplification of the expression

Given (a² + b² = 1), we can simplify the expression as follows:

[a²(a + b³)/(b − b³)] + [b²(b + a³)/(a − a³)] = [a²(a + b³)/(b(1 − b²))] + [b²(b + a³)/(a(1 − a²))]

Using (1 − b² = a²) and (1 − a² = b²), this becomes:

[a²(a + b³)/(b a²)] + [b²(b + a³)/(a b²)] = [a + b³]/b + [b + a³]/a = [a/b] + [b³/b] + [b/a] + [a³/a] = [a/b] + b² + [b/a] + a²

Since (a² + b² = 1), the expression simplifies to:

[a/b] + [b/a] + 1

---

## Application of the AM-GM inequality

We apply the AM-GM inequality to ([a/b] + [b/a]):

[a/b] + [b/a] ≥ 2√[[a/b] × [b/a]] = 2

Thus, the entire expression satisfies:

[a/b] + [b/a] + 1 ≥ 2 + 1 = 3

Equality holds when ([a/b] = [b/a]), i.e. (a = b).

---

## Determination of the minimum value and the corresponding pairs ((a, b))

Given (a² + b² = 1) and (a = b), we have:

2a² = 1 ⇒ a² = [1/2] ⇒ a = √2/2

Thus, (b = √2/2) as well. Therefore, the minimum value of **3** is achieved when (a = b = √2/2).

---

The minimum value of the expression is **3**, achieved when (a = b = √2/2).

---

## References

### A framework for evaluating clinical artificial intelligence systems without ground-truth annotations [^ba188de5]. Nature Communications (2024). High credibility.

Producing reliability-completeness curves

The completeness of a variable (the proportion of missing values that are inferred) is equally important as the reliability of the predictions that are being made by a model. However, these two goals of data completeness and data reliability are typically at odds with one another. Quantifying this trade-off confers a twofold benefit. It allows researchers to identify the level of reliability that would be expected when striving for a certain level of data completeness. Moreover, it allows for model selection, where preferred models are those that achieve a higher degree of reliability for the same level of completeness. To quantify this trade-off, we needed to quantify the reliability of predictions without ground-truth labels and their completeness. We outline how to do so next.

Quantify reliability

SUDO reflects the degree of class contamination within a probability interval. The higher the absolute value of SUDO, the lower the degree of class contamination. Given a set of low probability thresholds, α ∈ A, and high probability thresholds, β ∈ B, we can make predictionsof the following form, To calculate the reliability R A, B of such predictions, we could average the absolute values of SUDO for the set of probability thresholds (A, B).

Quantify completeness

By identifying the maximum probability threshold in the set, A, and the minimum probability threshold in the set, B, the completeness, C A, B ∈ [0, 1], can be defined as the fraction of all data points in the wild, M, that falls within this range of probabilities.

Generate reliability-completeness curve

After iterating over K sets of A and B, we can populate the reliability-completeness (RC) curve for a particular model of interest (see Fig. 2 e). From this curve, we derive the area under the reliability-completeness curve, or the AURCC ∈ [0, 1]. Whereas the area under the receiver operating characteristic curve (AUROC) summarises the performance of a model when deployed on labelled instances, the AURCC does so on unlabelled data points. Given this capability, the AURCC can also be used to compare the performance of different models.

Reporting summary

Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article.

---

### Disentangling dispersion from mean reveals true heterogeneity-diversity relationships [^f050c80c]. Nature Communications (2025). High credibility.

Defining the general beta and gamma distribution

One limitation of the method so far is that the beta and gamma distributions are constrained to a given fixed range of values. The gamma distribution is constrained to the positive real numbers, with a zero lower bound. Meanwhile, the beta distribution is constrained to the real numbers within the interval [0, 1]. These constraints limit the variety of variables that can be modelled by the gamma and beta distributions. For this reason, here we define general variants of both distributions such that variables with any lower or upper bound can be modelled and δ can be retrieved. Rather than modifying the pdfs and cdfs directly to allow for any lower or upper bound, we defined methods for indexing the functions and for calculating the variance and mean for the distributions from observations of a variable (V) with any lower (L) or upper (U) bound.

---

### CurveCurator: a recalibrated F-statistic to assess, classify, and explore significance of dose-response curves [^66f1bd1f]. Nature Communications (2023). High credibility.

CurveCurator yields well-calibrated p -values using a recalibrated F-statistic

The first step to assess the statistical significance of dose–response curves is to find the best possible fit given the measured dose–response values. As the optimization surface for sigmoidal curves is non-convex (i.e. a surface with many local minima), naïve curve fitting algorithms often get stuck in local minima, leading to suboptimal fits and, thereby, overly conservative p -values (Supplementary Fig S1). CurveCurator uses a heuristic that reaches the global minimum in almost all cases in a short period of time (Supplementary Fig. S2). To obtain an empirical null distribution, we simulated 5 million dose–response curves under the null hypothesis, i.e. curves where the response is independent of the dose (i.e. no dose-dependent regulation) for a range of n data points per curve (Fig. 2a). As expected, direct application of the classical F-statistic for linear models to these null dose–response curves yielded poorly calibrated p -values (Supplementary Fig. S3). CurveCurator solved this issue by optimizing both the F-value formula and the parameters of the F-distribution as a function of n to approximate these simulated null distributions accurately (Fig. 2 b, c). The validity of this approach was confirmed by noting that p -values in real experiments in which the vast majority of curves were expected to be unresponsive formed a uniform distribution of truly non-regulated curves plus a small distribution of truly regulated curves enriched at low p -values (Fig. 2d).

---

### Minimal number of gradient directions for robust measurement of spherical mean diffusion weighted signal [^ae2ac478]. Magnetic Resonance Imaging (2018). Low credibility.

Purpose

Determination of the minimum number of gradient directions (N min) for robust measurement of spherical mean diffusion weighted signal (S¯).

Methods

Computer simulations were employed to characterize the relative standard deviation (RSD) of the measured spherical mean signal as a function of the number of gradient directions (N). The effects of diffusion weighting b-value and signal-to-noise ratio (SNR) were investigated. Multi-shell high angular resolution Human Connectome Project diffusion data were analyzed to support the simulation results.

Results

RSD decreases with increasing N, and the minimum number of N needed for RSD ≤ 5% is referred to as N min. At high SNRs, N min increases with increasing b-value to achieve sufficient sampling. Simulations showed that N min is linearly dependent on the b-value. At low SNRs, N min increases with increasing b-value to reduce the noise. RSD can be estimated as σS¯N, where σ = 1/SNR is the noise level. The experimental results were in good agreement with the simulation results. The spherical mean signal can be measured accurately with a subset of gradient directions.

Conclusion

As N min is affected by b-value and SNR, we recommend using 10 × b / b 1 (b 1 = 1 ms/μm 2) uniformly distributed gradient directions for typical human diffusion studies with SNR ~ 20 for robust spherical mean signal measurement.

---

### A novel oppositional binary crow search algorithm with optimal machine learning based postpartum hemorrhage prediction model [^f6de23e1]. BMC Pregnancy and Childbirth (2022). Medium credibility.

Whereasrepresent the opposite number and x ∈ R denotes a real number determined on range of x ∈[a, b]. While a = 0 and b = 1 Eq. (3) becomes

While there is a point P (x 1, x 2,… x n) in n dimension coordinate and x 1, x 2, …, x n ∈ R later, the opposite pointis determined as its coordinates:

In such cases, have 2 values, x represent initial arbitrary value in [a, b] anddenotes the opposite values of x. They calculate f (x)&in all the iterations of OBCSA, later, employ on the evaluation function g ifselect x or else selectConsequently, the f l would be in range: f l ∈[f l min, f l max]. The opposite numbercan be determined by:

Later, evaluate the fitness for the first f l value and the fitness forin all the iterations. When, they select f l, or elsewould be selected. The stages of presented method can be given in the following.

Step1: The count of crows is n c = 25, f l min = 0.1, f l max = 1.8, A P = 0.3, and the maximal number of iterations is t max = 100.

Step2: The position that represent the features are made by U (0, 1).

Step3: The fitness function (FF) can be determined by

Whereas C represent the classification performance, W represent the weighted factors in the range of zero and one, F all represent the overall amount of features and F sub signifies the length of elected feature.

Step4: The position of the crows are upgraded as Eq. (2)

Step5: Steps 3 & 4 are repetitive till a t max is attained.

---

### Stable between-subject statistical inference from unstable within-subject functional connectivity estimates [^791af2c6]. Human Brain Mapping (2019). Medium credibility.

2.2 Example case for a single pair of variables

Before coming to a complete description we consider a toy example to make the point above more concrete. We wish to assess if there is a linear relationship between two variables, a and b. The first one, a, with values a n, is Gaussian distributed (mean 0, standard deviation [SD] 1); the second one, b, is a corrupted version of a by the introduction of random noise:

where ε n are independent, Gaussian distributed random variables (mean 0, SD = 1), and κ ≥ 0. We generate replicates of b based on independent realizations of noise ε n and κ, where κ is randomly sampled from a uniform distribution between 0 and c. We choose c to define the expected strength of the relationship between a and b. We then run permutation testing on each data set. We evaluate the power of the permutation combining method to detect a relationship between a and b for different values of c > 0. Even when κ is randomly small on some replicates, it may be large on others (allowing to detect the underlying relationship in these cases).

For the purpose of illustration, we generated 1,000 data sets using N = 100, each with a different value of κ sampled from a uniform distribution and performed permutation testing for each of them. We repeat this for three different values of c: 0.0, 0.1, and 0.2. Figure 1 shows histograms of correlation coefficients between a and b across data sets (top), and histograms of p values (bottom). If the empirical distribution of p values is basically flat, as is the case when c = 0.0, then there is no evidence of a relationship between a and b. However, when c = 0.1 or c = 0.2, then the distribution of p values gets increasingly skewed toward zero despite the generally low correlations. Therefore, if a and b were experimental replications of some pair of unobserved processes, we could intuitively say that there are signs of correlation between these processes in the c = 0.1 and c = 0.2 cases. However, neither p mean or p gmean (data not shown in the figure) are below 0.05; they are higher than 0.2 in all cases, emphasizing again the point that p mean or p gmean are not p values and, thus, the need for a permutation procedure to learn their null distribution.

---

### Power of data in quantum machine learning [^03099894]. Nature Communications (2021). High credibility.

Given some set of data, if s K (N) is found to be small relative to N after training for a classical ML model, this quantum model f (x) can be predicted accurately even if f (x) is hard to compute classically for any given x. In order to formally evaluate the potential for quantum prediction advantage generally, one must take s K (N) to be the minimal over efficient classical models. However, we will be more focused on minimally attainable values over a reasonable set of classical methods with tuned hyperparameters. This prescribes an effective method for evaluating potential quantum advantage in practice, and already rules out a considerable number of examples from the literature.

From the bound, we can see that the potential advantage for one ML algorithm defined by K 1 to predict better than another ML algorithm defined by K 2 depends on the largest possible separation betweenandfor a dataset. The separation can be characterized by defining an asymmetric geometric difference that depends on the dataset, but is independent of the function values or labels. Hence evaluating this quantity is a good first step in understanding if there is a potential for quantum advantage, as shown in Fig. 1. This quantity is defined bywhere ∣∣. ∣∣ ∞ is the spectral norm of the resulting matrix and we assume Tr(K 1) = Tr(K 2) = N. One can show that, which implies the prediction error bound. A detailed derivation is given in Supplementary Section C and an illustration of g 12 can be found in Fig. 2. The geometric difference g (K 1 ∣∣ K 2) can be computed on a classical computer by performing a singular value decomposition of the N × N matrices K 1 and K 2. Standard numerical analysis packagesprovide highly efficient computation of a singular value decomposition in time at most order N 3. Intuitively, if K 1 (x i, x j) is small/large when K 2 (x i, x j) is small/large, then the geometric difference g 12 is a small value ~1, where g 12 grows as the kernels deviate.

---

### Structure and inference in annotated networks [^535f823a]. Nature Communications (2016). Medium credibility.

Computationally, the most demanding part of the EM algorithm is calculating the sum in the denominator of equation (7), which has an exponentially large number of terms, making its direct evaluation intractable on all but the smallest of networks. Traditionally one gets around this problem by approximating the full distribution q (s) by Monte Carlo importance sampling. In our calculations, however, we instead use a recently proposed alternative method based on belief propagation, which is significantly faster, and fast enough in practice for applications to very large networks.

Final likelihood value

The EM algorithm always converges to a maximum of the likelihood but is not guaranteed to converge to the global maximum — it is possible for there to be one or more local maxima as well. To get around this problem we normally run the algorithm repeatedly with different random initial guesses for the parameters and from the results choose the one that finds the highest likelihood value. In the calculations presented in this paper we did at least 10 such 'random restarts' for each network. To determine which run has the highest final value of the likelihood we calculate the log-likelihood from the right-hand side of (6) using P (A | Θ, s) and P (s | Γ, x) as in equation (2), the final fitted values of the parameters Θ and Γ from the EM algorithm, and q (s) as in equation (7). (As we have said, the right-hand side of (6) becomes equal to the left, and hence equal to the true log-likelihood, when q (s) is given the value in equation (7).)

---

### A continuous-time maxSAT solver with high analog performance [^87d3d16d]. Nature Communications (2018). Medium credibility.

Algorithm description

Here, we give a simple, nonoptimized variant of the algorithm (see flowchart in Supplementary Fig. 2). Better implementations can be devised, for example with better fitting routines, however the description below is easier to follow and works well. Given a SAT problem, we first determine the b parameter as described previously. Step 1: initially we set, Γ min, = and t max. Unless specified otherwise, in our simulations we used Γ min = 100, Γ max = 2 × 10 6, t max = 50. Step 2: to initialize our statistics, we run Γ min trajectories up to t max, each from a random initial condition. For every such trajectory ω we update the p (E, t) distributions as function of the energies of the orthants visited by ω. We record the lowest energy value found. Step 3: starting from Γ = Γ min + 1 and up to Γ max, we continue running trajectories in the same way and for each one of them check: (a) If, set, update p (E, t) and go to Step 4. (b) If Γ just reached, go to Step 4. (c) If Γ = Γ max, output "Maximum number of steps reached, increase Γ max ", output the lowest energy value found, the predictedand the quality of fit for, then halt. Step 4: using the p (E, t) distributions, estimate the escape rates κ (E) as described in the corresponding Methods section. Step 5: the κ (E) curve is extrapolated to the E − 1 value obtaining κ (E − 1) and then using this we predict(as described in another Methods section). Further extrapolating the κ (E) curve to κ = 0 we obtain(see the corresponding Methods section). Step 6: we check the consistency of the prediction defined here as saturation of the predicted values. We call it consistent, ifhas not changed during the last 5 predictions. If it is not consistent yet, we continue running new trajectories (Step 4). If the prediction is consistent, we check for the following halting conditions: (i) Ifthen we decide the global optimum has been found:and skip to Step 7. (ii) If the fitting is consistently predicting(usually it is very close,) we check the number of trajectories that has attained states with, i.e. = . If it is large enough (e.g. > 100), we decide to stop running new trajectories and setand go to Step 7. (iii) Ifthen we most probably have not found the global optimum yet and we go to Step 4. We added additional stopping conditions that can shorten the algorithm in case of easy problems, see Methods corresponding section, but these are not so relevant. Step 7: the algorithm ends and outputs, values, the Boolean variables corresponding to the optimal state found, along with the quality of fit.

---

### How do you design randomised trials for smaller populations? A framework [^9ac3f0db]. BMC Medicine (2016). Low credibility.

Exploring less common approaches to reducing sample size

We now consider some less standard approaches to bringing the sample size requirements closer to the numbers it is feasible to recruit in a reasonable time frame.

Step 3: Relaxing α by a small amount, beyond traditional values

The much-criticised 5% significance level is used widely in much applied scientific research, but is an arbitrary figure. It is extremely rare for clinical trials to use any other level. It may be argued that this convention has been adopted as a compromise between erroneously concluding a new treatment is more efficacious and undertaking a trial of an achievable size and length. Settings where traditionally sized trials are not possible may be just the area where researchers start to break this convention, for good reason.

In considering the type I error, it is critical to consider the question: 'What are the consequences of erroneously deciding to use a new treatment routinely if it is truly not better?'

Taking the societal perspective as before, we might consider the probability of making a type I error, thus erroneously burdening patients with treatments that do not improve outcomes, or even worsen them, while potentially imposing unnecessary toxicity.

First, for conditions where there are only enough patients available to run one modestly sized randomised trial in a reasonable time frame, research progress will be relatively slow, and making a type I error may be less of a concern than a type II error. In contrast, making several type I errors in a common disease could lead in practice to patients taking several ineffective treatments; for a disease area where only one trial can run at any given time, the overall burden on patients is potentially taking one ineffective treatment that does not work.

Thus, if we take the societal perspective with the trials in Table 1 then, if each trial was analysed with α = 0.05 and we see (hypothetically) 40% positive results, then the expected number of false positive trials is given in the final column. We also assumed 10% and 70% positive results, with qualitatively similar conclusions.

---

### Fentanyl (Lazanda) [^0e38c680]. FDA (2021). Medium credibility.

2.4 Titration and Maintenance of Therapy

Titration

The objective of dose titration is to identify an effective and tolerable maintenance dose for ongoing management of breakthrough cancer pain episodes. The effective and tolerable dose of LAZANDA will be determined by dose titration in individual patients.

Titration steps: If adequate analgesia is not achieved with the first 100 mcg dose, dose escalate in a step-wise manner over consecutive episodes of breakthrough pain until adequate analgesia with tolerable side effects is achieved.

Patients MUST wait at least 2 hours before treating another episode of breakthrough cancer pain with LAZANDA.

The titration steps should be:

Patients should confirm the dose of LAZANDA that works for them with a second episode of breakthrough pain and review their experience with their physicians to determine if that dose is appropriate, or whether a further adjustment is warranted.

The safety and efficacy of doses higher than 800 mcg have not been evaluated in clinical studies. Avoid the use of a combination of dose strengths to treat an episode as this may cause confusion and dosing errors.

In order to minimize the risk of LAZANDA-related adverse reactions and to identify the appropriate dose, it is imperative that patients be supervised closely by health professionals during the titration process.

Maintenance Therapy

Once an appropriate dose has been established, instruct patients to use that dose for each subsequent breakthrough cancer pain episode. Limit LAZANDA use to four or fewer doses per day.

Patients MUST wait at least 2 hours before treating another episode of breakthrough cancer pain with LAZANDA.

During any episode of breakthrough cancer pain, if there is inadequate pain relief after 30 minutes following LAZANDA dosing or if a separate episode of breakthrough cancer pain occurs before the next dose of LAZANDA is permitted (i.e., within 2 hours), the patients may use a rescue medication as directed by their healthcare provider.

2.5 Dose Re-Adjustment

If the response (analgesia or adverse reactions) to the titrated LAZANDA dose markedly changes, an adjustment of dose may be necessary to ensure that an appropriate dose is maintained.

If more than four episodes of breakthrough pain are experienced per day, re-evaluate the dose of the long-acting opioid used for persistent underlying cancer pain. If the long-acting opioid or dose of long-acting opioid is changed, re-evaluate and re-titrate the LAZANDA dose as necessary to ensure the patient is on an appropriate dose.

Limit the use of LAZANDA to treat four or fewer episodes of breakthrough pain per day.

It is imperative that any dose re-titration is monitored carefully by a healthcare professional.

---

### Fundamental limits to learning closed-form mathematical models from data [^5954e9c9]. Nature Communications (2023). High credibility.

Fig. 1
Probabilistic model selection makes quasi-optimal predictions about unobserved data.

We select two models m *, whose expressions are shown at the top of each column. a, b From each model, we generate synthetic datasets D with N points (shown, N = 100) and different levels of noise s ϵ (shown, s ϵ = 1). Here and throughout the article, the values of the independent variables x 1 and x 2 are generated uniformly at random in [− 2, 2]. Vertical lines show the observation error ϵ i for each point in D. For a model not drawn from the prior and data generated differently, see Supplementary Fig. S1. c, d For each dataset D (with dataset sizes N ∈ {25, 50, 100, 200, 400}), we sample models from p (m ∣ D) using the Bayesian machine scientist, select the MDL model (maximum p (m ∣ D)) among those sampled, and use this model to make predictions on a test dataset, generated exactly as D. We show the prediction root mean squared error (RMSE) of the MDL model onas a function of N and s ϵ. For comparison, we also show the predictions from an artificial neural network (ANN, dotted lines; Methods). Since s ϵ is the irreducible error, predictions on the diagonal RMSE = s ϵ are optimal. e, f We plot the prediction RMSE scaled by the irreducible error s ϵ; optimal predictions satisfy RMSE/ s ϵ = 1 (dashed line).

---

### Computerized segmentation method for nonmasses on breast DCE-MRI images using resUNet + + with slice sequence learning and cross-phase convolution [^6706d9ea]. Journal of Imaging Informatics in Medicine (2024). Medium credibility.

Extraction of Breast Region

The breast regions were extracted from the pre-contrast and two post-contrast DCE-MRI images to reduce the effects on other structures, such as the chest. The foreground, including the breast region, was segmented by applying a gray-level thresholding technique to the post-contrast DCE-MRI early-phase images. The threshold was empirically set to a 10-pixel value. Figures 3 a and b depict examples of the input and segmented foreground images, respectively. Subsequently, the position in the-direction for the nipple regions was determined using the smaller value of the left nipple position in the-direction and the right nipple position in the-direction. The search range was 0–in the-direction, 0–in the-direction, and 0 for the number of slices in the-direction (through-plane), where, andrepresent 352, 352, and 144 slice images, respectively. Conversely, the search range for the-direction was set to/2–to calculate the right nipple position. The search ranges for the- and-directions were identical to those used to calculate the left nipple position. Subsequently, the top-center position between the left and right nipples was calculated using the following equation:

Fig. 3
Example of foreground segmentation

The bottom center position of the breastbone with the minimum value in the-direction was determined based on the top center position according to the segmented breast region using the raster scan technique. The search range wasin the-direction,–in the-direction, and 0–in the z-direction. Figure 3 c shows an example of the left nipple, right nipple, top center, and bottom center positions. The cropping area position for the breast region was determined in the range of 1–in the-direction, (-)–(+) in the-direction, and 1–in the-direction. In this case, andwere empirically set to 5 and 50 pixels, respectively. Based on the position, andincluding the breast region were extracted from the pre- and post-contrast DCE-MRI images of the early and delayed phases, respectively. Figure 3 d shows the clipping area range in light white. Each ROI was resized to 192 × 352 pixels.

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^aeeaae45]. Nature Communications (2025). High credibility.

Fitting models to data is an important part of the practice of science. Advances in machine learning have made it possible to fit more-and more complex-models, but have also exacerbated a problem: when multiple models fit the data equally well, which one(s) should we pick? The answer depends entirely on the modelling goal. In the scientific context, the essential goal is replicability: if a model works well to describe one experiment, it should continue to do so when that experiment is replicated tomorrow, or in another laboratory. The selection criterion must therefore be robust to the variations inherent to the replication process. In this work we develop a nonparametric method for estimating uncertainty on a model's empirical risk when replications are non-stationary, thus ensuring that a model is only rejected when another is reproducibly better. We illustrate the method with two examples: one a more classical setting, where the models are structurally distinct, and a machine learning-inspired setting, where they differ only in the value of their parameters. We show how, in this context of replicability or "epistemic uncertainty", it compares favourably to existing model selection criteria, and has more satisfactory behaviour with large experimental datasets.

---

### Rotator cuff tendinopathy diagnosis, nonsurgical medical care, and rehabilitation: a clinical practice guideline [^a4079329]. The Journal of Orthopaedic and Sports Physical Therapy (2025). High credibility.

Grades of recommendations — This table defines grades A–F and links each to a strength-of-evidence statement and corresponding Level of Obligation. Grade A is defined as "A preponderance of level I and/or level II studies support the recommendation", with Level of Obligation (Based on Treatment Effects): "Must: benefits substantially outweigh harms; Should: benefits moderately outweigh harms; May: benefits minimally outweigh harms or benefit/harm ratio is value dependent; Should not: harms minimally or moderately outweigh benefits or evidence of no effect; Must not: harms largely outweigh benefits". Grade B is "A single high-quality randomized controlled trial or a preponderance of level II studies support the recommendation", with obligations "Should: benefits substantially outweigh harms; May: benefits moderately or minimally outweigh harms or benefit/harm ratio is value dependent; Should not: evidence that harms outweigh benefits or evidence of no effect". Grade C is "A single level II study or a preponderance of level III and IV studies, including statements of consensus by content experts, support the recommendation", with obligations "Should: benefits substantially outweigh harms; May: benefits moderately or minimally outweigh harms or benefit/harm ratio is value dependent; Should not: harms minimally or moderately outweigh benefits". Grade D is "Higher-quality studies conducted on this topic disagree with respect to their conclusions", with obligation "May: conflicting evidence, the benefit/harm ratio is value dependent". Grade E is "A preponderance of evidence from animal or cadaver studies, from conceptual models/principles, or from basic science/bench research support this conclusion", with obligations "May: in the absence of evidence from clinical studies, theoretical and/or foundational evidence supports benefit" and "Should not: in the absence of evidence from clinical studies, theoretical and/or foundational evidence suggests strong risk of harms". Grade F is "Best practice based on the clinical experience of the guideline development team supports this conclusion", and the table also specifies Level of Obligation (Based on Assessment/Diagnosis): "Must: strongly supported by consensus-based best practice/standard of care; Should: moderately supported by best practice/standard of care; May: supported by expert opinion in the absence of consensus; Should not: best practice/standard of care indicates potential harms; Must not: potential harms are strongly supported by consensus-based best practice/standard of care".

---

### Optimal enzyme utilization suggests that concentrations and thermodynamics determine binding mechanisms and enzyme saturations [^118a67e1]. Nature Communications (2023). High credibility.

To overcome the nonlinear nature of the problem, we next linearized the bilinear terms and the nonlinear constraints. As the normalized elementary rate constantsare well-bounded between 0 and 1, we replaced the bilinear termsfor each elementary step by one new variable and one new constraint. To replace the nonlinear constraint posed by the thermodynamics, we eliminated one of the displacements using the overall thermodynamic displacement from equilibrium. We estimate the remaining elementary displacements or mechanistically meaningful independent combinations of them by a piecewise-constant function. The resulting problem is piecewise-linear and can be solved efficiently with a MILP formulation using the Peterson linearization scheme, (See Methods). The reformulation of the problem as a MILP ensures global optimality and enumeration of alternative solutions (Fig. 1 panel 3).

Finally, the resulting MILP allows us to optimize the net enzyme flux for the respective operating conditions, i.e. substrate, product concentrations, and standard Gibbs free energy. The optimization results will yield a set of elementary rate constants, displacements, and an enzyme distribution that allow for this optimal flux. With this constraint-based formulation of the problem, we can then assess potential alternative modes of operation by constraining the flux to its maximum and applying variability analysis on the operational variables. With this same principle, we can study the fitness landscape of optimal enzyme utilization for specific operating conditions by exploring the suboptimal space (Fig. 1 panel 4).

Optimally used Michaelis–Menten enzymes require condition-specific saturation regimes

We first applied our framework to study the modes of operation of the prototypical three-step reversible Michaelis–Menten Eq. (1) mechanism. Our results from our herein presented elementary MILP formulation capture those previously obtained by Wilhelm et al, we showed the division of the concentration space into distinct regions, with matching elementary rate constants and Michaelis constants at optimal state(Supplementary Figs. 1 and 2). We further calculated forward and backward turnover numbers at optimal state and showed their sensitivity over the concentration space (Supplementary Fig. 3).

Additionally, our formulation allowed us to assess optimal enzyme state distributions and thermodynamic forces directly. Our results show that the operating conditions govern the enzyme state and thermodynamic force distribution at a catalytically optimal state.

---

### Association between neurite metrics and tau / inflammatory pathology in Alzheimer's disease [^00168857]. Alzheimer's & Dementia (2020). Medium credibility.

3.2 Image correlations between PET and NODDI

The AD‐S group showed significant negative correlations between 18 F‐THK5351 and the NDI (Figure 2 ‐A) and between 18 F‐THK5351 and the ODI (Figure 2 ‐B). The bilateral temporal lobes showed more significant associations, although the right posterior cingulate gyrus was also correlated between 18 F‐THK5351 and the ODI (Figure 2 ‐B). In addition, there was a small negative correlation in the left orbitofrontal cortex between 11 C‐PiB and the ODI (Figure 2 ‐C).

FIGURE 2
Voxel‐wise correlations between PET and NODDI. (A) Negative correlations between 18 F‐THK5351 and the NDI in the AD‐S group, (B) negative correlations between 18 F‐THK5351 and the ODI in the AD‐S group, and (C) negative correlations between 11 C‐PiB and the ODI in the AD‐S group. Color bars denote the statistically significant T‐values from minimum to maximum

There were widespread positive correlations between 11 C‐PiB and 18 F‐THK5351 in the cortex in the AD‐S group (Figure 3 ‐A), although a more restricted area showed the same correlations in the HC group (Figure 3 ‐C). The NDI and ODI also showed positive correlations in the AD‐S group (Figure 3 ‐B), but the same correlations were much more widely found in the HC group (Figure 3 ‐D).

FIGURE 3
Voxel‐wise correlations between PET and NODDI. (A) Positive correlations between 18 F‐THK5351 and 11 C‐PiB in the AD‐S group, (B) positive correlations between the NDI and ODI in the AD‐S group, (C) positive correlations between 18 F‐THK5351 and 11 C‐PiB in the HC group, and (D) positive correlations between the NDI and ODI in the HC group. Color bars denote the statistically significant T‐values from minimum to maximum

The numerical relationships can be found in Supplementary Figures S2‐B and C. The other undescribed analyses in Figures 2 and 3 were not significant, and the list of them is shown in Supplementary Table S1.

---

### Validation of monte carlo estimates of three-class ideal observer operating points for normal data [^26b6e5d9]. Academic Radiology (2013). Low credibility.

Rationale and Objectives

Traditional two-class receiver operating characteristic (ROC) analysis is inadequate for the complete evaluation of observer performance in tasks with more than two classes.

Materials and Methods

Here, a Monte Carlo estimation method for operating point coordinates on a three-class ROC surface is developed and compared with analytically calculated coordinates in two special cases: (1) univariate and (2) restricted bivariate trinormal underlying data.

Results

In both cases, the statistical estimates were found to be good in the sense that the analytical values lay within the 95% confidence interval of the estimated values about 95% of the time.

Conclusions

The statistical estimation method should be key in the development of a pragmatic performance metric for evaluation of observers in classification tasks with three or more classes.

---

### Quantification of network structural dissimilarities [^3cf663e0]. Nature Communications (2017). Medium credibility.

Identifying and quantifying dissimilarities among graphs is a fundamental and challenging problem of practical importance in many fields of science. Current methods of network comparison are limited to extract only partial information or are computationally very demanding. Here we propose an efficient and precise measure for network comparison, which is based on quantifying differences among distance probability distributions extracted from the networks. Extensive experiments on synthetic and real-world networks show that this measure returns non-zero values only when the graphs are non-isomorphic. Most importantly, the measure proposed here can identify and quantify structural topological differences that have a practical impact on the information flow through the network, such as the presence or absence of critical links that connect or disconnect connected components.

---

### Self-directed online machine learning for topology optimization [^f3f1e6d5]. Nature Communications (2022). High credibility.

Methods

Enforcement of volume constraint

Compliance and heat transfer problems have volume constraints. The latter will be detailed in Section Interpolation of design variables, thus we only discuss the former here. In the two compliance problems, all matrices representing the density distribution ρ have the same weighted averagedue to the volume constraint where w i denotes the weight of linear Gaussian quadrature. A matrix from the initial batch is generated by three steps:
Generate a random matrix with elements uniformly distributed from 0 to 1.
Rescale the array to enforce the predefined weighted average.
Set the elements greater than one, if any, to one and then adjust those elements less than one to maintain the average.

Matrices for the second batch and afterward add random disturbance to optimized solutionsand then go through Step 2 and Step 3 above to make sure the volume satisfies the constraint.

Finite Element Method (FEM) and gradient-based baselines

The objective function values of material designs are calculated by FEM as the ground truth to train the DNN. In the compliance and fluid problems, the meshes of FEM are the same as the design variables. In the heat problem, the meshes are finer. Numerical results are obtained by COMSOL Multiphysics 5.4 (except the truss problems). Solutions from gradient-based methods (including "Approximated") are all solved by MMA via COMSOL with optimality tolerance as 0.001. In the fluid problems, the gradient-based baseline method produces a continuous array, and we use multiple thresholds to convert it to binary arrays and recompute their objective (pressure) to select the best binary array. In the heat problem, the "Approximated" method uses the same resolution as the other two methods (SOLO and "Direct") for a fair comparison. Specifically, we apply a Helmholtz filter, whose radius is half of the minimum distance of two design variable locations, to yield a mesh-independent solution. The solution is a continuous array; we use a threshold to convert it to a binary array which satisfies the volume constraint in Eq. (14).

---

### Fundamental limits to learning closed-form mathematical models from data [^a345300f]. Nature Communications (2023). High credibility.

Given a finite and noisy dataset generated with a closed-form mathematical model, when is it possible to learn the true generating model from the data alone? This is the question we investigate here. We show that this model-learning problem displays a transition from a low-noise phase in which the true model can be learned, to a phase in which the observation noise is too high for the true model to be learned by any method. Both in the low-noise phase and in the high-noise phase, probabilistic model selection leads to optimal generalization to unseen data. This is in contrast to standard machine learning approaches, including artificial neural networks, which in this particular problem are limited, in the low-noise phase, by their ability to interpolate. In the transition region between the learnable and unlearnable phases, generalization is hard for all approaches including probabilistic model selection.

---

### Optimization of breeding program design through stochastic simulation with evolutionary algorithms [^0be1d48f]. G3 (2025). Medium credibility.

Step 0: Definition of the optimization problem

Firstly, the breeding problem is formulated as an optimization problem, defining objectives, constraints, and decision variables. Two main types of decision variables are considered: class variables with a limited number of possible realizations (e.g. whether genomic selection is applied in a specific step of the breeding program) and continuous variables that can take values from a continuous scale, or at least a large number of discrete realizations. (e.g. number of candidates selected, phenotyped, and genotyped, or weights in a selection index).

Below, we present scenario 1, which serves as an example to illustrate the formulation of a breeding program design for an optimization problem as a baseline scenario. Subsequently, we showcase three additional examples to highlight the versatility of our EA framework, each representing a modification of scenario 1. These alternative scenarios are detailed towards the end of this section.

Scenario 1 — Traditional dairy cattle scheme

Here, we are considering a traditional dairy cattle scheme, illustrated schematically in Fig. 2 as suggested in our previous study. As performance traits of bulls cannot be directly measured, preselected test bulls must be mated with cows to produce test daughters. Bulls are selected as sires based on the performance of their offspring. For simplification purposes, we are explicitly not considering the genotyping as commonly done in dairy cattle breeding in the last 15 years, and both bulls and cows are chosen based on pedigree breeding value estimates. We are considering here only a single quantitative trait (milk yield), with heritability of 0.30.

Fig. 2.
A dairy cattle breeding scheme.

The three parameters we consider for optimization are:

number of test daughters
number of test bulls
number of selected sires

As constraints, the breeding program at hand is limited by an annual budget of 10,000,000 Euros with housing costs of 3,000 Euros per bull and 4,000 Euros per cow.

Note that, the initial bounds established during the initialization step are not constraints that are enforced throughout the optimization process. Instead, they serve as a starting point, and the variables may deviate from these bounds as the optimization algorithm iterates and refines the solution.

Exactly as in our previous study, the objective function (m) is a linear combination of the expected genetic gain (g) and the expected inbreeding level (f) after 10 generations (5 years of burn-in + 10 years of future breeding), to prioritize/weigh between the genetic gain and diversity:

---

### Rigorous location of phase transitions in hard optimization problems [^d98ebea5]. Nature (2005). Excellent credibility.

It is widely believed that for many optimization problems, no algorithm is substantially more efficient than exhaustive search. This means that finding optimal solutions for many practical problems is completely beyond any current or projected computational capacity. To understand the origin of this extreme 'hardness', computer scientists, mathematicians and physicists have been investigating for two decades a connection between computational complexity and phase transitions in random instances of constraint satisfaction problems. Here we present a mathematically rigorous method for locating such phase transitions. Our method works by analysing the distribution of distances between pairs of solutions as constraints are added. By identifying critical behaviour in the evolution of this distribution, we can pinpoint the threshold location for a number of problems, including the two most-studied ones: random k-SAT and random graph colouring. Our results prove that the heuristic predictions of statistical physics in this context are essentially correct. Moreover, we establish that random instances of constraint satisfaction problems have solutions well beyond the reach of any analysed algorithm.

---

### Bayesian strategy selection identifies optimal solutions to complex problems using an example from GP prescribing [^d1a4b1f1]. NPJ Digital Medicine (2020). Medium credibility.

Another aspect to be mindful of is that Thompson sampling trades between the type-1 error rate and statistical power, implying that to achieve a greater statistical power (relative to a same sample-size equal allocation trial), it is likely to inflate the type-1 error rate. By not trying the null strategy sufficiently, the type-1 error rate may exceed the acceptable threshold. Thus it is judicious to monitor the type-1 error rate and if it exceeds the significance level, adjustments may be required in the allocation algorithm. Recent workis starting to explore adaptive sampling approaches that can control the type-1 error rate, and further work is required to develop such techniques for a Thompson sampling setting.

We did not assess intervention fidelity, that is, whether the intervention was delivered as conceived and in accordance with the materials and instructions provided to participants. Given that the aims were to measure the effectiveness of the strategies in supporting GP-patient interactions where physical activity was discussed, failure to implement the strategies in consultations was ultimately what was tested, in essence assessing the utility of each strategy.

This case study has demonstrated an efficient process providing clear results in a short period of time. Further work should develop methods to understand interactions and sequencing of various and multiple combinations of interventions. Subsequent lines of investigation should also begin to consider the question of fidelity. Given this study demonstrates a process for supporting conversations about PA between GPs and patients, an important next step is to examine the quality and content of those conversations and their impact on measured patient PA.

While the reasons for low incidence of GP patient interaction about PA are complex, these interactions occur in a relatively stable setting with a clear, well-defined outcomes. Further work on other complex problems in similarly well-defined domains, or further investigations of the GP-patient relationship in expanded settings (i.e. with greater numbers of prioritised actions, or broader participant groups) represent exciting directions for future work.

The new frontier of public health is not what behaviours need to change, but rather how to change them. This study shows how new methods can be used to test and optimise implementation of intervention in multiple settings by engaging key actors in a system and moving quickly to the best set of interventions. For PA specifically, data suggested more than 80% of adults visit their GP at least once per year. Rapid identification of appropriate strategies that increase the discussion of PA between GPs and their patients provides new potential avenues for the improvement of public health.

---

### Three learning stages and accuracy-efficiency tradeoff of restricted boltzmann machines [^efe3f1d0]. Nature Communications (2022). High credibility.

Restricted Boltzmann Machines (RBMs) offer a versatile architecture for unsupervised machine learning that can in principle approximate any target probability distribution with arbitrary accuracy. However, the RBM model is usually not directly accessible due to its computational complexity, and Markov-chain sampling is invoked to analyze the learned probability distribution. For training and eventual applications, it is thus desirable to have a sampler that is both accurate and efficient. We highlight that these two goals generally compete with each other and cannot be achieved simultaneously. More specifically, we identify and quantitatively characterize three regimes of RBM learning: independent learning, where the accuracy improves without losing efficiency; correlation learning, where higher accuracy entails lower efficiency; and degradation, where both accuracy and efficiency no longer improve or even deteriorate. These findings are based on numerical experiments and heuristic arguments.

---

### Replicate sequencing libraries are important for quantification of allelic imbalance [^640aa216]. Nature Communications (2021). High credibility.

Fig. 4
QCC enables differential AI analysis and is correlated with abundance overdispersion.

a Number of genes with apparent differential AI in the same biological sample (false positives) before (left) and after QCC correction (right). Within each of three experiments, all 45 pairwise comparisons were performed. For comparisons between experiments, 45 sets of pairs were randomly chosen from 225 possible combinations. QCC values were calculated for each pair, and genes with significantly different AI in each set were identified. Experiments: 1 (blue), 2 (red), or 3 (green). Boxplot elements everywhere as in Fig. 2. Two-sided test was used here and in (b), (c), (d). b Impact of QCC on false positive (FP) rate. FPs are defined as genes for which the point AI estimate from all pooled replicates is not within CI from one replicate (left), two pooled replicates (center), and two replicates with QCC correction (right). Outlier from Experiment 1 (replicate 1) was removed from this analysis. All individual datapoints are shown in black. c, d Calculated QCC value (contained within the color bar) are close to optimal balance between FP and signal. c Number of FP genes (differential expression AI in the same biological sample) calculated for different potential values of QCC for all 45 possible combinations of two pairs of replicates from Experiment 3. d FP rate (as in (b)) for different possible QCC values calculated for all possible pairs of replicates in Experiment 3. a – d Note that the unit of comparison is composed of two technical replicates in both the binomial test and the QCC test. e Differences between experiments in abundance overdispersion are proportionate to QCC. Left: Abundance overdispersion for each experiment can be fitted as log-linear regression (solid lines) above expected Poisson dispersion (dotted line). Right: same after overdispersion was divided by QCC2. The outlier replicate from Experiment 1 was removed for (e) and (f). f Correlation of QCC and abundance overdispersion. QCC values are the same as in (e). Abundance overdispersion for each experiment calculated as exponent of intercept of log-linear regression (see (e)) between mean and dispersion of total counts: Left: for all replicates in an experiment; Right: for all possible pairs of replicates.

---

### Extensive benchmarking of a method that estimates external model performance from limited statistical characteristics [^691db016]. NPJ Digital Medicine (2025). Medium credibility.

One-sided positivity tests

The following tests are performed before running the weighting algorithm, to assess one-sided positivity:
Verify that Φ int and μ ext contain the same features and feature-outcome transformations and do not contain missing values.
Unary variables: In case one of the columns of Φ int has a constant value across all units, make sure that the difference between this value and the corresponding entry in μ ext is less than a threshold (default 0.01).
Binary variables: First, if the external statistics of binary features are exactly the same as one of the internal binary values, we assume that the external cohort is composed of a sub-population with this value and use only the corresponding sub-population in the internal cohort for weighting. Second, we test if univariate weighting on this variable will not result in assigning most of the weights to a few internal units. Specifically, let n 0 and n 1 be the number of internal units with feature values 0 and 1. Let p 0 and p 1 be the proportion of such units in the external dataset. Then, we assume that whenmost weights will be assigned to a few samples and the variance of any weighted estimator will be too large.
Continuous variables: Make sure that the external statistics is within the range of the internal cohort values. We allow a small slack (default 0.01).

Weighting algorithm pre-processing and parameter settings

Before running this algorithm, the features in the internal dataset are normalized by subtracting the mean and dividing by the standard deviations. The means and standard deviation are maintained and used later to apply the same transformation to the external statistics. This step maintains the relationships between the internal cohort and the external statistics while improving numerical stability and standardizing decision threshold. The convergence of the algorithm is examined by testing if the L 2 norm of the distance between the weighted internal and the external statistics is less than a threshold (default value 10 −5). The maximum number of iterations in this benchmark was set to 2000. If the weighting algorithm converges, we test that the maximum standardized mean difference between the weighted internal statistics and the external ones is less than a threshold (default 0.05).

---

### How I read an article that uses machine learning methods [^0f2f5676]. Blood Advances (2023). Medium credibility.

Step 1: Understand the problem being addressed. The first step in reading an ML paper is to understand the problem that the authors are trying to solve and, more importantly, understand the clinical or scientific impact of solving this problem. In other words, if the aim of the study is to solve a clinical problem, how does the answer or the recommendation provided by the algorithm help physicians or researchers in their day-to-day practice, and is this solution mature enough to be implemented in clinical workflows? Major clinical problems in health care can mainly affect either patient outcomes or operations (can I make the process easier and faster for the patient and the health care system?).

Step 2: Assess the quality of the data. The quality of the data used to build the ML model is crucial for the validity of the results. Following are some questions that can be used to evaluate the data:
1 Sample size: Is the size of the training, validation, and test sets enough to build a reproducible and generalizable ML model? Is this size of the data appropriate for the chosen methods (ie, some methods are "data-hungry" and understanding which methods require larger datasets is key)? However, different algorithms require different data types (image, tabular, text, or others) and sizes, and there are no rules of thumb or formulas that can estimate the perfect data.
2 Relevance: Are the data appropriate and relevant to the problem that the model is trying to solve?
3 Accuracy: How are the data collected and annotated (human vs natural language process). How are the data transformed to make it ready for ML use, etc.
4 Consistency: Are the data consistent? Do they have any missing values and how the authors dealt with this?
5 Representativeness: The data should be representative of the population being studied.
6 Balance: The data should be balanced, with roughly equal representation of all relevant classes or groups. However, most health care data are unbalanced. It is critical to understand how the authors dealt with unbalanced data.
7 Bias: To evaluate bias in data, it is important to look at the distribution of certain characteristics, such as race, gender, or socioeconomic status, among the samples in the data set, and how the data were collected. This will help to identify any disparities or overrepresentation of certain groups, which can indicate the presence of bias in the data. It is critical to evaluate bias at this stage because if this is not addressed properly, it could produce a biased model.

---

### A Bayesian optimization approach for rapidly mapping residual network function in stroke [^57609e47]. Brain (2021). Medium credibility.

Intra-subject reliability

To assess intra-subject reliability of our results, we computed the Spearman's rank correlation of the Bayesian predictions across the task space between the two runs of each subject. For statistical inference, we performed permutation testing (i.e. 10 000 permutations) where we shuffled the FPN > DMN values and corresponding task indices of the second run and then refitted the Gaussian process (hyperparameters were kept identical to the real-time scenario) for these shuffled values before computing the correlation coefficient between the two runs. For each permutation, we then computed the median of the Fisher z -transformed correlation values for each group separately. To correct for multiple comparisons, at each permutation we only kept the maximum of both median values (i.e. 'max statistic' method). The median of our true empirically obtained (Fisher z -transformed) correlation coefficients for patients and controls were then compared to the generated null distribution of maximum median values with a one-sided alpha-level set at 0.05.

---

### Practice guidelines for the prevention, detection, and management of respiratory depression associated with neuraxial opioid administration: an updated report by the American society of anesthesiologists task force on neuraxial opioids and the American Society of Regional Anesthesia and Pain Medicine [^c7d4883b]. Anesthesiology (2016). Medium credibility.

American Society of Anesthesiologists neuraxial opioid guideline — evidence grading and opinion methodology: Category B observational evidence permits directional inference, and for studies that report statistical findings, "the threshold for significance is P < 0.01"; Category B levels comprise "observational comparisons… with comparative statistics" (Level 1), "noncomparative observational studies with associative statistics" (Level 2), "noncomparative observational studies with descriptive statistics" (Level 3), and "case reports" (Level 4). Opinion-based inputs include formal surveys and informal sources; survey types are designated "Category A: Expert Opinion" and "Category B: Membership Opinion", with additional "Category C: Informal Opinion". Survey responses "are recorded using a five-point scale and summarized based on the median values", with median categories defined as follows: "Strongly Agree: Median score of 5 (at least 50% of the responses are 5)… Agree: Median score of 4 (at least 50% of the responses are 4 or 4 and 5)… Equivocal: Median score of 3 (at least 50% of the responses are 3, or no other response category or combination of similar categories contain at least 50% of the responses)… Disagree: Median score of 2 (at least 50% of responses are 2 or 1 and 2)… Strongly Disagree: Median score of 1 (at least 50% of responses are 1)". When equal numbers of distinct responses occur, "the median value is determined by calculating the arithmetic mean of the two middle values".

---

### An event-based architecture for solving constraint satisfaction problems [^14256d3f]. Nature Communications (2015). Medium credibility.

For solving CSPs, a subset of the nodes in the network will represent the actual problem variables, while others will represent helper variables that encode other problem-relevant quantities (for example, whether a constraint is satisfied or not). The value of a variable/node at a point in time is the index of the output port on which the node emitted its last event. Thus, a variable/node with M output ports can have M possible values. The output port of one node can connect to the input ports of one or more nodes and one input port can receive events from multiple output ports. One output port cannot be connected to multiple input ports on the same node. In the following sections, we describe how to connect nodes/variables together and how to define the nodes/variables behaviour (the f and g functions) so as to solve a number of hard CSPs. The procedure to map a CSP to this distributed architecture depends on the type of the CSP but in general, the mapping is done so that the distributed and parallel dynamics of the network of nodes tries to put the problem variables/nodes in a state where their outputs satisfy all the constraints.

Figures 1c and 1d show the definition and illustrate the behaviour of an example node that has two input ports, two output ports and two possible internal states (N = M = Q = 2). The state of the example node/variable is the index of the last external event it received and the node/variable advertises its state by generating an event on one of the output ports when it receives an event on the internal port 'in.0' as shown in Fig. 1c (we refer to this as 'updating'). Assume this example node is the target node receiving events from multiple sources nodes. Since it is only the last received event that determines the value advertised by an updating node, the phase relations between the analogue oscillators in the network determine which of the source nodes generates the decisive event that determines the event generated by the target node. This would be the source node that updated just before the target node updates. The phase relations are continuously changing in an aperiodic manner since the oscillation frequencies are incommensurable. The shifting phase relations thus continuously change which source node manages to influence the output events of the target node.

---

### The expected behaviour of random fields in high dimensions: contradictions in the results of bansal and peterson [^657f9d59]. Magnetic Resonance Imaging (2022). Medium credibility.

Bansal and Peterson (2018) found that in simple stationary Gaussian simulations Random Field Theory incorrectly estimates the number of clusters of a Gaussian field that lie above a threshold. Their results contradict the existing literature and appear to have arisen due to errors in their code. Using reproducible code we demonstrate that in their simulations Random Field Theory correctly predicts the expected number of clusters and therefore that many of their results are invalid.

---

### When to keep it simple-adaptive designs are not always useful [^bc2b8212]. BMC Medicine (2019). Medium credibility.

Background

Adaptive designs are a wide class of methods focused on improving the power, efficiency and participant benefit of clinical trials. They do this through allowing information gathered during the trial to be used to make changes in a statistically robust manner - the changes could include which treatment arms patients are enrolled to (e.g. dropping non-promising treatment arms), the allocation ratios, the target sample size or the enrolment criteria of the trial. Generally, we are enthusiastic about adaptive designs and advocate their use in many clinical situations. However, they are not always advantageous. In some situations, they provide little efficiency advantage or are even detrimental to the quality of information provided by the trial. In our experience, factors that reduce the efficiency of adaptive designs are routinely downplayed or ignored in methodological papers, which may lead researchers into believing they are more beneficial than they actually are.

Main Text

In this paper, we discuss situations where adaptive designs may not be as useful, including situations when the outcomes take a long time to observe, when dropping arms early may cause issues and when increased practical complexity eliminates theoretical efficiency gains.

Conclusion

Adaptive designs often provide notable efficiency benefits. However, it is important for investigators to be aware that they do not always provide an advantage. There should always be careful consideration of the potential benefits and disadvantages of an adaptive design.

---

### Rapid and sensitive digital droplet PCR assays for detecting HPV16 DNA in liquid biopsies [^a2e8933f]. Journal of Medical Virology (2025). Medium credibility.

Figure 2
Detection of HPV16 DNA in purified cfDNA. (A, B) Purified cfDNA from paired plasma and serum samples showing the numbers of detected droplets (A) and detection of HPV16‐positive cases (B). Right panel selectively showed the droplet counts for paired cases with droplet counts less than 150 in the left panel (A). (C, D) Purified cfDNA from paired plasma and surgical drain fluid (SDF) samples showing the numbers of detected droplets (C) and detection of HPV16‐positive cases (D). Right panel selectively showed the droplet counts for paired cases with droplet counts less than 5000 droplets in the left panel (C). A sample is considered positive with the presence of a minimum of one HPV16 DNA‐positive droplet in a single ddPCR reaction (B, D) with black box represents HPV16 DNA‐positive sample and white box represents HPV16 DNA‐negative sample. p ‐values were assessed by Wilcoxon matched‐pairs signed rank test with p < 0.05 considered having a significant difference (A, C). p ‐values were assessed by Fisher's exact test with p < 0.05 considered having a significant difference (B, D). p ‐val: p ‐value; NS, not significant.

3.3 Direct Detection of HPV16 DNA in Unpurified Liquid Biopsy

Given that cfDNA purification is a relatively time‐consuming and costly process, we evaluated the feasibility of directly detecting HPV16 DNA in plasma, serum, and SDF without cfDNA extraction (unpurified cfDNA). To assess potential sample inhibition, we spiked HPV16 plasmid DNA into HPV16 DNA‐negative liquid biopsies and compared HPV16 DNA‐positive droplet counts between non‐heated unpurified specimens and unpurified specimens subjected to 3 min of heat treatment. In samples spiked with 100 HPV16 plasmids, heated plasma, serum, and SDF samples showed average droplet counts of 17, 9, and 19, respectively, and were substantially higher than their nonheated counterparts, which displayed average droplet counts of 5, 6, and 15, respectively (Figure 3A). Heat treatment did not affect the baseline fluorescence of plasma, serum, or SDF, as all tested specimens, including non‐heated samples and controls, showed base fluorescence below 0 (Figures S4A–C).

---

### Recommendations for a standardized pulmonary function report. An official American Thoracic Society technical statement [^d6d8fcbc]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Spirometry reporting specifies that numerical values are given for the FEV1, the FVC, and the FEV1/FVC ratio; the latter should be reported as a decimal fraction and the space for percent predicted value left blank, and if bronchodilators are given the LLN column need not be repeated with absolute and percent change given only for FEV1 and FVC. Other numerical values such as the forced inspiratory flow at 75% of FVC (FEF75%) and FEF25–75% are not recommended for routine use. Graph requirements include that for the volume–time curve the volume scale should be at least 10 mm/L, the time scale at least 20 mm/s, and 1 second prior to the start of expiration should be displayed; on the flow–volume plot the flow display should be at least 5 l/min/L/s, and the ratio of flow to volume should be 2 L/s to 1 L, and linear and log scales where values are plotted as z-scores relative to the predicted value (z = 0) give an intuitive sense of severity.

---

### Semiparametric estimation of the relationship between ROC operating points and the test-result scale: application to the proper binormal model [^9a9d14d8]. Academic Radiology (2011). Low credibility.

Rationale and Objectives

Semiparametric methods provide smooth and continuous receiver operating characteristic (ROC) curve fits to ordinal test results and require only that the data follow some unknown monotonic transformation of the model's assumed distributions. The quantitative relationship between cutoff settings or individual test-result values on the data scale and points on the estimated ROC curve is lost in this procedure, however. To recover that relationship in a principled way, we propose a new algorithm for "proper" ROC curves and illustrate it by use of the proper binormal model.

Materials and Methods

Several authors have proposed the use of multinomial distributions to fit semiparametric ROC curves by maximum-likelihood estimation. The resulting approach requires nuisance parameters that specify interval probabilities associated with the data, which are used subsequently as a basis for estimating values of the curve parameters of primary interest. In the method described here, we employ those "nuisance" parameters to recover the relationship between any ordinal test-result scale and true-positive fraction, false-positive fraction, and likelihood ratio. Computer simulations based on the proper binormal model were used to evaluate our approach in estimating those relationships and to assess the coverage of its confidence intervals for realistically sized datasets.

Results

In our simulations, the method reliably estimated simple relationships between test-result values and the several ROC quantities.

Conclusion

The proposed approach provides an effective and reliable semiparametric method with which to estimate the relationship between cutoff settings or individual test-result values and corresponding points on the ROC curve.

---

### Maximum likelihood estimation of signal amplitude and noise variance from MR data [^bfd9a45c]. Magnetic Resonance in Medicine (2004). Low credibility.

In MRI, the raw data, which are acquired in spatial frequency space, are intrinsically complex valued and corrupted by Gaussian-distributed noise. After applying an inverse Fourier transform, the data remain complex valued and Gaussian distributed. If the signal amplitude is to be estimated, one has two options. It can be estimated directly from the complex valued data set, or one can first perform a magnitude operation on this data set, which changes the distribution of the data from Gaussian to Rician, and estimate the signal amplitude from the obtained magnitude image. Similarly, the noise variance can be estimated from both the complex and magnitude data sets. This article addresses the question whether it is better to use complex valued data or magnitude data for the estimation of these parameters using the maximum likelihood method. As a performance criterion, the mean-squared error (MSE) is used.

---

### Photon-efficient imaging with a single-photon camera [^eb349d0b]. Nature Communications (2016). Medium credibility.

We then proceed to step 1 of the reconstruction algorithm: we estimate reflectivityby combining the Poisson statistics of photon counts (Supplementary Fig. 8a) with a TV-norm smoothness constraint on the estimated reflectivity — while censoring the set of hot pixels — to write the optimization as a TV-regularized, Poisson image inpainting problem. This optimization problem is convex in the reflectivity image variable A, which allows us to solve it in a computationally efficient manner with simple projected gradient methods. This step of the algorithm inputs a parameter τ A that controls the degree of spatial smoothness of the final reflectivity image estimate. For a 384 × 384 image, the processing time of step 1 was ∼6 s on a standard laptop computer (Supplementary Note 1).

For step 2 of the reconstruction algorithm, we filtered the photon-detection data set to impose the longitudinal constraint that the scene has a sparse set of reflectors. This is because the scaled detection-time histogram hist(c T /2) that has been corrected for the average background-light plus dark-count detections per bin is a proxy solution for hist(Z Δ), where hist(Z Δ) is a size– N z histogram that bins the scene's 3D structure at the camera's c Δ/2 native range resolution. We used orthogonal matching pursuiton hist(c T /2), the coarsely binned histogram of photon detections, to find the non-zero spikes representing the object depth clusters. This step of the algorithm requires an integer parameter m that controls the number of depth clusters to be estimated; here we used m = 2, but our simulations show insensitivity to overestimation of the best choice of m (Supplementary Fig. 8b). We then discarded photon detections that implied depth values more than cT p /2 away from the estimated depth values, because they were presumably extraneous detections that are uniformly spread out during the acquisition time (Supplementary Fig. 8c). For a 384 × 384 image, the processing time of step 2 was ∼17 s on a standard laptop computer (Supplementary Note 1).

---

### What are small, medium and large effect sizes for exercise treatments of tendinopathy? A systematic review and meta-analysis [^6092a8e2]. BMJ Open Sport & Exercise Medicine (2023). High credibility.

Description of effect size distributions

From the initial 1454 outcomes extracted, a total of 38 outliers were removed from the analysis with a lower bound threshold of –0.82 (6 effect sizes below) and an upper bound threshold of 7.0 (32 effect sizes above). Across all outcomes and tendinopathy locations, direct calculation of the 0.25 (small), 0.5 (medium) and 0.75 quantiles (large) from the complete empirical data returned the following SMD pre values: 0.37, 0.77 and 1.31, respectively. Application of the meta-analysis model across the data with borrowing of information across studies resulted in similar but shrunken estimates (0.25 quantile 0.5 = 0.34 (95% CrI: 0.31 to 0.37); 0.5 quantile 0.5 = 0.73 (95% CrI: 0.70 to 0.77) and 0.75-quantile 0.5 = 1.21 (95% CrI: 1.17 to 1.27)). A forest plot of effect sizes illustrating effect sizes across studies is presented in online supplemental file 12. Sensitivity analyses checking the potential influences of study type and study quality are presented in online supplemental file 13. No evidence was obtained of greater effect sizes with quasi-experimental designs or with studies identified as high risk of bias.

Analyses of effect size distributions across the different tendinopathy locations are illustrated in figure 2 with numerical values presented in table 3. Analyses were pooled across all outcome domains as a means to compare tendinopathy locations with the largest amount of data possible. Analysis of the modelled small, medium and large thresholds showed considerable overlap in small and medium thresholds across all tendinopathy locations (0.25 quantile 0.5 ranged from 0.28 to 0.38; 0.5 quantile 0.5 ranged from 0.70 to 0.82). However, greater divergence was identified for large threshold estimates, with the greatest values estimated for the elbow (0.75 quantile 0.5 ranged from 1.18 to 1.49).

Figure 2
Effect size distributions across tendinopathy locations with identification of small, medium and large thresholds. black curve represents density plot of empirical effect size distribution. Diamonds with intervals represent small, medium and large thresholds with credible intervals (black: all outcomes; red: tendinopathy specific). RCRSP, rotator cuff related shoulder pain.

---

### Physical activity and exercise during pregnancy and the postpartum period: ACOG committee opinion, number 804 [^7aa306cd]. Obstetrics and Gynecology (2020). High credibility.

Recommended weight limits for lifting at work during pregnancy — an on-page algorithm and stepwise method describe how to determine a recommended weight limit (RWL). The algorithm asks, "At work do you perform lifting tasks more than once every 5 minutes?" and, if yes, "Do you lift more than three times per minute?", then queries "How many hours per day do you spend lifting at work?" with categories "Less than 1 continuous hour" or "1 or more hours per day", corresponding to infrequent, repetitive short duration, or repetitive long duration lifting panels. Steps instruct to "select the one graphic (A, B, or C) that best describes the lifting frequency or frequency/duration pattern", then "When less than 20 weeks pregnant, select the image on the left of the graphic; when pregnant for 20 weeks or more, select the image on the right", to underline all relevant numerical values along the lift path and circle the lowest, and that "The number circled in step 3 is the RWL (in pounds)" with a proviso that the short-duration category can include multiple hours per day provided each continuous bout is less than 1 hour and followed by at least 1 hour of nonlifting before resuming; the figure presents "Provisional recommended weight limits for lifting at work during pregnancy".

---

### Using the mean-to-sigma ratio as a measure of the improperness of binormal ROC curves [^70959dec]. Academic Radiology (2011). Low credibility.

Rationale and Objectives

A basic assumption for a meaningful diagnostic decision variable is that there is a monotone relationship between the decision variable and the likelihood of disease. This relationship, however, generally does not hold for the binormal model. As a result, receiver operating characteristic (ROC)-curve estimation based on the binormal model produces improper ROC curves that are not concave over the entire domain and cross the chance line. Although in practice the "improperness" is typically not noticeable, there are situations where the improperness is evident. Presently, standard statistical software does not provide diagnostics for assessing the magnitude of the improperness.

Materials and Methods

We show how the mean-to-sigma ratio can be a useful, easy-to-understand and easy-to-use measure for assessing the magnitude of the improperness of a binormal ROC curve by showing how it is related to the chance-line crossing. We suggest an improperness criterion based on the mean-to-sigma ratio.

Results

Using a real-data example, we illustrate how the mean-to-sigma ratio can be used to assess the improperness of binormal ROC curves, compare the binormal method with an alternative proper method, and describe uncertainty in a fitted ROC curve with respect to improperness.

Conclusions

By providing a quantitative and easily computable improperness measure, the mean-to-sigma ratio provides an easy way to identify improper binormal ROC curves and facilitates comparison of analysis strategies according to improperness categories in simulation and real-data studies.

---

### How do you design randomised trials for smaller populations? A framework [^45132483]. BMC Medicine (2016). Low credibility.

How should we approach trial design when we can get some, but not all, of the way to the numbers required for a randomised phase III trial?We present an ordered framework for designing randomised trials to address the problem when the ideal sample size is considered larger than the number of participants that can be recruited in a reasonable time frame. Staying with the frequentist approach that is well accepted and understood in large trials, we propose a framework that includes small alterations to the design parameters. These aim to increase the numbers achievable and also potentially reduce the sample size target. The first step should always be to attempt to extend collaborations, consider broadening eligibility criteria and increase the accrual time or follow-up time. The second set of ordered considerations are the choice of research arm, outcome measures, power and target effect. If the revised design is still not feasible, in the third step we propose moving from two- to one-sided significance tests, changing the type I error rate, using covariate information at the design stage, re-randomising patients and borrowing external information. We discuss the benefits of some of these possible changes and warn against others. We illustrate, with a worked example based on the Euramos-1 trial, the application of this framework in designing a trial that is feasible, while still providing a good evidence base to evaluate a research treatment. This framework would allow appropriate evaluation of treatments when large-scale phase III trials are not possible, but where the need for high-quality randomised data is as pressing as it is for common diseases.

---

### Practice guidelines for obstetric anesthesia: an updated report by the American society of anesthesiologists task force on obstetric anesthesia and the Society for Obstetric Anesthesia and Perinatology [^390eb025]. Anesthesiology (2016). Medium credibility.

Evidence classification for obstetric anesthesia recommendations — Category A randomized controlled trials (RCTs) and Category B observational studies or RCTs without pertinent comparison groups — is reported by evidence category, level, and direction, with Category A evidence given precedence over Category B for any particular outcome; for Category A, statistically significant (P < 0.01) outcomes are designated as beneficial (B) or harmful (H) and nonsignificant as equivocal (E), and levels are defined as Level 1 meta-analytic findings from a sufficient number of RCTs, Level 2 multiple RCTs not sufficient for a viable meta-analysis with findings reported separately, and Level 3 a single RCT, while for Category B the threshold for significance is a P value of less than 0.01 and levels include Level 1 observational comparisons with comparative statistics and Level 2 noncomparative observational studies with associative statistics; all meta-analyses are conducted by the ASA methodology group.

---

### Recommendations for a standardized pulmonary function report. An official American Thoracic Society technical statement [^64462d9d]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Lower limit of normal (LLN) and fixed cutoffs — percent predicted values should not be used to define abnormality because the true LLN is age- and/or height-dependent and varies among individuals; fixed cutoffs such as 80% predicted for forced vital capacity (FVC) or 0.70 for the FEV1/FVC ratio are based on middle-aged adults, can yield erroneous decisions in children and in older or shorter adults, and introduce sex bias. The LLN does not necessarily need to be the fifth percentile and can be adjusted with outcome data; caution is warranted near dichotomous boundaries, especially with results from a single occasion. In clinical testing where disease is suspected, the pretest probability is higher and the false positive rate is much lower than the lowest 5% implied by percentile thresholds.

---

### Challenging issues in clinical trial design: part 4 of a 4-part series on statistics for clinical trials [^9a41203a]. Journal of the American College of Cardiology (2015). Low credibility.

As a sequel to last week's paper on the fundamentals of clinical trial design, this paper tackles related controversial issues: noninferiority trials, the value of factorial designs, the importance and challenges of strategy trials, Data Monitoring Committees (including when to stop a trial early), and the role of adaptive designs. All topics are illustrated by relevant examples from cardiology trials.

---

### Disentangling dispersion from mean reveals true heterogeneity-diversity relationships [^d027f21a]. Nature Communications (2025). High credibility.

Understanding the effect of heterogeneity is fundamental to numerous fields. In community ecology, classical theory postulates that habitat heterogeneity determines niche dimensionality and drives biodiversity. However, disparate heterogeneity-diversity relationships have been empirically observed, generating increasingly complex theoretical developments. Here we show that spurious heterogeneity-diversity relationships and subsequent theories arise as artifacts of heterogeneity measures that are mean-biased for bounded continuous variables. To solve this, we derive an alternative mean-independent measure of heterogeneity for beta and gamma distributed variables that disentangles statistical dispersion from mean. Using the mean-independent measure of heterogeneity, true monotonic positive heterogeneity-diversity relationships, consistent with classical theory, are revealed in data previously presented as evidence for both hump-shaped heterogeneity-diversity relationships and theories of an area-heterogeneity trade-off for biodiversity. This work sheds light on the source of conflicting results that have hindered understanding of heterogeneity relationships in broader ecology and numerous other fields. The mean-independent measure of heterogeneity is provided as a solution, essential for understanding true mean-independent heterogeneity relationships in wider research.

---

### CurveCurator: a recalibrated F-statistic to assess, classify, and explore significance of dose-response curves [^d55d13ff]. Nature Communications (2023). High credibility.

F-statistics and p -values

The basic idea behind the F-value in classical linear regression problems is to quantify how much better a more complex model (M 1 with k linear parameters) fits the data compared to a simpler model (M 0 with j linear parameters and j < k) given the n observed data points and the corresponding sum-squared errors (SSE) (Eq. 6).

Although not a linear model by nature, the log-logistic function still meets the required assumptions of random sampling, independence of observations, residual normality, and equal variance of the errors. The basic rationality behind CurveCurator's recalibrated F-statistic is similar to the linear F-statistic above. It also quantifies how much better the fitted log-logistic model (M 1) is compared to the mean model (M 0), which describes that there is no relationship between the applied dose and the observed response. We found, however, that n/k was a more appropriate scaling factor for the 4-parameter log-logistic function.

The obtained recalibrated F-value (Eq. 7) can then be used to calculate a p -value that quantifies how often a curve with a similar or bigger F-value can be found by random chance. We observed that these F-values follow a parameterized F-distribution with degrees of freedom that diverged from the case of linear models. Using extensive simulations under the null hypothesis (5 million curves for n = 5… 50), we obtained a simple quasi-linear function to calculate the "effective "degrees of freedom as a function of n (Eqs. 8–10).

---

### Sources of confidence in value-based choice [^4f84ca43]. Nature Communications (2021). High credibility.

Fig. 5
The efficient coding model.

a The decision process with three distinct process stages, color-coded to match the graphs. The prior matches the distribution of subjective values v of supermarket products. When choosing between two items, subjects look repeatedly at them, spending unequal time on the two options. The subjective values are internally encoded, the corresponding likelihood functionis constrained by the prior p (v) via efficient coding. Lastly, noise that occurs after the decoding is taken into account. b Standardized posterior estimates of the relationship between confidence and variance in the encoding process, the variance in the comparison process, and attentional factors (β θ). is not significantly different from 0 (P = 0.39), both and (β θ) are significantly bigger than zero with P < 0.001. The effect size of is not significantly different from 0. Both the effect sizes of and (β θ) are significantly bigger than zero with P < 0.001. Error bars indicate the mean posterior estimate of the standard deviation. P -values are based on the highest density interval of the posterior estimates. c Left column: the empirical probabilities of choosing the upper item; up: as a function of value difference; down: as a function of the difference in dwell time. Right column: the same as left but for the predicted probabilities by the efficient coding model. The trials are median split in high/low confidence. Value difference and dwell time difference are split into eight groups of equal size. Data are presented as mean values ± SEM. Source data are provided as a Source Data file. d Comparison of parameter estimates of two alternative efficient coding models: a model with agent-specific estimates of σ enc and a model that allows for trial-to-trial fluctuations of σ enc. The median of the posterior estimate of σ enc of the agent-specific model is indicated by the horizontal green line, the shaded grey area indicates the 95% confidence interval. The diagonal green lines represent 100 random samples of the posterior distribution of how σ enc changes with confidence in the model allowing for trial-to-trial fluctuations. e, f Same as (e) but for σ comp and θ. g Comparison of the effect sizes of the posterior estimates of σ enc and σ comp. Vertical red dashed line indicates the median, black lines indicate the 95% confidence interval. h Comparison of the posterior estimates of the intercept of θ in the efficient coding model and the RUM. i Comparison of the posterior estimates of the slope of θ in the efficient coding model and the RUM. For the whole figure n = 33 independent participants.

---

### Single photon emission computed tomography (SPECT) myocardial perfusion imaging guidelines: instrumentation, acquisition, processing, and interpretation [^611e4bd9]. Journal of Nuclear Cardiology (2018). Medium credibility.

ASNC single photon emission computed tomography (SPECT) myocardial perfusion imaging — Two-day stress-rest 99mTc acquisition for Anger cameras specifies: Administered activity options are 18–30 mCi and 8–12 mCi for stress, and 18-30mCi and 8–12 mCi for rest, with notes of BMI ≥ 35 kg/m^2 and BMI < 35 kg/m^2. Position includes Supine (Standard) and Prone (Optional). Injection→imaging delay times are 15–60 minutes for stress and 30–60 minutes for rest. Acquisition protocol lists Step and shoot (Standard) and Continuous (Optional), and the energy window is 15–20% symmetric. Collimator is LEHR, and orbit is 180° (45° RAO to 45° LPO) where RAO denotes Right anterior oblique and LPO left posterior oblique. Orbit type includes Circular and Noncircular (Standard). Pixel size is 3–6 mm, the number of projections is 60–64, and matrix choices are 64 x 64 (Minimum) or 128 x 128 (Preferred). Time/projection is 25 s for stress and 20 s for rest. ECG gated is Standard for stress and Optional for rest. Frames/cycle entries include 8 and 16, and the R-to-R window is 100%.

---

### Identifying domains of applicability of machine learning models for materials science [^72c4ba76]. Nature Communications (2020). High credibility.

An illustrative example

Before describing the details of DA identification and its integration into the ML process, let us illustrate the concept and its utility via a synthetic example (see Fig. 1). We consider a simple two-dimensional representation consisting of independent features x 1 and x 2 that are each distributed according to a normal distribution with mean 0 and variance 2 (N (0, 2)) and a target property y that is a third-degree polynomial in x 1 with an additive noise component that scales exponentially in x 2 :That is, the y values are almost determined by the third-degree polynomial for low x 2 values but are almost completely random for high x 2 values. Discovering applicable domains reveals how different models cope differently with this setting even if they have a comparable average error. To show this, let us examine the error distributions obtained from three different kernelized regression models of the formwith parameter vector ν that are fitted around a training, or fitting (F), setwith three different choices for the kernel function k. We observe:
When using the linear (lin) kernel, the resulting linear model is globally incapable to trace the variation of the third-order polynomial except for a small stripe on the x 1 -axis where it can be approximated well by a linear function. Consequently, there is a very high error globally that is substantially reduced in the DA described by σ lin (x 1, x 2) ≡ −0.3 ≤ x 1 ≤ 0.3.
When using the Gaussian kernel), the resulting radial basis function (rbf) model is able to represent the target property well locally unless (a) the noise component is too large and (b) the variation of the target property is too high relative to the number of training points. The second restriction is because the rbfs have non-negligible values only within a small region around the training examples. Consequently, the discovered DA is not only restricted in x 2 -direction but also excludes high absolute x 1 -values: σ rbf ≡ −3.3 ≤ x 11 ≤ 3.1 ∧ x 2 ≤ 0.1.
In contrast, when using the non-local third-degree polynomial (poly) kernel, data sparsity does not prevent an accurate modeling of the target property along the x 1 -axis. However, this non-locality is counterproductive along the x 2 -axis where overfitting of the noise component has a global influence that results in higher prediction errors for the almost deterministic data points with low x 2 -values. This is reflected in the identified DA σ poly (x 1, x 2) ≡ −3.5 ≤ x 2 ≤ 0.1, which contains no restriction in x 1 -direction, but excludes both high and low x 2 -values. This highlights an important structural difference between the rbf and the polynomial model that is not reflected in their similar average errors.

---

### Novel clinical trial designs with dose optimization to improve long-term outcomes [^a1df19de]. Clinical Cancer Research (2023). Medium credibility.

Conventional designs for choosing a dose for a new therapy may select doses that are unsafe or ineffective and fail to optimize progression-free survival time, overall survival time, or response/remission duration. We explain and illustrate limitations of conventional dose-finding designs and make four recommendations to address these problems. When feasible, a dose-finding design should account for long-term outcomes, include screening rules that drop unsafe or ineffective doses, enroll an adequate sample size, and randomize patients among doses. As illustrations, we review three designs that include one or more of these features. The first illustration is a trial that randomized patients among two cell therapy doses and standard of care in a setting where it was assumed on biological grounds that dose toxicity and dose-response curves did not necessarily increase with cell dose. The second design generalizes phase I-II by first identifying a set of candidate doses, rather than one dose, randomizing additional patients among the candidates, and selecting an optimal dose to maximize progression-free survival over a longer follow-up period. The third design combines a phase I-II trial and a group sequential randomized phase III trial by using survival time data available after the first stage of phase III to reoptimize the dose selected in phase I-II. By incorporating one or more of the recommended features, these designs improve the likelihood that a selected dose or schedule will be optimal, and thus will benefit future patients and obtain regulatory approval.

---

### How I read an article that uses machine learning methods [^d784cc52]. Blood Advances (2023). Medium credibility.

Step 3: Familiarize yourself with the ML methods used. The next step is to understand the ML methods that the authors have used to solve the problem. Many papers will provide a brief overview of the methods used (in clinical or applied journals), but it is important to have a good understanding of the underlying concepts. It is critical to familiarize yourself with some of these terminologies presented in Table 1. There are many papers that explain these terminologies in a very simple manner. It is also important to understand the key issues in building ML (Figure 1) models and what the authors did to address these at each step.

Table 1.
ML terminologies

Figure 1.
Steps to build a machine learning model. Problem formulation: The first step is to clearly define the problem that you want to solve. This involves defining the inputs and outputs of your model, as well as the type of problem you are trying to solve (classification, regression, clustering, etc). It is important to have a clear understanding of the problem you are trying to solve before you start building a model. Data collection: Once you have formulated the problem, the next step is to collect the relevant data. This may involve scraping data from websites, downloading data sets from public repositories, or collecting data through surveys or experiments. It is important to collect enough data to train your model and validate its performance. Data preparation: After collecting the data, you will need to clean and preprocess it. This involves removing any irrelevant data, dealing with missing values, and transforming the data into a suitable format for ML algorithms. It also includes dividing the data set into training, validation, and test cohorts. This step can take a lot of time and effort, but it is essential for building an accurate and effective model. Feature engineering: Feature engineering is the process of selecting and transforming the input variables (features) in a way that will improve the performance of the model. This may involve selecting the most relevant features, transforming them into a different representation (eg, using one-hot encoding), or creating new features based on existing ones. Feature engineering can have a significant impact on the performance of the model. Model selection: Once you have prepared the data and engineered the features, the next step is to select a suitable ML algorithm. This involves choosing the type of algorithm (eg, decision trees, neural networks, support vector machines) and the specific parameters of the algorithm. This step requires some knowledge of ML and experience with different algorithms. Model training: After selecting the algorithm, the next step is to train the model on the prepared data. This involves feeding the input data into the algorithm and adjusting the model parameters to optimize its performance. This step can take a lot of time and computational resources, especially for large data sets and complex models. Model evaluation: Once the model has been trained, the next step is to evaluate its performance on a separate test set of data. This involves measuring metrics, such as accuracy, precision, recall, and F1 score, to assess the performance of the model. It is important to test the model on data that it has not seen before to ensure that it can be generalized to new data. Model optimization: If the model performance is not satisfactory, then the next step is to optimize the model. This involves tweaking the model parameters, changing the algorithm, or modifying the feature engineering process to improve the model's performance. This step may require several iterations until the desired level of performance is achieved. Model deployment: Once you have built a satisfactory model, the final step is to deploy it in a production environment. This may involve integrating the model into a web application, creating an application programming interface for other developers to use, or deploying it as a stand-alone application. It is important to ensure that the model is well documented and tested thoroughly before it is deployed.

---

### Is it time to reconsider the indications for surgery in patients with tennis elbow? [^cdae25f1]. The Bone & Joint Journal (2023). Medium credibility.

On an individual level, a steady half-life means that the duration of symptoms or length of follow-up could be largely irrelevant and provides little prognostic value. It also means that the future course is completely unpredictable. Recovery seems to be a random process akin to the biological half-life of drugs. A molecule lasting several half-lives is no different from a molecule that is eliminated first. This does not mean that a patient should expect their symptoms to halve every four months, but rather that the probability of recovery in future persists irrespective of the time passed.

While the underlying mechanisms for the half-life pattern are unclear, these findings have relevance for everyday clinical practice and future research. In this annotation we discuss what the findings mean for the usual practice of operating on patients whose symptoms fail to settle with nonoperative treatment. Second, we consider how the findings influence the interpretation of other studies. Finally, we discuss what our advice should be to patients who are frustrated by long-standing symptoms and seek advice about whether surgery should be undertaken.

---

### The impact of delayed transfers of care on emergency departments: common sense arguments, evidence and confounding [^105152b7]. Emergency Medicine Journal (2020). Medium credibility.

Results

Analysis 1 – synthetic time series analysis

Figure 1 illustrates the artificial data for: (A) Time Series A and (B) Time Series B. Figure 1C illustrates a scatter plot of the same points where the temporal ordering is not considered. Figure 1C indicates a strong positive correlation which was confirmed by the linear regression analysis (R 2 = 0.51). The shading of the scatter plot shows the temporal ordering of the points. The starting points of each time series are observed in the bottom left corner, while the ending points of each series are observed in the top right corner. This illustrates that the upward trends lead to the correlation observed between Time Series A and Time Series B which, before the upward trend was added, were otherwise uncorrelated.

Figure 1
Line plot showing synthetic data of (A) Time Series A and (B) Time Series B; (C) scatter plot of the same points which demonstrate a strong positive correlation (R 2 = 0.51).

Analysis 2 – use of real data to reproduce the previous study

Figure 2 illustrates the time series of: (A) number of breaches and (B) number of DTOCs. Figure 2C shows a reproduction of the analysis in the previous study, showing the scatter plot with high coefficient of determination value (R 2 = 0.50).

Figure 2
Line plot showing the time series of (A) number of breaches and (B) number of DTOCs; (C) a reproduction of the scatter plot previously published.

Analysis 2 – reanalysis of previous study with detrending

Augmented Dickey-Fuller tests yielded P-values = 0.51 and 0.99 respectively for the breaches and DTOC time series, demonstrating no statistical evidence to support trend-stationarity in either time series. This supports the visual evidence in figure 2A and B of an increasing mean value observed over time. The autocorrelation function indicated considerable autocorrelation was present (see online supplementary appendix B). Figure 3 illustrates the same analysis conducted above but applied to the detrended time series (R 2 = 0.01). The R 2 value obtained before detrending is much higher than for the detrended time series.

Figure 3
Scatter plot and analysis on the detrended NHS England data.

Table 1 reports the correlation coefficient and associated P-values for both the original data and the detrended time series. The coefficient values for the detrended data are lower than those found in the original unadjusted data.

Table 1
Correlation coefficient values for the original and the detrended data

---

### Designing accurate emulators for scientific processes using calibration-driven deep models [^5e03881e]. Nature Communications (2020). High credibility.

Predictive models that accurately emulate complex scientific processes can achieve speed-ups over numerical simulators or experiments and at the same time provide surrogates for improving the subsequent analysis. Consequently, there is a recent surge in utilizing modern machine learning methods to build data-driven emulators. In this work, we study an often overlooked, yet important, problem of choosing loss functions while designing such emulators. Popular choices such as the mean squared error or the mean absolute error are based on a symmetric noise assumption and can be unsuitable for heterogeneous data or asymmetric noise distributions. We propose Learn-by-Calibrating, a novel deep learning approach based on interval calibration for designing emulators that can effectively recover the inherent noise structure without any explicit priors. Using a large suite of use-cases, we demonstrate the efficacy of our approach in providing high-quality emulators, when compared to widely-adopted loss function choices, even in small-data regimes.

---

### Persistent tennis elbow symptoms have little prognostic value: a systematic review and meta-analysis [^47de1d20]. Clinical Orthopaedics and Related Research (2022). Medium credibility.

Background

Tennis elbow is a common painful enthesopathy of the lateral elbow that limits upper limb function and frequently results in lost time at work. Surgeons often recommend surgery if symptoms persist despite nonsurgical management, but operations for tennis elbow are inconsistent in their efficacy, and what we know about those operations often derives from observational studies that assume the condition does not continue to improve over time. This assumption is largely untested, and it may not be true; meta-analyzing results from the control arms of tennis elbow studies can help us to evaluate this premise, but to our knowledge, this has not been done.

Questions/Purposes

The aims of this systematic review were to describe the course of (1) global improvement, (2) pain, and (3) disability in participants who received no active treatment (placebo or no treatment) in published randomized controlled trials (RCTs) on tennis elbow. We also assessed (4) whether the duration of symptoms or placebo effect is associated with differences in symptom trajectories.

Methods

We searched MEDLINE, Embase, and CENTRAL from database inception to August 12, 2019, for trials including participants with tennis elbow and a placebo or a no-treatment arm and a minimum follow-up duration of 6 months. There were no language restrictions or exclusion criteria. We extracted global improvement, pain, and disability outcomes. We used the Cochrane Risk of Bias tool to assess the risk of bias of included trials. To estimate the typical course of tennis elbow without active treatment, we pooled global improvement (the proportion of participants who reported feeling much better or completely recovered), mean pain, and mean disability using baseline, 1-month, 3-month, 6-month, and 12-month follow-up data. We transformed pain and disability data from the original papers so that at each timepoint the relevant outcome was expressed as change relative to baseline to account for different baseline values. We used meta-regression to assess whether the placebo effect or duration of symptoms before enrollment was associated with differences in symptom trajectories. We included 24 trials with 1085 participants who received no active treatment.

Results

The number of patients who were not improved decreased exponentially over time. The half-life of global improvement was between 2.5 and 3 months (that is, every 2.5 to 3 months, 50% of the remaining symptomatic patients reported complete recovery or greatly improved symptoms). At 1 year, 89% (189 of 213; 95% CI 80% to 97%) of patients experienced global improvement. The mean pain and disability followed a similar pattern, halving every 3 to 4 months. Eighty-eight percent of pain (95% CI 70% to 100%) and 85% of disability (95% CI 60% to 100%) had resolved by 1 year. The mean duration of symptoms before trial enrollment was not associated with differences in symptom trajectories. The trajectories of the no-treatment and placebo arms were similar, indicating that the placebo effect of the studied active treatments likely is negligible.

Conclusion

Based on the placebo or no-treatment control arms of randomized trials, about 90% of people with untreated tennis elbow achieve symptom resolution at 1 year. The probability of resolution appears to remain constant throughout the first year of follow-up and does not depend on previous symptom duration, undermining the rationale that surgery is appropriate if symptoms persist beyond a certain point of time. We recommend that clinicians inform people who are frustrated with persisting symptoms that this is not a cause for apprehension, given that spontaneous improvement is about as likely during the subsequent few months as it was early after the symptoms first appeared. Because of the high likelihood of spontaneous recovery, any active intervention needs to be justified by high levels of early efficacy and little or no risk to outperform watchful waiting.

Level Of Evidence

Level I, therapeutic study.

---

### Usefulness of rotational spin for coronary angiography in patients with advanced renal insufficiency [^8e930a08]. The American Journal of Cardiology (2002). Low credibility.

Coronary angiography in patients with advanced renal insufficiency is typically restricted to cases of life-threatening circumstances such as acute myocardial infarction and unstable angina. To gather a large amount of visual information with a minimum number of cine runs, and consequently, with a minimum volume of contrast medium, we rotated the gantry at 40 degrees /s throughout an angle of 120 degrees, from the right toward the left anterior oblique positions. This technique of rotational spin during cinegraphic runs has not yet become established in invasive cardiology. Three experienced cardiologists independently evaluated all coronary segments in rotational versus standard coronary angiography modes for 15 patients, on the basis of an 11-point scale (0 = cardiac spin far better to 10 = standard mode far better). A score of 5 signified that there was no difference in quality between the 2 modes. The arithmetic mean of the assessment values was 4.9 ± 0.3 for coronary segments, 5.4 ± 1.3 for coronary lesions, 5.1 ± 1.4 for bifurcations, and 5.0 ± 0.1 for coronary flow. The arithmetic means for the volume of contrast medium (25 ± 4 ml), for the overall dose area product (8.6 ± 4.5 Gy x cm(2)), and for the number of cine graphic frames (203 ± 65) for a diagnostic cardiac spin were significantly below published typical values in standard mode. Cardiac spin enables 3-dimensional coronary impression under conditions of adequate image quality and represents a new, useful, and beneficial method in invasive cardiology for applications involving the special indication of advanced renal insufficiency.

---

### Photon-efficient imaging with a single-photon camera [^e1e3b62a]. Nature Communications (2016). Medium credibility.

The implementation of our reconstruction algorithm can be divided into the following three steps (Fig. 2; Supplementary Note 1).

Step 1: natural scenes have reflectivities that are spatially correlated — the reflectivity at a given pixel tends to be similar to the values at its nearest neighbours — with abrupt transitions at the boundaries between objects. We exploit these correlations by imposing a transverse-smoothness constraint using the total-variation (TV) normon our reflectivity image. In this process, we ignore data from the hot-pixel set. The final reflectivity imageis thus obtained by solving a regularized optimization problem.

Step 2: natural scenes have a finite number of reflectors that are clustered in depth. It follows that in an acquisition without background-light or dark-count detections, the set of detection times collected over the entire scene would have a histogram with N z bins that possesses non-zero entries in only a small number of small subintervals. This longitudinal sparsity constraint is enforced in our algorithm by solving a sparse deconvolution problem from the coarsely time-binned photon-detection data, which is specific to the array imaging set-up, to obtain a small number of representative scene depths. Raw photon-detection events at times corresponding to depths differing by more than cT p /2 from the representative scene depths are censored. As step 2 has identified coarse depth clusters of the scene objects, the next step of the algorithm uses the filtered set of photon detections to determine a high-resolution depth image within all identified clusters.

Step 3: similar to what was done in step 1 for reflectivity estimation, we impose a TV-norm spatial smoothness constraint on our depth image, where data from the hot-pixel setand censored detections at the remaining pixels are ignored. Thus, we obtainby solving a regularized optimization problem.

---

### Highly accurate retinotopic maps of the physiological blind spot in human visual cortex [^8ed2cf06]. Human Brain Mapping (2022). Medium credibility.

Resulting pRF profiles can be asymmetric or have multiple peaks. Given our use of a regular bar stimulus, profiles can also contain artifacts related to spatiotemporal correlations within the stimulus sequence. Consequently, we restricted further analyses to vertices where R 2 RC > 0.1. The distribution of R 2 RC values (Figure S1 a) suggests that this threshold removes the overwhelming majority of vertices that do not contain distinct reverse correlation profiles.

We then fit a two‐dimensional pRF model to these reverse correlation profiles, to estimate the location and extent of pRFs. We used a symmetric two‐dimensional Gaussian profile (Figure 1b, bottom‐right), defined by its x and y positions, its SD σ, and amplitude β. The goodness of pRF fit (R 2 pRF) is determined by calculating the sum of squared pixel‐wise residuals between the predicted pRF profile and the one estimated from reverse correlation. All further quantitative analyses of data are restricted to vertices where R 2 pRF > 0.5. The distribution of these values (Figure S1 b) shows that this threshold includes the majority of vertices that passed the R 2 RC threshold in the first analysis step. Vertices with R 2 pRF below threshold contain only fuzzy, ill‐defined reverse correlation profiles than cannot be fit well with a 2D Gaussian model.

---

### A continuous-time maxSAT solver with high analog performance [^2b617fbe]. Nature Communications (2018). Medium credibility.

Methods

SAT/MaxSAT definitions

Boolean satisfiability in conjunctive normal form (CNF) is a constraint satisfaction problem formulated on N Boolean variables x i ∈ {0, 1}, i = 1, …, N and M clauses C 1, …, C M. A clause is the disjunction (OR operation) of a set of literals, a literal being either the normal (x i) or the negated (NOT) formof a variable, an example clause being: C 4 = . The task is to find an assignment for the variables such that all clauses are satisfied, or alternatively, the conjunctive formula = C 1 ∧… ∧ C M evaluates to 1 (TRUE). If all clauses contain exactly k literals, the problem is k -SAT. For k ≥ 3 this is an NP-complete decision problem, meaning that a candidate solution is easily (poly-time) checked for satisfiability, but finding a solution can be hard (exp-time). Oftentimes, when studying the performance of algorithms over sets of randomly chosen problems the constraint density α = M / N is used as a statistical guide to problem hardness. MaxSAT has the same formulation as SAT, but the task is to maximize the number of satisfied clauses. For both SAT and MaxSAT, all known algorithms require exponentially many computational steps (in N) in the worst case, to find a solution. However, guaranteeing optimality of solution for MaxSAT is as hard as finding the solution itself, and thus MaxSAT is harder than SAT (NP-hard). Max 2-SAT (i.e. k = 2) is already NP-hard.

---

### ACOEM practice guidelines: elbow disorders [^4e95c510]. Journal of Occupational and Environmental Medicine (2013). Medium credibility.

ACOEM elbow disorders — strength-of-evidence criteria define levels and thresholds as follows: levels are labeled "A = Strong evidence base", "B = Moderate evidence base", "C = Limited evidence base", and "I = Insufficient evidence†"; A requires "Two or more high-quality studies", B requires "At least one high-quality study or multiple moderate-quality studies", C requires "At least one study of moderate quality", and I applies when "Evidence is insufficient or irreconcilable". Study quality scoring specifies "High-quality studies are scored 8.0 to 11.0 points. Moderate-quality studies are scored 4.0 to 7.5 points". The guideline notes "Insufficient evidence recommendations are, by definition, consensus-based guidance".

---

### Directly measuring mean and variance of infinite-spectrum observables such as the photon orbital angular momentum [^a1aca62f]. Nature Communications (2015). Medium credibility.

The standard method for experimentally determining the probability distribution of an observable in quantum mechanics is the measurement of the observable spectrum. However, for infinite-dimensional degrees of freedom, this approach would require ideally infinite or, more realistically, a very large number of measurements. Here we consider an alternative method which can yield the mean and variance of an observable of an infinite-dimensional system by measuring only a two-dimensional pointer weakly coupled with the system. In our demonstrative implementation, we determine both the mean and the variance of the orbital angular momentum of a light beam without acquiring the entire spectrum, but measuring the Stokes parameters of the optical polarization (acting as pointer), after the beam has suffered a suitable spin-orbit weak interaction. This example can provide a paradigm for a new class of useful weak quantum measurements.

---

### Defining the time-limited trial for patients with critical illness: an official American Thoracic Society workshop report [^8f1ffb68]. Annals of the American Thoracic Society (2024). High credibility.

Time-limited trial in critical care — essential elements include, with the patient and/or surrogate(s), to discuss the patient's goals and priorities, including what is an acceptable level of recovery after acute critical illness, and discuss estimates of the patient's prognosis, including level of uncertainty. Plan steps are to meet with the patient and/or surrogate(s) to discuss a time-limited trial and define trial guidelines: (1) Planned duration (2) Clinical criteria of improvement and/or deterioration; discuss which therapies are acceptable to the patient during the trial, discuss potential decisions to be made at the end of the trial, provide an opportunity for the patient and/or surrogate to agree or decline to participate in the trial, schedule a future meeting with the patient and/or surrogate(s) to discuss the patient's response to therapy, and document the meeting and plans in the health record. Support actions are to disseminate plans to entire interprofessional care team + whenever staffing rotation occurs, update patient and/or surrogate(s) about major changes in the patient's condition, and reconsider plans if necessary due to major changes in the patient's condition. Reassessment involves meeting with the patient and/or surrogate(s) and using clinical criteria to assess the patient's response to therapy, considering potential decisions — (1) Continue care focused on recovery (e.g., if the patient is improving), (2) Transition to care focused exclusively on comfort, (3) Extend the trial for a newly agreed-upon duration — and documenting assessment and next steps in the health record. Potentially beneficial steps to consider are to determine if additional interprofessional intensive care unit team members or other healthcare teams should be involved in the consider/plan phases, including longitudinal healthcare teams (e.g., primary care, oncology, surgery), specialty palliative care, and new consultant teams from current hospitalization, and during the plan phase, elicit preferences from the patient and/or surrogate about how they would like to communicate over the course of the trial.

---

### Neural and computational underpinnings of biased confidence in human reinforcement learning [^14c04b8b]. Nature Communications (2023). High credibility.

Fig. 4
Modeling choices in the learning phase.

a The learning model architecture. Color panels represent different components of value updating rules. Gray panel: Absolute model (ABS), which consists of basic delta update rule. Blue panel: Relative model (REL); Pink panel: Asymmetric updating model (ASYM); Purple panel: relative-asymmetric model (RELASYM). The contextualization panel illustrate how the unchosen option (X*) is updated in the partial information condition, when the unchosen option outcome (Ru) is not available. X* can take the value of 0, of the expected unchosen value (Qu), of the paired outcome (¬R) and of the last seen outcome associated with the option (Ru→t). b Left panels: Bayesian model comparison Between models included in the model space (X-axis). The Y axis indexes the value of three BMC criteria, namely exceedance probability (EP; gray histograms), expected frequencies (EF; diamonds) and protected exceedance probability (pEP; line and dots) of each model. The red dashed line represents the guessing level for EF. The blue dashed line represents the threshold (95%) for the exceedance probability. Right panels: Estimated parameter values of the winning model (RELASYM, X✱ = with ¬Rc). Dots represent individual data points (n = 40 independent participants). Error bars displayed within the violin plots indicate the sample mean ± SEM. The blue, dotted envelop represent the prior distribution. c Left: modeled trial-by-trial percentage of correct responses. Dots and error bars represent the mean ± SEM of the participant data. Filled, shaded colored areas represent mean ± SEM of the posterior predictive fits obtained from our winning computational model (RELASYM, X✱ = with ¬Rc). Right: average percentage of correct responses across conditions at the individual level (colored dots; n = 40 independent participants) and group-level (horizontal bars). The black error bars indicate the overall performance over conditions. The colored horizontal bar and error bar represent the mean and SEM of the real data, respectively. The large white dot and corresponding error bar represent mean ± SEM of the posterior predictive fits obtained from our winning computational model (RELASYM, X✱ = with ¬Rc). Q c/u, t: value of the chosen/unchosen option at trial t. R c/u: outcome associated to the chosen/unchosen option. δ c/u: prediction error for the chosen/unchosen option. α u/c: learning rate for the chosen/unchosen option. α conf/disc: learning rate for confirmatory/disconfirmatory information. V t: context value; δ V: prediction error for the context value. α V: learning rate for the context value.

---

### Dynamic construction of a coherent attentional state in a prefrontal cell population [^834a6f10]. Neuron (2013). Low credibility.

Data and Analysis

Physiological data were analyzed just from successfully completed trials, on average including 17 repetitions for each combination of cue, choice stimulus type, and hemifield/spatial arrangement. All statistical analyses were done using MATLAB (MathWorks). For all analyses, spike data were smoothed with a Gaussian kernel of SD 20 ms, cutoffs ± 1.5 SD.

To measure neural discrimination between any two choice stimuli X and Y, we used Euclidean distance between activity vectors in the selected cell population. For each cell in the population, we measured the difference in absolute firing rate for X and Y. As is standard, Euclidean distance was defined as the square root of the sum of these squared differences. For MDS (Figure 2 B), we used raw Euclidean distances. For quantitative analysis (Figures 3 and 6), we used a correction for the fact that Euclidean distance must always be positive and scales with absolute firing rate. For each distance measure, we calculated the expected chance value by randomly permuting X, Y labels across trials, then subtracted the median permuted value (across 1,000 permutations) from the obtained raw value. Distances plotted in Figures 3 and 6 are raw values minus median permuted values. Significance bars are based on 95% confidence intervals of the permuted distribution. Analyses were repeated at each 1 ms time point from −100 ms to 600 ms from choice stimulus onset.

---

### Quantum walks and dirac cellular automata on a programmable trapped-ion quantum computer [^dcbde025]. Nature Communications (2020). High credibility.

After evolving a number of steps, we sample the corresponding probability distribution 3000 times and correct the results for readout errors. For the DQW evolution up to five steps shown in Fig. 3, a balanced coin (θ 1 = θ 2 = π /4) is used where the initial position isfor different initial particle states, in Fig. 3 b i, in Fig. 3 b ii, and an equal superposition of both in Fig. 3 b iii. In Fig. 3 b iv, b v, and b vi we show the ideal output from classical simulation of the circuit for comparison (see Methods for a plot of the difference). With a balanced coin the particle evolves in equal superposition to the left and right position at each time step and upon measurement, there is a 50/50 probability of finding the particle to the left or right of its previous position, just as in classical walk. If we let the DQW evolve for more than three steps before we perform a position measurement, we will find a very different probability distribution compared to the classical random walk.

The same experimental setup can be used to recover a DCA with a two-period DQW. Here we set θ 1 = 0 and varied θ 2 to recover the Dirac equation for different mass values. In Fig. 3 c, we show experimental results for θ 2 = π /4, π /10 and π /20, corresponding to a mass 1.1357, 0.3305, and 0.159 in units of ℏc −2 s −1, with the initial particle state in the superposition. The main signature of a DCA for small mass values is the presence of peaks moving outward and a flat distribution in the middle as shown for the cases with small values of θ 2, Figs. 3 c ii-iii. This bimodal probability distribution in position space is an indication of the one-dimensional analog of an initially localized Dirac particle, with positive energy, evolving in time which spreads in all directions in position space at speeds close to the speed of light. In contrast, a DCA with θ 2 = π /4, Fig. 3 c i corresponds to a massive particle and hence there is a slow spread rather than a ballistic trajectory in position space.

---

### Quantifying the dynamics of failure across science, startups and security [^6acaa77f]. Nature (2019). Excellent credibility.

Human achievements are often preceded by repeated attempts that fail, but little is known about the mechanisms that govern the dynamics of failure. Here, building on previous research relating to innovation 1–7, human dynamics 8–11 and learning 12–17, we develop a simple one-parameter model that mimics how successful future attempts build on past efforts. Solving this model analytically suggests that a phase transition separates the dynamics of failure into regions of progression or stagnation and predicts that, near the critical threshold, agents who share similar characteristics and learning strategies may experience fundamentally different outcomes following failures. Above the critical point, agents exploit incremental refinements to systematically advance towards success, whereas below it, they explore disjoint opportunities without a pattern of improvement. The model makes several empirically testable predictions, demonstrating that those who eventually succeed and those who do not may initially appear similar, but can be characterized by fundamentally distinct failure dynamics in terms of the efficiency and quality associated with each subsequent attempt. We collected large-scale data from three disparate domains and traced repeated attempts by investigators to obtain National Institutes of Health (NIH) grants to fund their research, innovators to successfully exit their startup ventures, and terrorist organizations to claim casualties in violent attacks. We find broadly consistent empirical support across all three domains, which systematically verifies each prediction of our model. Together, our findings unveil detectable yet previously unknown early signals that enable us to identify failure dynamics that will lead to ultimate success or failure. Given the ubiquitous nature of failure and the paucity of quantitative approaches to understand it, these results represent an initial step towards the deeper understanding of the complex dynamics underlying failure.

---

### Spine injury [^f27c236c]. ACS (2022). High credibility.

Spine injury — motor level of injury assessment uses bilateral testing of five key muscle groups in the upper extremities and five in the lower extremities, with muscle strength graded using a 6-point score (0 to 5). Grades include 0/5 = Total paralysis and 5/5 = Active movement, full ROM against gravity and full resistance in a functional muscle position, and NT = Not testable. An asterisk " tag is added to 0, 1, 2, 3, 4 or NT when an impairment is due to a non-SCI condition in the ASIA worksheet. Additional guidance states to assign a score for each key muscle group of the right and left upper and lower extremities and to complete the motor assessment by evaluating the contraction of the external anal sphincter around the examiner's finger, with scoring reported as present or absent voluntary anal contraction. The normal motor level on each side is determined by a minimum score of 3 with all the proximal key muscle groups being 5 (normal), and the overall motor level score is the last normal for both (a minimum of 3 bilaterally); if the motor level cannot be tested or determined, the motor level score is considered to be the same as the sensory level score.

---

### Development of a core outcome set for lateral elbow tendinopathy (COS-LET) using best available evidence and an international consensus process [^f580e8eb]. British Journal of Sports Medicine (2022). Medium credibility.

Introduction

Background and objectives

Pain arising from the tendons on the lateral side of the elbow is common in adults, particularly in middle age. Historically, it has been known by various names such as lateral epicondylitis or tennis elbow, but the current accepted description is lateral elbow tendinopathy (LET). It is acknowledged that there is substantial heterogeneity of outcome measure instrument use in elbow research and specifically for LET. With no clear consensus on which instruments most accurately represent a patient's LET-related health status, comparison of effectiveness research and evidence synthesis/meta-analysis has been hampered.

In 2019, an international group of experts in the field of tendinopathy (International Scientific Tendinopathy Symposium Consensus (ICON) Group), comprising researchers, healthcare professionals and patients, published a consensus document defining the nine health-related core domains of tendinopathy. That group recommended researchers and clinicians measure outcomes for specific regional tendinopathies against these domains.

The aim of this project was to develop a core outcome set (COS) for LET that maps to the nine domains. A COS is a minimal set of outcome measures to be used in future research and clinical practice involving people with LET. It enables meta-analysis of findings from different studies using a consistent set of measures. To be included in a COS, measures need to be both practical to perform (based on cost, patient burden and availability) and of high quality (valid, responsive, reliable, interpretable and of acceptable burden for patients and investigators). The result will be a minimum set of outcome measurement instruments to be used in future LET research that allows direct comparison between different studies across the nine domains.

Scope

This COS relates to all adults diagnosed with LET and applies to interventional research (including surgical and non-surgical) and longitudinal assessment. The COS will only apply to the English language.

---

### Cladribine (Mavenclad) [^adfd0eb6]. FDA (2025). Medium credibility.

2.2 Recommended Dosage

The recommended cumulative dosage of MAVENCLAD is 3.5 mg per kg body weight administered orally and divided into 2 yearly treatment courses (1.75 mg per kg per treatment course) (see Table 1). Each treatment course is divided into 2 treatment cycles:

Administration of First Treatment Course

First Course/First Cycle: start any time.
First Course/Second Cycle: administer 23 to 27 days after the last dose of First Course/First Cycle.

Administration of Second Treatment Course

Second Course/First Cycle: administer at least 43 weeks after the last dose of First Course/Second Cycle.
Second Course/Second Cycle: administer 23 to 27 days after the last dose of Second Course/First Cycle.

Administer the cycle dosage as 1 or 2 tablets once daily over 4 or 5 consecutive days [see How Supplied/Storage and Handling (16.1)]. Do not administer more than 2 tablets daily.

Following the administration of 2 treatment courses, do not administer additional MAVENCLAD treatment during the next 2 years. Treatment during these 2 years may further increase the risk of malignancy [see Warnings and Precautions (5.1)]. The safety and efficacy of reinitiating MAVENCLAD more than 2 years after completing 2 treatment courses has not been studied.

2.3 Missed Dose

If a dose is missed, patients should not take double or extra doses.

If a dose is not taken on the scheduled day, then the patient must take the missed dose on the following day and extend the number of days in that treatment cycle. If two consecutive doses are missed, the treatment cycle is extended by 2 days.

2.4 Administration

MAVENCLAD tablets are taken orally, with water, and swallowed whole without chewing. MAVENCLAD can be taken with or without food.

Separate administration of MAVENCLAD and any other oral drugs by at least 3 hours during the 4 to 5 day MAVENCLAD treatment cycles [see Clinical Pharmacology (12.6)].

MAVENCLAD is a cytotoxic drug. Follow applicable special handling and disposal procedures [see References (15)]. MAVENCLAD is an uncoated tablet and must be swallowed immediately once removed from the blister. If a tablet is left on a surface, or if a broken or fragmented tablet is released from the blister, the area must be thoroughly washed with water.

The patient's hands must be dry when handling the tablets and washed thoroughly afterwards. Avoid prolonged contact with skin.

---

### Guidelines for the echocardiographic assessment of the right heart in adults: a report from the American Society of Echocardiography endorsed by the European association of echocardiography, a registered branch of the European Society of Cardiology, and the Canadian society of echocardiography [^b0a30f7f]. Journal of the American Society of Echocardiography (2010). Medium credibility.

Guidelines overview and aims — these guidelines are "to be viewed as a starting point to establish a standard uniform method for obtaining right heart images for assessing RV size and function and as an impetus for the development of databases to refine the normal values". They aim to "Provide revised reference values for right-sided measures with cutoff limits representing 95% confidence intervals based on the current available literature". Methodology notes that "An extensive systematic literature search was performed to identify all studies reporting echocardiographic right heart measurements in normal subjects".

---

### Guidelines for the cardiac sonographer in the performance of contrast echocardiography: a focused update from the American Society of Echocardiography [^04c207f8]. Journal of the American Society of Echocardiography (2014). Medium credibility.

Update on knowledge of ultrasound physics and instrumentation — mechanical index (MI) categories and imaging guidance are defined, with very low MI representing values < 0.2, low MI < 0.3, intermediate MI 0.3 to 0.5, and high MI exceeding 0.5; power modulation improves the signal-to-noise ratio at very low MIs (0.05–0.20); contrast pulse–sequence imaging can be used at very low MIs (< 0.2) to assess left ventricular opacification (LVO) and myocardial contrast perfusion in real time; when real-time very low MI software is not available, low-MI (< 0.3) harmonic imaging should be used; and although myocardial perfusion imaging is not an approved indication for ultrasound contrast agents (UCAs), very low MI techniques have been used in multiple clinical studies, with pulse–sequence methods offering better tissue cancellation and enhanced contrast from microbubbles compared with B-mode low-MI harmonic imaging.

---

### Identifying domains of applicability of machine learning models for materials science [^196a563d]. Nature Communications (2020). High credibility.

Fig. 1
Domains of applicability of three 2d-models of a noisy third-degree polynomial.

Three different models, linear (top), radial basis function (rbf, center), and polynomial (poly, bottom), are shown approximating the same distribution of two independent features x 1 ~ N (0, 2) and x 2 ~ N (0, 2), and the target property, where N (μ, ϵ 2) denotes a normal distribution with mean μ and standard deviation ϵ. Test points are plotted in 3d plots against the prediction surface of the models (color corresponds to absolute error) where the DA is highlighted in gray. The distributions of individual errors for the DA (gray) and globally (black) are shown in the 2d plots of each panel with the mean error (solid) and the 95th percentile (95 perc./dashed) marked by vertical lines. Note that the global error distribution of the linear model has a considerably long tail, which is capped in the image.

---

### Clinical practice guidelines for healthy eating for the prevention and treatment of metabolic and endocrine diseases in adults: cosponsored by the American Association of Clinical Endocrinologists / the American college of endocrinology and the obesity society [^2288b87f]. Endocrine Practice (2013). Medium credibility.

Regarding preventative measures for vitamin D deficiency, more specifically with respect to vitamin D requirements, AACE/ACE/OS 2013 guidelines recommend to prescribe vitamin D supplementation of at least 1,000–2,000 IU of vitamin D2 or vitamin D3 daily in most patients, to keep the plasma 25-hydroxyvitamin D level > 30 ng/mL.

---

### Ocrelizumab (Ocrevus) [^0933df30]. FDA (2025). Medium credibility.

2.3	Recommended Dosage and Dose Administration

Administer OCREVUS under the close supervision of an experienced healthcare professional with access to appropriate medical support to manage severe reactions such as serious infusion reactions.

Initial dose: 300 mg intravenous infusion, followed two weeks later by a second 300 mg intravenous infusion.
Subsequent doses: single 600 mg intravenous infusion every 6 months.
Observe the patient for at least one hour after the completion of the infusion [see Warnings and Precautions (5.1)].

2.4	Delayed or Missed Doses

If a planned infusion of OCREVUS is missed, administer OCREVUS as soon as possible; do not wait until the next scheduled dose. Reset the dose schedule to administer the next sequential dose 6 months after the missed dose is administered. Doses of OCREVUS must be separated by at least 5 months [see Dosage and Administration (2.3)].

2.5	Dose Modifications Because of Infusion Reactions

Dose modifications in response to infusion reactions depends on the severity.

Life-threatening Infusion Reactions

Immediately stop and permanently discontinue OCREVUS if there are signs of a life-threatening or disabling infusion reaction [see Warnings and Precautions (5.1)]. Provide appropriate supportive treatment.

Severe Infusion Reactions

Immediately interrupt the infusion and administer appropriate supportive treatment, as necessary [see Warnings and Precautions (5.1)]. Restart the infusion only after all symptoms have resolved. When restarting, begin at half of the infusion rate at the time of onset of the infusion reaction. If this rate is tolerated, increase the rate as described in Table 1 [see Dosage and Administration (2.3)]. This change in rate will increase the total duration of the infusion but not the total dose.

Mild to Moderate Infusion Reactions

Reduce the infusion rate to half the rate at the onset of the infusion reaction and maintain the reduced rate for at least 30 minutes [see Warnings and Precautions (5.1)]. If this rate is tolerated, increase the rate as described in Table 1 [see Dosage and Administration (2.3)]. This change in rate will increase the total duration of the infusion but not the total dose.

---

### The effect of two priors on Bayesian estimation of "Proper" binormal ROC curves from common and degenerate datasets [^d19d800a]. Academic Radiology (2010). Low credibility.

Rationale and Objectives

We showed previously that maximum-likelihood (ML) and Bayesian (with a flat prior on a common parameterization of the model) estimates of "proper" binormal receiver operating characteristic (ROC) curves produce similar results. We propose a new prior that is flat over the area under the ROC curve (AUC) and investigate its effect on the Bayesian estimates.

Materials and Methods

In two simulation studies, we compared Bayesian estimation of the AUC with the two prior probability distributions against ML estimation in terms of root mean squared error (RMSE) and the coverage of 95% confidence (or credible) intervals (both abbreviated CIs). In the first study, we simulated categorical data that tend to be "well-behaved" and produce ROC curve estimates that most would consider reasonable. In the second study, we simulated coarsely discretized categorical data that often included so-called degenerate datasets that cause the ML estimate to be the perfect ROC curve.

Results

For the well-behaved datasets, all three AUC estimates were similar in terms of RMSE and 95% CI coverage. For the coarsely discretized datasets, the RMSE of ML was consistently greater than that of Bayesian estimation and the 95% CI coverage of ML estimation was consistently below nominal, whereas the 95% CI coverage of Bayesian estimation was consistently equal to, or greater than, nominal.

Conclusion

Bayesian estimation with a flat prior on the AUC can provide reasonable inference from datasets with coarsely categorized data that are prone to be degenerate and produce results similar to other estimation methods on well-behaved datasets.

---

### Defining the time-limited trial for patients with critical illness: an official American Thoracic Society workshop report [^cf1ee11e]. Annals of the American Thoracic Society (2024). High credibility.

Time-limited trial in critical care — essential elements and phased process were derived through a Delphi process: investigators "identified 18 potential steps" and found first-round consensus that "11 of these were essential elements"; during the second round, they "identified seven additional essential elements". The committee then "combined 4 related items into two steps (to reach 16 essential elements)" and "organized steps into four phases of care: consider, plan, support, and reassess". Additional steps "may be helpful in some cases, but not necessary for all trials", and involvement of other disciplines is "highly dependent on a patient's specific situation and on the available hospital resources".

---

### Clinical pilot study of different hand positions during manual chest compressions monitored with capnography [^5cf4d9af]. Resuscitation (2013). Low credibility.

Purpose Of The Study

Optimal hand position for chest compressions during cardiopulmonary resuscitation is unknown. Recent imaging studies indicate significant inter-individual anatomical variations, which might cause varying haemodynamic responses with standard chest compressions. This prospective clinical pilot study intended to assess the feasibility of utilizing capnography to optimize chest compressions and identify the optimal hand position.

Materials and Methods

Intubated cardiac arrest patients treated by the physician manned ambulance between February and December 2011 monitored with continuous end-tidal CO2 (EtCO2) measurements were included. One minute of chest compressions at the inter-nipple line (INL) optimized using EtCO2 feedback, was followed by four 30-s intervals with compressions at four different sites; INL, 2 cm below the INL, 2 cm below and to the left of INL and 2 cm below and to the right of INL.

Results

Thirty patients were included. At the end of each 30-s interval median (range) EtCO2 was 3.1 kPa (0.7–8.7 kPa) at INL, 3.5 kPa (0.5–10.7) 2 cm below INL, 3.5 kPa (0.5–10.3 kPa) 2 cm below and to the left of INL, and 3.8 kPa (0.4–8.8 kPa) 2 cm below and to the right of INL (p = 0.4). The EtCO2 difference within each subject between hand positions with maximum and minimum values varied between individuals from 0.2 to 3.4 kPa (median 0.9 kPa).

Conclusion

Monitoring and optimizing chest compressions using capnography was feasible. We could not demonstrate one superior hand position, but inter-individual differences suggest optimal hand position might vary significantly among patients.

---

### Defining the time-limited trial for patients with critical illness: an official American Thoracic Society workshop report [^f52940f6]. Annals of the American Thoracic Society (2024). High credibility.

Table 2: Major themes in defining time-limited trials in critical care — Healthcare literature reflects operational approaches (essential elements and pragmatic steps) and conceptual approaches (objectives and rationales). Operational themes include Interventions (life-sustaining care in general, specific life-sustaining therapies) and Engagement with patients or surrogates (agreement, planning, standardization). Conceptual themes include Prognosis (uncertain, poor) and Decision making (uncertainty, readiness, disagreement).

---

### Six steps in quality intervention development (6SQuID) [^139a29a2]. Journal of Epidemiology and Community Health (2016). Low credibility.

Improving the effectiveness of public health interventions relies as much on the attention paid to their design and feasibility as to their evaluation. Yet, compared to the vast literature on how to evaluate interventions, there is little to guide researchers or practitioners on how best to develop such interventions in practical, logical, evidence based ways to maximise likely effectiveness. Existing models for the development of public health interventions tend to have a strong social-psychological, individual behaviour change orientation and some take years to implement. This paper presents a pragmatic guide to six essential Steps for Quality Intervention Development (6SQuID). The focus is on public health interventions but the model should have wider applicability. Once a problem has been identified as needing intervention, the process of designing an intervention can be broken down into six crucial steps: (1) defining and understanding the problem and its causes; (2) identifying which causal or contextual factors are modifiable: which have the greatest scope for change and who would benefit most; (3) deciding on the mechanisms of change; (4) clarifying how these will be delivered; (5) testing and adapting the intervention; and (6) collecting sufficient evidence of effectiveness to proceed to a rigorous evaluation. If each of these steps is carefully addressed, better use will be made of scarce public resources by avoiding the costly evaluation, or implementation, of unpromising interventions.

---

### Guidelines for the diagnosis and treatment of cobalamin and folate disorders [^1cec0493]. British Journal of Haematology (2014). Medium credibility.

Regarding screening and diagnosis for vitamin B12 deficiency, more specifically with respect to diagnostic criteria, BSH 2014 guidelines recommend to diagnose vitamin B12 deficiency based on vitamin B12 levels of < 148 pmol/L (200 ng/L) or if vitamin B12 levels are below the LLN for the local reference range, in the presence of strong clinical suspicion.

---

### Recommendations for the evaluation of left ventricular diastolic function by echocardiography and for heart failure with preserved ejection fraction diagnosis: an update from the American Society of Echocardiography [^447c9be2]. Journal of the American Society of Echocardiography (2025). High credibility.

Diastolic function values and prognosis — ARIC cohort data show that the values provided are based on an estimate of the most extreme 10% of values from disease-free individuals, and among > 5,700 older adults (> 65 years of age) increases in incidence of heart failure (HF) or death were observed at septal e′ velocity < 6 cm/s (lower 10th percentile limit 4.6 cm/s) and lateral e′ velocity < 7 cm/s (lower 10th percentile limit 5.2 cm/s); additionally, in this study E/e′ ratio and LAVi demonstrated monotonic and near linear associations with incidence of HF or death without evidence of a clear threshold.

---

### KDOQI US commentary on the 2017 KDIGO clinical practice guideline on the evaluation and care of living kidney donors [^bc193d94]. American Journal of Kidney Diseases (2020). High credibility.

KDOQI commentary — living kidney donor risk prediction tool limitations and related consent guidance are described as follows: The tool "does not include family history of kidney disease or donation to an immediate relative with kidney failure" and "does not include other important clinical variables", such that "for donor candidates with these risk factors, the tool may underestimate risk". It "was constructed using a life-table approach" with follow-up "from only 4 to 16 years", whereas donor risk spans "multiple decades", and "The lifetime estimates generated are therefore inherently imprecise approximations". The website "(http://www.transplantmodels.com/esrdrisk) does not readily acknowledge most of these limitations", and outcomes compared were "only to 15 years postdonation". Accordingly, "The tool should be used only with a clear understanding that it may well generate incorrect information about long-term outcomes" and "should be considered as a possibly valuable step". An "additional predictive tool by Massie et al" exists, but "Both the pre- and postdonation tools require additional validation and study before being considered standard of care". Given current constraints, "the lack of tools to estimate future risk with precision makes the goal unreachable at this time", and programs "currently lack both the data and the experience to be able to follow the KDIGO recommendation to set their own acceptable absolute thresholds for long-term risks of kidney failure". The commentary "question[s] the prescription in recommendation 1.11 to express risks 'as absolute as well as relative risks'", agreeing the absolute risk is key because the increase "will usually be relatively low", while stating informed consent would include "the disclosure of the best-available estimates of relative risk of kidney failure due to kidney donation", and it notes recommendation "1.17" "is a UNOS requirement to provide what is termed the 'Independent Living Donor Advocate'.

---

### Cyclosporine (Verkazia) [^382ebe7e]. FDA (2025). Medium credibility.

Using Verkazia

Follow the instructions (Step 1 to Step 12) each time you use Verkazia. If there is anything that you do not understand, ask your healthcare provider.

Step 1. Wash your hands.

Step 2. Open the aluminum pouch, which contains 5 single-dose vials.

Step 3. Take 1 vial from the aluminum pouch. Leave the remaining vials in the pouch.

Step 4. Gently shake the vial.

Step 5. Twist off the cap (see Figure A).

Figure A

Step 6. Pull down your lower eyelid (see Figure B).

Figure B

Step 7. Tilt your head back and look up.

Step 8. Gently squeeze 1 drop of the medicine onto your eye. Make sure that the tip of the vial does not touch your eye. If a drop misses your eye, try again.

Step 9. Blink a few times so that the medicine spreads across your eye.

Step 10. After using Verkazia, press the corner of your eye closest to your nose and gently close the eyelid for 2 minutes (see Figure C).

Figure C

Step 11. If you need to use drops in both eyes, repeat Steps 6 to 10 for your other eye. There is enough medicine in one single-dose vial to use in both eyes.

Step 12. Throw away the vial with any remaining medicine after use.

Storing Verkazia

Store Verkazia at room temperature between 68°F to 77°F (20°C to 25°C).
Do not freeze.
After opening the pouch, keep vials in the pouch to protect from light and evaporation.

Throwing away Verkazia

Any open vial with any medicine left over should be thrown out right after use.

Talk with your healthcare provider if you want more information about Verkazia.

Distributed by:

Santen Incorporated

6401 Hollis St. Suite 125

Emeryville, CA 94608

This Instructions for Use has been approved by the U.S. Food and Drug Administration.

Approved: June 2021

---

### European Stroke Organisation (ESO) and European Association of Neurosurgical Societies (EANS) guideline on stroke due to spontaneous intracerebral haemorrhage [^a24b784e]. European Stroke Journal (2025). High credibility.

Regarding diagnostic investigations for intracerebral hemorrhage, more specifically with respect to initial evaluation, EANS/ESO 2025 guidelines recommend to consider using algorithms such as the DIAGRAM for targeted investigation of the cause of spontaneous ICH to improve the performance of prediction regarding the underlying cause, compared to standard care.

---

### Basal cell skin cancer, version 2.2024, NCCN clinical practice guidelines in oncology [^e9cc4f83]. Journal of the National Comprehensive Cancer Network (2023). High credibility.

Regarding medical management for basal cell carcinoma - NCCN, more specifically with respect to management of low-risk basal cell carcinoma (additional treatment), NCCN 2023 guidelines recommend to proceed to follow-up for patients with low-risk basal cell carcinoma and negative margins.

---

### Six ways not to improve patient flow: a qualitative study [^056121c3]. BMJ Quality & Safety (2017). Medium credibility.

Implications

Figure 1 prompts decision-makers to rigorously interrogate proposed initiatives in terms of population, capacity and process. In addition, the following guiding questions provide a framework for considering the three aspects in tandem.
What populations are experiencing flow problems, and what are their needs? Differentiate among the major subgroups with different clusters of need; prioritise those that account for the greatest amount of potentially conservable utilisation.
What capacity is most suitable for meeting the identified needs, bearing in mind that different capacity may be required as patients' needs evolve? If appropriate, consider multiple suitable options, noting which ones are already present in the system.
What process does this population currently experience? Map the process, using data to locate the prime constraint that impedes or delays access to suitable capacity; the process' endpoint should be defined as the point at which the identified needs have been met.
What alternative process (es) could straightforwardly link the population to suitable capacity, avoiding the constraint? If new capacity and/or processes are proposed, how will these impact other populations? What is needed to ensure that new or released capacity is used for its intended purpose?

The suggested thought process conforms to standard flow-improvement methodology, but adds repeated reminders to take a system view, scrutinising how different parts of the system impinge on each other. Such thinking may help organisations move beyond a proliferation of piecemeal initiatives to a coherent strategy of identifying the most important constraints and following these as they move around the system. Without a system perspective to inform improvement efforts, the most promising initiative may become just another dismal entry in 'The How-Not-To Guide' to patient flow.

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^6e468ce5]. Annals of the American Thoracic Society (2023). High credibility.

General principles of biophysical computational models — biophysical computational models can be used for forward simulations of function or inverse identification of system structure and parameters. Forward models are described as useful for sensitivity analyses to evaluate the contribution of normal subject variability to imaging metrics to define a threshold for abnormal and can also predict emergent behavior after parameter changes at the cellular or tissue level. Inverse models are described as useful for deriving information that cannot be measured directly, including distributions of airway or vascular obstruction that produce specific V˙A/Q˙ patterns. Both forward and inverse models can be used to analyze, interpret, and predict.

---

### Evidence-based guidelines for the pharmacological treatment of schizophrenia: updated recommendations from the British Association for Psychopharmacology [^eacb6a16]. Journal of Psychopharmacology (2020). High credibility.

Regarding follow-up and surveillance for schizophrenia, more specifically with respect to assessment of treatment response, BAP 2020 guidelines recommend to consider defining a sufficient response to initial treatment as a reduction of 25% in total score on a symptom assessment scale or a one-point improvement on the Clinical Global Impressions scale within 2 weeks. Define a definitive response as a 40–50% reduction or a two-point improvement on the Clinical Global Impressions scale after 6 weeks. Aim for symptom remission, such as no more than a mild severity rating for a range of Positive and Negative Syndrome Scale items, as the target for the overall treatment of the initial episode.

---

### Vitamin B2 [^7943cae3]. FDA. Low credibility.

The drug Vitamin B2.

---

### Standards of care in diabetes – 2025 [^31be65a6]. Diabetes Care (2025). High credibility.

Regarding diagnostic investigations for hypoglycemia, more specifically with respect to screening for cognitive impairment, ADA 2025 guidelines recommend to simplify diabetes treatment plans as much as possible for patients with cognitive impairment and tailor to minimize the risk of hypoglycemia.

---

### Colorectal cancer screening and prevention [^c270f773]. American Family Physician (2025). High credibility.

Regarding screening and diagnosis for colon cancer, more specifically with respect to indications for screening, general population, aged 76–85 years, AAFP 2025 guidelines recommend to consider obtaining screening for CRC in adults aged 76–85 years at average risk based on overall health status, prior screening history, and patient preferences.

---

### Evidence for delayed presbyopia after photorefractive keratectomy for myopia [^48963048]. Ophthalmology (2006). Low credibility.

Purpose

To evaluate uncorrected near visual acuity (NVA), accommodation, corneal aberrations, and the optical quality of the retinal image in presbyopic eyes after photorefractive keratectomy (PRK).

Design

Nonrandomized comparative retrospective study.

Participants

Ten post-PRK patients and 10 normal patients.

Methods

Twenty eyes (10 right and 10 left after PRK for myopia, minimum of 10 years after the operation; group A) were compared with 20 eyes (10 right and 10 left age- and gender-matched normal controls; group B). All subjects were over 40 years of age.

Main Outcome Measures

With best distance correction, NVA was measured at 40 cm (Jaeger, J series), and the range of accommodation (diopters [D]) was measured subjectively (negative relative amplitude and positive relative amplitude). The modulation transfer function (MTF) and corneal aberrations such as comalike, spherical (SAs), and higher order (HOAs) were measured with a Hartmann-Shack aberrometer. From the MTF curves, the spatial frequencies corresponding to contrast values of 0.1 and 0.5 were noted.

Results

Mean ages (± standard deviations [SDs]) were 46.3 years (4.7) for group A and 47.6 years (4.9) for group B (P > 0.05). Near acuity was J1 or better in 12 of 20 post-PRK and 4 of 20 control eyes. Mean accommodations (± SDs) were 3.2 D (1.14) for right group A eyes and 2.1 D (0.94) for right group B eyes (P = 0.0152), and 3.4 D (0.99) for left A eyes and 2.3 D (1.02) for left B eyes (P = 0.0168). Total HOA indexes (± SDs) were 1.449 (0.409) for right group A eyes and 0.824 (0.241) for right group B eyes (P = 0.008), and 1.464 (0.388) for left A eyes and 1.067 (0.542) for left B eyes (P = 0.0752). Pooling the data from post-PRK and control eyes, a significant correlation was found between near acuity and SA (right eyes, r = -0.535, P = 0.015; left eyes, r = -0.493, P = 0.027). Significant associations were found between accommodation, near acuity, HOA, and comalike aberration for right eyes only. Mean spatial frequencies (± SDs) corresponding to contrast values of 0.1 for right and left eyes were 14.96 (5.71) for right group A eyes and 22.02 (6.85) for right group B eyes (P = 0.074), and 15.11 (7.80) for left A eyes and 21.41 (9.00) for left B eyes (P = 0.271). Mean spatial frequencies (± SDs) corresponding to contrast values of 0.5 for right and left eyes were 2.86 (0.63) for right group A eyes and 3.21 (0.35) for right group B eyes (P = 0.596), and 2.76 (0.98) for left A eyes and 3.22 (0.27) for left B eyes (P = 0.194).

Conclusions

Compared with normal eyes, in previously myopic eyes treated with first-generation PRK lasers there is a tendency for (1) the optical quality of the retinal image to be reduced at low contrast, (2) the aberrations attributed to the corneal surface to increase, and (3) both measured subjective accommodation and near acuity to be greater than expected. We postulate that the corneal aberrations induced by PRK for myopia may reduce the quality of the retinal image for distance but enhance near acuity by way of a multifocal effect that can delay the onset of age-related near vision symptoms.

---

### Missing data: a special challenge in aging research [^73b86f84]. Journal of the American Geriatrics Society (2009). Low credibility.

Scientific evidence should guide clinical care, but special methodological challenges influence interpretation of the medical literature pertaining to older adults. Missing data, ranging from lack of individual items in questionnaires to complete loss to follow-up, affect the quality of the evidence and are more likely to occur in studies of older adults because older adults have more health and functional problems that interfere with all aspects of data collection than do younger people. The purpose of this article is to promote knowledge about the risks and consequences of missing data in clinical aging research and to provide an organized approach to prevention and management. Although it is almost never possible to achieve complete data capture, efforts to prevent missing data are more effective than analytical "cure". Strategies to prevent missing data include selecting a primary outcome that is easy to determine and devising valid alternate definitions, adapting data collection to the special needs of the target population, pilot testing data collection plans, and monitoring missing data rates during the study and adapting data collection procedures as needed. Key steps in the analysis of missing data include assessing the extent and types of missing data before analysis, exploring potential mechanisms that contributed to the missing data, and using multiple analytical approaches to assess the effect of missing data on the results. Manuscripts should disclose rates of missing data and losses to follow-up, compare dropouts with participants who completed the study, describe how missing data were managed in the analysis phase, and discuss the potential effect of missing data on the conclusions of the study.

---

### Follitropin alfa (Gonal-F) [^edcfa993]. FDA (2025). Medium credibility.

2.3 Dosing for Ovulation Induction

Prior to initiation of treatment with GONAL-f:

Perform a complete gynecologic and endocrinologic evaluation
Exclude primary ovarian failure
Exclude the possibility of pregnancy
Demonstrate tubal patency
Evaluate the fertility status of the male partner

The dosing scheme is stepwise and is individualized for each woman [see Clinical Studies (14.1)].

Administer a starting dose of 75 International Units of GONAL-f subcutaneously daily for 14 days in the first cycle of use.
In subsequent cycles of treatment, determine the starting dose (and dosage adjustments) of GONAL-f based on the woman's history of the ovarian response to GONAL-f.
If indicated by the ovarian response after the initial 14 days, make an incremental adjustment in dose of up to 37.5 International Units.
If indicated by the ovarian response, make additional incremental adjustments in the dose, up to 37.5 International Units, every 7 days.
Continue treatment until follicular growth and/or serum estradiol levels indicate an adequate ovarian response.
Consider the following when planning the woman's individualized dose:
Use the lowest dose of GONAL-f consistent with the expectation of good results.
Use appropriate GONAL-f dose adjustment(s) to prevent multiple follicular growth and cycle cancellation.
The maximum, individualized, daily dose of GONAL-f is 300 International Units per day.
In general, do not exceed 35 days of treatment, unless an estradiol rise indicates imminent follicular development.
When pre-ovulatory conditions are reached, administer human chorionic gonadotropin (hCG) to induce final oocyte maturation and ovulation. Human chorionic gonadotropin, hCG, (5,000 USP units) should be given 1 day after the last dose of GONAL-f.
Encourage the woman and her partner to have intercourse daily, beginning on the day prior to the administration of hCG and until ovulation becomes apparent.
Withhold hCG in cases where the ovarian monitoring suggests an increased risk of ovarian hyperstimulation syndrome (OHSS) on the last day of GONAL-f therapy (for example estradiol greater than 2,000 pg per mL) [see Warnings and Precautions (5.2, 5.3, 5.5, 5.11)].
Discourage intercourse when the risk for OHSS is increased [see Warnings and Precautions (5.2, 5.5)].
Schedule a follow-up visit in the luteal phase.
Individualize the initial dose administered in subsequent cycles based on the woman's response in the preceding cycle.
As in the initial cycle, do not administer doses larger than 300 International Units of FSH per day. Administer 5,000 USP units of hCG 1 day after the last dose of GONAL-f to complete follicular development and induce ovulation.
Follow the above recommendations to minimize the chance of development of OHSS.